==================== ./legal_compliance.py ====================
#!/usr/bin/env python3
"""
‚öñÔ∏è Legal & Compliance Agent (Stage-only view)
---------------------------------------------
Lightweight Streamlit page derived from the Asset Appraisal template. It presents
only the compliance stages that sit between KYC / Anti-Fraud / Asset intel and
the downstream Credit Appraisal decisioning stack.

Outputs are stored in ``st.session_state["credit_policy_df"]`` so Credit Appraisal
can overlay hard-policy constraints onto the shared scoring outputs.
"""
from __future__ import annotations

import os
from datetime import datetime, timezone
from typing import Any, Dict, List

import numpy as np
import pandas as pd
import streamlit as st
import plotly.graph_objects as go

from services.ui.theme_manager import (
    apply_theme as apply_global_theme,
    get_palette,
    get_theme,
    render_theme_toggle,
)
from services.ui.components.operator_banner import render_operator_banner
from services.ui.components.chat_assistant import render_chat_assistant
from services.common.model_registry import get_hf_models, get_llm_lookup, get_llm_display_info
from services.ui.utils.llm_selector import render_llm_selector
from services.ui.utils.ai_insights import llm_generate_summary


st.set_page_config(page_title="Legal Compliance Agent", layout="wide")
ss = st.session_state


def _init_state() -> None:
    llm_lookup = get_llm_lookup()
    default_label = llm_lookup["labels"][0]
    ss.setdefault("stage", "legal_compliance_agent")
    ss.setdefault("legal_compliance_stage", "stages_only")
    ss.setdefault(
        "legal_compliance_user",
        ss.get("compliance_user")
        or ss.get("credit_scoring_user")
        or ss.get("credit_user")
        or {"name": "Operator", "email": "operator@demo.local"},
    )
    ss.setdefault("legal_compliance_pending", 11)
    ss.setdefault("legal_compliance_flags", 2)
    ss.setdefault("legal_compliance_avg_time", "6 min")
    ss.setdefault("legal_compliance_last_run_ts", None)
    ss.setdefault("legal_compliance_status", "idle")
    ss.setdefault("legal_compliance_llm_label", default_label)
    ss.setdefault("legal_compliance_llm_model", llm_lookup["value_by_label"][default_label])
    ss.setdefault("legal_compliance_llm_score", 88.0)


_init_state()
apply_global_theme(get_theme())


def _safe_switch(target: str) -> None:
    ss["stage"] = target
    try:
        st.switch_page("app.py")
        return
    except Exception:
        pass
    try:
        st.query_params["stage"] = target
    except Exception:
        pass
    try:
        st.experimental_rerun()
    except Exception:
        pass


def _render_nav():
    stage = ss.get("stage", "legal_compliance_agent")
    c1, c2, c3 = st.columns([1, 1, 2.5])
    with c1:
        if st.button("üè† Home", key=f"lc_nav_home_{stage}"):
            _safe_switch("landing")
    with c2:
        if st.button("ü§ñ Agents", key=f"lc_nav_agents_{stage}"):
            _safe_switch("agents")
    with c3:
        render_theme_toggle("üåó Theme", key="legal_compliance_top_theme")


_render_nav()


# ---------------------------------------------------------------------------
# HELPERS
# ---------------------------------------------------------------------------
def _collect_multi_source_view() -> pd.DataFrame:
    """
    Merge whatever artefacts are present in session state. Priority:
    1) Real compliance df
    2) Credit policy df
    3) Anti-fraud / KYC
    4) Asset policy exposures
    5) Synthetic fallback
    """
    candidate_keys = [
        "legal_compliance_df",
        "credit_policy_df",
        "afk_kyc_df",
        "afk_fraud_df",
        "asset_policy_df",
        "credit_scoring_df",
    ]
    frames: List[pd.DataFrame] = []
    for key in candidate_keys:
        df = ss.get(key)
        if isinstance(df, pd.DataFrame) and not df.empty:
            frames.append(df.copy())

    if frames:
        base = frames[0]
    else:
        rng = np.random.default_rng(20251110)
        base = pd.DataFrame(
            {
                "customer_id": [f"CUST-{2000 + i}" for i in range(8)],
                "jurisdiction": rng.choice(["US", "UK", "NG", "ZA", "AE", "SG", "MX"], 8),
                "pep_flag": rng.choice([0, 1], 8, p=[0.84, 0.16]),
                "sanctions_match": rng.choice([0, 1], 8, p=[0.92, 0.08]),
                "license_required": rng.choice([0, 1], 8, p=[0.55, 0.45]),
                "ticket_type": rng.choice(["Retail", "SME", "Project Finance"], 8),
                "ask_amount": rng.integers(50_000, 3_500_000, 8),
            }
        )
    return base.reset_index(drop=True)


def _run_compliance_checks(df: pd.DataFrame) -> pd.DataFrame:
    """Simulate the shared compliance reasoning pass (same signals as Asset)."""
    df = df.copy()
    rng = np.random.default_rng(42)
    if "pep_flag" not in df:
        df["pep_flag"] = rng.choice([0, 1], len(df), p=[0.82, 0.18])
    if "sanctions_match" not in df:
        df["sanctions_match"] = rng.choice([0, 1], len(df), p=[0.93, 0.07])
    if "license_required" not in df:
        df["license_required"] = rng.choice([0, 1], len(df), p=[0.58, 0.42])

    df["kyc_risk_score"] = df.get("kyc_risk_score", rng.uniform(0.05, 0.55, len(df))).astype(float)
    df["llm_alignment_score"] = (1 - df["kyc_risk_score"]) * (1 - df["sanctions_match"] * 0.6)
    df["llm_alignment_score"] = df["llm_alignment_score"].clip(0.15, 0.99).round(3)

    def _status(row: pd.Series) -> str:
        if row["sanctions_match"] >= 1 or row["pep_flag"] >= 1:
            return "üö´ Hold ‚Äì escalate"
        if row["license_required"] >= 1 and row["llm_alignment_score"] < 0.55:
            return "üü† Conditional"
        return "‚úÖ Cleared"

    df["compliance_status"] = df.apply(_status, axis=1)
    df["stage"] = np.where(df["compliance_status"] == "‚úÖ Cleared", "Compliance OK", "Compliance Hold")
    df["legal_reason"] = [
        f"Stage {idx%3 + 1}: Shared model highlighted {('PEP' if pep else 'policy match')} w/ score {score:.2f}"
        for idx, (pep, score) in enumerate(zip(df["pep_flag"], df["llm_alignment_score"]))
    ]
    df["last_reviewed_at"] = datetime.now(timezone.utc).isoformat()
    return df


def _build_chat_context() -> Dict[str, Any]:
    ctx = {
        "agent_type": "legal_compliance",
        "stage": ss.get("legal_compliance_stage"),
        "user": (ss.get("legal_compliance_user") or {}).get("name"),
        "pending_cases": ss.get("legal_compliance_pending"),
        "flagged_cases": ss.get("legal_compliance_flags"),
        "avg_time": ss.get("legal_compliance_avg_time"),
        "last_run": ss.get("legal_compliance_last_run_ts"),
        "status": ss.get("legal_compliance_status"),
        "llm_model": ss.get("legal_compliance_llm_model"),
        "llm_label": ss.get("legal_compliance_llm_label"),
        "ollama_url": os.getenv("OLLAMA_URL", f"http://localhost:{os.getenv('GEMMA_PORT', '7001')}"),
    }
    return {k: v for k, v in ctx.items() if v not in (None, "", [])}


# ---------------------------------------------------------------------------
# HEADER + OVERVIEW
# ---------------------------------------------------------------------------
pal = get_palette()
render_operator_banner(
    operator_name=(ss["legal_compliance_user"] or {}).get("name", "Operator"),
    title="Legal & Compliance Agent",
    summary="Confirms regulatory readiness before the Credit Appraisal agent finalizes a decision.",
    bullets=[
        "Stage 1 ‚Üí Align Anti-Fraud/KYC + Asset/Collateral context",
        "Stage 2 ‚Üí Shared LLM legal reasoning (sanctions, PEP, licensing)",
        "Stage 3 ‚Üí Push compliance verdicts to Credit Appraisal / unified agent",
    ],
    metrics=[
        {"label": "Pending reviews", "value": ss.get("legal_compliance_pending"), "delta": "+1 new"},
        {"label": "Flags", "value": ss.get("legal_compliance_flags"), "delta": "stable"},
        {"label": "Avg SLA", "value": ss.get("legal_compliance_avg_time"), "delta": "-6%"},
    ],
)

source_df = _collect_multi_source_view()
fraud_hits = int(source_df.get("sanctions_match", pd.Series(dtype=int)).sum() or 0)
kyc_feeds = len(source_df)
agreement_pct = float(ss.get("legal_compliance_confidence", 0.95) or 0.95) * 100
agreement_pct = max(0, min(100, agreement_pct))
refresh_age_val = 10.0
ts = ss.get("legal_compliance_last_run_ts")
if isinstance(ts, str):
    try:
        ts_dt = datetime.fromisoformat(ts.replace("Z", "+00:00"))
        refresh_age_val = max(
            0.0, (datetime.now(timezone.utc) - ts_dt).total_seconds() / 3600.0
        )
    except Exception:
        pass
telemetry = {
    "kyc_feeds": kyc_feeds,
    "fraud_hits": fraud_hits,
    "refresh_age": min(72.0, refresh_age_val),
    "agreement": agreement_pct,
}

llm_score_value = float(ss.get("legal_compliance_llm_score", telemetry["agreement"]) or telemetry["agreement"])
llm_score_value = max(0.0, min(100.0, llm_score_value))
ss["legal_compliance_llm_score"] = llm_score_value

llm_confidence_fig = go.Figure(
    go.Indicator(
        mode="gauge+number",
        value=llm_score_value,
        title={"text": "LLM Confidence / Explanation Strength"},
        gauge={
            "axis": {"range": [0, 100]},
            "bar": {"color": "lime"},
            "steps": [
                {"range": [0, 30], "color": "#f87171"},
                {"range": [30, 70], "color": "#fb923c"},
                {"range": [70, 100], "color": "#4ade80"},
            ],
        },
    )
)
llm_confidence_fig.update_layout(
    height=300,
    margin=dict(l=0, r=0, t=60, b=0),
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(0,0,0,0)",
)
st.plotly_chart(llm_confidence_fig, use_container_width=True)

donut = go.Figure(
    data=[
        go.Pie(
            values=[telemetry["agreement"], 100 - telemetry["agreement"]],
            labels=["Agreement", "Gap"],
            hole=0.65,
            marker_colors=["#3b82f6", "#1e293b"],
        )
    ]
)
donut.update_layout(
    height=320,
    showlegend=False,
    margin=dict(l=0, r=0, t=40, b=0),
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(0,0,0,0)",
)

bullet = go.Figure(
    go.Indicator(
        mode="number+gauge",
        value=telemetry["refresh_age"],
        title={"text": "Data Refresh Age (hours)"},
        gauge={
            "shape": "bullet",
            "axis": {"range": [0, 72]},
            "bar": {"color": "#22c55e"},
            "steps": [
                {"range": [0, 24], "color": "#4ade80"},
                {"range": [24, 48], "color": "#fbbf24"},
                {"range": [48, 72], "color": "#f87171"},
            ],
        },
        domain={"x": [0, 1], "y": [0, 1]},
    )
)
bullet.update_layout(
    height=160,
    margin=dict(l=0, r=0, t=40, b=0),
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(0,0,0,0)",
)

bar = go.Figure(
    go.Bar(
        x=[telemetry["kyc_feeds"], telemetry["fraud_hits"]],
        y=["KYC Feeds", "Fraud Hits"],
        orientation="h",
        marker=dict(color=["#0ea5e9", "#ef4444"]),
    )
)
bar.update_layout(
    height=260,
    margin=dict(l=0, r=0, t=40, b=0),
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(0,0,0,0)",
)

c1, c2 = st.columns(2)
with c1:
    st.markdown("<div class='metric-title'>LLM ‚Üî Human Agreement</div>", unsafe_allow_html=True)
    st.plotly_chart(donut, use_container_width=True)
    st.markdown(
        "<div class='metric-explain'>How closely the LLM's valuation/explanation matches human analysts. "
        "90%+ = high alignment; <70% = review for drift or inconsistencies.</div>",
        unsafe_allow_html=True,
    )
with c2:
    st.markdown("<div class='metric-title'>Data Freshness (Hours Since Last Sync)</div>", unsafe_allow_html=True)
    st.plotly_chart(bullet, use_container_width=True)
    st.markdown(
        "<div class='metric-explain'>Measures how recent Anti-Fraud / KYC signals were last synchronized. "
        "Green <24h = fresh, Yellow <48h = acceptable, Red >48h = stale data risk.</div>",
        unsafe_allow_html=True,
    )
st.markdown("<div class='metric-title'>Operational Signals (KYC / Fraud)</div>", unsafe_allow_html=True)
st.plotly_chart(bar, use_container_width=True)
st.markdown(
    "<div class='metric-explain'>Live operational workload indicators. Fraud hits represent flagged anomalies. "
    "KYC feeds show validated identity signals.</div>",
    unsafe_allow_html=True,
)

ai_summary_text = llm_generate_summary(telemetry)
st.markdown("### üß† AI Recommendation Summary")
st.write(ai_summary_text)

if False:
    st.markdown("### Shared LLM & Hardware Profile")
    hf_models_df = pd.DataFrame(get_hf_models())
    llm_lookup_ui = get_llm_lookup()
    OPENSTACK_FLAVORS = {
        "m4.medium": "4 vCPU / 8 GB RAM (CPU-only small)",
        "m8.large": "8 vCPU / 16 GB RAM (CPU-only medium)",
        "g1.a10.1": "8 vCPU / 32 GB RAM + 1√óA10 24 GB",
        "g1.l40.1": "16 vCPU / 64 GB RAM + 1√óL40 48 GB",
        "g2.a100.1": "24 vCPU / 128 GB RAM + 1√óA100 80 GB",
    }
    with st.expander("üß† Local/HF lineup (shared with Asset & Credit)", expanded=False):
        st.dataframe(hf_models_df, use_container_width=True)
        c1, c2 = st.columns([1.2, 1])
        labels = llm_lookup_ui["labels"]
        value_by_label = llm_lookup_ui["value_by_label"]
        hint_by_label = llm_lookup_ui["hint_by_label"]
        saved_label = ss.get("legal_compliance_llm_label", labels[0])
        if saved_label not in labels:
            saved_label = labels[0]
        with c1:
            selected_label = st.selectbox(
                "üî• Local/HF LLM (legal reasoning)",
                labels,
                index=labels.index(saved_label),
                key="legal_compliance_llm_label",
            )
            st.caption(f"Hint: {hint_by_label[selected_label]}")
        with c2:
            flavor = st.selectbox(
                "OpenStack flavor / host profile",
                list(OPENSTACK_FLAVORS.keys()),
                index=0,
                key="legal_compliance_flavor",
            )
            st.caption(OPENSTACK_FLAVORS[flavor])
        ss["legal_compliance_llm_model"] = value_by_label[selected_label]

OPENSTACK_FLAVORS = {
    "m4.medium": "4 vCPU / 8 GB RAM (CPU-only small)",
    "m8.large": "8 vCPU / 16 GB RAM (CPU-only medium)",
    "g1.a10.1": "8 vCPU / 32 GB RAM + 1√óA10 24 GB",
    "g1.l40.1": "16 vCPU / 64 GB RAM + 1√óL40 48 GB",
    "g2.a100.1": "24 vCPU / 128 GB RAM + 1√óA100 80 GB",
}

selected_llm = render_llm_selector(context="legal_compliance")
ss["legal_compliance_llm_label"] = selected_llm["model"]
ss["legal_compliance_llm_model"] = selected_llm["value"]

flavor = st.selectbox(
    "OpenStack flavor / host profile",
    list(OPENSTACK_FLAVORS.keys()),
    index=0,
    key="legal_compliance_flavor",
)
st.caption(OPENSTACK_FLAVORS[flavor])

st.markdown("### Stage-Only Compliance Flow")


# ---------------------------------------------------------------------------
# STAGE 1 ‚Äî MULTI-SOURCE ALIGNMENT
# ---------------------------------------------------------------------------
with st.container():
    st.subheader("Stage 1 ¬∑ Align Anti-Fraud/KYC + Asset context")
    st.write(
        "We expose only the necessary columns for compliance to keep the scope tight. "
        "This table reflects whichever upstream agent populated session_state most recently."
    )
    st.dataframe(source_df.head(12), use_container_width=True, height=320)
    st.caption(
        "Tip: refresh Anti-Fraud/KYC or Asset pages first to propagate their most recent outputs here."
    )


# ---------------------------------------------------------------------------
# STAGE 2 ‚Äî SHARED LEGAL REASONING
# ---------------------------------------------------------------------------
with st.container():
    st.subheader("Stage 2 ¬∑ Shared legal reasoning")
    st.write(
        "Leverages the shared Phi/Mistral/Gemma lineup (selected above) to summarise sanctions, PEP and licensing gates."
    )
    run_checks = st.button("‚öñÔ∏è Run compliance pass", use_container_width=True)
    compliance_df = ss.get("legal_compliance_df")
    if run_checks:
        compliance_df = _run_compliance_checks(source_df)
        ts = datetime.now(timezone.utc).isoformat()
        ss["legal_compliance_df"] = compliance_df.copy()
        ss["credit_policy_df"] = compliance_df.copy()
        ss["legal_compliance_last_run_ts"] = ts
        ss["legal_compliance_status"] = "completed"
        st.success(
            f"Compliance sweep completed at {ts} using {ss['legal_compliance_llm_label']}. Stored in credit_policy_df for downstream use."
        )

    if isinstance(compliance_df, pd.DataFrame) and not compliance_df.empty:
        st.dataframe(compliance_df.head(12), use_container_width=True, height=340)
        st.download_button(
            "‚¨áÔ∏è Export compliance log",
            data=compliance_df.to_csv(index=False).encode("utf-8"),
            file_name=f"legal_compliance_{datetime.now():%Y%m%d-%H%M}.csv",
            mime="text/csv",
            use_container_width=True,
        )
    else:
        st.info("Run the compliance pass to populate this stage.")


# ---------------------------------------------------------------------------
# STAGE 3 ‚Äî HANDOFF
# ---------------------------------------------------------------------------
with st.container():
    st.subheader("Stage 3 ¬∑ Handoff to Credit Appraisal")
    if isinstance(compliance_df, pd.DataFrame) and not compliance_df.empty:
        ready_count = len(compliance_df)
        st.success(f"{ready_count} compliance verdicts synchronized. Credit Appraisal can now layer policy checks.")
        st.markdown(
            """
            - üìò [Launch Credit Appraisal](/credit_appraisal)  
            - ‚úÖ ``credit_policy_df`` updated in session state  
            - üß† Selected LLM: `{label}` (`{model}`)
            """.format(
                label=ss.get("legal_compliance_llm_label"),
                model=ss.get("legal_compliance_llm_model"),
            )
        )
    else:
        st.warning("Generate compliance verdicts before attempting the handoff.")


# ---------------------------------------------------------------------------
# SHARED CHAT
# ---------------------------------------------------------------------------
FAQ = [
    "Stage 1 ‚Üí What docs are mandatory for jurisdiction mapping?",
    "Stage 2 ‚Üí How does the shared LLM justify a sanctions escalation?",
    "Stage 3 ‚Üí What columns does Credit Appraisal read from credit_policy_df?",
]
render_chat_assistant(
    page_id="legal_compliance",
    context=_build_chat_context(),
    title="üí¨ Compliance assistant",
    default_open=False,
    faq_questions=FAQ,
)


# ---------------------------------------------------------------------------
# FOOTER CONTROLS
# ---------------------------------------------------------------------------
st.divider()
footer_cols = st.columns([1, 1, 1, 2])
with footer_cols[0]:
    render_theme_toggle("üåó Theme", key="legal_compliance_theme_footer")
with footer_cols[1]:
    if st.button("‚Ü©Ô∏è Back to Agents", use_container_width=True, key="legal_compliance_back_agents"):
        ss["stage"] = "agents"
        try:
            st.switch_page("app.py")
        except Exception:
            st.experimental_set_query_params(stage="agents")
            st.experimental_rerun()
with footer_cols[2]:
    if st.button("üè† Back to Home", use_container_width=True, key="legal_compliance_back_home"):
        ss["stage"] = "landing"
        try:
            st.switch_page("app.py")
        except Exception:
            st.experimental_set_query_params(stage="landing")
            st.experimental_rerun()
st.markdown(
    "<h1 style='font-size:2.2rem;font-weight:700;'>üî• Local/HF LLM (narratives + explainability)</h1>",
    unsafe_allow_html=True,
)



==================== ./asset_appraisal.py ====================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üè¶ Asset Appraisal Agent ‚Äî Full E2E Flow (Inputs ‚Üí Anonymize ‚Üí AI ‚Üí Human Review ‚Üí Training)
Author:  Nguyen Dzoan
Version: 2025-11-01

Includes:
- Stage 1: CSV + evidence (images/PDFs) + manual row; synthetic fallback + "why" table
- Stage 2: Explicit anonymization pipeline (RAW & ANON kept)
- Stage 3: AI appraisal with runtime flavor selector, agent discovery+probe, rule_reasons when backend omits
  + Production banner + asset-trained model selector + promote inside Stage 3
- Stage 4: Human Review with AI‚ÜîHuman agreement gauge; export feedback CSV
- Stage 5: Training (upload feedback) ‚Üí Train candidate ‚Üí Promote to PRODUCTION
"""

import os
import io
import re
import json
import threading
import time
from datetime import datetime, timezone  # ‚úÖ clean, safe, supports datetime.now()
from pathlib import Path
from textwrap import dedent
from typing import Any, Dict

# ‚îÄ‚îÄ Third-party
import requests
import numpy as np
import pandas as pd
import streamlit as st
import streamlit.components.v1 as components
import plotly.express as px
import plotly.graph_objects as go
import csv
import zipfile  # ‚úÖ ADD THIS

from services.ui.theme_manager import (
    apply_theme as apply_global_theme,
    get_palette,
    get_theme,
    render_theme_toggle,
)
from services.ui.components.operator_banner import render_operator_banner
from services.ui.components.feedback import render_feedback_tab
from services.ui.components.chat_assistant import render_chat_assistant
from services.common.model_registry import get_hf_models
from services.common.personas import get_persona_for_agent
from services.ui.utils.llm_selector import render_llm_selector







# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# PAGE CONFIG + THEME
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import streamlit as st

st.set_page_config(page_title="Asset Appraisal Agent", layout="wide")
ss = st.session_state

st.markdown(
    """
    > **Unified Risk Checklist**  
    > ‚úÖ Is the borrower real & safe? (handled by KYC/Fraud)  
    > ‚úÖ Is the collateral worth enough? (this agent)  
    > ‚úÖ Can they afford the loan? (Credit)  
    > ‚úÖ Should the bank approve overall? (Unified agent)
    """
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SESSION DEFAULTS (idempotent)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _init_defaults():
    ss.setdefault("asset_logged_in", True)
    ss.setdefault("asset_stage", "asset_flow")   # login ‚Üí asset_flow
    ss.setdefault("asset_user", {"name": "Operator", "email": "operator@demo.local"})
    ss.setdefault("asset_pending_cases", 18)
    ss.setdefault("asset_flagged_cases", 3)
    ss.setdefault("asset_avg_time", "22 min")
    # Working tables/artifacts per our matrix (placeholders)
    ss.setdefault("asset_intake_df", None)
    ss.setdefault("asset_evidence_index", None)
    ss.setdefault("asset_anon_df", None)
    ss.setdefault("asset_features_df", None)
    ss.setdefault("asset_comps_used", None)
    ss.setdefault("asset_valued_df", None)
    ss.setdefault("asset_verified_df", None)
    ss.setdefault("asset_policy_df", None)
    ss.setdefault("asset_decision_df", None)
    ss.setdefault("asset_human_review_df", None)
    ss.setdefault("asset_feedback_csv", None)
    ss.setdefault("asset_trained_model_meta", None)
    ss.setdefault("asset_gpu_profile", None)  # will be set only in C.4
    ss.setdefault("asset_ai_performance", 0.88)
    os.makedirs("./.tmp_runs", exist_ok=True)

_init_defaults()
ASSET_PERSONA = get_persona_for_agent("asset_appraisal")


def _build_asset_chat_context() -> Dict[str, Any]:
    ss_local = st.session_state
    ctx = {
        "agent_type": "asset",
        "stage": ss_local.get("asset_stage"),
        "user": (ss_local.get("asset_user") or {}).get("name"),
        "pending_cases": ss_local.get("asset_pending_cases"),
        "flagged_cases": ss_local.get("asset_flagged_cases"),
        "avg_time": ss_local.get("asset_avg_time"),
        "ai_performance": ss_local.get("asset_ai_performance"),
        "last_run_id": ss_local.get("asset_last_run_id"),
        "last_runner": ss_local.get("asset_last_runner"),
        "last_error": ss_local.get("asset_last_error"),
        "next_step": ss_local.get("asset_next_step"),
    }
    return {k: v for k, v in ctx.items() if v not in (None, "", [])}


ASSET_FAQ = [
    "Explain ai_adjusted vs FMV.",
    "Show comps that drove the valuation.",
    "What encumbrances were detected?",
    "How do I rerun Stage C ‚Äì Valuation?",
]

# Always bypass login step during operator demos
ss["asset_logged_in"] = True
if ss.get("asset_stage") == "login":
    ss["asset_stage"] = "asset_flow"


def _coerce_minutes(value, fallback: float = 0.0) -> float:
    """Convert values like '22 min' to floats for gauge percentages."""
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        cleaned = "".join(ch for ch in value if ch.isdigit() or ch == ".")
        try:
            return float(cleaned)
        except (TypeError, ValueError):
            pass
    return float(fallback)

def render_nav_bar_app():
    st.markdown(
        "<div style='display:flex;gap:12px;align-items:center'>"
        "<a href='?stage=agents' class='macbtn'>ü§ñ Agents</a>"
        "<span style='opacity:.6'>/</span>"
        "<span>üèõÔ∏è Asset Appraisal Agent</span>"
        "</div>",
        unsafe_allow_html=True,
    )


# ---- Global runs dir (used everywhere) ----
RUNS_DIR = os.path.abspath("./.tmp_runs")
os.makedirs(RUNS_DIR, exist_ok=True)






# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# API CONFIG
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

API_URL = os.getenv("API_URL", "http://localhost:8090")

_CHATBOT_REFRESH_STATE = {"last_ts": 0.0}

def _ping_chatbot_refresh(reason: str = "asset", *, min_interval: float = 300.0) -> None:
    now = time.time()
    if (now - _CHATBOT_REFRESH_STATE.get("last_ts", 0.0)) < min_interval:
        return
    _CHATBOT_REFRESH_STATE["last_ts"] = now

    def _fire():
        try:
            requests.post(f"{API_URL}/chatbot/refresh", json={"reason": reason}, timeout=5)
        except Exception:
            pass

    threading.Thread(target=_fire, daemon=True).start()

# Default fallbacks (will be superseded by discovery)

ASSET_AGENT_IDS = [a.strip() for a in os.getenv("ASSET_AGENT_IDS", "asset_appraisal,asset").split(",") if a.strip()]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# NAV (reliable jump to Home / Agents from a page)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _set_query_params_safe(**kwargs):
    # New API (Streamlit ‚â•1.40)
    try:
        for k, v in kwargs.items():
            st.query_params[k] = v
        return True
    except Exception:
        pass
    # Older versions
    try:
        st.experimental_set_query_params(**kwargs)
        return True
    except Exception:
        return False

def _go_stage(target_stage: str):
    # 1) let app.py‚Äôs router know what to show
    st.session_state["stage"] = target_stage

    # 2) preferred: jump to main app file
    try:
        # path is relative to the run root when you launch:
        #   streamlit run services/ui/app.py
        st.switch_page("app.py")
        return
    except Exception:
        pass

    # 3) fallback: set query param and rerun so app.py picks it up
    _set_query_params_safe(stage=target_stage)
    st.rerun()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# UTILITIES ‚Äî DataFrame selection helpers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ---- DataFrame selection helpers (avoid boolean ambiguity) ----
def first_nonempty_df(*candidates):
    """Return the first candidate that is a non-empty pandas DataFrame, else None."""
    for df in candidates:
        if isinstance(df, pd.DataFrame) and not df.empty:
            return df
    return None

def is_nonempty_df(x) -> bool:
    return isinstance(x, pd.DataFrame) and not x.empty

def render_nav_bar_app():
    stage = st.session_state.get("stage", "landing")

    # three columns: home, agents, theme toggle
    c1, c2, c3 = st.columns([1, 1, 2.5])

    with c1:
        if st.button("üè† Back to Home", key=f"btn_home_{stage}"):
            _go_stage("landing")
            st.stop()

    with c2:
        if st.button("ü§ñ Back to Agents", key=f"btn_agents_{stage}"):
            _go_stage("agents")
            st.stop()

    with c3:
        render_theme_toggle(
            label="üåó Dark mode",
            key="asset_theme_toggle",
            help="Switch theme",
        )

    st.markdown("---")




# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GEO UTILITIES: EXIF GPS, Geocode, Geohash   ‚Üê PASTE START
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from typing import Optional, Tuple

def _exif_to_degrees(value):
    try:
        d = float(value[0][0]) / float(value[0][1])
        m = float(value[1][0]) / float(value[1][1])
        s = float(value[2][0]) / float(value[2][1])
        return d + (m / 60.0) + (s / 3600.0)
    except Exception:
        return None

def extract_gps_from_image(path: str) -> Optional[Tuple[float, float]]:
    try:
        from PIL import Image
        from PIL.ExifTags import TAGS, GPSTAGS
        img = Image.open(path)
        exif = img._getexif() or {}
        tagged = {TAGS.get(k, k): v for k, v in exif.items()}
        gps_info = tagged.get("GPSInfo")
        if not gps_info:
            return None
        gps_data = {GPSTAGS.get(k, k): v for k, v in gps_info.items()}
        lat = _exif_to_degrees(gps_data.get("GPSLatitude"))
        lon = _exif_to_degrees(gps_data.get("GPSLongitude"))
        if lat is None or lon is None:
            return None
        lat_ref = gps_data.get("GPSLatitudeRef", "N")
        lon_ref = gps_data.get("GPSLongitudeRef", "E")
        if lat_ref == "S": lat = -lat
        if lon_ref == "W": lon = -lon
        return (lat, lon)
    except Exception:
        return None

_GEOCODE_CACHE_PATH = "./.tmp_runs/geocode_cache.json"

def _load_geocode_cache():
    try:
        with open(_GEOCODE_CACHE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _save_geocode_cache(cache: dict):
    os.makedirs("./.tmp_runs", exist_ok=True)
    with open(_GEOCODE_CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def geocode_freeform(addr: str) -> Optional[Tuple[float, float]]:
    """Nominatim via geopy; cached locally. Returns None if offline."""
    try:
        cache = _load_geocode_cache()
        key = addr.strip().lower()
        if key in cache:
            v = cache[key]
            return (v["lat"], v["lon"])
        from geopy.geocoders import Nominatim
        geolocator = Nominatim(user_agent="asset-appraisal-agent")
        loc = geolocator.geocode(addr, timeout=10)
        if not loc:
            return None
        cache[key] = {"lat": float(loc.latitude), "lon": float(loc.longitude)}
        _save_geocode_cache(cache)
        return (float(loc.latitude), float(loc.longitude))
    except Exception:
        return None

def geohash_decode(s: str) -> Optional[Tuple[float, float]]:
    try:
        import geohash  # pip install python-geohash
        lat, lon = geohash.decode(s)
        return (float(lat), float(lon))
    except Exception:
        return None



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SESSION
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ss = st.session_state
ss.setdefault("asset_stage", "login")
ss.setdefault("asset_logged_in", False)
ss.setdefault("asset_user", None)

# Stage caches
ss.setdefault("asset_raw_df", None)     # Stage 1 raw (after CSV/manual merge)
ss.setdefault("asset_evidence", [])     # evidence filenames (images/pdfs)
ss.setdefault("asset_anon_df", None)    # Stage 2 anonymized
ss.setdefault("asset_stage2_df", None)  # Stage 3 input (resolved source)
ss.setdefault("asset_ai_df", None)      # Stage 3 AI output
ss.setdefault("asset_selected_model", None)  # trained model path

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# HELPERS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def anonymize_text_cols(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for col in out.columns:
        if out[col].dtype == "object":
            out[col] = (
                out[col].astype(str)
                .apply(lambda x: re.sub(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+", "[EMAIL]", x))
            )
    return out

def quick_synth(rows: int = 150) -> pd.DataFrame:
    """Generate asset rows + finance metrics for demo/backup."""
    rng = np.random.default_rng(42)
    cities = [
        ("Hanoi", 21.0285, 105.8542),
        ("HCMC", 10.7769, 106.7009),
        ("Da Nang", 16.0544, 108.2022),
        ("Hue", 16.4637, 107.5909),
        ("Can Tho", 10.0452, 105.7469),
    ]
    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, rows + 1)],
        "asset_id": [f"A{i:04d}" for i in range(1, rows + 1)],
        "asset_type": rng.choice(["House","Apartment","Car","Land","Factory"], rows),
        "age_years": rng.integers(1, 40, rows),
        "market_value": rng.integers(50_000, 2_000_000, rows),
        "condition_score": rng.uniform(0.6, 1.0, rows),
        "legal_penalty": rng.uniform(0.95, 1.0, rows),          # legal/title risk adj
        "employment_years": rng.integers(0, 30, rows),
        "credit_history_years": rng.integers(0, 25, rows),
        "delinquencies": rng.integers(0, 6, rows),
        "current_loans": rng.integers(0, 8, rows),
        "loan_amount": rng.integers(10_000, 200_000, rows),
        "customer_type": rng.choice(["bank","non-bank"], rows, p=[0.7,0.3]),
    })
    cdf = pd.DataFrame(cities, columns=["city","lat","lon"])
    df["city"] = rng.choice(cdf["city"], rows)
    df = df.merge(cdf, on="city", how="left")
    df["depreciation_rate"] = (1 - df["condition_score"]) * 100
    df["market_segment"] = np.where(df["market_value"] > 500_000, "High", "Mass")
    df["DTI"] = rng.uniform(0.05, 0.9, rows)
    df["LTV"] = np.clip(df["loan_amount"] / np.maximum(df["market_value"], 1), 0.05, 1.5)
    df["evidence_files"] = [[] for _ in range(rows)]
    return df

def synth_why_table() -> pd.DataFrame:
    return pd.DataFrame([
        {"Metric": "DTI", "Why": "Debt service relative to income ‚Äî proxy for payability."},
        {"Metric": "LTV", "Why": "Loan vs asset value ‚Äî proxy for collateral adequacy."},
        {"Metric": "condition_score", "Why": "Asset physical state impacts fair value/depreciation."},
        {"Metric": "legal_penalty", "Why": "Legal/title flags reduce realizable value."},
        {"Metric": "employment_years / credit_history_years", "Why": "Stability/track record."},
        {"Metric": "delinquencies / current_loans", "Why": "Current risk pressure."},
        {"Metric": "market_segment / city / lat,lon", "Why": "Market & location effects on pricing."},
    ])

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DATAFRAME SELECTION (avoid boolean ambiguity)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def first_nonempty_df(*candidates):
    """Return the first candidate that is a non-empty pandas DataFrame, else None."""
    for df in candidates:
        if isinstance(df, pd.DataFrame) and not df.empty:
            return df
    return None



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# UNIVERSAL INGEST + NORMALIZATION HELPERS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _slug(name: str) -> str:
    return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

def _ts() -> str:
    return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

def _read_any_table(uploaded_file) -> pd.DataFrame:
    """
    Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback.
    Accepts Streamlit UploadedFile or a file-like object.
    """
    name = getattr(uploaded_file, "name", "").lower()

    # Excel first
    if name.endswith((".xlsx", ".xls")):
        return pd.read_excel(uploaded_file)

    # Text (CSV/TSV/TXT): try utf-8, then latin-1; sniff delimiter.
    raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
    for enc in ("utf-8", "latin-1"):
        try:
            text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
            lines = text.splitlines()
            sample = "\n".join(lines[:5]) if lines else ""
            try:
                dialect = csv.Sniffer().sniff(sample) if sample else csv.excel
                sep = getattr(dialect, "delimiter", ",")
            except Exception:
                sep = ","
            return pd.read_csv(io.StringIO(text), sep=sep)
        except Exception:
            continue
    # last resort
    return pd.read_csv(io.BytesIO(raw), engine="python")

def _normalize_for_agents(df: pd.DataFrame) -> pd.DataFrame:
    """
    Light normalization for credit/asset agents.
    Creates a consistent thin schema if columns exist; leaves extras intact.
    """
    out = df.copy()

    # alias map (extend freely)
    aliases = {
        "application_id": ["application_id", "app_id", "loan_id", "id", "request_id"],
        "asset_id":       ["asset_id", "property_id", "house_id", "assetid"],
        "asset_type":     ["asset_type", "type", "category"],
        "address":        ["address", "addr", "street", "location"],
        "city":           ["city", "town"],
        "state":          ["state", "province", "region"],
        "country":        ["country"],
        "price":          ["price", "value", "market_value", "listing_price", "sale_price"],
        "bedrooms":       ["bedrooms", "beds"],
        "bathrooms":      ["bathrooms", "baths"],
        "parking_space":  ["parking_space", "parking", "garage"],
        "title":          ["title", "name"],
    }

    # rename by first matching alias
    rename_map = {}
    cols = set(out.columns)
    for target, cands in aliases.items():
        for c in cands:
            if c in cols:
                rename_map[c] = target
                break
    out = out.rename(columns=rename_map)

    # ensure presence of common columns
    required = ["application_id", "asset_id", "asset_type", "address", "city", "state", "price"]
    for col in required:
        if col not in out.columns:
            out[col] = None

    # numeric coercions
    for col in ("price", "bedrooms", "bathrooms", "parking_space"):
        if col in out.columns:
            out[col] = pd.to_numeric(out[col], errors="coerce")

    # provenance
    out["source_dataset"] = st.session_state.get("asset_intake_source_name", "uploaded")
    return out


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# THEME SYSTEM (Light/Dark CSS + map style)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from textwrap import dedent

THEME_EXTRAS = {
    "light": {
        "success": "#16A34A",
        "warn": "#D97706",
        "danger": "#DC2626",
        "stripe": "#F1F5F9",
        "shadow": "0 6px 24px rgba(15,23,42,0.08)",
    },
    "dark": {
        "success": "#22C55E",
        "warn": "#FBBF24",
        "danger": "#F87171",
        "stripe": "#111827",
        "shadow": "0 8px 30px rgba(0,0,0,0.35)",
    },
}


def _theme_tokens(theme: str) -> dict[str, str]:
    pal = get_palette(theme)
    extra = THEME_EXTRAS.get(theme, THEME_EXTRAS["dark"])
    return {
        "bg": pal["bg"],
        "panel": pal["card"],
        "text": pal["text"],
        "muted": pal["subtext"],
        "primary": pal["accent"],
        "accent": pal["accent_alt"],
        "success": extra["success"],
        "warn": extra["warn"],
        "danger": extra["danger"],
        "stripe": extra["stripe"],
        "shadow": extra["shadow"],
    }


def _theme_css(theme: str) -> str:
    t = _theme_tokens(theme)
    return dedent(f"""
    <style>
      /* Fonts: Inter + JetBrains Mono */
      @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=JetBrains+Mono:wght@400;600&display=swap');

      :root {{
        --bg: {t['bg']};
        --panel: {t['panel']};
        --text: {t['text']};
        --muted: {t['muted']};
        --primary: {t['primary']};
        --success: {t['success']};
        --warn: {t['warn']};
        --danger: {t['danger']};
        --accent: {t['accent']};
        --stripe: {t['stripe']};
        --shadow: {t['shadow']};
        --radius: 14px;
      }}

      html, body, .stApp {{
        background: var(--bg) !important;
        color: var(--text) !important;
        font-family: 'Inter', system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif !important;
      }}

      /* Panel-like boxes (use .left-box/.right-box or your custom containers) */
      .left-box, .right-box, .stExpander, .stTabs [data-baseweb="tab-highlight"] {{
        background: var(--panel) !important;
        border-radius: var(--radius);
        box-shadow: var(--shadow);
      }}

      /* Headings */
      h1, h2, h3, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 {{
        color: var(--text) !important;
        font-weight: 700;
        letter-spacing: -0.01em;
      }}

      /* Buttons */
      .stButton>button, button[kind="primary"] {{
        background: var(--primary) !important;
        color: #fff !important;
        border-radius: 12px !important;
        border: none !important;
        box-shadow: 0 3px 12px rgba(37,99,235,0.35) !important;
      }}
      .stDownloadButton button {{
        background: var(--success) !important;
        color: #fff !important;
        border-radius: 12px !important;
        border: none !important;
        box-shadow: 0 3px 12px rgba(34,197,94,0.35) !important;
      }}

      /* Tables (dataframe) */
      .stDataFrame thead tr th {{
        background: var(--panel) !important;
        color: var(--muted) !important;
        font-weight: 600 !important;
      }}
      .stDataFrame tbody tr:nth-child(odd) {{
        background: var(--stripe) !important;
      }}

      /* Chips / small badges */
      .chip {{
        display:inline-block; padding:4px 10px; border-radius:999px;
        background: var(--panel); color: var(--muted); border:1px solid rgba(148,163,184,0.35);
      }}

      /* Code font */
      code, pre, .stCodeBlock, .st-emotion-cache-ffhzg2 {{
        font-family: 'JetBrains Mono', ui-monospace, SFMono-Regular, Menlo, Consolas, monospace !important;
      }}
      
      /* NEW  Optional: hide sidebar without touching theme */
      [data-testid="stSidebar"], section[data-testid="stSidebar"], nav[data-testid="stSidebarNav"] {{
        display: none !important;
      }}
      [data-testid="stAppViewContainer"] {{
        margin-left: 0 !important;
        padding-left: 0 !important;
      }}
      
      
    </style>
    """)

def apply_theme(theme: str | None = None):
    """Inject shared Streamlit theme plus asset-specific tokens."""
    theme = theme or get_theme()
    apply_global_theme(theme)
    st.markdown(_theme_css(theme), unsafe_allow_html=True)


apply_theme()



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# MAP THEME HELPERS (Mapbox style + token + adapters)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os
import streamlit as st

def get_mapbox_token() -> str | None:
    """Find a Mapbox token from secrets or env. Return None if not set."""
    try:
        tok = st.secrets.get("MAPBOX_TOKEN")
    except Exception:
        tok = None
    if not tok:
        tok = os.environ.get("MAPBOX_TOKEN") or os.environ.get("MAPBOX_ACCESS_TOKEN")
    return tok or None


def plotly_map_style() -> str:
    """
    Return a Plotly-compatible map style.
    - Uses bright style in light mode even without a Mapbox token.
    - Falls back to CARTO 'positron' if Mapbox token not set.
    """
    theme = get_theme()
    token = get_mapbox_token()
    if token:
        return "light" if theme == "light" else "dark"
    else:
        # fallback to open CARTO tiles (bright)
        return "carto-positron" if theme == "light" else "carto-darkmatter"


def get_map_style() -> str:
    """
    Return a Mapbox style URL (for pydeck only).
    Light ‚Üí bright, Dark ‚Üí dark. Defaults to light for safety.
    """
    theme = get_theme()
    return "mapbox://styles/mapbox/light-v11" if theme == "light" else "mapbox://styles/mapbox/dark-v11"


def apply_plotly_mapbox_defaults():
    """Set Plotly's Mapbox token globally (if available)."""
    import plotly.express as px
    token = get_mapbox_token()
    if token:
        px.set_mapbox_access_token(token)
    else:
        st.info("‚ÑπÔ∏è Mapbox token not set ‚Äî using free bright map style (carto-positron).")


def make_pydeck_view_state(lat=10.7769, lon=106.7009, zoom=10, pitch=0, bearing=0):
    import pydeck as pdk
    return pdk.ViewState(latitude=lat, longitude=lon, zoom=zoom, pitch=pitch, bearing=bearing)


def pydeck_map_style() -> str:
    """pydeck uses the same Mapbox style URLs when a token is available."""
    return get_map_style()



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# AGENT DISCOVERY & PROBE
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _safe_get_json(url: str, timeout: int = 8):
    try:
        r = requests.get(url, timeout=timeout)
        if r.ok:
            try:
                return True, r.json()
            except Exception as e:
                return False, f"parse error: {e}\nBody:\n{r.text[:2000]}"
        return False, f"{r.status_code} {r.reason}\nBody:\n{r.text[:2000]}"
    except Exception as e:
        return False, f"request error: {e}"

def discover_asset_agents() -> list[str]:
    """Try common discovery endpoints and extract agent ids. Cache in session."""
    cached = st.session_state.get("asset_agent_ids")
    if isinstance(cached, list) and cached:
        return cached

    candidates = []

    # 1) /v1/agents (prefer)
    ok, data = _safe_get_json(f"{API_URL}/v1/agents")
    if ok:
        try:
            if isinstance(data, dict) and "agents" in data:
                items = data["agents"]
                if isinstance(items, list):
                    for it in items:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name") or it.get("agent") or it.get("slug")
                            if aid: candidates.append(aid)
            elif isinstance(data, list):
                for it in data:
                    if isinstance(it, str):
                        candidates.append(it)
                    elif isinstance(it, dict):
                        aid = it.get("id") or it.get("name")
                        if aid: candidates.append(aid)
        except Exception:
            pass

    # 2) /v1/agents/list (alt)
    if not candidates:
        ok2, data2 = _safe_get_json(f"{API_URL}/v1/agents/list")
        if ok2:
            try:
                if isinstance(data2, dict):
                    for k in ("agents", "data", "items"):
                        if k in data2 and isinstance(data2[k], list):
                            for it in data2[k]:
                                if isinstance(it, str):
                                    candidates.append(it)
                                elif isinstance(it, dict):
                                    aid = it.get("id") or it.get("name")
                                    if aid: candidates.append(aid)
                elif isinstance(data2, list):
                    for it in data2:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name")
                            if aid: candidates.append(aid)
            except Exception:
                pass

    # 3) /v1/health (sometimes lists agents)
    if not candidates:
        ok3, data3 = _safe_get_json(f"{API_URL}/v1/health")
        if ok3 and isinstance(data3, dict):
            for k in ("agents", "services", "available_agents"):
                val = data3.get(k)
                if isinstance(val, list):
                    for it in val:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name")
                            if aid: candidates.append(aid)

    discovered = [c for c in dict.fromkeys(candidates) if c]  # de-dupe
    if not discovered:
        discovered = ASSET_AGENT_IDS[:]  # fallback to env/defaults

    st.session_state["asset_agent_ids"] = discovered
    return discovered

def probe_api() -> dict:
    """Collect quick diagnostics for UI."""
    diag = {}
    for path in ("/v1/health", "/v1/agents", "/v1/agents/list"):
        ok, data = _safe_get_json(f"{API_URL}{path}")
        diag[path] = data if ok else {"error": data}
    diag["API_URL"] = API_URL
    diag["discovered_agents"] = discover_asset_agents()
    return diag

# NEW: run_id extractor for various API payload shapes
def _extract_run_id(obj) -> str | None:
    """Find a run_id in a nested dict/list API response."""
    if isinstance(obj, dict):
        rid = obj.get("run_id")
        if isinstance(rid, str) and rid:
            return rid
        for k in ("data", "meta", "result", "payload"):
            v = obj.get(k)
            if isinstance(v, dict):
                rid = v.get("run_id")
                if isinstance(rid, str) and rid:
                    return rid
    elif isinstance(obj, list):
        for it in obj:
            rid = _extract_run_id(it)
            if rid:
                return rid
    return None

def try_run_asset_agent(csv_bytes: bytes, form_fields: dict, timeout_sec: int = 180):
    """
    Discover agent ids, then try each. Rebuild multipart for each attempt.
    Preferred: use run_id to GET merged CSV and DataFrame it.
    Fallback: normalize 'result' only (not whole JSON).

    Returns (ok: bool, DataFrame | error_string)
    """
    agent_ids = discover_asset_agents()
    errors = []
    for agent_id in agent_ids:
        files = {"file": ("asset_verified.csv", io.BytesIO(csv_bytes), "text/csv")}
        url = f"{API_URL}/v1/agents/{agent_id}/run"
        try:
            resp = requests.post(url, files=files, data=form_fields, timeout=timeout_sec)
        except Exception as e:
            errors.append(f"[{agent_id}] request error: {e}")
            continue

        if resp.ok:
            body_text = resp.text[:4000]
            try:
                payload = resp.json()
            except Exception as e:
                errors.append(f"[{agent_id}] parse error: {e}\nBody:\n{body_text}")
                continue

            rid = _extract_run_id(payload)
            if rid:
                # Preferred: fetch merged CSV
                try:
                    r_csv = requests.get(f"{API_URL}/v1/runs/{rid}/report?format=csv", timeout=60)
                    if r_csv.ok:
                        df = pd.read_csv(io.BytesIO(r_csv.content))
                        st.session_state["asset_last_run_id"] = rid
                        st.session_state["asset_last_runner"] = ((payload.get("meta") or {}).get("runner_used"))
                        return True, df
                    else:
                        errors.append(
                            f"[{agent_id}] report GET {r_csv.status_code} {r_csv.reason} for run_id={rid}\n"
                            f"Body:\n{r_csv.text[:2000]}"
                        )
                except Exception as e:
                    errors.append(f"[{agent_id}] report GET error for run_id={rid}: {e}")

            # Fallback: try to render just 'result'
            result_part = payload.get("result")
            if isinstance(result_part, list):
                try:
                    df = pd.json_normalize(result_part)
                    return True, df
                except Exception as e:
                    errors.append(f"[{agent_id}] fallback normalize error: {e}\nBody:\n{body_text}")
            elif isinstance(result_part, dict):
                try:
                    df = pd.json_normalize(result_part)
                    return True, df
                except Exception as e:
                    errors.append(f"[{agent_id}] fallback normalize error: {e}\nBody:\n{body_text}")
            else:
                errors.append(f"[{agent_id}] no run_id and empty/unknown 'result'.\nBody:\n{body_text}")
        else:
            errors.append(f"[{agent_id}] {resp.status_code} {resp.reason}\nBody:\n{resp.text[:2000]}")

    combined = "All agent attempts failed (discovered=" + ", ".join(agent_ids) + "):\n" + "\n\n".join(errors)
    if errors and all(
        any(token in err.lower() for token in ("connection refused", "failed to establish", "errno 111"))
        for err in errors
    ):
        combined += (
            "\n\nHint: the backend API on port 8090 is not reachable. "
            "Start your agent server (uvicorn/fastapi) and rerun this stage."
        )
    return False, combined


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# LOGIN
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if ss["asset_stage"] == "login" and not ss["asset_logged_in"]:
    render_nav_bar_app()
    st.title("üîê Login to AI Asset Appraisal Platform")
    c1, c2, c3 = st.columns([1,1,1])
    with c1:
        user = st.text_input("Username", placeholder="e.g. dzoan")
    with c2:
        email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
    with c3:
        pwd = st.text_input("Password", type="password", placeholder="Enter any password")
    if st.button("Login", key="btn_asset_login", use_container_width=True):
        if (user or "").strip() and (email or "").strip():
            ss["asset_user"] = {
                "name": user.strip(),
                "email": email.strip(),
                "timestamp": datetime.now(timezone.utc).isoformat(),  # ‚úÖ fixed
            }
            ss["asset_logged_in"] = True
            ss["asset_stage"] = "asset_flow"
            st.rerun()
        else:
            st.error("‚ö†Ô∏è Please fill all fields before continuing.")
    st.stop()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# WORKFLOW (A‚ÜíG)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if ss.get("asset_logged_in") and ss.get("asset_stage") in ("asset_flow", "asset_agent"):
    render_nav_bar_app()
    st.title("üèõÔ∏è Asset Appraisal Agent")
    st.caption(
        "A‚ÜíG pipeline ‚Äî Intake ‚Üí Privacy ‚Üí Valuation ‚Üí Policy ‚Üí Human Review ‚Üí Model Training ‚Üí Reporting "
        f"| üëã {ss['asset_user']['name']}"
    )

    asset_ai_minutes = _coerce_minutes(ss.get("asset_avg_time"), 22.0)

    render_operator_banner(
        operator_name=ss.get("asset_user", {}).get("name", "Operator"),
        title="Asset Appraisal Command",
        summary="Automate collateral intake, anonymization, valuation, and policy checks with AI copilots.",
        bullets=[
            "Unify intake sources (CSV, HF, Kaggle) and auto-enrich with geospatial context.",
            "Apply FMV modeling + haircut & LTV policy thresholds before human review.",
            "Capture reviewer feedback to retrain valuation models and improve reports.",
        ],
        metrics=[
            {
                "label": "Avg AI Turnaround",
                "value": ss.get("asset_avg_time") or f"{asset_ai_minutes:.0f} min",
                "delta": "-5 min vs last week",
                "delta_color": "#60a5fa",
                "color": "#60a5fa",
                "percent": min(1.0, asset_ai_minutes / 50.0),
                "context": "AI valuation cycle time",
            },
            {
                "label": "Assets Pending Valuation",
                "value": ss.get("asset_pending_cases"),
                "delta": "+4 vs prior cycle",
                "delta_color": "#34d399",
                "color": "#34d399",
                "percent": min(1.0, ss.get("asset_pending_cases", 0) / 40.0),
                "context": "Manual backlog avg: 35",
            },
            {
                "label": "Flagged Risk Cases",
                "value": ss.get("asset_flagged_cases"),
                "delta": "+1 escalation",
                "delta_color": "#f87171",
                "color": "#f87171",
                "percent": min(1.0, ss.get("asset_flagged_cases", 0) / 12.0),
                "context": "Human avg: 6 risk flags",
            },
        ],
        icon="üèõÔ∏è",
    )
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # TABS (How-To + A..H) ‚Äî Live tabs
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    tabHow, tabA, tabB, tabC, tabD, tabE, tabF, tabG, tabH, tabFeedback = st.tabs([
        "üìò How-To",
        "üü¶ A) Intake & Evidence",
        "üü© B) Privacy & Features",
        "üü® C) Valuation & Verification",
        "üüß D) Policy & Decision",
        "üü™ E) Human Review & Feedback",
        "üü´ F) Model Training & Promotion",
        "üü´ G) Deployment & Export üöÄ",     # ‚úÖ corrected label (was double F)
        "‚¨ú H) Reporting & Handoff üßæ",
        "üó£Ô∏è Feedback"
    ])

    with tabHow:
        st.title("üìò How to Use This Agent")
        st.markdown("""
### What
An AI-powered agent that performs collateral and asset valuation automatically using market data, machine learning, and verification logic.

### Goal
To provide instant, data-driven, and consistent asset valuations for credit, insurance, or regulatory use.

### How
1. Import property or asset data from CSV, Kaggle, or Hugging Face sources.
2. The agent anonymizes and enriches data, extracts geospatial and condition features, and predicts fair market value (FMV).
3. Applies haircut and LTV policy thresholds to derive realizable value.
4. Verifies ownership, encumbrances, and generates professional reports.

### So What (Benefits)
- Cuts appraisal turnaround from days to minutes.
- Standardizes valuations across asset classes.
- Increases confidence with explainable AI valuations.
- Ensures data consistency and legal traceability.

### What Next
1. Run a test with your asset dataset or public examples.
2. Contact our team to customize valuation rules, haircut logic, and report templates.
3. Once validated, integrate the agent into your production credit or appraisal systems to automate end-to-end asset evaluation.
        """)


    # Runtime tip
    st.caption(
        "üìò Tip: Move sequentially from A‚ÜíG or revisit individual stages. "
        "If a stage reports missing data, rerun the previous one or load demo data."
    )

    with tabFeedback:
        render_feedback_tab("üè¶ Asset Appraisal Agent")

else:
    st.warning("Please log in first to access the Asset Appraisal workflow.")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üü¶ STAGE A ‚Äî INTAKE & EVIDENCE
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tabA:
    import io, os, json, hashlib, pandas as pd
    from datetime import datetime, timezone
    
    
    ss = st.session_state  # ‚úÖ make 'ss' available in this scope
    st.subheader("A. Intake & Evidence")
    st.caption("Steps: (1) Upload / Import, (2) Normalize, (3) Generate unified intake CSV")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üìò Quick User Guide (updated)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    with st.expander("üìò Quick User Guide", expanded=False):
        st.markdown("""
        **Goal:** Collect, normalize, and unify all asset-related data before appraisal.

        **1Ô∏è‚É£ Upload Your Data**
        - Upload **field agent reports**, **loan lists with collateral**, and **legal property documents**.
        - Supported: `.csv`, `.xlsx`, `.zip` (evidence images/docs).

        **2Ô∏è‚É£ Import Open Data**
        - Search **Kaggle** or **Hugging Face** for relevant valuation datasets.
        - You can mix public + internal uploads ‚Äî AI will normalize columns.

        **3Ô∏è‚É£ Normalize**
        - After upload/import, click **"Normalize Data"** to merge and standardize features.
        - Output: `intake_table.csv` ready for Stage B (Anonymization).

        **4Ô∏è‚É£ Generate Synthetic Data**
        - If no input data is available, the AI can synthesize a demo dataset representing:
          `asset_id, asset_type, city, market_value, loan_amount, legal_source, condition_score`.

        **5Ô∏è‚É£ Output**
        - A unified CSV file is produced ‚Üí download or proceed directly to **Stage B**.
        """)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # (A.1) UPLOAD ZONE ‚Äî Human Inputs
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üì§ Upload Data Files (Field Agents / Loans / Legal Docs)")
    uploaded_files = st.file_uploader(
        "Upload multiple files",
        type=["csv", "xlsx", "zip"],
        accept_multiple_files=True,
        key="asset_upload_files"
    )

    uploaded_dfs = []
    if uploaded_files:
        for f in uploaded_files:
            try:
                if f.name.endswith(".csv"):
                    df = pd.read_csv(f)
                elif f.name.endswith(".xlsx"):
                    df = pd.read_excel(f)
                else:
                    st.info(f"üì¶ Skipping non-tabular file: {f.name}")
                    continue
                st.success(f"‚úÖ Loaded `{f.name}` ({len(df)} rows, {len(df.columns)} cols)")
                uploaded_dfs.append(df)
            except Exception as e:
                st.error(f"‚ùå Failed to read {f.name}: {e}")


    # ‚îÄ‚îÄNew  global runs dir (shared across stages) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    RUNS_DIR = os.path.abspath("./.tmp_runs")
    os.makedirs(RUNS_DIR, exist_ok=True)
    
   

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # (A.2) PUBLIC DATASETS ‚Äî Kaggle / HF / OpenML
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üåç Import Public Datasets (Kaggle / Hugging Face / OpenML / Portals)")

    # keep a place to persist search results across reruns
    ss.setdefault("kaggle_search_df", pd.DataFrame())

    src = st.selectbox(
        "Select source",
        ["Kaggle (API)", "Hugging Face", "OpenML", "Public Domain Portals"],
        key="asset_pubsrc"
    )
    query = st.text_input("Search keywords", "house prices real estate valuation", key="asset_pubquery")

    # helpers
    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    
    def _safe_read_csv(fp: str) -> pd.DataFrame:
        """
        Robust CSV reader:
        - Tries multiple encodings (utf-8, utf-8-sig, cp1252, latin-1)
        - Tries common separators
        - Skips bad rows rather than crashing
        """
        encodings = ["utf-8", "utf-8-sig", "cp1252", "latin-1"]
        seps = [",", ";", "\t", "|"]

        last_err = None
        for enc in encodings:
            for sep in seps:
                try:
                    df_try = pd.read_csv(
                        fp,
                        encoding=enc,
                        sep=sep,
                        engine="python",
                        on_bad_lines="skip",   # pandas >=1.3
                    )
                    # Require at least 2 columns to consider it valid
                    if df_try.shape[1] >= 2:
                        return df_try
                except Exception as e:
                    last_err = e
                    continue

        # Final fallback: read bytes, decode with latin-1 replacement, then parse in-memory
        try:
            with open(fp, "rb") as f:
                raw = f.read()
            text = raw.decode("latin-1", errors="replace")
            for sep in seps:
                try:
                    return pd.read_csv(
                        io.StringIO(text),
                        sep=sep,
                        engine="python",
                        on_bad_lines="skip",
                    )
                except Exception:
                    pass
        except Exception as e:
            last_err = e

        raise RuntimeError(f"Could not parse CSV with common encodings/separators. Last error: {last_err}")

    


    # Kaggle search
    if st.button("üîé Search dataset", key="btn_asset_pubsearch"):
        with st.spinner("Searching datasets..."):
            try:
                if src == "Kaggle (API)":
                    import subprocess, io
                    cmd = ["kaggle", "datasets", "list", "-s", query, "-v"]  # -v => CSV output
                    out = subprocess.run(cmd, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                    if out.returncode != 0:
                        st.error(f"Kaggle CLI failed: {out.stderr.strip() or out.stdout.strip()}")
                        st.info("üí° Ensure ~/.kaggle/kaggle.json exists and has chmod 600.")
                        ss["kaggle_search_df"] = pd.DataFrame()
                    else:
                        df_pub = pd.read_csv(io.StringIO(out.stdout))
                        keep = [c for c in ["ref","title","size","lastUpdated","downloadCount","voteCount","usabilityRating"] if c in df_pub.columns]
                        ss["kaggle_search_df"] = df_pub[keep]
                        st.success("‚úÖ Kaggle API results shown.")
                elif src == "Hugging Face":
                    from huggingface_hub import list_datasets
                    results = list_datasets(search=query)
                    df_pub = pd.DataFrame([{"Dataset": r.id, "Tags": ", ".join(r.tags)} for r in results[:50]])
                    st.dataframe(df_pub, use_container_width=True, hide_index=True)
                    st.success("‚úÖ Hugging Face datasets retrieved.")
                elif src == "OpenML":
                    st.markdown(f"[üìä OpenML Search ‚ÜóÔ∏è](https://www.openml.org/search?type=data&q={query})")
                elif src == "Public Domain Portals":
                    st.markdown("""
                    - [üåé data.gov](https://www.data.gov/)
                    - [üá™üá∫ data.europa.eu](https://data.europa.eu/)
                    - [üá∏üá¨ data.gov.sg](https://data.gov.sg/)
                    - [üáªüá≥ data.gov.vn](https://data.gov.vn/)
                    """)
            except Exception as e:
                st.error(f"Search failed: {e}")

    # If we have Kaggle results, show table + import controls
    if src == "Kaggle (API)" and not ss["kaggle_search_df"].empty:
        st.dataframe(ss["kaggle_search_df"], use_container_width=True, hide_index=True)

        with st.expander("‚¨áÔ∏è Import Selected Kaggle Dataset", expanded=True):
            refs = ss["kaggle_search_df"]["ref"].astype(str).tolist()
            selected_ref = st.selectbox("Choose a dataset (ref)", refs, key="asset_kaggle_ref")

            kag_dir = os.path.join(RUNS_DIR, "kaggle")
            os.makedirs(kag_dir, exist_ok=True)
            safe_ref = re.sub(r"[^a-zA-Z0-9._/-]+", "_", selected_ref)
            safe_ref_for_file = safe_ref.replace("/", "__")
            dest = os.path.join(kag_dir, safe_ref_for_file)
            os.makedirs(dest, exist_ok=True)

            # Optional: let user set a server-side save folder (relative to project root)
            st.markdown("**Optional server-side save folder (relative to project root)**")
            default_svdir = os.path.join(RUNS_DIR, "kaggle_exports")
            svdir = st.text_input(
                "Save to folder (server-side)",
                value=default_svdir,
                key="asset_kaggle_svdir",
                help="This saves on the server/WSL side (not your desktop). Use Download button below for local Save As."
            )

            # Main import button
            if st.button("üì• Download & Import Selected", key="btn_asset_kaggle_dl", use_container_width=True):
                try:
                    import subprocess
                    cmd = ["kaggle", "datasets", "download", "-d", selected_ref, "-p", dest, "--unzip"]
                    r = subprocess.run(cmd, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                    if r.returncode != 0:
                        raise RuntimeError(r.stderr.strip() or r.stdout.strip())

                    csvs = [f for f in os.listdir(dest) if f.lower().endswith(".csv")]
                    if not csvs:
                        raise FileNotFoundError("No CSV found in the downloaded archive.")
                    fp = os.path.join(dest, csvs[0])

                    # Load & stash for downstream stages + download button
                    df_imp = _safe_read_csv(fp)
                    ss["asset_intake_df"] = df_imp

                    # Save a unified copy with timestamp in RUNS_DIR
                    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
                    uni_fp = os.path.join(RUNS_DIR, f"intake_table.{ts}.csv")
                    df_imp.to_csv(uni_fp, index=False)

                    st.success(f"‚úÖ Imported {len(df_imp):,} rows from `{selected_ref}`")
                    st.caption(f"Saved unified intake copy: `{uni_fp}`")
                    st.dataframe(df_imp.head(100), use_container_width=True)

                    # ‚îÄ‚îÄ NEW: Local Download (browser) with dataset-name.csv ‚îÄ‚îÄ
                    st.markdown("#### üíæ Download")
                    default_fname = f"{safe_ref_for_file}.csv"
                    st.download_button(
                        label="‚¨áÔ∏è Download CSV",
                        file_name=default_fname,
                        data=df_imp.to_csv(index=False).encode("utf-8"),
                        mime="text/csv",
                        key="asset_kaggle_download"
                    )

                    # ‚îÄ‚îÄ NEW: Optional server-side save with user folder ‚îÄ‚îÄ
                    st.markdown("#### üóÇÔ∏è Save on Server (optional)")
                    # sanitize: keep within project root
                    project_root = os.path.abspath(os.path.join(RUNS_DIR, "..", ".."))
                    svdir_abs = os.path.abspath(svdir)
                    if not svdir_abs.startswith(project_root):
                        st.warning("‚ö†Ô∏è Path is outside project root; resetting to default exports folder.")
                        svdir_abs = os.path.abspath(default_svdir)

                    os.makedirs(svdir_abs, exist_ok=True)
                    save_name = f"{safe_ref_for_file}.csv"
                    server_save_path = os.path.join(svdir_abs, save_name)

                    if st.button("üíΩ Save CSV on Server", key="btn_asset_kaggle_save_server"):
                        try:
                            df_imp.to_csv(server_save_path, index=False)
                            rel_path = os.path.relpath(server_save_path, start=project_root)
                            st.success(f"‚úÖ Saved on server: `{server_save_path}`")
                            st.caption(f"(Relative to project root: ./{rel_path})")
                        except Exception as e:
                            st.error(f"Server save failed: {e}")

                except Exception as e:
                    st.error(f"Import failed: {e}")
                    st.info("Tip: check Kaggle auth and try another dataset.")

    
   

    # Quick HF import (optional direct load)
    if src == "Hugging Face":
        st.markdown("#### Or load directly by repo id")
        hf_repo = st.text_input("ü§ó Dataset repo (e.g. uciml/real-estate-valuation)", value="uciml/real-estate-valuation", key="asset_hf_repo")
        if st.button("üì• Load from HF", key="btn_asset_hf_load", use_container_width=True):
            try:
                from datasets import load_dataset
                ds = load_dataset(hf_repo)
                split = next(iter(ds.keys()))
                df_imp = ds[split].to_pandas()
                ss["asset_intake_df"] = df_imp
                uni_fp = os.path.join(RUNS_DIR, f"intake_table.{_ts()}.csv")
                df_imp.to_csv(uni_fp, index=False)
                st.success(f"‚úÖ Loaded {len(df_imp):,} rows from {hf_repo} (split: {split})")
                st.caption(f"Saved unified intake copy: `{uni_fp}`")
                st.dataframe(df_imp.head(100), use_container_width=True)
            except Exception as e:
                st.error(f"HF load failed: {e}")

    st.divider()
    
    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # (A.3) NORMALIZE & GENERATE UNIFIED CSV
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üßπ Normalize & Combine All Inputs")

    # ---------- helpers ----------
    import io, csv, re
    from pathlib import Path

    def _slug(name: str) -> str:
        return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

    def _read_any_table(uploaded_file) -> pd.DataFrame:
        """Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback."""
        name = uploaded_file.name.lower()

        # Excel
        if name.endswith((".xlsx", ".xls")):
            return pd.read_excel(uploaded_file)

        # Text (CSV/TSV/TXT): try utf-8 then latin-1; sniff delimiter
        raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
        for enc in ("utf-8", "latin-1"):
            try:
                text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
                head = "\n".join(text.splitlines()[:10]) or text
                try:
                    dialect = csv.Sniffer().sniff(head)
                    sep = dialect.delimiter
                except Exception:
                    sep = ","
                return pd.read_csv(io.StringIO(text), sep=sep)
            except Exception:
                continue
        # last resort (python engine)
        return pd.read_csv(io.BytesIO(raw), engine="python")

    # ---------- optional upload right here ----------
    st.markdown("#### ‚¨ÜÔ∏è Optional: Upload a CSV/TSV/TXT/XLSX to normalize")
    uploaded = st.file_uploader(
        "Upload a dataset file (or skip if you already imported via Kaggle/HF).",
        type=["csv", "tsv", "txt", "xlsx"],
        key="norm_upload_once",
        accept_multiple_files=False
    )

    if uploaded is not None:
        try:
            df_up = _read_any_table(uploaded)
            ss["asset_intake_df"] = df_up
            ss["last_dataset_name"] = Path(uploaded.name).stem  # remember original name
            st.success(f"‚úÖ Loaded {len(df_up):,} rows from **{uploaded.name}**")
            st.dataframe(df_up.head(100), use_container_width=True, hide_index=True)
        except Exception as e:
            st.error(f"Could not read file: {e}")

    # ---------- normalization source ----------
    df_src = ss.get("asset_intake_df")
    # Best-effort name priority: last_dataset_name (Kaggle/HF/upload) ‚Üí fallback
    base_name = _slug(ss.get("last_dataset_name") or ss.get("asset_intake_source_name") or "dataset")

    if df_src is None or len(df_src) == 0:
        st.info("Upload/import a dataset first (Kaggle/HF/Upload), then come back to normalize.")
    else:
        with st.expander("‚öôÔ∏è Normalization options", expanded=False):
            drop_dupes = st.checkbox("Drop duplicate rows", value=True)
            trim_whitespace = st.checkbox("Trim whitespace in string columns", value=True)
            lower_columns = st.checkbox("Lowercase column names", value=True)

        def _normalize(df: pd.DataFrame) -> pd.DataFrame:
            out = df.copy()
            # 1) basic cleanup
            if lower_columns:
                out.columns = [c.strip().lower() for c in out.columns]
            if trim_whitespace:
                for c in out.select_dtypes(include=["object"]).columns:
                    out[c] = out[c].astype(str).str.strip()
            if drop_dupes:
                out = out.drop_duplicates().reset_index(drop=True)
            # (Optional) add any schema harmonization here later
            return out

        if st.button("üß™ Normalize & Generate Unified CSV", key="btn_normalize", use_container_width=True):
            norm_df = _normalize(df_src)

            # Ensure output dir
            norm_dir = os.path.join(RUNS_DIR, "normalized")
            os.makedirs(norm_dir, exist_ok=True)

            # Build file name: <original>-Normalized.csv   (slug-safe base_name)
            norm_name = f"{base_name}-Normalized.csv"
            norm_path = os.path.join(norm_dir, norm_name)

            # Save to disk with utf-8-sig (friendlier for Excel)
            norm_df.to_csv(norm_path, index=False, encoding="utf-8-sig")

            # Prepare one bytes blob for both download buttons
            _norm_bytes = norm_df.to_csv(index=False).encode("utf-8-sig")
            _rows, _cols = len(norm_df), len(norm_df.columns)
            _size_kb = max(1, int(len(_norm_bytes) / 1024))
            _norm_file_only = Path(norm_path).name

            # Sticky banner: filename ‚Ä¢ rows√ócols ‚Ä¢ size
            st.markdown(
                f"""
                <div style="
                    position: sticky;
                    top: 64px;
                    z-index: 50;
                    background: rgba(16,185,129,0.10);
                    border: 1px solid #10b981;
                    padding: 12px 16px;
                    border-radius: 12px;
                    margin: 8px 0 14px 0;
                ">
                <b>‚úÖ Normalized CSV:</b> <code>{_norm_file_only}</code>
                &nbsp;‚Ä¢&nbsp; {_rows:,} rows √ó {_cols} cols &nbsp;‚Ä¢&nbsp; {_size_kb} KB
                </div>
                """,
                unsafe_allow_html=True
            )

            # BIG centered primary download button (TOP)
            cL, cM, cR = st.columns([1, 2.5, 1])
            with cM:
                st.download_button(
                    "‚¨áÔ∏è  Download Normalized CSV",
                    data=_norm_bytes,
                    file_name=_norm_file_only,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_norm_top"
                )

            # Copyable saved path
            st.text_input("Saved to (server path)", norm_path, disabled=True, label_visibility="collapsed")

            # Preview table
            st.dataframe(norm_df.head(100), use_container_width=True, hide_index=True)

            # BIG centered primary download button (BOTTOM)
            cL2, cM2, cR2 = st.columns([1, 2.5, 1])
            with cM2:
                st.download_button(
                    "‚¨áÔ∏è  Download Normalized CSV",
                    data=_norm_bytes,
                    file_name=_norm_file_only,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_norm_bottom"
                )

    # # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # # (A.1b) QUICK START ‚Äî Generate Synthetic Data (always visible here)
    # # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # st.markdown("### üé≤ Quick Start: Generate Synthetic Data")
    # c_syn1, c_syn2 = st.columns([2, 1])
    # with c_syn1:
    #     nrows = st.slider("Number of synthetic rows", 20, 1000, 150, step=10, key="slider_synth_rows_A_quick")
    # with c_syn2:
    #     if st.button("üöÄ Generate Synthetic Dataset Now", key="btn_generate_synth_A_quick", use_container_width=True):
    #         try:
    #             df_synth = quick_synth(nrows)
    #             ss["asset_intake_df"] = df_synth
    #             os.makedirs("./.tmp_runs", exist_ok=True)
    #             synth_path = f"./.tmp_runs/intake_table_synth_{_ts()}.csv"
    #             df_synth.to_csv(synth_path, index=False, encoding="utf-8-sig")
    #             st.success(f"‚úÖ Synthetic dataset created ({len(df_synth)} rows). Saved: `{synth_path}`")
    #             st.dataframe(df_synth.head(20), use_container_width=True, hide_index=True)
    #             st.download_button(
    #                 "‚¨áÔ∏è Download Synthetic CSV",
    #                 df_synth.to_csv(index=False).encode("utf-8-sig"),
    #                 file_name="synthetic_intake.csv",
    #                 mime="text/csv",
    #                 key="dl_synth_A_quick"
    #             )
    #         except Exception as e:
    #             st.error(f"Synthetic generation failed: {e}")

       
        
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # (A.4) SYNTHETIC DATA GENERATION
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### ü§ñ Generate Synthetic Data (Fallback)")
    nrows = st.slider("Number of synthetic rows", 10, 500, 150, step=10, key="slider_synth_rows")
    if st.button("üé≤ Generate Synthetic Dataset", key="btn_generate_synth"):
        try:
            df_synth = quick_synth(nrows)
            ss["asset_intake_df"] = df_synth
            os.makedirs("./.tmp_runs", exist_ok=True)
            synth_path = f"./.tmp_runs/intake_table_synth_{datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')}.csv"
            df_synth.to_csv(synth_path, index=False)
            st.success(f"‚úÖ Synthetic dataset created ({len(df_synth)} rows).")
            st.dataframe(df_synth.head(20), use_container_width=True)
            st.download_button("üíæ Download Synthetic CSV", df_synth.to_csv(index=False), "synthetic_intake.csv", "text/csv")
        except Exception as e:
            st.error(f"Synthetic generation failed: {e}")





# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# B ‚Äî PRIVACY & FEATURES (2..3)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tabB:
    st.subheader("B. Privacy & Features")
    st.caption("Steps: **2) Anonymize**, **3) Feature Engineering + Comps**")

    import re, math, json, os, time
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    
    # ----------------------------
    # B.2 ‚Äî Anonymize / Sanitize PII
    # ----------------------------
    st.markdown("### **2) Anonymize / Sanitize PII**")

    import io, csv, re, os
    from pathlib import Path

    ss = st.session_state  # make sure this exists globally

    def _slug(name: str) -> str:
        return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

    def _read_any_table(uploaded_file) -> pd.DataFrame:
        """Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback."""
        name = uploaded_file.name.lower()
        if name.endswith((".xlsx", ".xls")):
            return pd.read_excel(uploaded_file)

        raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
        for enc in ("utf-8", "latin-1"):
            try:
                text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
                head = "\n".join(text.splitlines()[:10]) or text
                try:
                    sep = csv.Sniffer().sniff(head).delimiter
                except Exception:
                    sep = ","
                return pd.read_csv(io.StringIO(text), sep=sep)
            except Exception:
                continue
        return pd.read_csv(io.BytesIO(raw), engine="python")

    def _anonymize(df: pd.DataFrame) -> pd.DataFrame:
        """Mask likely-PII text while preserving common join keys."""
        if df is None or df.empty:
            return df
        out = df.copy()
        join_keys = {"loan_id", "asset_id", "application_id"}
        pii_like = re.compile(r"(name|email|phone|addr|address|national|passport|id|nid)", re.I)

        for col in out.columns:
            if col in join_keys:
                continue
            if out[col].dtype == "object" and pii_like.search(col):
                s = out[col].astype(str)
                s = s.str.replace(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+", "[EMAIL]", regex=True)
                s = s.str.replace(r"\b(\+?\d{1,3}[-.\s]?)?\d{7,}\b", "[PHONE]", regex=True)
                s = s.str.replace(r"\b\d{9,16}\b", "[ID]", regex=True)
                out[col] = s
        return out.drop_duplicates().reset_index(drop=True)

    # ---------- Source picker (Stage A or Upload here) ----------
    st.markdown("#### üîÑ Choose data source")
    src_choice = st.radio(
        "Pick how you want to provide data:",
        ["Use data from Stage A", "Upload a new file here"],
        horizontal=True,
        key="b_src_choice"
    )

    df_src, base_name = None, "dataset"

    if src_choice == "Use data from Stage A":
        df_src = ss.get("asset_intake_df")
        if isinstance(df_src, pd.DataFrame) and not df_src.empty:
            base_name = _slug(ss.get("last_dataset_name") or "dataset")
            st.success(f"Using Stage-A dataset: **{base_name}** ‚Ä¢ {len(df_src):,} rows")
            st.dataframe(df_src.head(10), use_container_width=True, hide_index=True)
        else:
            st.warning("No data found from Stage A. Upload below instead.")
    else:
        st.markdown(
            """
            <div style="border:2px dashed #22c55e;padding:14px;border-radius:12px;
                        background:rgba(34,197,94,0.06);margin:6px 0 12px 0;">
            <b>‚¨ÜÔ∏è Upload CSV/TSV/TXT/XLSX for anonymization</b>
            </div>
            """,
            unsafe_allow_html=True,
        )
        up = st.file_uploader(
            "Upload a dataset file",
            type=["csv","tsv","txt","xlsx"],
            key="b_upload",
            accept_multiple_files=False,
        )
        if up is not None:
            try:
                df_src = _read_any_table(up)
                base_name = _slug(Path(up.name).stem)
                ss["asset_intake_df"] = df_src
                ss["last_dataset_name"] = base_name
                st.success(f"‚úÖ Loaded {len(df_src):,} rows from **{up.name}**")
                st.dataframe(df_src.head(20), use_container_width=True, hide_index=True)
            except Exception as e:
                st.error(f"Could not read file: {e}")

    if not (isinstance(df_src, pd.DataFrame) and not df_src.empty):
        st.info("Provide data (via Stage A or upload here) to enable anonymization.")
    else:
        if st.button("üõ°Ô∏è Run Anonymization & Export CSV", type="primary", use_container_width=True, key="btn_b_anon"):
            anon_df = _anonymize(df_src)
            out_dir = os.path.join(RUNS_DIR, "normalized")
            os.makedirs(out_dir, exist_ok=True)
            out_name = f"{base_name}-Anonymized.csv"
            out_path = os.path.join(out_dir, out_name)
            anon_df.to_csv(out_path, index=False, encoding="utf-8-sig")
            ss["asset_anon_df"] = anon_df

            _bytes = anon_df.to_csv(index=False).encode("utf-8-sig")

            st.markdown(
                f"""
                <div style="position:sticky;top:64px;z-index:50;background:rgba(59,130,246,0.10);
                            border:1px solid #3b82f6;padding:12px 16px;border-radius:12px;margin:8px 0 14px 0;">
                <b>‚úÖ Anonymized CSV:</b> <code>{out_name}</code> ‚Ä¢ {len(anon_df):,} rows √ó {len(anon_df.columns)} cols
                </div>
                """,
                unsafe_allow_html=True,
            )

            cL, cM, cR = st.columns([1, 2.6, 1])
            with cM:
                st.download_button(
                    "‚¨áÔ∏è  Download Anonymized CSV",
                    data=_bytes,
                    file_name=out_name,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_b_anon_top",
                )

            st.text_input("Saved to (server path)", out_path, disabled=True, label_visibility="collapsed")
            st.dataframe(anon_df.head(100), use_container_width=True, hide_index=True)

            cL2, cM2, cR2 = st.columns([1, 2.6, 1])
            with cM2:
                st.download_button(
                    "‚¨áÔ∏è  Download Anonymized CSV",
                    data=_bytes,
                    file_name=out_name,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_b_anon_bottom",
                )

    st.markdown("---")

    
    

    
    # ----------------------------
    # B.3 ‚Äî Feature Engineering & Comps
    # ----------------------------
    st.markdown("### **3) Feature Engineering & Comps**")

    def feature_engineer(df: pd.DataFrame, evidence_index=None) -> pd.DataFrame:
        """
        Light feature engineering (safe):
        - Ensure city/lat/lon/age_years/delinquencies/current_loans
        - Coerce lat/lon including "10,762" ‚Üí 10.762
        - Create stable 'geohash' (lat,lon preferred; fallback short city hash)
        - Derive condition_score (0..1) heuristically if inputs exist
        - Ensure legal_penalty numeric
        - Keep join keys up front if present
        """
        if not isinstance(df, pd.DataFrame) or df.empty:
            return pd.DataFrame()

        out = df.copy()

        # Ensure expected columns exist
        for c in ("city", "lat", "lon", "age_years", "delinquencies", "current_loans"):
            if c not in out.columns:
                out[c] = pd.NA

        # Coerce lat/lon
        for c in ("lat", "lon"):
            if out[c].dtype == "object":
                out[c] = out[c].astype(str).str.replace(",", ".", regex=False)
            out[c] = pd.to_numeric(out[c], errors="coerce")

        # Row-wise geokey: prefer lat/lon else short md5(city)
        import hashlib
        def _row_geokey(row) -> str:
            lat = row.get("lat")
            lon = row.get("lon")
            if pd.notna(lat) and pd.notna(lon):
                return f"{float(lat):.3f},{float(lon):.3f}"
            city_val = row.get("city")
            city_txt = "" if pd.isna(city_val) else str(city_val)
            return hashlib.md5(city_txt.encode("utf-8")).hexdigest()[:7]

        out["geohash"] = out.apply(_row_geokey, axis=1).astype(str)

        # Heuristic condition_score (0..1)
        age = pd.to_numeric(out.get("age_years"), errors="coerce").fillna(0.0)
        delinq = pd.to_numeric(out.get("delinquencies"), errors="coerce").fillna(0.0)
        curr_loans = pd.to_numeric(out.get("current_loans"), errors="coerce").fillna(0.0)
        cond = 1.0 - (0.02 * age) - (0.05 * delinq) - (0.03 * curr_loans)
        out["condition_score"] = pd.Series(cond, index=out.index).clip(0.10, 0.98)

        # legal_penalty safe numeric
        if "legal_penalty" not in out.columns:
            out["legal_penalty"] = 0.0
        else:
            out["legal_penalty"] = pd.to_numeric(out["legal_penalty"], errors="coerce").fillna(0.0)

        # Keep join keys in front
        front_cols = [c for c in ["loan_id", "application_id", "asset_id"] if c in out.columns]
        other_cols = [c for c in out.columns if c not in front_cols]
        out = out[front_cols + other_cols]

        return out


    def _fmt_mean(df, col, fmt="{:.2f}"):
        if isinstance(df, pd.DataFrame) and col in df.columns:
            v = pd.to_numeric(df[col], errors="coerce").mean()
            if pd.notna(v):
                return fmt.format(v)
        return "‚Äî"


    def fetch_and_clean_comps(df_feats: pd.DataFrame) -> dict:
        """Deterministic stub; replace with real comps feed."""
        mv = pd.to_numeric(df_feats.get("market_value", pd.Series(dtype=float)), errors="coerce")
        base = float(mv.median()) if mv.notna().any() else 100000.0
        comps = [{"comp_id": f"C-{i+1:03d}", "price": round(base * (0.95 + 0.02 * i), 2)} for i in range(5)]
        return {"used": comps, "count": len(comps), "median_baseline": base}


    if not isinstance(ss.get("asset_anon_df"), pd.DataFrame) or ss["asset_anon_df"].empty:
        st.info("Run **Anonymization (B.2)** first to prepare inputs for features.")
    else:
        c3a, c3b = st.columns([1.2, 0.8])

        with c3a:
            if st.button("Build Features & Fetch Comps", key="btn_build_features", use_container_width=True):
                # Build features in this rerun and persist
                feats = feature_engineer(ss["asset_anon_df"], ss.get("asset_evidence_index"))
                ss["asset_features_df"] = feats

                # Metrics (from feats)
                m1, m2, m3 = st.columns(3)
                with m1:
                    st.metric("Avg condition_score", _fmt_mean(feats, "condition_score"))
                with m2:
                    st.metric("Avg market_value", _fmt_mean(feats, "market_value", "{:,.0f}"))
                with m3:
                    st.metric("Avg loan_amount", _fmt_mean(feats, "loan_amount", "{:,.0f}"))

                # Persist features.parquet (BytesIO for download)
                features_path = os.path.join(RUNS_DIR, f"features.{_ts()}.parquet")
                feats.to_parquet(features_path, index=False)
                st.success(f"Saved features ‚Üí `{features_path}`")

                import io as _io
                _buf = _io.BytesIO()
                feats.to_parquet(_buf, index=False)
                st.download_button(
                    "‚¨áÔ∏è Download features.parquet",
                    data=_buf.getvalue(),
                    file_name="features.parquet",
                    mime="application/octet-stream",
                    key="dl_features_parquet",
                    use_container_width=True
                )

                # Comps (and persist)
                comps = fetch_and_clean_comps(feats)
                ss["asset_comps_used"] = comps
                comps_path = os.path.join(RUNS_DIR, f"comps_used.{_ts()}.json")
                with open(comps_path, "w", encoding="utf-8") as fp:
                    json.dump(comps, fp, ensure_ascii=False, indent=2)
                st.success(f"Saved comps ‚Üí `{comps_path}`")

        with c3b:
            # Show last built features (if any) and offer a second download
            df_feats = ss.get("asset_features_df")
            if isinstance(df_feats, pd.DataFrame) and not df_feats.empty:
                import io as _io2
                _buf2 = _io2.BytesIO()
                df_feats.to_parquet(_buf2, index=False)
                st.download_button(
                    "‚¨áÔ∏è Download last features.parquet",
                    data=_buf2.getvalue(),
                    file_name="features.parquet",
                    mime="application/octet-stream",
                    key="dl_features_parquet_last",
                    use_container_width=True
                )

    # Outside the columns: show current features + comps if present
    df_feats = ss.get("asset_features_df")
    if isinstance(df_feats, pd.DataFrame) and not df_feats.empty:
        k1, k2, k3 = st.columns(3)
        with k1:
            st.metric("Rows (features)", f"{len(df_feats):,}")
        with k2:
            st.metric("Avg condition_score", _fmt_mean(df_feats, "condition_score"))
        with k3:
            # evidence_count column is optional; show 0 if absent
            ev = pd.to_numeric(df_feats.get("evidence_count", pd.Series([0]*len(df_feats))), errors="coerce").fillna(0)
            st.metric("Evidence count (stub)", int(ev.mean()))

        st.dataframe(df_feats.head(30), use_container_width=True)

    if ss.get("asset_comps_used") is not None:
        st.caption("Comps used (stub)")
        st.json(ss["asset_comps_used"])

    


# ========== 3) AI APPRAISAL & VALUATION ==========
with tabC:
    st.subheader("ü§ñ Stage 3 ‚Äî AI Appraisal & Valuation")

    import os, json, numpy as np, requests, pandas as pd, plotly.express as px
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß≠ HOW TO USE THIS STAGE
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("""
    ### üß≠ How to Use This Stage
    1. **Select a model** ‚Äî choose between production, trained, or open-source (Hugging Face) models.  
    2. **Check hardware** ‚Äî confirm your GPU/CPU profile supports the chosen model.  
    3. **Select dataset** ‚Äî use Stage 2 outputs (Features / Anonymized) or fallback synthetic data.  
    4. **Run appraisal** ‚Äî compute AI-based valuation (`fmv`, `ai_adjusted`, `confidence`, `why`).  
    5. **Review outputs** ‚Äî compare customer vs AI results, run projections, dashboards, and reports.  
    6. **Verify ownership** ‚Äî perform Legal / Encumbrance checks (C.5).  
    """)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß† MODEL FAMILY TABLE
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üß† Model Families & Recommended Use-Cases")

    model_ref = pd.DataFrame([
        {"Category": "Local / Trained", "Model": "LightGBM / XGBoost / CatBoost",
         "Use Case": "Numeric ‚Üí FMV prediction", "GPU": "CPU OK",
         "Notes": "Fast, explainable baseline model"},
        {"Category": "Production (‚≠ê)", "Model": "asset_lgbm-v1 / credit_lr",
         "Use Case": "Enterprise-grade deployed valuation", "GPU": "CPU OK",
         "Notes": "Stable, low-latency predictions"},
        {"Category": "LLM (HF)", "Model": "Mistral 7B / Gemma 2 9B",
         "Use Case": "Text reasoning + narratives", "GPU": "‚â• 8 GB",
         "Notes": "Fast reasoning for appraisal explanations"},
        {"Category": "LLM (HF)", "Model": "LLaMA 3 8B / Qwen 2 7B",
         "Use Case": "Multilingual valuation reports", "GPU": "‚â• 12 GB",
         "Notes": "Strong contextual generation"},
        {"Category": "LLM (HF)", "Model": "Mixtral 8√ó7B",
         "Use Case": "High-end MoE valuation", "GPU": "‚â• 24 GB",
         "Notes": "Premium precision for portfolios"},
    ])
    st.dataframe(model_ref, use_container_width=True)
    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üü¢ PRODUCTION MODEL BANNER
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.status_code == 200:
            meta = resp.json()
            if meta.get("has_production"):
                ver = (meta.get("meta") or {}).get("version", "1.x")
                src = (meta.get("meta") or {}).get("source", "production")
                st.success(f"üü¢ Production model active ‚Äî version {ver} ‚Ä¢ source {src}")
            else:
                st.warning("‚ö†Ô∏è No production model promoted yet ‚Äî using baseline.")
        else:
            st.info("‚ÑπÔ∏è Could not fetch production model meta.")
    except Exception:
        st.info("‚ÑπÔ∏è Production meta unavailable.")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß© Model Selection (list all trained models) ‚Äî HARD-CODED TEST
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    from datetime import datetime
    import os, shutil, streamlit as st

    # Hardcoded absolute paths for your environment
    trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/trained"
    production_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/production"

    # Debug info
    st.caption(f"üìÇ Trained dir: `{trained_dir}`")
    st.caption(f"üì¶ Production dir: `{production_dir}`")

    # Refresh button ‚Äî use unique key for asset
    if st.button("‚Üª Refresh models", key="asset_refresh_models"):
        st.session_state.pop("asset_selected_model", None)
        st.rerun()

    models = []
    if os.path.isdir(trained_dir):
        for f in os.listdir(trained_dir):
            if f.endswith(".joblib"):
                fpath = os.path.join(trained_dir, f)
                ctime = os.path.getctime(fpath)
                created = datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
                models.append((f, fpath, created))
    else:
        st.error(f"‚ùå Trained dir not found: {trained_dir}")

    if models:
        # Sort by creation time (latest first)
        models.sort(key=lambda x: os.path.getctime(x[1]), reverse=True)
        display_names = [f"{m[0]} ‚Äî {m[2]}" for m in models]

        selected_display = st.selectbox("üì¶ Select trained model to use", display_names, key="asset_model_select")
        selected_model = models[display_names.index(selected_display)][1]
        st.success(f"‚úÖ Using model: {os.path.basename(selected_model)}")

        st.session_state["asset_selected_model"] = selected_model

        # Promote to production
        if st.button("üöÄ Promote this model to Production", key="asset_promote_button"):
            try:
                os.makedirs(production_dir, exist_ok=True)
                prod_path = os.path.join(production_dir, "model.joblib")
                shutil.copy2(selected_model, prod_path)
                st.balloons()
                st.success(f"‚úÖ Model promoted to production: {prod_path}")
            except Exception as e:
                st.error(f"‚ùå Promotion failed: {e}")
    else:
        st.warning("‚ö†Ô∏è No trained models found ‚Äî train one in Stage F first.")


    

    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß† LLM + HARDWARE PROFILE (LOCAL + HUGGING FACE)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üß† LLM & Hardware Profile (Local + Hugging Face Models)")

    st.dataframe(pd.DataFrame(get_hf_models()), use_container_width=True)

    OPENSTACK_FLAVORS = {
        "m4.medium": "4 vCPU / 8 GB RAM (CPU-only small)",
        "m8.large": "8 vCPU / 16 GB RAM (CPU-only medium)",
        "g1.a10.1": "8 vCPU / 32 GB RAM + 1√óA10 24 GB",
        "g1.l40.1": "16 vCPU / 64 GB RAM + 1√óL40 48 GB",
        "g2.a100.1": "24 vCPU / 128 GB RAM + 1√óA100 80 GB",
    }

    with st.expander("üß† Choose Model & Hardware Profile", expanded=True):
        st.info("CPU picks land first so you can generate valuation narratives without waiting on GPUs. Jump to the GPU section only if you need deeper reasoning or longer context windows.", icon="‚öôÔ∏è")
        selected_llm = render_llm_selector(context="asset_appraisal")
        st.session_state["asset_llm_label"] = selected_llm["model"]
        st.session_state["asset_llm_model"] = selected_llm["value"]
        llm_value = selected_llm["value"]
        use_llm = st.checkbox(
            "Use LLM narrative (explanations)",
            value=st.session_state.get("asset_use_llm", False),
            key="asset_use_llm",
        )
        flavor = st.selectbox(
            "OpenStack flavor / host profile",
            list(OPENSTACK_FLAVORS.keys()),
            index=0,
            key="asset_flavor",
        )
        st.caption(OPENSTACK_FLAVORS[flavor])
        st.caption("These parameters are passed to backend (Ollama / Flowise / RunAI).")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # GPU PROFILE AND DATASET SOURCE (keep existing logic)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### **C.4 ‚Äî Valuation (AI)**")
    gpu_profile = st.selectbox(
        "GPU Profile (for valuation compute)",
        ["CPU (slow)", "GPU: 1√óA100", "GPU: 1√óH100", "GPU: 2√óL40S"],
        index=1, key="asset_gpu_profile_c4")
    ss["asset_gpu_profile"] = gpu_profile

    # Gather candidates from prior stages (they may be None/empty)
    cand_features = ss.get("asset_features_df")
    cand_anon     = ss.get("asset_anon_df")
    cand_intake   = ss.get("asset_intake_df")

    src = st.selectbox(
        "Data source for AI run",
        [
            "Use FEATURES (Stage 2/3)",
            "Use ANON (Stage 2)",
            "Use RAW ‚Üí auto-sanitize",
            "Use synthetic (fallback)",
        ],
        key="asset_c4_source"
    )

    # Decide df2 explicitly based on choice
    df2 = None
    if src == "Use FEATURES (Stage 2/3)":
        # First non-empty among features ‚Üí anon ‚Üí intake
        df2 = first_nonempty_df(cand_features, cand_anon, cand_intake)

    elif src == "Use ANON (Stage 2)":
        df2 = cand_anon

    elif src == "Use RAW ‚Üí auto-sanitize":
        # If intake exists, sanitize; else leave None
        df2 = anonymize_text_cols(cand_intake) if isinstance(cand_intake, pd.DataFrame) and not cand_intake.empty else None

    else:  # "Use synthetic (fallback)"
        df2 = quick_synth(150)

    # Final safety check
    if not isinstance(df2, pd.DataFrame) or df2.empty:
        st.warning("No usable dataset found. Please complete Stage A (Intake) and Stage B (Privacy/Features), or choose the synthetic fallback.")
        st.stop()

    # Preview selected data
    st.dataframe(df2.head(10), use_container_width=True)



    # Probe API (health & agents)
    with st.expander("üîé Probe API (health & agents)", expanded=False):
        if st.button("Run probe now", key="btn_probe_api"):
            diag = probe_api()
            st.json(diag)

    # Run model button (runtime flavor + gpu_profile included)
    if st.button("üöÄ Run AI Appraisal now", key="btn_run_ai"):
        health_ok, health_payload = _safe_get_json(f"{API_URL}/v1/health")
        if not health_ok:
            st.error("Backend API is not reachable. Start your API server (port 8090) via newstart.sh and rerun.")
            st.caption("Details from /v1/health probe:")
            st.code(str(health_payload)[:2000])
            st.stop()

        csv_bytes = df2.to_csv(index=False).encode("utf-8")

        form_fields = {
            "use_llm": str(use_llm).lower(),
            "llm": llm_value,
            "flavor": flavor,
            "gpu_profile": gpu_profile,  # NEW: pass GPU profile to backend
            "selected_model": ss.get("asset_selected_model", ""),
            "agent_name": "asset_appraisal",
        }

        with st.spinner("Calling asset agent‚Ä¶"):
            ok, result = try_run_asset_agent(csv_bytes, form_fields=form_fields, timeout_sec=180)

        if not ok:
            st.error("‚ùå Model API error.")
            st.info("Tip: open 'üîé Probe API' above to see health and discovered agent ids.")
            st.code(str(result)[:8000])
            st.stop()

        df_app = result.copy()

        # Ensure core valuation columns per blueprint
        if "ai_adjusted" not in df_app.columns and "market_value" in df_app.columns:
            df_app["ai_adjusted"] = df_app["market_value"]
        if "fmv" not in df_app.columns:
            # heuristics: if model returns fmv, keep; else set fmv ~ ai_adjusted
            df_app["fmv"] = pd.to_numeric(df_app.get("ai_adjusted", np.nan), errors="coerce")
        if "confidence" not in df_app.columns:
            df_app["confidence"] = 80.0
        if "why" not in df_app.columns:
            df_app["why"] = ["Condition, comps, and features (placeholder)"] * len(df_app)

        # Persist valuation artifact
        val_path = os.path.join(RUNS_DIR, f"valuation_ai.{_ts()}.csv")
        df_app.to_csv(val_path, index=False)
        
        _ping_chatbot_refresh("asset_run")
        
        st.success(f"Saved valuation artifact ‚Üí `{val_path}`")

        # # ‚úÖ PATCH: Save Stage C valuation table for Stage H (use df_app, not ai_df)
        # try:
        #     st.session_state["asset_ai_df"] = df_app.copy()
        #     ss["asset_ai_df"] = df_app.copy()
        #     st.info("‚úÖ Stage C valuation stored for Stage D / E / H.")
        # except Exception as e:
        #     st.warning(f"Could not store Stage C output: {e}")

        # st.success(f"Saved valuation artifact ‚Üí `{val_path}`")
        # # ‚úÖ PATCH: Save Stage C valuation table for Stage H
        # try:
        #     st.session_state["asset_ai_df"] = ai_df.copy()
        #     st.info("‚úÖ Stage C results stored for Stage H portfolio view.")
        # except Exception as e:
        #     st.warning(f"Could not store Stage C output: {e}")


        # Keep table for downstream steps
        ss["asset_ai_df"] = df_app

        # Display minimal KPIs
        k1, k2, k3 = st.columns(3)
        try:
            k1.metric("Avg FMV", f"{pd.to_numeric(df_app['fmv'], errors='coerce').mean():,.0f}")
        except Exception:
            k1.metric("Avg FMV", "‚Äî")
        try:
            k2.metric("Avg Confidence", f"{pd.to_numeric(df_app['confidence'], errors='coerce').mean():.2f}")
        except Exception:
            k2.metric("Avg Confidence", "‚Äî")
        k3.metric("Rows", len(df_app))

        st.markdown("### üßæ Valuation Output (preview)")
        cols_first = [c for c in [
            "application_id","asset_id","asset_type","city",
            "fmv","ai_adjusted","confidence","why"
        ] if c in df_app.columns]
        st.dataframe(df_app[cols_first].head(50), use_container_width=True)

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # Customer vs AI ‚Äî Details & 5-Year Deltas
        # (Place this right after the valuation preview table)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        st.markdown("### üìã Customer & Loan Details (Declared) + AI Alignment")

        import numpy as np
        from datetime import datetime, timezone
        import os

        RUNS_DIR = "./.tmp_runs"
        os.makedirs(RUNS_DIR, exist_ok=True)
        _ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

        
        # ‚úÖ Save Stage C output for Stage H (single source of truth)
        st.session_state["asset_ai_df"] = df_app.copy()
        ss["asset_ai_df"] = df_app.copy()
        st.info("‚úÖ Stage C valuation stored for Stage D / E / H.")

        # ‚úÖ Use the saved valuation table
        ai_df = st.session_state["asset_ai_df"]

        # ‚úÖ Merge intake (customer-declared) if available
        intake_df = ss.get("asset_intake_df")
        if intake_df is not None and not intake_df.empty:
            join_keys = [
                k for k in ["application_id", "asset_id"]
                if k in ai_df.columns and k in intake_df.columns
            ]
            if join_keys:
                merged = intake_df.merge(
                    ai_df, on=join_keys,
                    suffixes=("_cust", "_ai"),
                    how="left"
                )
            else:
                merged = ai_df.copy()
        else:
            merged = ai_df.copy()

        # Optional: show preview table
        st.markdown("### üîç Stage C Output Preview")
        st.dataframe(merged, use_container_width=True)

        
        # # ‚úÖ Save Stage C output for Stage H (using df_app, not undefined ai_df)
        # st.session_state["asset_ai_df"] = df_app.copy()
        # #ai_df = df_app.copy()
        # ai_df = st.session_state["asset_ai_df"]
        

        

        # ‚úÖ Merge intake (customer-declared) if available
        intake_df = ss.get("asset_intake_df")
        if intake_df is not None and not intake_df.empty:
            # Choose join keys available in both frames
            join_keys = [
                k for k in ["application_id", "asset_id"]
                if k in ai_df.columns and k in intake_df.columns
            ]
            if join_keys:
                merged = intake_df.merge(
                    ai_df, on=join_keys,
                    suffixes=("_cust", "_ai"),
                    how="left"
                )
            else:
                merged = ai_df.copy()
        else:
            merged = ai_df.copy()

        
        # # ‚úÖ Save Stage C output for Stage H
        # #st.session_state["asset_ai_df"] = ai_df.copy()
        # ai_df = ss.get("asset_ai_df")
        # if ai_df is None or len(ai_df) == 0:
        #     st.info("Run the AI appraisal first to populate these tables.")
        # else:
        #     # Merge intake (customer-declared) if available
        #     intake_df = ss.get("asset_intake_df")
        #     if intake_df is not None and not intake_df.empty:
        #         # Choose join keys available in both frames
        #         join_keys = [k for k in ["application_id", "asset_id"] if k in ai_df.columns and k in intake_df.columns]
        #         if join_keys:
        #             merged = intake_df.merge(ai_df, on=join_keys, suffixes=("_cust", "_ai"), how="left")
        #         else:
        #             merged = ai_df.copy()
        #     else:
        #         merged = ai_df.copy()

            # Canonical column mapping
            # customer declared value (prefer *_cust if merge happened)
            customer_val_col = "market_value_cust" if "market_value_cust" in merged.columns else (
                "market_value" if "market_value" in merged.columns else None
            )
            # AI value (prefer fmv, fallback ai_adjusted)
            ai_val_col = "fmv" if "fmv" in merged.columns else (
                "ai_adjusted" if "ai_adjusted" in merged.columns else None
            )

            # Build Customer & Loan Details table
            details_cols = [c for c in [
                "application_id","asset_id","asset_type","city",
                customer_val_col,
                "loan_amount",
                ai_val_col, "confidence","why"
            ] if c and c in merged.columns]

            details_tbl = merged[details_cols].copy() if details_cols else merged.copy()

            # Rename for clarity in the UI
            rename_map = {}
            if customer_val_col:
                rename_map[customer_val_col] = "customer_declared_value"
            if ai_val_col:
                rename_map[ai_val_col] = "ai_estimate_value"
            details_tbl = details_tbl.rename(columns=rename_map)

            # Explanation / Source
            selected_model = os.path.basename(str(ss.get("asset_selected_model","") or ""))
            comps_count = int((ss.get("asset_comps_used") or {}).get("count", 0))
            details_tbl["explanation_source"] = details_tbl.apply(
                lambda r: f"Customer input CSV vs AI model {selected_model or 'production'} (comps={comps_count})",
                axis=1
            )

            st.dataframe(details_tbl.head(50), use_container_width=True)

            # Persist details table
            details_path = os.path.join(RUNS_DIR, f"customer_loan_details.{_ts}.csv")
            details_tbl.to_csv(details_path, index=False)
            st.download_button(
                "‚¨áÔ∏è Download Customer & Loan Details (CSV)",
                data=details_tbl.to_csv(index=False).encode("utf-8"),
                file_name="customer_loan_details.csv",
                mime="text/csv"
            )

            st.markdown("---")
            st.markdown("### üìà 5-Year Deltas: Customer vs AI (per-year Œî and %Œî)")

            # Controls for forward projections
            cgr_a, cgr_b = st.columns(2)
            with cgr_a:
                cust_cagr = st.slider("Customer Expected CAGR (%)", min_value=-20, max_value=40, value=5, step=1) / 100.0
            with cgr_b:
                ai_cagr = st.slider("AI Expected CAGR (%)", min_value=-20, max_value=40, value=4, step=1) / 100.0

            if not customer_val_col or not ai_val_col:
                st.warning("Missing base columns to compute deltas. Ensure both customer and AI values exist.")
            else:
                base_cust = merged[customer_val_col].astype(float)
                base_ai   = merged[ai_val_col].astype(float)

                # Build long-format 5-year projection table
                rows = []
                years = [1, 2, 3, 4, 5]
                for idx in range(len(merged)):
                    cust0 = base_cust.iloc[idx]
                    ai0   = base_ai.iloc[idx]
                    app_id = merged.iloc[idx].get("application_id", None)
                    asset_id = merged.iloc[idx].get("asset_id", None)
                    asset_type = merged.iloc[idx].get("asset_type", None)
                    city = merged.iloc[idx].get("city", None)

                    for y in years:
                        cust_y = cust0 * ((1.0 + cust_cagr) ** y) if np.isfinite(cust0) else np.nan
                        ai_y   = ai0   * ((1.0 + ai_cagr) ** y)   if np.isfinite(ai0)   else np.nan
                        delta  = ai_y - cust_y if (np.isfinite(ai_y) and np.isfinite(cust_y)) else np.nan
                        pct    = (delta / cust_y * 100.0) if (np.isfinite(delta) and cust_y not in [0, np.nan]) else np.nan

                        rows.append({
                            "application_id": app_id,
                            "asset_id": asset_id,
                            "asset_type": asset_type,
                            "city": city,
                            "year_ahead": y,
                            "customer_value": cust_y,
                            "ai_value": ai_y,
                            "delta": delta,
                            "delta_pct": pct,
                            "explanation_source": f"Customer CAGR={cust_cagr*100:.1f}% vs AI CAGR={ai_cagr*100:.1f}%; AI model {selected_model or 'production'} (comps={comps_count})"
                        })

                deltas_tbl = pd.DataFrame(rows)

            # Display & export
            # Round for readability
            for c in ["customer_value","ai_value","delta","delta_pct"]:
                if c in deltas_tbl.columns:
                    deltas_tbl[c] = pd.to_numeric(deltas_tbl[c], errors="coerce")

            st.dataframe(deltas_tbl.head(100), use_container_width=True)

            deltas_path = os.path.join(RUNS_DIR, f"valuation_deltas_5y.{_ts}.csv")
            deltas_tbl.to_csv(deltas_path, index=False)
            st.download_button(
                "‚¨áÔ∏è Download 5-Year Deltas (CSV)",
                data=deltas_tbl.to_csv(index=False).encode("utf-8"),
                file_name="valuation_deltas_5y.csv",
                mime="text/csv"
            )


        st.markdown("---")
        # üîí C.5 ‚Äî Legal/Ownership Verification (encumbrances, liens, fraud)
        st.markdown("### **C.5 ‚Äî Legal/Ownership Verification**")

        def _verify_stub(df_in: pd.DataFrame) -> pd.DataFrame:
            df = df_in.copy()
            if "verification_status" not in df.columns:
                df["verification_status"] = "verified"
            if "encumbrance_flag" not in df.columns:
                df["encumbrance_flag"] = False
            if "verified_owner" not in df.columns:
                df["verified_owner"] = np.where(df.get("asset_type","").astype(str).str.lower().str.contains("car"), "DMV Registry", "Land Registry")
            if "notes" not in df.columns:
                df["notes"] = "Registry check passed (stub)"
            return df

        if st.button("üîç Run Legal/Ownership Checks", key="btn_run_verification"):
            base_df = ss.get("asset_ai_df")
            if base_df is None:
                st.warning("Run valuation first.")
            else:
                verified_df = _verify_stub(base_df)
                ss["asset_verified_df"] = verified_df
                ver_path = os.path.join(RUNS_DIR, f"verification_status.{_ts()}.csv")
                verified_df.to_csv(ver_path, index=False)
                st.success(f"Saved verification artifact ‚Üí `{ver_path}`")

                v1, v2 = st.columns(2)
                with v1:
                    try:
                        pct = (verified_df["verification_status"] == "verified").mean()
                        st.metric("Verified %", f"{pct:.0%}")
                    except Exception:
                        st.metric("Verified %", "‚Äî")
                with v2:
                    try:
                        st.metric("Encumbrance Flags", int(pd.to_numeric(verified_df["encumbrance_flag"]).sum()))
                    except Exception:
                        st.metric("Encumbrance Flags", "‚Äî")

                cols_ver = [c for c in [
                    "application_id","asset_id","verified_owner","verification_status","encumbrance_flag","notes"
                ] if c in verified_df.columns]
                st.dataframe(verified_df[cols_ver].head(50), use_container_width=True)

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # üìä Executive Portfolio Dashboard (Spectacular)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        st.divider()
        st.subheader("üìä Executive Portfolio Dashboard")

        df_src = ss.get("asset_ai_df")
        ft = ss.get("asset_first_table")  # loan-centric projection you already built
        if df_src is None or (hasattr(df_src, "empty") and df_src.empty):
            st.info("Run appraisal to populate the dashboard.")
        else:
            df = df_src.copy()

            # ---- Safe numerics
            def _num(series, default=None):
                s = pd.to_numeric(series, errors="coerce")
                if default is not None:
                    s = s.fillna(default)
                return s

            for c in ["ai_adjusted","realizable_value","loan_amount",
                    "valuation_gap_pct","ltv_ai","ltv_cap","confidence",
                    "condition_score","legal_penalty"]:
                if c in df.columns:
                    df[c] = _num(df[c])

            # ---- KPIs row
            k1, k2, k3, k4, k5 = st.columns(5)
            total_ai        = float(df.get("ai_adjusted", pd.Series(dtype=float)).sum()) if "ai_adjusted" in df.columns else 0.0
            total_realiz    = float(df.get("realizable_value", pd.Series(dtype=float)).sum()) if "realizable_value" in df.columns else 0.0
            avg_conf        = float(df.get("confidence", pd.Series(dtype=float)).mean()) if "confidence" in df.columns else 0.0
            ltv_breach_rate = 0.0
            if {"ltv_ai","ltv_cap"}.issubset(df.columns):
                ltv_breach_rate = float((df["ltv_ai"] > df["ltv_cap"]).mean() * 100)
            approved_cnt = int(df.get("decision","").astype(str).str.lower().eq("approved").sum()) if "decision" in df.columns else 0

            k1.metric("AI Gross Value",       f"${total_ai:,.0f}")
            k2.metric("Realizable Value",     f"${total_realiz:,.0f}")
            k3.metric("Avg Confidence",       f"{avg_conf:.1f}%")
            k4.metric("LTV Breach Rate",      f"{ltv_breach_rate:.1f}%")
            k5.metric("Approved Count",       f"{approved_cnt:,}")

            # ---- Row 1: Top-10 Assets & Decision Mix
            r1c1, r1c2 = st.columns([1.2, 1])
            with r1c1:
                value_col = "realizable_value" if "realizable_value" in df.columns else ("ai_adjusted" if "ai_adjusted" in df.columns else None)
                if value_col:
                    df_top = (df.assign(_val=df[value_col])
                                .sort_values("_val", ascending=False)
                                .head(10))
                    fig_top = px.bar(
                        df_top,
                        x="_val", y=df_top.get("asset_id", df_top.index).astype(str),
                        color="asset_type" if "asset_type" in df_top.columns else None,
                        orientation="h",
                        title=f"Top 10 Assets by {value_col.replace('_',' ').title()}",
                        hover_data=[c for c in ["application_id","asset_id","asset_type","city","_val"] if c in df_top.columns]
                    )
                    fig_top.update_layout(template="plotly_dark", height=380, yaxis_title=None, xaxis_title=value_col)
                    st.plotly_chart(fig_top, use_container_width=True)
            with r1c2:
                names_series = (df["decision"].astype(str).str.title()
                                if "decision" in df.columns
                                else np.where(df.get("policy_breaches","").astype(str).str.len().gt(0),
                                            "Has Breach","No Breach"))
                fig_mix = px.pie(df, names=names_series, title="Decision / Breach Mix")
                fig_mix.update_layout(template="plotly_dark", height=380)
                st.plotly_chart(fig_mix, use_container_width=True)

            # ---- Row 2: By Asset Type & City Concentration
            r2c1, r2c2 = st.columns(2)
            with r2c1:
                if "asset_type" in df.columns:
                    df_type = (df
                            .assign(value=df[value_col] if value_col else 0)
                            .groupby("asset_type", dropna=False)["value"]
                            .sum().sort_values(ascending=False).reset_index())
                    fig_type = px.bar(df_type, x="asset_type", y="value",
                                    title="Value by Asset Type",
                                    text_auto=True)
                    fig_type.update_layout(template="plotly_dark", height=360, xaxis_title=None, yaxis_title="Value")
                    st.plotly_chart(fig_type, use_container_width=True)
            with r2c2:
                if "city" in df.columns and value_col:
                    df_city = (df.groupby("city", dropna=False)[value_col]
                                .sum().sort_values(ascending=False)
                                .head(10).reset_index())
                    fig_city = px.pie(df_city, values=value_col, names="city",
                                    title="Top-10 City Concentration")
                    fig_city.update_layout(template="plotly_dark", height=360)
                    st.plotly_chart(fig_city, use_container_width=True)

            # ---- Row 3: LTV vs Cap & Condition√óLegal Heat
            r3c1, r3c2 = st.columns(2)
            with r3c1:
                if {"ltv_ai","ltv_cap"}.issubset(df.columns):
                    fig_sc = px.scatter(
                        df, x="ltv_cap", y="ltv_ai",
                        color="asset_type" if "asset_type" in df.columns else None,
                        hover_data=[c for c in ["application_id","asset_id","asset_type","city","loan_amount"] if c in df.columns],
                        title="LTV (AI) vs LTV Cap"
                    )
                    try:
                        max_cap = float((df["ltv_cap"].max() or 1.2))
                        fig_sc.add_shape(type="line", x0=0, y0=0, x1=max_cap, y1=max_cap, line=dict(dash="dash"))
                    except Exception:
                        pass
                    fig_sc.update_layout(template="plotly_dark", height=360,
                                        xaxis_title="LTV Cap", yaxis_title="LTV (AI)")
                    st.plotly_chart(fig_sc, use_container_width=True)
            with r3c2:
                if {"condition_score","legal_penalty"}.issubset(df.columns):
                    try:
                        cond_bins  = pd.cut(df["condition_score"], bins=[0,0.70,0.85,1.00], labels=["<0.70","0.70‚Äì0.85",">0.85"])
                        legal_bins = pd.cut(df["legal_penalty"],  bins=[0,0.97,0.99,1.00], labels=["<0.97","0.97‚Äì0.99",">=0.99"])
                        heat = (df.assign(cond=cond_bins, legal=legal_bins)
                                .groupby(["cond","legal"]).size().reset_index(name="count"))
                        fig_hm = px.density_heatmap(heat, x="legal", y="cond", z="count",
                                                    title="Condition vs Legal ‚Äî Density")
                        fig_hm.update_layout(template="plotly_dark", height=360)
                        st.plotly_chart(fig_hm, use_container_width=True)
                    except Exception:
                        pass

            # ---- Row 4: City Leaderboard + Per-City Asset List
            st.markdown("### üèôÔ∏è City Leaderboard & Assets")
            if "city" in df.columns:
                value_col = value_col or "ai_adjusted"
                city_sum = (df.groupby("city", dropna=False)[value_col]
                            .sum().sort_values(ascending=False).reset_index()
                            .rename(columns={value_col: "total_value"}))
                left, right = st.columns([1, 2])
                with left:
                    st.dataframe(city_sum, use_container_width=True)
                with right:
                    # show top assets per top city
                    top_cities = city_sum["city"].astype(str).head(5).tolist()
                    for city in top_cities:
                        with st.expander(f"üìç {city} ‚Äî top assets", expanded=False):
                            sub = (df[df["city"].astype(str)==city]
                                .assign(value=df[value_col])
                                .sort_values("value", ascending=False)
                                [[c for c in ["application_id","asset_id","asset_type","value","loan_amount","confidence"] if c in df.columns]]
                                .head(15))
                            st.dataframe(sub, use_container_width=True)


            # ---- Optional Map (if lat/lon present)
            st.markdown("### üó∫Ô∏è Map (optional)")
            st.caption("Visualize asset locations ‚Äî map color and style follow the current UI theme.")

            map_cols = [("lat","lon"), ("latitude","longitude"), ("gps_lat","gps_lon")]
            have_map = False

            for la, lo in map_cols:
                if la in df.columns and lo in df.columns:
                    have_map = True
                    map_df = df[[la, lo] + [
                        c for c in ["asset_id","asset_type","city","ai_adjusted","realizable_value","confidence"]
                        if c in df.columns
                    ]].copy()
                    map_df = map_df.rename(columns={la: "lat", lo: "lon"})
                    map_df = map_df.dropna(subset=["lat", "lon"])

                    if not map_df.empty:
                        try:
                            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                            # Prefer Plotly (bright light / dark dark)
                            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                            import plotly.express as px
                            apply_plotly_mapbox_defaults()
                            # ============================================
                            # ‚úÖ CODEX INSTRUCTION ‚Äî MAP IMPROVEMENTS
                            # ============================================
                            value_source = None
                            for candidate in ["ai_adjusted", "realizable_value", "loan_amount"]:
                                if candidate in map_df.columns:
                                    value_source = candidate
                                    break

                            if value_source:
                                value_metric = map_df[value_source]
                            else:
                                value_metric = pd.Series([0] * len(map_df), index=map_df.index)

                            working_df = map_df.assign(
                                city=map_df.get("city", "Unknown").fillna("Unknown"),
                                value_metric=value_metric,
                            )

                            # Aggregate per city so bubble size reflects asset counts
                            city_bubbles = (
                                working_df.groupby("city", dropna=False)
                                .agg(
                                    lat=("lat", "mean"),
                                    lon=("lon", "mean"),
                                    n_assets=("city", "size"),
                                    max_value=("value_metric", "max"),
                                    min_value=("value_metric", "min"),
                                )
                                .reset_index()
                            )

                            if not city_bubbles.empty:
                                lat_min, lat_max = city_bubbles["lat"].min(), city_bubbles["lat"].max()
                                lon_min, lon_max = city_bubbles["lon"].min(), city_bubbles["lon"].max()

                                def _compute_zoom(lat_range: float, lon_range: float) -> float:
                                    span = max(lat_range, lon_range)
                                    if span <= 0.05:
                                        return 11
                                    if span <= 0.1:
                                        return 10
                                    if span <= 0.5:
                                        return 8
                                    if span <= 1.0:
                                        return 7
                                    if span <= 5.0:
                                        return 6
                                    return 5

                                zoom = _compute_zoom(lat_max - lat_min, lon_max - lon_min)
                                center = {
                                    "lat": float((lat_min + lat_max) / 2.0),
                                    "lon": float((lon_min + lon_max) / 2.0),
                                }

                                map_style = "carto-positron"

                                fig = px.scatter_mapbox(
                                    city_bubbles,
                                    lat="lat",
                                    lon="lon",
                                    size="n_assets",
                                    color="city",
                                    hover_name="city",
                                    hover_data={
                                        "n_assets": True,
                                        "max_value": value_source is not None,
                                        "min_value": value_source is not None,
                                    },
                                    size_max=50,
                                    height=420,
                                    color_discrete_sequence=px.colors.qualitative.Bold,
                                )

                                def fmt_currency(value: float | int | None) -> str:
                                    if value is None or (isinstance(value, float) and np.isnan(value)):
                                        return "n/a"
                                    return f"{value:,.0f}"

                                city_bubbles["bubble_text"] = city_bubbles.apply(
                                    lambda row: (
                                        f"{row['city']} ‚Ä¢ {row['n_assets']} assets"
                                        + (
                                            f"\nTop: {fmt_currency(row['max_value'])} ¬∑ Low: {fmt_currency(row['min_value'])}"
                                            if value_source
                                            else ""
                                        )
                                    ),
                                    axis=1,
                                )

                                fig.update_traces(
                                    text=city_bubbles["bubble_text"],
                                    textposition="top center",
                                )
                                fig.update_layout(
                                    mapbox_style=map_style,
                                    mapbox_center=center,
                                    mapbox_zoom=zoom,
                                    margin=dict(l=0, r=0, t=0, b=0),
                                    paper_bgcolor="rgba(0,0,0,0)",
                                    plot_bgcolor="rgba(0,0,0,0)",
                                )
                                st.plotly_chart(fig, use_container_width=True)
                            else:
                                st.info("‚ÑπÔ∏è Unable to build city bubbles ‚Äî no valid coordinates after grouping.")

                        except Exception as e:
                            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                            # Fallback to pydeck if Plotly unavailable
                            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                            import pydeck as pdk
                            view_state = make_pydeck_view_state(
                                lat=float(map_df["lat"].mean()),
                                lon=float(map_df["lon"].mean()),
                                zoom=8
                            )
                            layer = pdk.Layer(
                                "ScatterplotLayer",
                                data=map_df,
                                get_position='[lon, lat]',
                                get_color='[0, 128, 255, 200]',
                                get_radius=120,
                                pickable=True,
                            )
                            deck = pdk.Deck(
                                map_style=pydeck_map_style(),
                                initial_view_state=view_state,
                                layers=[layer],
                                tooltip={"text": "{asset_id} ¬∑ {asset_type}\n{city}\nAI: {ai_adjusted}\nRealiz: {realizable_value}\nConf: {confidence}"}
                            )
                            st.pydeck_chart(deck)
                    else:
                        st.info("‚ÑπÔ∏è No valid coordinates found to display on the map.")
                    break

            if not have_map:
                st.caption("No lat/lon columns found (lat/lon or latitude/longitude or gps_lat/gps_lon). Map hidden.")


            # ---- Exports of aggregates
            st.markdown("#### üì§ Export dashboard aggregates")
            exports = {}
            if "asset_type" in df.columns:
                exports["by_asset_type.csv"] = df_type.to_csv(index=False) if 'df_type' in locals() else ""
            if "city" in df.columns and value_col:
                exports["by_city_top10.csv"] = df_city.to_csv(index=False) if 'df_city' in locals() else ""
            if 'df_top' in locals():
                exports["top_assets.csv"] = df_top.drop(columns=["_val"], errors="ignore").to_csv(index=False)

            ex1, ex2, ex3 = st.columns(3)
            for i, (fname, data) in enumerate(exports.items()):
                if not data:
                    continue
                col = [ex1, ex2, ex3][i % 3]
                with col:
                    st.download_button(f"‚¨áÔ∏è {fname}", data=data.encode("utf-8"), file_name=fname, mime="text/csv")
                    
            
            # ‚úÖ NEW: Export full AI decision file for Stage E (Human Review)
            st.markdown("### üßæ Export AI Decision for Human Review (Stage E)")

            if 'ai_df' in locals() and isinstance(ai_df, pd.DataFrame) and not ai_df.empty:
                ai_export_name = f"ai_decision_stageC_{datetime.now().strftime('%Y%m%d-%H%M%S')}.csv"
                ai_csv_data = ai_df.to_csv(index=False, encoding="utf-8-sig")

                st.download_button(
                    "‚¨áÔ∏è Export AI Decisions (send to Stage E)",
                    data=ai_csv_data,
                    file_name=ai_export_name,
                    mime="text/csv",
                    key="dl_ai_stagec_export"
                )
            else:
                st.info("AI table (ai_df) not available ‚Äî run valuation first.")

            # ========================
            # Stage C ‚Äî AI Appraisal & Valuation
            # ========================
            # Now send AI results to Stage E for review
            if st.button("üí¨ Review in Stage E"):
                # Store AI appraisal results in session_state for Stage E
                st.session_state["ai_review_df"] = ai_df  # ai_df should be the AI results dataframe from the current stage
                st.session_state["current_stage"] = "human_review"
                st.success("AI results sent to Stage E for human review!")
                st.rerun()

            
            # # ========================
            # # Stage C ‚Äî AI Appraisal & Valuation
            # # ========================
            # # Now send AI results to Stage E for review
            # if st.button("üí¨ Review in Stage E"):
            #     # Store AI appraisal results in session_state for Stage E
            #     st.session_state["ai_review_df"] = ai_df  # ai_df should be the AI results dataframe from the current stage
            #     st.session_state["current_stage"] = "human_review"
            #     st.success("AI results sent to Stage E for human review!")
            #     st.rerun()  # Trigger a page refresh to go to the next stage




# ========== 4) POLICY & DECISION (Stage D: steps 6‚Äì7) ==========
with tabD:
    st.subheader("üßÆ Stage 4 ‚Äî Policy & Decision (D.6 / D.7)")

    import os, json
    import numpy as np
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)
    def _ts(): return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    # ---- Input table: prefer verified ‚Üí else AI valuation (safe selector) ----
    base_df = first_nonempty_df(ss.get("asset_verified_df"), ss.get("asset_ai_df"))
    if not is_nonempty_df(base_df):
        st.warning("Run Stage C first (valuation, and optionally verification).")
        st.stop()

    st.caption("Input: valuation + (optional) verification outputs.")

    # ‚îÄ‚îÄ D.6 and D.7 continue here (your existing haircuts / caps / breaches / decision code) ‚îÄ‚îÄ

    # -------- D.6 ‚Äî Policy & Haircuts ‚Üí realizable_value --------
    st.markdown("### **D.6 ‚Äî Policy & Haircuts**")
    p1, p2, p3 = st.columns(3)
    with p1:
        base_haircut_pct = st.slider("Base haircut (%)", 0, 60, 10, 1, key="policy_base_haircut")
    with p2:
        condition_weight = st.slider("Condition multiplier min", 0.50, 1.00, 0.80, 0.01, key="policy_cond_min")
    with p3:
        legal_weight = st.slider("Legal multiplier min", 0.50, 1.00, 0.95, 0.01, key="policy_legal_min")

    if st.button("Apply Haircuts", key="btn_apply_haircuts"):
        df = base_df.copy()

        # Ensure necessary inputs exist
        for col, default in [("ai_adjusted", np.nan), ("condition_score", 0.9), ("legal_penalty", 1.0)]:
            if col not in df.columns:
                df[col] = default

        ai_adj = pd.to_numeric(df["ai_adjusted"], errors="coerce")
        cond   = pd.to_numeric(df["condition_score"], errors="coerce").clip(condition_weight, 1.0)
        legal  = pd.to_numeric(df["legal_penalty"],  errors="coerce").clip(legal_weight, 1.0)
        base_cut = (1.0 - float(base_haircut_pct) / 100.0)

        df["realizable_value"] = ai_adj * cond * legal * base_cut

        # Persist artifact
        policy_path = os.path.join(RUNS_DIR, f"policy_haircuts.{_ts()}.csv")
        df.to_csv(policy_path, index=False)

        # ‚úÖ Save Stage D policy results for Stage H
        try:
            # Save into BOTH namespaces safely
            st.session_state["asset_policy_df"] = df.copy()
            ss["asset_policy_df"] = df.copy()

            st.info("‚úÖ Stage D policy results stored for Stage H.")
        except Exception as e:
            st.warning(f"Could not store Stage D output: {e}")

        st.success(f"Saved: `{policy_path}`")

        # KPIs
        k1, k2, k3 = st.columns(3)
        with k1:
            st.metric(
                "Avg Realizable Value",
                f"{pd.to_numeric(df['realizable_value'], errors='coerce').mean():,.0f}"
            )
        with k2:
            st.metric("Rows", len(df))
        with k3:
            st.metric("Base Haircut", f"{base_haircut_pct}%")

        st.dataframe(df.head(30), use_container_width=True)

        
        # # ‚úÖ Save Stage D policy results for Stage H
        # try:
        #     st.session_state["asset_policy_df"] = df.copy()
        #     ss["asset_policy_df"] = df.copy()
        #     st.info("‚úÖ Stage D policy results stored for Stage H.")
        # except Exception as e:
        #     st.warning(f"Could not store Stage D output: {e}")

        # st.success(f"Saved: `{policy_path}`")

        # # KPIs
        # k1, k2, k3 = st.columns(3)
        # with k1:
        #     st.metric("Avg Realizable Value", f"{pd.to_numeric(df['realizable_value'], errors='coerce').mean():,.0f}")
        # with k2:
        #     st.metric("Rows", len(df))
        # with k3:
        #     st.metric("Base Haircut", f"{base_haircut_pct}%")

        # st.dataframe(df.head(30), use_container_width=True)

    # # -------- D.6 ‚Äî Policy & Haircuts ‚Üí realizable_value --------
    # st.markdown("### **D.6 ‚Äî Policy & Haircuts**")
    # p1, p2, p3 = st.columns(3)
    # with p1:
    #     base_haircut_pct = st.slider("Base haircut (%)", 0, 60, 10, 1, key="policy_base_haircut")
    # with p2:
    #     condition_weight = st.slider("Condition multiplier min", 0.50, 1.00, 0.80, 0.01, key="policy_cond_min")
    # with p3:
    #     legal_weight = st.slider("Legal multiplier min", 0.50, 1.00, 0.95, 0.01, key="policy_legal_min")

    # if st.button("Apply Haircuts", key="btn_apply_haircuts"):
    #     df = base_df.copy()

    #     # Ensure necessary inputs exist
    #     for col, default in [("ai_adjusted", np.nan), ("condition_score", 0.9), ("legal_penalty", 1.0)]:
    #         if col not in df.columns:
    #             df[col] = default

    #     ai_adj = pd.to_numeric(df["ai_adjusted"], errors="coerce")
    #     cond   = pd.to_numeric(df["condition_score"], errors="coerce").clip(condition_weight, 1.0)
    #     legal  = pd.to_numeric(df["legal_penalty"],  errors="coerce").clip(legal_weight, 1.0)
    #     base_cut = (1.0 - float(base_haircut_pct) / 100.0)

    #     df["realizable_value"] = ai_adj * cond * legal * base_cut

    #     # Persist policy_haircuts artifact
    #     policy_path = os.path.join(RUNS_DIR, f"policy_haircuts.{_ts()}.csv")
    #     df.to_csv(policy_path, index=False)
        
    #     # ‚úÖ Save Stage D policy for Stage H
    #     st.session_state["asset_policy_df"] = policy_df.copy()

    #     ss["asset_policy_df"] = df
    #     st.success(f"Saved: `{policy_path}`")

    #     # KPIs
    #     k1, k2, k3 = st.columns(3)
    #     with k1:
    #         st.metric("Avg Realizable Value", f"{pd.to_numeric(df['realizable_value'], errors='coerce').mean():,.0f}")
    #     with k2:
    #         st.metric("Rows", len(df))
    #     with k3:
    #         st.metric("Base Haircut", f"{base_haircut_pct}%")

    #     st.dataframe(df.head(30), use_container_width=True)

    # st.markdown("---")

    # -------- D.7 ‚Äî Risk / Decision --------
    st.markdown("### **D.7 ‚Äî Risk / Decision**")

    if ss.get("asset_policy_df") is None:
        st.info("Run D.6 first to compute `realizable_value`.")
    else:
        df = ss["asset_policy_df"].copy()

        # Inputs
        r1, r2, r3 = st.columns(3)
        with r1:
            loan_amount_default = float(pd.to_numeric(df.get("loan_amount", pd.Series([60000])).median()))
            loan_amount = st.number_input("Loan amount (default=median)", value=loan_amount_default, min_value=0.0, step=1000.0, key="risk_loan_amt")
        with r2:
            ltv_mode = st.selectbox("LTV cap mode", ["Fixed cap", "Per asset_type"], index=0, key="risk_ltv_mode")
        with r3:
            fixed_ltv_cap = st.slider("Fixed LTV cap (√ó)", 0.10, 2.00, 0.80, 0.05, key="risk_ltv_cap_fixed")

        # Per-type caps if requested
        type_caps = {}
        if ltv_mode == "Per asset_type":
            types = sorted(list(map(str, (df.get("asset_type") or pd.Series(["Asset"])).dropna().unique())))[0:10]
            st.caption("Tune LTV caps per asset_type")
            grid = st.columns(4 if len(types) > 3 else max(1, len(types)))
            for i, t in enumerate(types):
                with grid[i % len(grid)]:
                    type_caps[t] = st.number_input(f"{t} cap √ó", 0.10, 2.00, 0.80, 0.05, key=f"cap_{t}")

        # Thresholds for decisioning
        t1, t2, t3 = st.columns(3)
        with t1:
            min_conf = st.slider("Min confidence (%)", 0, 100, 70, 1, key="risk_min_conf")
        with t2:
            min_cond = st.slider("Min condition_score", 0.60, 1.00, 0.75, 0.01, key="risk_min_cond")
        with t3:
            min_legal = st.slider("Min legal_penalty", 0.80, 1.00, 0.97, 0.01, key="risk_min_legal")

        if st.button("Compute Decision", key="btn_compute_decision"):
            # Compute ltv_ai
            df["ltv_ai"] = pd.to_numeric(loan_amount, errors="coerce") / pd.to_numeric(df.get("ai_adjusted", np.nan), errors="coerce")

            # ltv_cap
            if ltv_mode == "Fixed cap":
                df["ltv_cap"] = float(fixed_ltv_cap)
            else:
                atypes = df.get("asset_type").astype(str) if "asset_type" in df.columns else pd.Series(["Asset"] * len(df))
                df["ltv_cap"] = atypes.map(lambda t: float(type_caps.get(t, fixed_ltv_cap)))

            # Breaches
            conf = pd.to_numeric(df.get("confidence", 100.0), errors="coerce")
            cond = pd.to_numeric(df.get("condition_score", 1.0), errors="coerce")
            legal= pd.to_numeric(df.get("legal_penalty", 1.0),  errors="coerce")
            ltv  = pd.to_numeric(df["ltv_ai"], errors="coerce")
            lcap = pd.to_numeric(df["ltv_cap"], errors="coerce")

            breaches = []
            for i in range(len(df)):
                b = []
                if pd.notna(conf.iat[i]) and conf.iat[i] < min_conf:
                    b.append(f"confidence<{min_conf}%")
                if pd.notna(cond.iat[i]) and cond.iat[i] < min_cond:
                    b.append(f"condition<{min_cond:.2f}")
                if pd.notna(legal.iat[i]) and legal.iat[i] < min_legal:
                    b.append(f"legal<{min_legal:.2f}")
                if pd.notna(ltv.iat[i]) and pd.notna(lcap.iat[i]) and ltv.iat[i] > lcap.iat[i]:
                    b.append("ltv>cap")
                breaches.append(", ".join(b))
            df["policy_breaches"] = breaches

            # Decision rule
            # - reject if LTV>cap OR confidence << min_conf (<= min_conf-10)
            # - review if any breach but not hard reject
            # - approve otherwise
            hard_reject = (
                (ltv > lcap) |
                (pd.to_numeric(conf, errors="coerce") <= (min_conf - 10))
            )
            any_breach = df["policy_breaches"].str.len().gt(0)

            df["decision"] = np.select(
                [
                    hard_reject,
                    any_breach
                ],
                ["reject", "review"],
                default="approve"
            )

            # Persist risk_decision artifact
            risk_path = os.path.join(RUNS_DIR, f"risk_decision.{_ts()}.csv")
            df.to_csv(risk_path, index=False)
            ss["asset_decision_df"] = df
            st.success(f"Saved: `{risk_path}`")

            # KPIs + Table
            k1, k2, k3 = st.columns(3)
            with k1:
                st.metric("Avg LTV (AI)", f"{pd.to_numeric(df['ltv_ai'], errors='coerce').mean():.2f}")
            with k2:
                try:
                    st.metric("Breach Rate", f"{(df['policy_breaches'].str.len().gt(0)).mean():.0%}")
                except Exception:
                    st.metric("Breach Rate", "‚Äî")
            with k3:
                mix = df["decision"].value_counts(dropna=False)
                st.metric("Approve/Review/Reject", f"{int(mix.get('approve',0))}/{int(mix.get('review',0))}/{int(mix.get('reject',0))}")

            cols_view = [c for c in [
                "application_id","asset_id","asset_type","city",
                "ai_adjusted","realizable_value",
                "loan_amount","ltv_ai","ltv_cap",
                "confidence","condition_score","legal_penalty",
                "policy_breaches","decision"
            ] if c in df.columns]
            st.dataframe(df[cols_view].head(50), use_container_width=True)

            st.download_button(
                "‚¨áÔ∏è Download Policy+Decision (CSV)",
                data=df.to_csv(index=False).encode("utf-8"),
                file_name="risk_decision.csv",
                mime="text/csv"
            )



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# E ‚Äî HUMAN REVIEW & FEEDBACK DASHBOARD
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from datetime import datetime, timezone  # ensure available inside this block
import os, glob, json
import numpy as np
import pandas as pd
import plotly.graph_objects as go

with tabE:
    st.subheader("üßë‚Äç‚öñÔ∏è Stage E ‚Äî Human Review & Feedback")
    st.caption("Compare AI-estimated collateral values against business metrics, adjust valuations, and record justification for retraining.")

    
    # Workspace
    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Stage C loader controls (Auto-load + picker)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _find_stage_c_candidates():
        pats = ["valuation_ai*.csv", "valuation_ai*.parquet"]
        files = []
        for pat in pats:
            files.extend(glob.glob(os.path.join(RUNS_DIR, pat)))
        return sorted(files, key=os.path.getmtime, reverse=True)

    if "stage_c_selected_path" not in st.session_state:
        st.session_state["stage_c_selected_path"] = None

    ctrl1, ctrl2, ctrl3 = st.columns([1.2, 1, 2.8])
    with ctrl1:
        btn_autoload = st.button("üîÑ Auto-load latest Stage C", use_container_width=True)
    with ctrl2:
        btn_refresh = st.button("üîÅ Refresh list", use_container_width=True)
    with ctrl3:
        st.caption("Looks for `valuation_ai*.csv|.parquet` under `./.tmp_runs`")

    if btn_refresh:
        pass  # triggers rerun ‚Üí list will refresh

    candidates = _find_stage_c_candidates()
    if not candidates:
        st.warning("‚ö†Ô∏è No AI appraisal results found. Please complete Stage C first.")
        st.stop()

    # Pick newest on first load or when autoload pressed
    if btn_autoload:
        st.session_state["stage_c_selected_path"] = candidates[0]
    elif not st.session_state["stage_c_selected_path"]:
        st.session_state["stage_c_selected_path"] = candidates[0]
    # Ensure the selected one still exists
    if st.session_state["stage_c_selected_path"] not in candidates:
        st.session_state["stage_c_selected_path"] = candidates[0]

    # Human-friendly label
    def _fmt(p):
        ts = datetime.fromtimestamp(os.path.getmtime(p)).strftime("%Y-%m-%d %H:%M:%S")
        return f"{os.path.basename(p)}  ‚Ä¢  {ts}"

    current_idx = candidates.index(st.session_state["stage_c_selected_path"])
    picked = st.selectbox(
        "Stage C output to review",
        options=candidates,
        index=current_idx,
        format_func=_fmt,
    )
    st.session_state["stage_c_selected_path"] = picked

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‚úÖ NEW: Direct Upload of Stage C Export (CSV)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üì§ Or Upload Stage C Export")
    uploaded_c = st.file_uploader(
        "Upload a Stage C file (valuation_ai*.csv)",
        type=["csv"],
        key="stage_c_upload"
    )

    if uploaded_c is not None:
        try:
            df_ai = pd.read_csv(uploaded_c)
            #st.success(f"‚úÖ Imported uploaded file ({len[df_ai)} rows).")
            st.success(f"‚úÖ Imported uploaded file ({len(df_ai)} rows).")


            temp_path = os.path.join(RUNS_DIR, f"uploaded_stage_c_{datetime.now().timestamp()}.csv")
            df_ai.to_csv(temp_path, index=False, encoding="utf-8-sig")

            st.session_state["stage_c_selected_path"] = temp_path
            st.session_state["df_ai_current"] = df_ai.copy()

            st.rerun()

        except Exception as e:
            st.error(f"Upload failed: {e}")
    

    # Load the selected Stage C table ‚Üí df_ai
    ai_path = st.session_state["stage_c_selected_path"]
    try:
        if ai_path.lower().endswith(".parquet"):
            df_ai = pd.read_parquet(ai_path)
        else:
            df_ai = pd.read_csv(ai_path)
        st.success(f"‚úÖ Loaded Stage C: {os.path.basename(ai_path)}  ({len(df_ai)} rows √ó {df_ai.shape[1]} cols)")
    except Exception as e:
        st.error(f"Failed to read `{ai_path}`: {e}")
        st.stop()

    # Ensure join keys exist to avoid editor KeyErrors later
    for col in ["application_id", "asset_id", "asset_type", "city"]:
        if col not in df_ai.columns:
            df_ai[col] = None

    # Ensure human_value / justification columns for adjustments
    if "human_value" not in df_ai.columns:
        df_ai["human_value"] = pd.to_numeric(df_ai["fmv"], errors="coerce") if "fmv" in df_ai.columns else np.nan
    if "justification" not in df_ai.columns:
        df_ai["justification"] = ""

    

    # ‚îÄ‚îÄ Market Projections (safe)
    st.markdown("### üìà Market Projections")
    horizon = st.select_slider("Projection Horizon (years)", options=[3, 5, 10], value=5)
    growth = st.slider("Expected Market Growth (%)", -10, 25, 4) / 100

    df_proj = df_ai.copy()
    if "fmv" in df_proj.columns:
        fmv_num = pd.to_numeric(df_proj["fmv"], errors="coerce")
        df_proj[f"fmv_proj_{horizon}y"] = (fmv_num * ((1 + growth) ** horizon)).round(0)
        st.line_chart(df_proj[["fmv", f"fmv_proj_{horizon}y"]])
    else:
        st.info("FMV column not found; projection chart will appear after you run Stage C.")

    # ‚úÖ Helper: return the first column present in dataframe
    def _first_present(df, candidates):
        for c in candidates:
            if c in df.columns:
                return c
        return None


    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‚úÖ Human Adjustment Table (LIVE + REFRESH SAFE)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### ‚úèÔ∏è Human Adjustments & Justification")

    


    # Ensure editable columns exist
    if "human_value" not in df_ai.columns:
        df_ai["human_value"] = pd.to_numeric(df_ai["fmv"], errors="coerce") if "fmv" in df_ai.columns else np.nan
    if "justification" not in df_ai.columns:
        df_ai["justification"] = ""

     # Editable columns for the reviewer
    base_editable = ["application_id", "asset_id", "asset_type", "city", "fmv", "ai_adjusted", "confidence", "loan_amount", "human_value", "justification"]
    editable_cols = [c for c in base_editable if c in df_ai.columns]  # filter to present
    if not editable_cols:
        editable_cols = df_ai.columns.tolist()  # last resort: allow full frame
    
    # Display the editable table for human review

    edited = st.data_editor(df_ai[editable_cols], num_rows="dynamic", use_container_width=True)

    # ‚îÄ‚îÄ Agreement / Deviation Gauge + Mismatch list
    st.markdown("### üéØ Human vs AI Agreement / Deviation")

    # Resolve decision columns if present
    def _first_present(df, candidates):
        return next((c for c in candidates if c in df.columns), None)

    ai_dec_col = _first_present(edited, ["ai_decision", "ai_label", "ai_outcome", "decision_ai"])
    human_dec_col = _first_present(edited, ["human_decision", "human_label", "final_decision", "decision_human"])

    if ai_dec_col and human_dec_col:
        # Agreement gauge (%)
        a = edited[ai_dec_col].astype(str).str.strip().str.lower()
        h = edited[human_dec_col].astype(str).str.strip().str.lower()
        matches = (a == h)
        agree_pct = float(matches.mean() * 100.0) if len(matches) else 0.0

        fig = go.Figure(go.Indicator(
            mode="gauge+number",
            value=round(agree_pct, 2),
            number={'suffix': '%'},
            gauge={
                'axis': {'range': [0, 100]},
                'bar': {'thickness': 0.35},
                'steps': [
                    {'range': [0, 50], 'color': '#fee2e2'},
                    {'range': [50, 80], 'color': '#fef9c3'},
                    {'range': [80, 100], 'color': '#dcfce7'},
                ],
                'threshold': {'line': {'color': '#2563eb', 'width': 4}, 'thickness': 0.9, 'value': round(agree_pct, 2)}
            },
            title={'text': "AI ‚Üî Human Agreement"}
        ))
        st.plotly_chart(fig, use_container_width=True)

        # Mismatch table (if any)
        mis_df = edited.loc[~matches].copy()
        key_cols = [c for c in ["application_id", "asset_id", "asset_type", "city"] if c in edited.columns]
        value_ai = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
        value_hu = _first_present(edited, ["human_value", "reviewed_value", "final_value"])

        if not mis_df.empty:
            show_cols = key_cols + [c for c in [ai_dec_col, human_dec_col, value_ai, value_hu, "justification"] if c]
            show_cols = [c for c in show_cols if c in mis_df.columns]
            st.markdown("#### üîé Mismatches ‚Äî what did humans change?")
            st.dataframe(mis_df[show_cols].head(300), use_container_width=True, hide_index=True)
        else:
            st.success("üéâ Perfect agreement ‚Äî no mismatches.")
    else:
        # Fall back to deviation score if decisions are not present
        if all(c in edited.columns for c in ("human_value", "fmv")):
            hv = pd.to_numeric(edited["human_value"], errors="coerce")
            fmv = pd.to_numeric(edited["fmv"], errors="coerce").replace(0, np.nan)
            deviation = (hv - fmv).abs() / fmv
            score = max(0.0, 100.0 - float(np.nanmean(deviation) * 200.0)) if len(deviation) else 0.0

            fig = go.Figure(go.Indicator(
                mode="gauge+number",
                value=round(score, 1),
                number={'suffix': ' / 100'},
                gauge={
                    'axis': {'range': [0, 100]},
                    'bar': {'thickness': 0.35},
                    'steps': [
                        {'range': [0, 50], 'color': '#fee2e2'},
                        {'range': [50, 80], 'color': '#fef9c3'},
                        {'range': [80, 100], 'color': '#dcfce7'},
                    ],
                    'threshold': {'line': {'color': '#2563eb', 'width': 4}, 'thickness': 0.9, 'value': round(score, 1)}
                },
                title={'text': "Alignment Score (by value deviation)"}
            ))
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Provide decision columns (ai_decision / human_decision) for agreement gauge, or both FMV and human_value for deviation.")

    
        # ‚îÄ‚îÄ Human Changes Only (colored)
        st.markdown("### üñçÔ∏è Human Changes Only (colored)")

        # Reuse helper and edited df from above
        value_ai = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
        value_hu = _first_present(edited, ["human_value", "reviewed_value", "final_value"])
        ai_dec_col = _first_present(edited, ["ai_decision", "ai_label", "ai_outcome", "decision_ai"])
        human_dec_col = _first_present(edited, ["human_decision", "human_label", "final_decision", "decision_human"])

        if value_ai and value_hu:
            ai_vals = pd.to_numeric(edited[value_ai], errors="coerce")
            hu_vals = pd.to_numeric(edited[value_hu], errors="coerce")

            # decisions -> Series aligned to edited.index
            if ai_dec_col and human_dec_col:
                a = edited[ai_dec_col].astype(str).str.strip().str.lower()
                h = edited[human_dec_col].astype(str).str.strip().str.lower()
                dec_changed = (a != h)  # Series
            else:
                dec_changed = pd.Series(False, index=edited.index)

            # justification -> Series aligned
            just_present = edited.get("justification", pd.Series("", index=edited.index)) \
                                .astype(str).str.strip().ne("")

            # treat tiny diffs as equal
            rel_tol = 1e-9
            val_changed = (ai_vals.fillna(np.nan) - hu_vals.fillna(np.nan)).abs() > (
                (ai_vals.abs() + hu_vals.abs()).fillna(0) * rel_tol
            )

            changed_mask = val_changed | dec_changed | just_present
            diff_df = edited.loc[changed_mask].copy()

            if diff_df.empty:
                st.success("üéâ No human changes detected.")
            else:
                # compute deltas on the FILTERED subset ONLY
                ai_sub = ai_vals.reindex(diff_df.index)
                hu_sub = hu_vals.reindex(diff_df.index)

                diff_df["Œî_value"] = (hu_sub - ai_sub)
                base = ai_sub.replace(0, np.nan)
                diff_df["Œî_%"] = ((diff_df["Œî_value"] / base) * 100.0).round(2)

                key_cols = [c for c in ["application_id", "asset_id", "asset_type", "city"] if c in diff_df.columns]
                show_cols = key_cols + [c for c in [value_ai, value_hu, "Œî_value", "Œî_%", ai_dec_col, human_dec_col, "justification"] if c in diff_df.columns]
                show_df = diff_df[show_cols].copy()

                def _color_row(row):
                    styles = [""] * len(row.index)

                    def _idx(colname):
                        try:
                            return show_df.columns.get_loc(colname)
                        except Exception:
                            return None

                    idx_ai = _idx(value_ai)
                    idx_hu = _idx(value_hu)
                    idx_dv = _idx("Œî_value")
                    idx_dp = _idx("Œî_%")

                    # Value changes
                    try:
                        ai_v = float(row.get(value_ai, np.nan))
                        hu_v = float(row.get(value_hu, np.nan))
                    except Exception:
                        ai_v, hu_v = np.nan, np.nan

                    if pd.notna(ai_v) and pd.notna(hu_v):
                        if hu_v > ai_v:  # green for up
                            for i in [idx_hu, idx_dv, idx_dp]:
                                if i is not None:
                                    styles[i] = "background-color:#dcfce7; color:#065f46; font-weight:600;"
                            if idx_ai is not None:
                                styles[idx_ai] = "background-color:#ecfdf5; color:#064e3b;"
                        elif hu_v < ai_v:  # red for down
                            for i in [idx_hu, idx_dv, idx_dp]:
                                if i is not None:
                                    styles[i] = "background-color:#fee2e2; color:#7f1d1d; font-weight:600;"
                            if idx_ai is not None:
                                styles[idx_ai] = "background-color:#fef2f2; color:#7f1d1d;"

                    # Decision changes ‚Üí amber
                    if ai_dec_col in show_df.columns and human_dec_col in show_df.columns:
                        ai_d = str(row.get(ai_dec_col, "")).strip().lower()
                        hu_d = str(row.get(human_dec_col, "")).strip().lower()
                        if ai_d != "" and hu_d != "" and ai_d != hu_d:
                            for colname in [ai_dec_col, human_dec_col]:
                                j = _idx(colname)
                                if j is not None:
                                    styles[j] = "background-color:#fef9c3; color:#7c2d12; font-weight:600;"

                    # Justification present ‚Üí blue
                    if "justification" in show_df.columns:
                        just = str(row.get("justification", "")).strip()
                        if just:
                            j = _idx("justification")
                            if j is not None:
                                styles[j] = "background-color:#e0f2fe; color:#0c4a6e;"

                    return styles

                styled = show_df.style.apply(_color_row, axis=1) \
                                    .format({value_ai: "{:,.0f}", value_hu: "{:,.0f}", "Œî_value": "{:,.0f}", "Œî_%": "{:.2f}%"})
                st.dataframe(styled, use_container_width=True, hide_index=True)
        else:
            st.info("To show the colorful Human-Changes table, ensure value columns exist (e.g., ai_adjusted/fmv and human_value).")

        

    
    # ‚îÄ‚îÄ Export for Retraining
    st.markdown("### üíæ Save & Export for Training")
    # Lightweight export view for training: keep keys + AI/Human value/decisions if present
    train_cols_base = ["application_id", "asset_id", "asset_type", "city"]
    ai_val_col = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
    hu_val_col = _first_present(edited, ["human_value", "reviewed_value", "final_value"])
    keep_cols = [c for c in train_cols_base if c in edited.columns] + \
                [c for c in [ai_dec_col, human_dec_col, ai_val_col, hu_val_col, "confidence", "loan_amount", "justification"] if c in edited.columns]
    export_df = edited[keep_cols].copy() if keep_cols else edited.copy()

    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
    out_path = os.path.join(RUNS_DIR, f"reviewed_appraisal.{ts}.csv")

    cE1, cE2 = st.columns([1.2, 1])
    with cE1:
        st.text_input("Will save to (server path)", out_path, label_visibility="collapsed")
    with cE2:
        st.download_button(
            "‚¨áÔ∏è Download Human vs AI CSV",
            export_df.to_csv(index=False).encode("utf-8-sig"),
            file_name=os.path.basename(out_path),
            mime="text/csv",
            key="dl_reviewed_appraisal_stageE"
        )

    if st.button("üíæ Save Human Feedback (server)", key="btn_save_feedback"):
        try:
            export_df.to_csv(out_path, index=False, encoding="utf-8-sig")
            st.success(f"‚úÖ Saved human-reviewed data ‚Üí `{out_path}`")
        except Exception as e:
            st.error(f"Save failed: {e}")


# # ============================================================
# # ‚úÖ STAGE F FOOTER FUNCTION ‚Äî must be defined BEFORE Stage F
# # ============================================================

def render_stage_f_footer(
    new_m, prod_m, RUNS_DIR, model_path, report,
    df_train=None, yte=None, y_pred_new=None
):
    import streamlit as st
    import pandas as pd
    import numpy as np
    import os, json, glob, shutil, zipfile
    from datetime import datetime, timezone
    import plotly.express as px

    st.markdown("## üß≠ Executive Model Evaluation Dashboard (Stage F)")

    # -----------------------------------------
    # ‚úÖ Compute deltas
    # -----------------------------------------
    if prod_m:
        delta_mae = (prod_m["MAE"] - new_m["MAE"]) / prod_m["MAE"] * 100
        delta_rmse = (prod_m["RMSE"] - new_m["RMSE"]) / prod_m["RMSE"] * 100
        delta_mape = (prod_m["MAPE%"] - new_m["MAPE%"]) / prod_m["MAPE%"] * 100
        delta_r2  = (new_m["R2"] - prod_m["R2"]) * 100

        improved = {
            "MAE": delta_mae > 0,
            "RMSE": delta_rmse > 0,
            "MAPE%": delta_mape > 0,
            "R2": delta_r2 > 0
        }

        # Main headline message
        headline = f"‚úÖ The new model outperforms the production model by **{delta_mae:+.1f}% MAE** and **{delta_r2:+.2f} R¬≤ points**."
        headline_color = "#D1FAE5"  # greenish
        reward_phrase = "‚úî This is a strong improvement and beneficial for production use."
    else:
        headline = "üü¢ First model trained ‚Äî this will become the initial production baseline."
        headline_color = "#DBEAFE"  # blueish
        reward_phrase = "‚úî You can safely promote this model."

    # -----------------------------------------
    # ‚úÖ WHAT ‚Äî Big one-sentence discovery
    # -----------------------------------------
    st.markdown(
        f"""
        <div style="
            padding: 18px;
            border-radius: 12px;
            background-color: {headline_color};
            font-size: 1.3rem;
            font-weight: 600;
        ">
        {headline}
        </div>
        """,
        unsafe_allow_html=True
    )

    # -----------------------------------------
    # ‚úÖ SO WHAT ‚Äî Why does this matter?
    # -----------------------------------------
    st.markdown("### üßê SO WHAT ‚Äî Why does this matter?")
    if prod_m:
        st.write(
            f"""
            The new model shows measurable improvements across key financial and ML metrics:

            - **MAE** (Average absolute error) improved by **{delta_mae:+.1f}%**  
            - **RMSE** (Hard penalties on large mismatches) improved by **{delta_rmse:+.1f}%**  
            - **MAPE** (Percentage error relative to asset value) improved by **{delta_mape:+.1f}%**  
            - **R¬≤** (How well the model explains variance) improved by **{delta_r2:+.2f} points**  

            These metrics together mean:
            - ‚úÖ More accurate valuation predictions  
            - ‚úÖ Smaller high-error outliers  
            - ‚úÖ Better stability with fewer ‚Äúshocks‚Äù  
            - ‚úÖ Higher confidence for underwriting, credit, and collateral decisions  
            """
        )
    else:
        st.info(
            """
            Since there is **no existing production model**, this trained model becomes 
            the best available baseline for your valuation pipeline.
            """
        )

    # -----------------------------------------
    # ‚úÖ KEY COMPARISON TABLE
    # -----------------------------------------
    st.markdown("### üìä Metric Comparison (New vs Production)")

    if prod_m:
        df_cmp = pd.DataFrame([
            ["MAE",   f"{new_m['MAE']:,.0f}",   f"{prod_m['MAE']:,.0f}",   f"{delta_mae:+.1f}%",  "Lower is better"],
            ["RMSE",  f"{new_m['RMSE']:,.0f}",  f"{prod_m['RMSE']:,.0f}",  f"{delta_rmse:+.1f}%", "Penalizes large errors"],
            ["MAPE%", f"{new_m['MAPE%']:.2f}%", f"{prod_m['MAPE%']:.2f}%", f"{delta_mape:+.1f}%", "Percent accuracy"],
            ["R¬≤",    f"{new_m['R2']:.3f}",     f"{prod_m['R2']:.3f}",     f"{delta_r2:+.2f}",    "Explained variance"],
        ], columns=["Metric", "New Model", "Production", "Œî (Change)", "Meaning"])
    else:
        df_cmp = pd.DataFrame([
            ["MAE",   f"{new_m['MAE']:,.0f}",   "‚Äî",  "‚Äî", "Lower is better"],
            ["RMSE",  f"{new_m['RMSE']:,.0f}",  "‚Äî",  "‚Äî", "Penalizes large errors"],
            ["MAPE%", f"{new_m['MAPE%']:.2f}%", "‚Äî",  "‚Äî", "Percent accuracy"],
            ["R¬≤",    f"{new_m['R2']:.3f}",     "‚Äî",  "‚Äî", "Explained variance"],
        ], columns=["Metric", "New Model", "Production", "Œî (Change)", "Meaning"])

    st.table(df_cmp)

    # -----------------------------------------
    # ‚úÖ NOW WHAT ‚Äî Recommended Action
    # -----------------------------------------
    st.markdown("### üöÄ NOW WHAT ‚Äî Recommended Next Action")

    if not prod_m or (delta_mae > 0 and delta_r2 > 0):
        st.success(
            f"""
            ### ‚úÖ Recommendation: **Promote the new model to production.**

            {reward_phrase}

            #### Why?
            - It reduces valuation errors.
            - It improves consistency and confidence scores.
            - It captures market variance better (higher R¬≤).
            - It reduces underwriting risk.
            - It generates more stable predictions for credit, risk & collateral workflows.
            """
        )
        promote_ready = True
    else:
        st.warning(
            f"""
            ### ‚ö†Ô∏è Recommendation: **Do NOT promote yet.**

            Some metrics degrade when compared to production.

            #### Before promoting:
            - Tune hyperparameters  
            - Add more diverse training samples  
            - Validate anomalies / outliers  
            - Re-check human_value labels from Stage E  
            """
        )
        promote_ready = False

    # -----------------------------------------
    # ‚úÖ Next Steps Checklist
    # -----------------------------------------
    st.markdown("### ‚úÖ Next Steps Checklist")

    if promote_ready:
        st.markdown(
            """
            ‚úÖ Promote to production  
            ‚úÖ Export ZIP bundle  
            ‚úÖ Notify Credit / Risk agents  
            ‚úÖ Schedule monitoring in Stage I  
            ‚úÖ Optional: widen training dataset  
            """
        )
    else:
        st.markdown(
            """
            üîÑ Retrain with more data  
            üßπ Clean labeling inconsistencies  
            üîç Inspect outliers via residual plots  
            üîß Try Gradient Boosting or Random Forest  
            """
        )

    # -----------------------------------------
    # ‚úÖ Show Drift Trend (mini chart)
    # -----------------------------------------
    st.markdown("### üìà Performance Trend (MAE & R¬≤ over time)")
    reports = sorted(glob.glob(os.path.join(RUNS_DIR, "training_report_*.json")), reverse=True)[:10]
    trend = []
    for f in reports:
        try:
            with open(f) as jf:
                rep = json.load(jf)
            trend.append({
                "timestamp": rep["timestamp"],
                "MAE": rep["metrics_new"]["MAE"],
                "R2": rep["metrics_new"]["R2"],
            })
        except:
            pass

    if trend:
        df_tr = pd.DataFrame(trend).sort_values("timestamp")
        st.line_chart(df_tr.set_index("timestamp")[["MAE", "R2"]])

    # -----------------------------------------
    # ‚úÖ Promotion Button
    # -----------------------------------------
    st.markdown("### üì§ Promote to Production")

    if st.button("‚úÖ Promote This Model Now"):
        try:
            #prod_dir = "./agents/asset_appraisal/models/production"
            prod_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/production"

            
            os.makedirs(prod_dir, exist_ok=True)

            shutil.copy(model_path, os.path.join(prod_dir, "model.joblib"))
            json.dump(
                {"model_path": model_path, "promoted_at": datetime.now(timezone.utc).isoformat(), "report": report},
                open(os.path.join(prod_dir, "production_meta.json"), "w"),
                indent=2
            )
            st.balloons()
            st.success("‚úÖ Model promoted successfully!")
        except Exception as e:
            st.error(f"‚ùå Promotion failed: {e}")
    



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# F ‚Äî MODEL TRAINING & PROMOTION (A/B with Prod)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tabF:
    import os, json, glob
    from datetime import datetime, timezone
    import numpy as np
    import pandas as pd
    import plotly.graph_objects as go
    import plotly.express as px
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
    from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
    from sklearn.linear_model import LinearRegression
    import joblib
    import shutil
    from pathlib import Path


    st.subheader("üß™ Stage F ‚Äî Model Training & Promotion")
    st.caption("Train or retrain with human feedback, compare against production (A/B), and promote if better.")
    
    
    # -----------------------------------------
    # ‚úÖ HOW TO USE THIS TRAINING STAGE (Dark / Collapsible)
    # -----------------------------------------
    with st.expander("üß≠ How this stage works", expanded=False):
        st.markdown("""
        <div style="
            padding:18px;
            border-radius:12px;
            background:linear-gradient(145deg,#0d1829,#10243d);
            border:1px solid #1e3a5f;
            color:#e2e8f0;
            font-size:1.05rem;
            line-height:1.55;
        ">
        <b>üìò How this stage works:</b><br><br>
        You are now in <b>Stage F</b>, where your appraisal model is trained, compared, and prepared for production.
        <br>This stage takes the <b>human-reviewed values</b> produced in Stage E and builds a model that predicts
        future valuations with better accuracy.
        <br><br><b>‚úÖ Follow these steps:</b><br>
        <b>1Ô∏è‚É£ Load Training Data</b><br>‚Ä¢ Auto-detect latest <code>reviewed_appraisal*.csv</code> from Stage E.<br>
        ‚Ä¢ Or upload CSV with <code>human_value</code> labels.
        <br><br><b>2Ô∏è‚É£ Select Features</b><br>‚Ä¢ Numeric columns auto-selected; leakage columns excluded.
        <br><br><b>3Ô∏è‚É£ Choose a Model</b><br>Select GradientBoosting, RandomForest, or LinearRegression (fast cycle).
        <br><br><b>4Ô∏è‚É£ Train & Compare</b><br>‚Ä¢ Train on data ‚Üí evaluate holdout ‚Üí A/B compare if baseline exists.
        <br><br><b>5Ô∏è‚É£ Review Metrics & Insights</b><br>‚Ä¢ Actual vs predicted charts, residuals, importance, summary.
        <br><br><b>6Ô∏è‚É£ Save / Promote / Export</b><br>‚Ä¢ Promote best model ‚Üí Stage G ZIP bundle.
        <br><br><b>üéØ Goal:</b> Produce a model that‚Äôs <b>more accurate, more stable, and more explainable</b>.
        </div>
        """, unsafe_allow_html=True)

    # # -----------------------------------------
    # # ‚úÖ HOW TO USE THIS TRAINING STAGE
    # # -----------------------------------------
    # st.markdown("""
    # <div style="
    #     padding: 18px;
    #     border-radius: 12px;
    #     background-color: #EFF6FF;
    #     border-left: 6px solid #2563EB;
    #     font-size: 1.05rem;
    # ">
    # <b>üìò How this stage works:</b><br><br>

    # You are now in <b>Stage F</b>, where your appraisal model is trained, compared, and prepared for production.

    # This stage takes the <b>human-reviewed values</b> produced in Stage E and builds a model that predicts future valuations with better accuracy.

    # <br><br>

    # <b>‚úÖ Follow these steps:</b><br>

    # <b>1Ô∏è‚É£ Load Training Data</b><br>
    # ‚Ä¢ The system auto-detects your latest <code>reviewed_appraisal*.csv</code> from Stage E.  
    # ‚Ä¢ If you prefer, upload a new CSV containing <code>human_value</code> labels.  

    # <br>

    # <b>2Ô∏è‚É£ Select Features</b><br>
    # ‚Ä¢ Numeric and relevant features are automatically selected.  
    # ‚Ä¢ ID columns and leakage columns (asset_id, ai_adjusted, etc.) are excluded.

    # <br>

    # <b>3Ô∏è‚É£ Choose a Model</b><br>
    # Select an algorithm (GradientBoosting, RandomForest, LinearRegression).  
    # The system will auto-tune nothing‚Äîthis is a fast-iteration training cycle.

    # <br>

    # <b>4Ô∏è‚É£ Train & Compare</b><br>
    # ‚Ä¢ The model trains on your data.  
    # ‚Ä¢ A holdout test set evaluates performance.  
    # ‚Ä¢ If a production model exists, an A/B comparison is displayed.  

    # <br>

    # <b>5Ô∏è‚É£ Review Metrics & Insights</b><br>
    # ‚Ä¢ Actual vs predicted charts  
    # ‚Ä¢ Residual distributions  
    # ‚Ä¢ Feature importance analysis  
    # ‚Ä¢ Executive summary (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)  
    # ‚Ä¢ AI recommendation (promote / retrain)

    # <br>

    # <b>6Ô∏è‚É£ Save, Promote or Export</b><br>
    # ‚Ä¢ Save trained models  
    # ‚Ä¢ Promote to production (Stage G uses it for ZIP packaging)  
    # ‚Ä¢ Export full ZIP bundles for deployment (AWS S3, Swift, GitHub)

    # <br><br>

    # <b>üéØ Goal of Stage F:</b><br>
    # Produce a model that is <b>more accurate, more stable, and more explainable</b> than your current production baseline ‚Äî and ready for deployment in Stage G.
    # </div>
    # """, unsafe_allow_html=True)

    

    # ---------- helpers ----------
    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    def _rmse(y_true, y_pred):
        return float(np.sqrt(mean_squared_error(y_true, y_pred)))

    def _mape(y_true, y_pred):
        y_true = np.asarray(y_true, dtype=float)
        y_pred = np.asarray(y_pred, dtype=float)
        mask = (y_true != 0) & np.isfinite(y_true) & np.isfinite(y_pred)
        if not mask.any():
            return float("nan")
        return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0)

    def _safe_len_df(x):
        return (0 if not isinstance(x, pd.DataFrame) else len(x))

    # ---------- diagnostics (always visible) ----------
    st.markdown("#### üîé Data availability (snapshots)")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("decision_df",  _safe_len_df(ss.get("asset_decision_df")))
    c2.metric("policy_df",    _safe_len_df(ss.get("asset_policy_df")))
    c3.metric("verified_df",  _safe_len_df(ss.get("asset_verified_df")))
    c4.metric("ai_df",        _safe_len_df(ss.get("asset_ai_df")))

    with st.expander("Load demo portfolio (if earlier stages not run)"):
        if st.button("Load demo portfolio (10 rows)", key="btn_demo_portfolio"):
            rng = np.random.default_rng(42)
            demo = pd.DataFrame({
                "application_id": [f"APP_{i:04d}" for i in range(10)],
                "asset_id":      [f"A{i:04d}" for i in range(10)],
                "asset_type":    rng.choice(["House","Apartment","Car","Land"], 10),
                "city":          rng.choice(["HCMC","Hanoi","Da Nang","Hue"], 10),
                "market_value":  rng.integers(80_000, 800_000, 10),
                "ai_adjusted":   rng.integers(75_000, 820_000, 10),
                "loan_amount":   rng.integers(30_000, 500_000, 10),
                "confidence":    rng.integers(60, 98, 10),
                "condition_score": rng.uniform(0.6, 1.0, 10).round(3),
                "legal_penalty":   rng.uniform(0.95, 1.0, 10).round(3),
                "human_value":   rng.integers(75_000, 820_000, 10),
            })
            ss["asset_decision_df"] = demo
            st.success("Demo portfolio loaded into ss['asset_decision_df'].")

    st.divider()

    # ---------- training data source ----------
    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    # Auto-pick latest reviewed CSV from Stage E
    reviewed = sorted([f for f in os.listdir(RUNS_DIR)
                       if f.startswith("reviewed_appraisal") and f.endswith(".csv")], reverse=True)
    df_train = None
    auto_path = None
    if reviewed:
        auto_path = os.path.join(RUNS_DIR, reviewed[0])
        try:
            df_train = pd.read_csv(auto_path)
        except Exception as e:
            st.warning(f"Could not read `{auto_path}`: {e}")

    st.markdown("#### üì• Training dataset")
    colU1, colU2 = st.columns([1.4, 1])
    with colU1:
        st.text_input("Auto-detected Stage E file", value=(auto_path or "‚Äî"), disabled=True)
    with colU2:
        up = st.file_uploader("Or upload CSV with human_value", type=["csv"], key="train_csv_upload")

    if up is not None:
        try:
            df_train = pd.read_csv(up)
            st.success(f"Loaded uploaded CSV ({len(df_train)} rows).")
        except Exception as e:
            st.error(f"Upload read failed: {e}")

    if df_train is None or df_train.empty:
        st.warning("‚ö†Ô∏è No training data available. Use Stage E to export `reviewed_appraisal*.csv` or upload a CSV above.")
        st.stop()

    st.markdown(f"**Using training rows:** {len(df_train):,}")
    st.dataframe(df_train.head(20), use_container_width=True)

    # ---------- feature building ----------
    st.markdown("#### üß± Feature selection")
    target_col = "human_value"
    if target_col not in df_train.columns:
        st.error("CSV must include a 'human_value' column (target).")
        st.stop()

    # Exclude obvious leak/IDs/targets from X
    drop_cols = {
        target_col, "fmv", "ai_adjusted",  # avoid leakage; AI numbers used only for comparison
        "ai_decision", "human_decision", "decision", "final_decision",
        "justification", "reviewed_value", "final_value",
        "application_id", "asset_id", "asset_type", "city"
    }
    num_cols = [c for c in df_train.columns
                if c not in drop_cols and pd.api.types.is_numeric_dtype(df_train[c])]

    if not num_cols:
        st.error("No numeric features left after filtering. Please include numeric columns for training.")
        st.stop()

    dataset_rows = len(df_train)
    numeric_feature_count = len(num_cols)

    X = df_train[num_cols].copy()
    y = pd.to_numeric(df_train[target_col], errors="coerce")

    # Drop rows with missing target
    mask = pd.notna(y)
    X, y = X.loc[mask], y.loc[mask]

    st.markdown("#### ‚≠ê Recommended models (EQACh signal)")
    st.caption(f"{dataset_rows:,} labeled assets ¬∑ {numeric_feature_count} numeric features")

    def score_asset_model(name: str) -> tuple[int, str]:
        """Coarse scoring so operators see why a regressor fits their data."""
        reason = ""
        score = 0

        if name == "GradientBoostingRegressor":
            score = 5 if dataset_rows > 5_000 else 3
            reason = "Captures nonlinear patterns and handles wide appraisal signals."
        elif name == "RandomForestRegressor":
            score = 4 if 1_000 < dataset_rows <= 10_000 else 2
            reason = "Stable when you have mixed-quality human feedback and want robustness."
        elif name == "LinearRegression":
            score = 3 if dataset_rows <= 2_000 else 1
            reason = "Fast, fully explainable baseline for regulators or smoke tests."

        if numeric_feature_count >= 8 and name != "LinearRegression":
            score += 1
            reason += " Extra numeric features boost tree ensembles."

        return score, reason

    model_profiles = []
    for candidate in ["GradientBoostingRegressor", "RandomForestRegressor", "LinearRegression"]:
        score, reason = score_asset_model(candidate)
        model_profiles.append(
            {
                "name": candidate,
                "score": score,
                "tagline": {
                    "GradientBoostingRegressor": "Enterprise-ready EQACh default",
                    "RandomForestRegressor": "Resilient midsize option",
                    "LinearRegression": "Audit-friendly baseline",
                }[candidate],
                "reason": reason,
            }
        )

    model_profiles.sort(key=lambda x: x["score"], reverse=True)
    rec_cols = st.columns(len(model_profiles))
    for col, profile in zip(rec_cols, model_profiles):
        with col:
            st.markdown(f"**{profile['name']}**")
            st.caption(profile["tagline"])
            st.write(profile["reason"])
            if st.button(f"Use {profile['name']}", key=f"use_asset_{profile['name']}"):
                ss["asset_model_choice"] = profile["name"]

    # Train/Test split
    test_size = st.slider("Holdout size", 10, 40, 20, step=5) / 100.0
    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, random_state=42)

    # ---------- model choice ----------
    st.markdown("#### ü§ñ Choose model")
    model_options = ["GradientBoostingRegressor", "RandomForestRegressor", "LinearRegression"]
    default_choice = ss.get("asset_model_choice", model_profiles[0]["name"])
    if default_choice not in model_options:
        default_choice = model_options[0]

    model_choice = st.selectbox(
        "Select model algorithm",
        model_options,
        index=model_options.index(default_choice)
    )
    ss["asset_model_choice"] = model_choice
    ModelCls = {
        "GradientBoostingRegressor": GradientBoostingRegressor,
        "RandomForestRegressor": RandomForestRegressor,
        "LinearRegression": LinearRegression,
    }[model_choice]

    # ---------- train & compare ----------
    if st.button("üöÄ Train & Compare (A/B vs Production)", key="btn_train_model"):
        # Train new
        new_model = ModelCls().fit(Xtr, ytr)
        y_pred_new = new_model.predict(Xte)

        # Load production baseline if exists
        prod_model_path = "./agents/asset_appraisal/models/production/model.joblib"
        prod_exists = os.path.exists(prod_model_path)
        y_pred_prod = None
        if prod_exists:
            try:
                prod_model = joblib.load(prod_model_path)
                # guard: try only if shapes align
                y_pred_prod = prod_model.predict(Xte)
            except Exception as e:
                st.warning(f"Production model failed to score holdout: {e}")

        # Metrics
        def _metrics(y_true, y_pred):
            return {
                "MAE": float(mean_absolute_error(y_true, y_pred)),
                "RMSE": _rmse(y_true, y_pred),
                "MAPE%": _mape(y_true, y_pred),
                "R2": float(r2_score(y_true, y_pred)),
            }

        new_m = _metrics(yte, y_pred_new)
        prod_m = _metrics(yte, y_pred_prod) if y_pred_prod is not None else None

        # ===== Dashboard: KPIs & deltas =====
        st.markdown("### üìä A/B Metrics (Holdout)")
        k1, k2, k3, k4, k5 = st.columns(5)
        with k1:
            st.metric("New MAE", f"{new_m['MAE']:,.0f}",
                      delta=(f"{(new_m['MAE'] - prod_m['MAE']):+.0f}" if prod_m else None))
        with k2:
            st.metric("New RMSE", f"{new_m['RMSE']:,.0f}",
                      delta=(f"{(new_m['RMSE'] - prod_m['RMSE']):+.0f}" if prod_m else None))
        with k3:
            st.metric("New MAPE", f"{new_m['MAPE%']:.2f}%",
                      delta=(f"{(new_m['MAPE%'] - prod_m['MAPE%']):+.2f}%" if prod_m else None))
        with k4:
            st.metric("New R¬≤", f"{new_m['R2']:.3f}",
                      delta=(f"{(new_m['R2'] - prod_m['R2']):+.3f}" if prod_m else None))
        with k5:
            st.metric("Test rows", f"{len(yte):,}")

        # ===== Plots: Actual vs Pred, Residuals =====
        plot_df = pd.DataFrame({
            "y_true": yte.values,
            "y_pred_new": y_pred_new,
            "y_pred_prod": (y_pred_prod if y_pred_prod is not None else np.full_like(y_pred_new, np.nan))
        })

        # Actual vs Pred overlay
        fig_scatter = go.Figure()
        fig_scatter.add_trace(go.Scatter(
            x=plot_df["y_true"], y=plot_df["y_pred_new"],
            mode="markers", name="New", opacity=0.7
        ))
        if y_pred_prod is not None:
            fig_scatter.add_trace(go.Scatter(
                x=plot_df["y_true"], y=plot_df["y_pred_prod"],
                mode="markers", name="Production", opacity=0.6
            ))
        # diagonal reference
        minv, maxv = np.nanmin(plot_df[["y_true","y_pred_new","y_pred_prod"]].values), np.nanmax(plot_df[["y_true","y_pred_new","y_pred_prod"]].values)
        fig_scatter.add_trace(go.Scatter(x=[minv, maxv], y=[minv, maxv], mode="lines", name="Ideal", line=dict(dash="dash")))
        fig_scatter.update_layout(title="Actual vs Predicted (Holdout)", xaxis_title="Actual", yaxis_title="Predicted")
        st.plotly_chart(fig_scatter, use_container_width=True)

        # Residuals hist
        plot_df["res_new"]  = plot_df["y_true"] - plot_df["y_pred_new"]
        if y_pred_prod is not None:
            plot_df["res_prod"] = plot_df["y_true"] - plot_df["y_pred_prod"]

        fig_res = go.Figure()
        fig_res.add_trace(go.Histogram(x=plot_df["res_new"], name="New", opacity=0.7))
        if y_pred_prod is not None:
            fig_res.add_trace(go.Histogram(x=plot_df["res_prod"], name="Production", opacity=0.6))
        fig_res.update_layout(barmode="overlay", title="Residuals Distribution (Actual - Predicted)")
        fig_res.update_traces(nbinsx=40)
        st.plotly_chart(fig_res, use_container_width=True)

        # ===== Feature importance / coefficients =====
        st.markdown("### üß† Feature Importance / Coefficients")
        if hasattr(new_model, "feature_importances_"):
            imp = pd.DataFrame({
                "feature": num_cols,
                "importance": new_model.feature_importances_
            }).sort_values("importance", ascending=False)
            st.bar_chart(imp.set_index("feature"))
        elif hasattr(new_model, "coef_"):
            coef = pd.DataFrame({
                "feature": num_cols,
                "coef": np.ravel(new_model.coef_)
            }).sort_values("coef", key=np.abs, ascending=False)
            st.bar_chart(coef.set_index("feature"))
        else:
            st.info("This model does not expose importances/coefficients.")

        # ===== Persist artifacts =====
        #trained_dir = "./agents/asset_appraisal/models/trained"
        trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/trained"
        
        os.makedirs(trained_dir, exist_ok=True)
        ts = _ts()
        model_path = os.path.join(trained_dir, f"{model_choice}_asset_{ts}.joblib")
        joblib.dump(new_model, model_path)

        preds_csv = os.path.join(RUNS_DIR, f"training_preds_{ts}.csv")
        plot_df.to_csv(preds_csv, index=False)

        report = {
            "timestamp": ts,
            "model_choice": model_choice,
            "trained_model_path": model_path,
            "features": num_cols,
            "metrics_new": new_m,
            "metrics_prod": prod_m,
            "holdout_rows": int(len(yte)),
            "source_file": (auto_path or "uploaded"),
            "preds_csv": preds_csv,
        }
        report_path = os.path.join(RUNS_DIR, f"training_report_{ts}.json")
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2)

        st.success(f"‚úÖ Trained model saved ‚Üí `{model_path}`")
        st.caption(f"Report ‚Üí `{report_path}` | Predictions ‚Üí `{preds_csv}`")

        # Download helpers
        cdl1, cdl2 = st.columns(2)
        with cdl1:
            st.download_button("‚¨áÔ∏è Download training report (JSON)",
                               data=json.dumps(report, indent=2).encode("utf-8"),
                               file_name=os.path.basename(report_path),
                               mime="application/json")
        with cdl2:
            st.download_button("‚¨áÔ∏è Download holdout predictions (CSV)",
                               data=plot_df.to_csv(index=False).encode("utf-8-sig"),
                               file_name=os.path.basename(preds_csv),
                               mime="text/csv")
        # ‚úÖ ‚úÖ ‚úÖ CALL DASHBOARD ‚Äî FIX FOR YOUR ISSUE
        render_stage_f_footer(
            new_m=new_m,
            prod_m=prod_m,
            RUNS_DIR=RUNS_DIR,
            model_path=model_path,
            report=report,
            df_train=df_train,
            yte=yte,
            y_pred_new=y_pred_new
        )
        
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚úÖ Helper functions required by Stage G
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def is_nonempty_df(x):
    import pandas as pd
    return isinstance(x, pd.DataFrame) and not x.empty

def first_nonempty_df(*candidates):
    for c in candidates:
        if is_nonempty_df(c):
            return c
    return None


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# G ‚Äî DEPLOYMENT & DISTRIBUTION STAGE
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tabG:
    import os, json, hashlib, zipfile, requests
    from datetime import datetime, timezone
    from pathlib import Path
    import streamlit as st

    st.title("üöÄ Stage G ‚Äî Deployment & Distribution")
    st.caption("Package ‚Üí Verify ‚Üí Upload ‚Üí Release ‚Üí Distribute to Credit / Legal / Risk units.")
    EXPORT_DIR = Path("./exports")
    EXPORT_DIR.mkdir(exist_ok=True)

    st.markdown("## üì¶ Build Project Bundle (Model + Reports + Artifacts)")
    build_zip_name = f"asset_project_bundle_{_ts()}.zip"
    build_zip_path = EXPORT_DIR / build_zip_name

    if st.button("‚¨áÔ∏è Build & Download Project ZIP", key="btn_build_stage_g_zip"):
        try:
            with zipfile.ZipFile(build_zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
                for root, dirs, files in os.walk(RUNS_DIR):
                    for f in files:
                        full = os.path.join(root, f)
                        arc = os.path.relpath(full, RUNS_DIR)
                        zf.write(full, f"runs/{arc}")

                if os.path.exists("./agents/asset_appraisal/models/production"):
                    for root, dirs, files in os.walk("./agents/asset_appraisal/models/production"):
                        for f in files:
                            full = os.path.join(root, f)
                            zf.write(full, f"production_models/{f}")

                if os.path.exists("./agents/asset_appraisal/models/trained"):
                    for root, dirs, files in os.walk("./agents/asset_appraisal/models/trained"):
                        for f in files:
                            full = os.path.join(root, f)
                            zf.write(full, f"trained_models/{f}")

                latest_report = sorted(Path(RUNS_DIR).glob("training_report_*.json"), reverse=True)
                if latest_report:
                    zf.write(latest_report[0], "training_report.json")

            st.success(f"‚úÖ Exported: {build_zip_name}")
            with open(build_zip_path, "rb") as fp:
                st.download_button(
                    "‚¨áÔ∏è Download ZIP Now",
                    data=fp,
                    file_name=build_zip_name,
                    mime="application/zip",
                    use_container_width=True,
                    key="btn_download_stage_g_zip",
                )
        except Exception as e:
            st.error(f"‚ùå ZIP creation failed: {e}")

    # ---------------------------------------------
    # 1) Load the latest ZIP bundle created in Stage F
    # ---------------------------------------------
    st.markdown("## üì¶ 1) Project Package (Generated in Stage F)")

    # Find ZIP files
    zip_files = sorted(EXPORT_DIR.glob("asset_project_bundle_*.zip"), reverse=True)
    
    if not zip_files:
        st.warning("‚ö†Ô∏è No project ZIP found. Run Stage F and export a bundle first.")
        st.stop()

    latest_zip = zip_files[0]

    st.success(f"‚úÖ Latest bundle detected: `{latest_zip.name}`")
    st.caption(f"Size: **{latest_zip.stat().st_size/1e6:.2f} MB**")
    
    # # Show preview
    # with zipfile.ZipFile(latest_zip, "r") as z:
    #     preview = z.namelist()[:20]
    #     st.code("\n".join(preview), language="text")
    

    # ---------------------------------------------
    # 2) Integrity Check (SHA256)
    # ---------------------------------------------
    st.markdown("## ‚úÖ 2) File Integrity Check (SHA256)")

    sha256 = hashlib.sha256(latest_zip.read_bytes()).hexdigest()
    st.code(sha256)

    checksum_path = latest_zip.with_suffix(".sha256")
    checksum_path.write_text(sha256)
    st.caption(f"Checksum written ‚Üí `{checksum_path.name}`")

    # Simple signature
    sig_path = latest_zip.with_suffix(".sig")
    sig_path.write_text(f"AI-Agent-Hub signed @ {datetime.now(timezone.utc).isoformat()}")
    st.caption(f"Signature stub ‚Üí `{sig_path.name}`")


    # ---------------------------------------------
    # 3) Upload Targets (S3 / Swift / GitHub Release)
    # ---------------------------------------------
    st.markdown("## ‚òÅÔ∏è 3) Upload / Publish Package")

    dest = st.radio(
        "Choose destination",
        ["AWS S3", "OpenStack Swift", "GitHub Release"],
        horizontal=True
    )

    if dest == "AWS S3":
        st.info("Upload to S3 (requires AWS credentials)")
        bucket = st.text_input("Bucket Name", "my-ai-models")
        key = st.text_input("Object Key", latest_zip.name)

        if st.button("‚¨ÜÔ∏è Upload to S3"):
            try:
                import boto3
                s3 = boto3.client("s3")
                s3.upload_file(str(latest_zip), bucket, key)
                st.success(f"‚úÖ Uploaded to `s3://{bucket}/{key}`")
            except Exception as e:
                st.error(f"‚ùå Failed: {e}")

    elif dest == "OpenStack Swift":
        st.info("Upload to Swift (requires Swift credentials)")
        container = st.text_input("Container Name", "ai-models")
        if st.button("‚¨ÜÔ∏è Upload to Swift"):
            try:
                from swiftclient.service import SwiftService, SwiftUploadObject
                with SwiftService() as swift:
                    swift.upload(container, [SwiftUploadObject(str(latest_zip))])
                st.success(f"‚úÖ Uploaded to Swift container `{container}`")
            except Exception as e:
                st.error(f"‚ùå Failed: {e}")

    elif dest == "GitHub Release":
        st.info("Publish as a GitHub release asset")
        repo = st.text_input("Repo (owner/repo)", "RackspaceAI/asset-appraisal-agent")
        token = st.text_input("GitHub Personal Access Token", type="password")
        tag = datetime.now().strftime("v%Y%m%d-%H%M%S")

        if st.button("‚¨ÜÔ∏è Publish Release on GitHub"):
            try:
                headers = {
                    "Authorization": f"token {token}",
                    "Accept": "application/vnd.github+json",
                }

                # Create release
                r = requests.post(
                    f"https://api.github.com/repos/{repo}/releases",
                    headers=headers,
                    json={"tag_name": tag, "name": f"Release {tag}", 
                          "body": "Automated export from Stage G"}
                )
                r.raise_for_status()

                upload_url = r.json()["upload_url"].split("{")[0]

                # Upload asset
                with open(latest_zip, "rb") as f:
                    ur = requests.post(
                        f"{upload_url}?name={latest_zip.name}",
                        headers={**headers, "Content-Type": "application/zip"},
                        data=f,
                    )
                ur.raise_for_status()

                st.success(f"‚úÖ GitHub Release `{tag}` published successfully!")
            except Exception as e:
                st.error(f"‚ùå Failed: {e}")


    # ---------------------------------------------
    # 4) Deployment Audit Log
    # ---------------------------------------------
    st.markdown("## üßæ 4) Deployment Audit Log")

    audit = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "export_file": latest_zip.name,
        "checksum": sha256,
        "target": dest,
    }

    audit_path = EXPORT_DIR / "deployment_audit.jsonl"
    with open(audit_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(audit) + "\n")

    st.success(f"Audit record added ‚Üí `{audit_path.name}`")


    # ---------------------------------------------
    # 5) Next Steps Checklist
    # ---------------------------------------------
    st.markdown("## ‚úÖ 5) Next Steps for DevOps / IT")

    st.markdown("""
    ### ‚úî For Credit Underwriting
    - Import CSV assets into the Credit Appraisal Agent  
    - Validate LTV, confidence, breaches  
    - Promote selected assets for loan approval  

    ### ‚úî For Legal & Compliance
    - Use verification subset (ownership, encumbrances)  
    - Run through Legal Verification Agent  
    - Flag encumbrances & fraud paths  

    ### ‚úî For Risk Management
    - Use realizable_value, condition_score, legal_penalty  
    - Re-run LTV stress tests  
    - Update risk dashboards monthly  

    ### ‚úî For DevOps / Platform Teams
    - Push ZIP to GitHub / Swift / S3  
    - Deploy production model into RunAI / SageMaker / OpenStack MLOps  
    - Update production_meta.json  
    """)

    st.info("Stage G is complete ‚Äî continue to Stage H for Inter-Department Handoff.")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚úÖ STAGE H ‚Äî Executive Dashboard + Handoff Export
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

with tabH:
    import os, json, zipfile
    from pathlib import Path
    from datetime import datetime, timezone
    import pandas as pd
    import numpy as np
    import streamlit as st
    import plotly.express as px
    import plotly.graph_objects as go

    st.markdown("## üß≠ Stage H ‚Äî Unified Portfolio, Insights & Handoff Export")
    st.caption("Executive summary ‚Ä¢ Asset insights ‚Ä¢ Fraud/risk signals ‚Ä¢ Department deliverables")


    
    # ---------------------------------------------------------
    # ‚úÖ Required outputs from earlier stages ‚Äî MUST COME FIRST
    # ---------------------------------------------------------
    ai_df        = st.session_state.get("asset_ai_df")
    policy_df    = st.session_state.get("asset_policy_df")
    decision_df  = st.session_state.get("asset_decision_df")

    missing = []

    if ai_df is None or ai_df.empty:
        missing.append("Stage C (valuation)")
    if decision_df is None or decision_df.empty:
        missing.append("Stage D (risk & decision)")

    if missing:
        st.error("‚ö†Ô∏è Missing required data: " + ", ".join(missing))
        st.info("Please run the missing stages before returning to Stage H.")
        st.stop()

    # ‚úÖ Only now is dfv allowed to be created
    dfv = decision_df.copy()


    # ---------------------------------------------------------
    # ‚úÖ STATUS LABEL (Validated / Risky / Fraud)
    # ---------------------------------------------------------
    def label_row(r):
        if r.get("fraud_flag") in [True, "True", 1]:
            return "FRAUD"
        if r.get("encumbrance_flag") in [True, "True", 1]:
            return "ENCUMBERED"
        if str(r.get("decision", "")).lower() == "reject":
            return "RISKY"
        if str(r.get("policy_breaches", "")).strip():
            return "RISKY"
        return "VALIDATED"

    dfv["status"] = dfv.apply(label_row, axis=1)

    # ---------------------------------------------------------
    # ‚úÖ EXECUTIVE SUMMARY METRICS
    # ---------------------------------------------------------
    st.markdown("### üìä Executive Summary")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Assets", len(dfv))
    with col2:
        st.metric("Validated", (dfv["status"] == "VALIDATED").sum())
    with col3:
        st.metric("Risky", (dfv["status"] == "RISKY").sum())
    with col4:
        st.metric("Fraud / Encumbered", (dfv["status"].isin(["FRAUD","ENCUMBERED"])).sum())

    # ---------------------------------------------------------
    # ‚úÖ HEATMAP ‚Äî Asset Risk & Fraud Signals
    # ---------------------------------------------------------
    st.markdown("### üî• Fraud / Risk Heatmap")
    
    try:
        hm = dfv[["confidence", "ltv_ai"]].copy()
        hm = hm.dropna()

        fig_hm = px.density_heatmap(
            hm, x="confidence", y="ltv_ai",
            nbinsx=30, nbinsy=30,
            color_continuous_scale="YlOrRd",
            title="Fraud/Anomaly Density ‚Äî (Low confidence + High LTV = Hot Zones)"
        )
        st.plotly_chart(fig_hm, use_container_width=True)
    except Exception:
        st.info("Heatmap unavailable until confidence / LTV data is complete.")

    # ---------------------------------------------------------
    # ‚úÖ MARKET INSIGHTS ‚Äî CITY LEVEL DISTRIBUTION
    # ---------------------------------------------------------
    st.markdown("### üåç Asset Distribution by City")

    if "city" in dfv.columns:
        fig_city = px.histogram(
            dfv, x="city", color="status",
            title="Asset Count per City by Status",
            barmode="group"
        )
        st.plotly_chart(fig_city, use_container_width=True)

    # ---------------------------------------------------------
    # ‚úÖ VALUE INSIGHTS ‚Äî Realizable Value Curve
    # ---------------------------------------------------------
    st.markdown("### üí∞ Value Distribution ‚Äî FMV vs Realizable Value")

    if "realizable_value" in dfv.columns:
        fig_val = go.Figure()
        fig_val.add_trace(go.Violin(y=dfv["fmv"], name="FMV", box_visible=True))
        fig_val.add_trace(go.Violin(y=dfv["realizable_value"], name="Realizable", box_visible=True))
        st.plotly_chart(fig_val, use_container_width=True)

    # ---------------------------------------------------------
    # ‚úÖ FULL PORTFOLIO TABLE
    # ---------------------------------------------------------
    st.markdown("### üìÇ Unified Portfolio (with status)")
    st.dataframe(dfv, use_container_width=True)

    # ---------------------------------------------------------
    # ‚úÖ DEPARTMENT HANDOFF EXPORTS (bulletproof)
    # ---------------------------------------------------------
    st.markdown("## üè¶ Department Handoff Packages")
    st.caption("Each team receives only what they need. Clear, simple, compliant.")

    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    HANDOFF_DIR = Path("./handoff")
    ZIP_DIR      = HANDOFF_DIR / "zips"
    HANDOFF_DIR.mkdir(exist_ok=True)
    ZIP_DIR.mkdir(exist_ok=True)

    # ---------------------------
    # ‚úÖ CREDIT APPRAISAL EXPORT
    # ---------------------------
    credit_cols = [ 
        "application_id","asset_id","asset_type","city",
        "ai_adjusted","fmv","realizable_value",
        "loan_amount","ltv_ai","ltv_cap",
        "decision","policy_breaches"
    ]
    credit = dfv[[c for c in credit_cols if c in dfv.columns]].copy()
    credit_path = HANDOFF_DIR / f"credit_appraisal_{ts}.csv"
    credit.to_csv(credit_path, index=False)

    # Download button
    with open(credit_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Credit Appraisal CSV", f, file_name=credit_path.name, mime="text/csv")

    # ---------------------------
    # ‚úÖ LEGAL / TITLE EXPORT
    # ---------------------------
    legal_cols = [
        "application_id","asset_id","verified_owner",
        "encumbrance_flag","legal_penalty","condition_score","notes"
    ]
    legal = dfv[[c for c in legal_cols if c in dfv.columns]].copy()
    legal_path = HANDOFF_DIR / f"legal_pack_{ts}.csv"
    legal.to_csv(legal_path, index=False)

    with open(legal_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Legal & Title CSV", f, file_name=legal_path.name, mime="text/csv")

    # ---------------------------
    # ‚úÖ RISK MANAGEMENT EXPORT
    # ---------------------------
    risk_cols = [
        "application_id","asset_id","confidence",
        "ltv_ai","ltv_cap","policy_breaches","decision","status"
    ]
    risk = dfv[[c for c in risk_cols if c in dfv.columns]].copy()
    risk_path = HANDOFF_DIR / f"risk_management_{ts}.csv"
    risk.to_csv(risk_path, index=False)

    with open(risk_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Risk Management CSV", f, file_name=risk_path.name, mime="text/csv")

    # ---------------------------
    # ‚úÖ CUSTOMER SERVICE EXPORT
    # ---------------------------
    cust_cols = [
        "application_id","asset_id","asset_type","city",
        "fmv","ai_adjusted","decision","status","why"
    ]
    cust = dfv[[c for c in cust_cols if c in dfv.columns]].copy()
    cust_path = HANDOFF_DIR / f"customer_service_{ts}.csv"
    cust.to_csv(cust_path, index=False)

    with open(cust_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Customer Service CSV", f, file_name=cust_path.name, mime="text/csv")

    # ---------------------------
    # ‚úÖ PORTFOLIO SUMMARY
    # ---------------------------
    portfolio_path = HANDOFF_DIR / f"portfolio_{ts}.csv"
    dfv.to_csv(portfolio_path, index=False)

    with open(portfolio_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Portfolio Summary CSV", f, file_name=portfolio_path.name, mime="text/csv")

    # ---------------------------
    # ‚úÖ AUDIT RECORD
    # ---------------------------
    audit = {
        "timestamp": ts,
        "rows": len(dfv),
        "status": dfv["status"].value_counts().to_dict(),
        "avg_confidence": float(dfv["confidence"].mean() if "confidence" in dfv else 0.0),
    }
    audit_path = HANDOFF_DIR / f"audit_{ts}.json"
    with open(audit_path, "w") as f:
        json.dump(audit, f, indent=2)

    with open(audit_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Audit Record (JSON)", f, file_name=audit_path.name, mime="application/json")

    # # ---------------------------
    # # ‚úÖ FULL ZIP BUNDLE
    # # ---------------------------
    # zip_path = ZIP_DIR / f"handoff_bundle_{ts}.zip"
    # with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
    #     for fp in [credit_path, legal_path, risk_path, cust_path, portfolio_path, audit_path]:
    #         zf.write(fp, arcname=os.path.basename(fp))

    # with open(zip_path, "rb") as f:
    #     st.download_button("‚¨áÔ∏è Download FULL Handoff ZIP", f,
    #                     file_name=zip_path.name, mime="application/zip",
    #                     use_container_width=True)

    
    
   
    # ---------------------------------------------------------
    # ‚úÖ ZIP bundle
    # ---------------------------------------------------------
    zip_path = ZIP_DIR / f"handoff_bundle_{ts}.zip"
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
        for fp in [credit_path, legal_path, risk_path, cust_path, portfolio_path, audit_path]:
            zf.write(fp, arcname=os.path.basename(fp))

    st.markdown("### üì¶ Download Unified Handoff Bundle")
    with open(zip_path, "rb") as f:
        st.download_button(
            "‚¨áÔ∏è Download Full Handoff ZIP",
            data=f,
            file_name=os.path.basename(zip_path),
            mime="application/zip",
            use_container_width=True
        )


# Legacy macOS blue theme (kept for optional reuse)
LEGACY_ASSET_THEME_SNIPPET = '''
def legacy_asset_theme(theme: str = "dark"):
    import streamlit as st

    st.markdown("""
    <style>
    /* ===============================================
       üåô MACOS BLUE DARK THEME ‚Äî GLOBAL BASE
    =============================================== */
    html, body, [data-testid="stAppViewContainer"] {
        background: radial-gradient(circle at 20% 20%, #0b0f16, #060a12 85%) !important;
        color: #f8fafc !important;
        font-family: "Inter","SF Pro Display","Segoe UI",system-ui,sans-serif !important;
    }

    h1,h2,h3,h4,h5,h6 {
        color: #f8fafc !important;
        font-weight: 700 !important;
        letter-spacing: -0.02em !important;
    }

    p, li, label, span, div {
        color: #e2e8f0 !important;
    }
    small, .stCaption { color: #94a3b8 !important; }

    a, a:link, a:visited { color: #339dff !important; }
    a:hover { color: #60a5fa !important; text-decoration: underline; }

    hr {
        border: none !important;
        height: 1px !important;
        background: linear-gradient(90deg,transparent,#007aff,transparent) !important;
    }

    /* ===============================================
       üß± CONTAINERS & CARDS
    =============================================== */
    .stMarkdown, .stContainer, .stAlert, [class*="stCard"], [class*="block-container"] {
        background: #0f172a !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 12px !important;
        box-shadow: 0 4px 16px rgba(0,0,0,0.5) !important;
    }

    /* ===============================================
       üîò BUTTONS ‚Äî macOS BLUE
    =============================================== */
    button[kind="primary"], .stButton>button, .stDownloadButton>button, .stDownloadButton button {
        background: linear-gradient(180deg,#007aff,#005ecb) !important;
        color: #ffffff !important;
        border: 1px solid #0051b8 !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
        padding: 0.5rem 1rem !important;
        box-shadow: 0 4px 10px rgba(0,122,255,0.35),
                    inset 0 -1px 0 rgba(255,255,255,0.2) !important;
        transition: all 0.25s ease-in-out !important;
    }
    button[kind="primary"]:hover, .stButton>button:hover, .stDownloadButton>button:hover {
        background: linear-gradient(180deg,#339dff,#006ae6) !important;
        box-shadow: 0 4px 14px rgba(0,122,255,0.45) !important;
        transform: translateY(-1px) !important;
    }
    button[kind="primary"]:active, .stButton>button:active, .stDownloadButton>button:active {
        background: linear-gradient(180deg,#004fc4,#0042a8) !important;
        box-shadow: inset 0 2px 6px rgba(0,122,255,0.3) !important;
        transform: translateY(0) !important;
    }
    .stButton button[disabled], .stDownloadButton button[disabled] {
        background: #1e293b !important;
        color: #64748b !important;
        border: 1px solid #334155 !important;
    }

    /* ===============================================
    üß† INPUTS (Text, Select, Number) & FOCUS STATE
    =============================================== */
    .stTextInput>div>div>input,
    .stSelectbox>div>div>div,
    .stNumberInput input {
        background: #111827 !important;
        color: #f8fafc !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 8px !important;
        padding: 6px 10px !important;
        transition: all 0.25s ease;
    }
    .stTextInput>div>div>input:focus,
    .stSelectbox>div>div>div:focus-within,
    .stNumberInput input:focus {
        outline: none !important;
        border-color: #007aff !important;
        box-shadow: 0 0 0 2px rgba(0,122,255,0.4) !important;
    }
    ::placeholder {
        color: #9ca3af !important;
        opacity: 1 !important;
    }
    /* ===============================================
   üéõ DROPDOWN MENUS
    =============================================== */
    [data-baseweb="popover"], [role="listbox"] {
        background: #0f172a !important;
        color: #f8fafc !important;
        border: 1px solid #1e3a8a !important;
        box-shadow: 0 4px 20px rgba(0,0,0,0.6) !important;
    }
    [data-baseweb="menu-item"] {
        background: #0f172a !important;
        color: #f8fafc !important;
    }
    [data-baseweb="menu-item"]:hover {
        background: #1e3a8a !important;
        color: #ffffff !important;
    }
    /* ===============================================
    üß≠ SIDEBAR THEME
    =============================================== */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg,#0d1320,#060a12) !important;
        border-right: 1px solid #1e3a8a !important;
        color: #f8fafc !important;
    }

    
    /* ===============================================
       ‚òëÔ∏è CHECKBOXES / RADIOS / SLIDERS
    =============================================== */
    input[type="checkbox"], input[type="radio"] {
        accent-color: #007aff !important;
    }
    .stSlider [role="slider"] {
        background-color: #007aff !important;
    }

    /* ===============================================
       üóÇÔ∏è TABS
    =============================================== */
    .stTabs [data-baseweb="tab-list"] button {
        color: #e2e8f0 !important;
        background: #111827 !important;
        border: 1px solid #1e293b !important;
        border-radius: 10px !important;
        font-weight: 500 !important;
        margin-right: 4px !important;
    }
    .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {
        background: #007aff !important;
        color: #ffffff !important;
        box-shadow: 0 0 12px rgba(0,122,255,0.4) !important;
    }

    /* ===============================================
       üß≠ EXPANDERS / ACCORDIONS
    =============================================== */
    .streamlit-expanderHeader {
        background: linear-gradient(90deg,#0d284d,#0a1f3a) !important;
        color: #dbeafe !important;
        border: 1px solid #1e3a5f !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
    }
    .streamlit-expanderContent {
        background: #0f172a !important;
        color: #e2e8f0 !important;
        border: 1px solid #1e3a5f !important;
        border-radius: 0 0 8px 8px !important;
    }

    /* ===============================================
       üìä METRIC CARDS (st.metric)
    =============================================== */
    [data-testid="stMetric"] {
        background: linear-gradient(180deg,#0b1220,#101a2c) !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 10px !important;
        box-shadow: inset 0 0 10px rgba(255,255,255,0.03),
                    0 3px 10px rgba(0,0,0,0.6) !important;
        padding: 10px 14px !important;
        text-align: center !important;
    }
    div[data-testid="stMetricLabel"] {
        color: #94a3b8 !important;
        font-size: 0.85rem !important;
        font-weight: 500 !important;
    }
    div[data-testid="stMetricValue"] {
        color: #ffffff !important;
        font-size: 1.3rem !important;
        font-weight: 600 !important;
    }

    /* ===============================================
       üìä METRIC COMPARISON TABLE ‚Äî FINAL
    =============================================== */
    [data-testid="stDataFrame"] {
        background: radial-gradient(circle at 50% 50%, #0b1220, #060a12 90%) !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 12px !important;
        box-shadow:
            0 0 14px rgba(0,0,0,0.6) inset,
            0 4px 18px rgba(0,0,0,0.7),
            0 0 12px rgba(0,122,255,0.15) !important;
        margin-top: 12px !important;
        padding: 8px !important;
    }
    [data-testid="stDataFrame"] thead tr th {
        background: linear-gradient(90deg,#004fc4,#007aff) !important;
        color: #ffffff !important;
        border-bottom: 2px solid #007aff !important;
        font-weight: 700 !important;
        text-transform: uppercase !important;
        letter-spacing: 0.02em !important;
        font-size: 0.92rem !important;
        padding: 10px 14px !important;
    }
    [data-testid="stDataFrame"] tbody tr {
        background-color: #0b1220 !important;
        color: #ffffff !important;
        transition: background 0.25s ease;
    }
    [data-testid="stDataFrame"] tbody tr:nth-child(even) {
        background-color: #101a2c !important;
    }
    [data-testid="stDataFrame"] tbody tr:hover {
        background-color: #112a52 !important;
        box-shadow: 0 0 8px rgba(0,122,255,0.25) inset !important;
    }
    [data-testid="stDataFrame"] tbody td {
        border-top: 1px solid #1e3a8a !important;
        color: #ffffff !important;
        padding: 9px 14px !important;
        font-size: 0.95rem !important;
        font-weight: 500 !important;
    }
    [data-testid="stDataFrame"] tbody td:last-child {
        color: #60a5fa !important;
        font-weight: 500 !important;
    }

    /* ===============================================
       üìÅ FILE UPLOADER
    =============================================== */
    [data-testid="stFileUploaderDropzone"] {
        background: rgba(255,255,255,0.03) !important;
        border: 1px dashed #1e3a8a !important;
        border-radius: 10px !important;
        color: #cbd5e1 !important;
        transition: all 0.25s ease;
    }
    [data-testid="stFileUploaderDropzone"]:hover {
        border-color: #007aff !important;
        background: rgba(0,122,255,0.1) !important;
    }

    /* ===============================================
       ‚ö†Ô∏è ALERT BOXES
    =============================================== */
    [data-testid^="stAlert"] {
        border-radius: 10px !important;
        border: 1px solid #1e3a8a !important;
        color: #e2e8f0 !important;
        box-shadow: 0 3px 15px rgba(0,0,0,0.4) !important;
    }
    [data-testid="stAlertInfo"]    { background: linear-gradient(145deg,#0d1829,#10243d)!important; }
    [data-testid="stAlertSuccess"] { background: linear-gradient(145deg,#0f2414,#183820)!important; }
    [data-testid="stAlertError"]   { background: linear-gradient(145deg,#2b1617,#1a0c0d)!important; }
    [data-testid="stAlertWarning"] { background: linear-gradient(145deg,#2f2a10,#1c1a0a)!important; }

    </style>
    """, unsafe_allow_html=True)

'''

render_chat_assistant(
    page_id="asset_appraisal",
    context=_build_asset_chat_context(),
    faq_questions=ASSET_FAQ,
    persona=ASSET_PERSONA,
)



==================== ./chatbot_assistant.py ====================
import os
from typing import Dict, Any, List

import requests
import streamlit as st
from services.ui.theme_manager import apply_theme, render_theme_toggle

API_URL = os.getenv("API_URL", "http://localhost:8090")

ROLE_CONFIG: Dict[str, Dict[str, Any]] = {
    "credit": {
        "label": "üí≥ Credit Appraisal Agent",
        "page_id": "credit_appraisal",
        "context": {"agent_type": "credit", "stage": "credit_review"},
        "faqs": [
            "How does the Credit Appraisal agent work end-to-end?",
            "What are the step-by-step stages in this agent?",
            "Explain the lexical definitions for PD, DTI, LTV, and other credit terms.",
            "What inputs and outputs does the credit agent expect?",
            "What benefits do we get from using this AI credit agent?",
            "How do I explain an approve vs review decision?",
            "What credit score threshold are we using for SMEs?",
            "Summarize PD, LTV, and DTI for the current borrower.",
            "Which policy rules can trigger an automatic reject?",
            "How do I export the credit decision narrative?",
            "What inputs feed the probability of default model?",
            "How do I rerun the credit model after adjusting collateral?",
            "Where can I see recent manual overrides?",
            "How do I share credit results with the Unified agent?",
            "What datasets power the explainability section?",
        ],
    },
    "asset": {
        "label": "üè¶ Asset Appraisal Agent",
        "page_id": "asset_appraisal",
        "context": {"agent_type": "asset", "stage": "valuation"},
        "faqs": [
            "How does the Asset Appraisal agent work from intake to report?",
            "What are the stage-by-stage steps in the asset workflow?",
            "Define the key terms (FMV, AI-adjusted, realizable, encumbrance).",
            "What inputs and outputs does the asset agent consume/produce?",
            "What benefits do we gain from automating asset valuation with AI?",
            "How are AI-adjusted FMVs derived?",
            "List comps used to price construction equipment.",
            "What encumbrance flags should I watch for?",
            "How do I upload new evidence (images/PDFs)?",
            "Where do I see anonymized vs raw intake data?",
            "How does the agent detect secondary liens?",
            "Can I export the appraisal report for auditors?",
            "What asset types have custom models?",
            "How do I sync FMV outputs with the Unified agent?",
            "How is valuation confidence calculated?",
        ],
    },
    "anti_fraud": {
        "label": "üõ°Ô∏è Anti-Fraud & KYC Agent",
        "page_id": "anti_fraud_kyc",
        "context": {"agent_type": "fraud_kyc", "stage": "fraud_review"},
        "faqs": [
            "How does the Anti-Fraud/KYC agent work?",
            "What are the detailed steps (Intake ‚Üí Privacy ‚Üí Verification ‚Üí Fraud ‚Üí Review ‚Üí Reporting)?",
            "Define the key lexical terms (sanction hits, fraud_score, kyc_passed).",
            "What inputs and outputs does this fraud agent use?",
            "What benefits does this AI fraud/KYC agent provide?",
            "Walk me through the fraud workflow A‚ÜíH.",
            "Where do sanction hits appear for the borrower?",
            "How can I rerun the fraud rules for this application?",
            "What does the privacy scrub remove?",
            "How do I export the KYC audit packet?",
            "How is the fraud risk score calculated?",
            "Can the agent anonymize documents before sharing?",
            "Where do I see verification status by stage?",
            "How do I hand off a case to human review?",
            "How do I refresh watchlist data?",
        ],
    },
    "chatbot": {
        "label": "ü§ñ Chatbot Ops",
        "page_id": "chatbot_assistant",
        "context": {"agent_type": "chatbot", "stage": "testing"},
        "faqs": [
            "How does the chatbot assistant work behind the scenes?",
            "What are the steps from ingestion ‚Üí retrieval ‚Üí reply?",
            "Define lexical terms like retrieved snippet, agent_type, context_summary.",
            "What inputs and outputs does the chatbot endpoint expect?",
            "What are the benefits of using this AI chatbot with local RAG?",
            "What data sources are indexed in the chatbot?",
            "How do I refresh the local RAG store?",
            "Explain how the chatbot answers Unified Risk questions.",
            "How do I switch personas (credit, asset, fraud)?",
            "Where are chat logs stored?",
            "Can I push chatbot answers into a report?",
            "How do I update the FAQ entries?",
            "How do I reset the embeddings cache?",
            "How do I test from the command line?",
            "How can I add more CSV sources?",
        ],
    },
}

st.set_page_config(
    page_title="Chatbot Assistant ‚Äî Preview",
    layout="wide",
    initial_sidebar_state="collapsed",
)

apply_theme()

st.markdown(
    """
    <style>
    [data-testid="stSidebar"], section[data-testid="stSidebar"] { display: none !important; }
    [data-testid="stAppViewContainer"] { margin-left: 0 !important; padding-left: 0 !important; }
    .chatbot-nav {
        display: flex;
        gap: 0.5rem;
        flex-wrap: wrap;
        margin-bottom: 1rem;
    }
    .chatbot-nav button {
        font-weight: 600 !important;
    }
    </style>
    """,
    unsafe_allow_html=True,
)


def _launch_page(target: str):
    mapping = {
        "asset": "pages/asset_appraisal.py",
        "credit": "pages/credit_appraisal.py",
        "anti_fraud": "pages/anti_fraud_kyc.py",
        "unified": "pages/unified_risk.py",
        "agents": "app.py",
    }
    path = mapping.get(target)
    if not path:
        return
    try:
        st.switch_page(path)
    except Exception:
        pass


nav_cols = st.columns([1, 1, 1, 1, 1])
with nav_cols[0]:
    if st.button("üè† Home", use_container_width=True):
        _launch_page("agents")
with nav_cols[1]:
    if st.button("üß© Unified", use_container_width=True):
        _launch_page("unified")
with nav_cols[2]:
    if st.button("üí≥ Credit", use_container_width=True):
        _launch_page("credit")
with nav_cols[3]:
    if st.button("üè¶ Asset", use_container_width=True):
        _launch_page("asset")
with nav_cols[4]:
    if st.button("üõ°Ô∏è Anti-Fraud", use_container_width=True):
        _launch_page("anti_fraud")

_, theme_col = st.columns([5, 1])
with theme_col:
    render_theme_toggle(key="chatbot_theme_toggle")

st.title("üí¨ Chatbot Assistant (Preview)")
st.caption("Context-aware copilot that stays in sync with local RAG + agent blueprints.")

st.markdown(
    """
    ### üß† What it does today
    - Answers FAQs about every agent blueprint (credit, asset, anti-fraud, unified risk).
    - Surfaces ingestion status and signals if the local vector store is stale.
    - Streams reasoning steps so operators can audit every suggestion.

    ### üõ£Ô∏è Roadmap
    1. Multi-turn workflows so the chatbot can kick off asset/credit decisions directly.
    2. Inline analytics cards powered by the same sentence-transformer embeddings.
    3. Agent-to-agent orchestration so this copilot can dispatch work to the others.

    ### ‚úÖ Try it out
    - Seed the local RAG store with `seed_local_rag_agent_docs.py`.
    - Ask *‚ÄúHow does the asset appraisal agent price farm equipment?‚Äù*
    - Run `/refresh_rag` from the chat sidebar to pull the latest code comments into memory.
    """
)

st.success("Preview ready ‚Äî wire this page into the nav once the full chat stack ships.")

st.markdown("---")
st.subheader("Chatbot Test Bench")

left_col, right_col = st.columns([1, 1])

st.session_state.setdefault("chatbot_test_runs", [])
st.session_state.setdefault("chatbot_selected_role", "credit")

with left_col:
    st.subheader("Role & Shortcuts")
    role_options = list(ROLE_CONFIG.keys())
    default_role = st.session_state.get("chatbot_selected_role", "credit")
    role_key = ROLE_CONFIG.get(default_role, ROLE_CONFIG[role_options[0]])
    selected_role = st.selectbox(
        "Choose assistant persona",
        role_options,
        index=role_options.index(default_role) if default_role in role_options else 0,
        format_func=lambda key: ROLE_CONFIG[key]["label"],
    )
    st.session_state["chatbot_selected_role"] = selected_role
    role_key = ROLE_CONFIG[selected_role]

    st.markdown(
        """
        **How to test**
        - Paste FAQs or operator questions.
        - Reference agent files (`credit_appraisal.py`, `asset_appraisal.py`, etc.).
        - Ask follow ups like *"Show me the fraud workflow"*.
        - Responses on the right come directly from the local `/v1/chat` endpoint.
        """
    )
    st.info("‚öôÔ∏è Uses local embeddings + CSV fallback. Ensure the API server is running on port 8090.")

    st.markdown("**Starter FAQs**")
    faqs: List[str] = role_key.get("faqs", [])
    for idx, question in enumerate(faqs):
        if st.button(question, key=f"faq_{selected_role}_{idx}"):
            st.session_state["chatbot_test_prompt"] = question
            st.rerun()

with right_col:
    with st.form("chatbot_test_form"):
        prompt = st.text_area(
            "Prompt",
            height=320,
            placeholder="e.g. Explain how the Unified Risk agent combines fraud + credit",
            key="chatbot_test_prompt",
        )
        submitted = st.form_submit_button("Send", use_container_width=True)

    reply_box = st.empty()
    if submitted:
        trimmed = prompt.strip()
        if not trimmed:
            st.warning("Enter a prompt before sending.")
        else:
            try:
                resp = requests.post(
                    f"{API_URL}/v1/chat",
                    json={
                        "message": trimmed,
                        "page_id": role_key["page_id"],
                        "context": role_key.get("context", {}),
                        "history": [
                            {"role": "user", "content": run["prompt"]}
                            for run in st.session_state["chatbot_test_runs"][-3:]
                        ],
                    },
                    timeout=30,
                )
                resp.raise_for_status()
                data: Dict[str, Any] = resp.json()
                st.session_state["chatbot_test_runs"].append(
                    {
                        "prompt": trimmed,
                        "reply": data.get("reply", "(No reply)"),
                        "retrieved": data.get("retrieved", []),
                        "timestamp": data.get("timestamp"),
                    }
                )
            except requests.RequestException as exc:
                st.error(f"Chat API error: {exc}")

    if st.session_state["chatbot_test_runs"]:
        last = st.session_state["chatbot_test_runs"][-1]
        reply_box.text_area(
            "Assistant Response",
            value=last["reply"],
            height=320,
        )
        with st.expander("Retrieved context", expanded=False):
            for idx, doc in enumerate(last.get("retrieved", []), start=1):
                st.markdown(f"**Match {idx}: {doc.get('title')}** (score={doc.get('score')})")
                st.write(doc.get("snippet"))
    else:
        reply_box.text_area(
            "Assistant Response",
            value="Awaiting first prompt‚Ä¶",
            height=320,
        )



==================== ./credit_appraisal.py.ok.py ====================
# services/ui/pages/credit_appraisal.py
from __future__ import annotations

import os
import io
import re
import json
import shutil
from pathlib import Path
from datetime import datetime, timezone
from typing import Optional, Dict, List, Any
import pandas as pd
import numpy as np
import streamlit as st
import requests
import plotly.express as px
import plotly.graph_objects as go
import logging
import sys

from pandas import json_normalize  # ADD



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# PAGE CONFIG ‚Äî must be the first Streamlit call
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

st.set_page_config(page_title="Asset Appraisal Agent", layout="wide")
ss = st.session_state


def apply_theme(theme: str = "dark"):
    import streamlit as st

    st.markdown("""
    <style>
    /* ===============================================
       üåô MACOS BLUE DARK THEME ‚Äî GLOBAL BASE
    =============================================== */
    html, body, [data-testid="stAppViewContainer"] {
        background: radial-gradient(circle at 20% 20%, #0b0f16, #060a12 85%) !important;
        color: #f8fafc !important;
        font-family: "Inter","SF Pro Display","Segoe UI",system-ui,sans-serif !important;
    }

    h1,h2,h3,h4,h5,h6 {
        color: #f8fafc !important;
        font-weight: 700 !important;
        letter-spacing: -0.02em !important;
    }

    p, li, label, span, div {
        color: #e2e8f0 !important;
    }
    small, .stCaption { color: #94a3b8 !important; }

    a, a:link, a:visited { color: #339dff !important; }
    a:hover { color: #60a5fa !important; text-decoration: underline; }

    hr {
        border: none !important;
        height: 1px !important;
        background: linear-gradient(90deg,transparent,#007aff,transparent) !important;
    }

    /* ===============================================
       üß± CONTAINERS & CARDS
    =============================================== */
    .stMarkdown, .stContainer, .stAlert, [class*="stCard"], [class*="block-container"] {
        background: #0f172a !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 12px !important;
        box-shadow: 0 4px 16px rgba(0,0,0,0.5) !important;
    }

    /* ===============================================
       üîò BUTTONS ‚Äî macOS BLUE
    =============================================== */
    button[kind="primary"], .stButton>button, .stDownloadButton>button, .stDownloadButton button {
        background: linear-gradient(180deg,#007aff,#005ecb) !important;
        color: #ffffff !important;
        border: 1px solid #0051b8 !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
        padding: 0.5rem 1rem !important;
        box-shadow: 0 4px 10px rgba(0,122,255,0.35),
                    inset 0 -1px 0 rgba(255,255,255,0.2) !important;
        transition: all 0.25s ease-in-out !important;
    }
    button[kind="primary"]:hover, .stButton>button:hover, .stDownloadButton>button:hover {
        background: linear-gradient(180deg,#339dff,#006ae6) !important;
        box-shadow: 0 4px 14px rgba(0,122,255,0.45) !important;
        transform: translateY(-1px) !important;
    }
    button[kind="primary"]:active, .stButton>button:active, .stDownloadButton>button:active {
        background: linear-gradient(180deg,#004fc4,#0042a8) !important;
        box-shadow: inset 0 2px 6px rgba(0,122,255,0.3) !important;
        transform: translateY(0) !important;
    }
    .stButton button[disabled], .stDownloadButton button[disabled] {
        background: #1e293b !important;
        color: #64748b !important;
        border: 1px solid #334155 !important;
    }

    /* ===============================================
    üß† INPUTS (Text, Select, Number) & FOCUS STATE
    =============================================== */
    .stTextInput>div>div>input,
    .stSelectbox>div>div>div,
    .stNumberInput input {
        background: #111827 !important;
        color: #f8fafc !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 8px !important;
        padding: 6px 10px !important;
        transition: all 0.25s ease;
    }
    .stTextInput>div>div>input:focus,
    .stSelectbox>div>div>div:focus-within,
    .stNumberInput input:focus {
        outline: none !important;
        border-color: #007aff !important;
        box-shadow: 0 0 0 2px rgba(0,122,255,0.4) !important;
    }
    ::placeholder {
        color: #9ca3af !important;
        opacity: 1 !important;
    }
    /* ===============================================
   üéõ DROPDOWN MENUS
    =============================================== */
    [data-baseweb="popover"], [role="listbox"] {
        background: #0f172a !important;
        color: #f8fafc !important;
        border: 1px solid #1e3a8a !important;
        box-shadow: 0 4px 20px rgba(0,0,0,0.6) !important;
    }
    [data-baseweb="menu-item"] {
        background: #0f172a !important;
        color: #f8fafc !important;
    }
    [data-baseweb="menu-item"]:hover {
        background: #1e3a8a !important;
        color: #ffffff !important;
    }
    /* ===============================================
    üß≠ SIDEBAR THEME
    =============================================== */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg,#0d1320,#060a12) !important;
        border-right: 1px solid #1e3a8a !important;
        color: #f8fafc !important;
    }

    
    /* ===============================================
       ‚òëÔ∏è CHECKBOXES / RADIOS / SLIDERS
    =============================================== */
    input[type="checkbox"], input[type="radio"] {
        accent-color: #007aff !important;
    }
    .stSlider [role="slider"] {
        background-color: #007aff !important;
    }

    /* ===============================================
       üóÇÔ∏è TABS
    =============================================== */
    .stTabs [data-baseweb="tab-list"] button {
        color: #e2e8f0 !important;
        background: #111827 !important;
        border: 1px solid #1e293b !important;
        border-radius: 10px !important;
        font-weight: 500 !important;
        margin-right: 4px !important;
    }
    .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {
        background: #007aff !important;
        color: #ffffff !important;
        box-shadow: 0 0 12px rgba(0,122,255,0.4) !important;
    }

    /* ===============================================
       üß≠ EXPANDERS / ACCORDIONS
    =============================================== */
    .streamlit-expanderHeader {
        background: linear-gradient(90deg,#0d284d,#0a1f3a) !important;
        color: #dbeafe !important;
        border: 1px solid #1e3a5f !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
    }
    .streamlit-expanderContent {
        background: #0f172a !important;
        color: #e2e8f0 !important;
        border: 1px solid #1e3a5f !important;
        border-radius: 0 0 8px 8px !important;
    }

    /* ===============================================
       üìä METRIC CARDS (st.metric)
    =============================================== */
    [data-testid="stMetric"] {
        background: linear-gradient(180deg,#0b1220,#101a2c) !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 10px !important;
        box-shadow: inset 0 0 10px rgba(255,255,255,0.03),
                    0 3px 10px rgba(0,0,0,0.6) !important;
        padding: 10px 14px !important;
        text-align: center !important;
    }
    div[data-testid="stMetricLabel"] {
        color: #94a3b8 !important;
        font-size: 0.85rem !important;
        font-weight: 500 !important;
    }
    div[data-testid="stMetricValue"] {
        color: #ffffff !important;
        font-size: 1.3rem !important;
        font-weight: 600 !important;
    }

    /* ===============================================
       üìä METRIC COMPARISON TABLE ‚Äî FINAL
    =============================================== */
    [data-testid="stDataFrame"] {
        background: radial-gradient(circle at 50% 50%, #0b1220, #060a12 90%) !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 12px !important;
        box-shadow:
            0 0 14px rgba(0,0,0,0.6) inset,
            0 4px 18px rgba(0,0,0,0.7),
            0 0 12px rgba(0,122,255,0.15) !important;
        margin-top: 12px !important;
        padding: 8px !important;
    }
    [data-testid="stDataFrame"] thead tr th {
        background: linear-gradient(90deg,#004fc4,#007aff) !important;
        color: #ffffff !important;
        border-bottom: 2px solid #007aff !important;
        font-weight: 700 !important;
        text-transform: uppercase !important;
        letter-spacing: 0.02em !important;
        font-size: 0.92rem !important;
        padding: 10px 14px !important;
    }
    [data-testid="stDataFrame"] tbody tr {
        background-color: #0b1220 !important;
        color: #ffffff !important;
        transition: background 0.25s ease;
    }
    [data-testid="stDataFrame"] tbody tr:nth-child(even) {
        background-color: #101a2c !important;
    }
    [data-testid="stDataFrame"] tbody tr:hover {
        background-color: #112a52 !important;
        box-shadow: 0 0 8px rgba(0,122,255,0.25) inset !important;
    }
    [data-testid="stDataFrame"] tbody td {
        border-top: 1px solid #1e3a8a !important;
        color: #ffffff !important;
        padding: 9px 14px !important;
        font-size: 0.95rem !important;
        font-weight: 500 !important;
    }
    [data-testid="stDataFrame"] tbody td:last-child {
        color: #60a5fa !important;
        font-weight: 500 !important;
    }

    /* ===============================================
       üìÅ FILE UPLOADER
    =============================================== */
    [data-testid="stFileUploaderDropzone"] {
        background: rgba(255,255,255,0.03) !important;
        border: 1px dashed #1e3a8a !important;
        border-radius: 10px !important;
        color: #cbd5e1 !important;
        transition: all 0.25s ease;
    }
    [data-testid="stFileUploaderDropzone"]:hover {
        border-color: #007aff !important;
        background: rgba(0,122,255,0.1) !important;
    }

    /* ===============================================
       ‚ö†Ô∏è ALERT BOXES
    =============================================== */
    [data-testid^="stAlert"] {
        border-radius: 10px !important;
        border: 1px solid #1e3a8a !important;
        color: #e2e8f0 !important;
        box-shadow: 0 3px 15px rgba(0,0,0,0.4) !important;
    }
    [data-testid="stAlertInfo"]    { background: linear-gradient(145deg,#0d1829,#10243d)!important; }
    [data-testid="stAlertSuccess"] { background: linear-gradient(145deg,#0f2414,#183820)!important; }
    [data-testid="stAlertError"]   { background: linear-gradient(145deg,#2b1617,#1a0c0d)!important; }
    [data-testid="stAlertWarning"] { background: linear-gradient(145deg,#2f2a10,#1c1a0a)!important; }

    </style>
    """, unsafe_allow_html=True)



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üß≠ THEME BOOTSTRAP ‚Äî Default to DARK
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import streamlit as st

# Define default session theme early
if "ui_theme" not in st.session_state:
    st.session_state["ui_theme"] = "dark"

# Apply immediately before any Streamlit renders
apply_theme(st.session_state["ui_theme"])

# ‚úÖ Then continue with Streamlit config
st.set_page_config(page_title="Credit Appraisal Agent", layout="wide")



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# UTILITIES ‚Äî DataFrame selection helpers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ---- DataFrame selection helpers (avoid boolean ambiguity) ----
def first_nonempty_df(*candidates):
    """Return the first candidate that is a non-empty pandas DataFrame, else None."""
    for df in candidates:
        if isinstance(df, pd.DataFrame) and not df.empty:
            return df
    return None

def is_nonempty_df(x) -> bool:
    return isinstance(x, pd.DataFrame) and not x.empty

def render_nav_bar_app():
    stage = st.session_state.get("stage", "landing")

    # three columns: home, agents, theme toggle
    c1, c2, c3 = st.columns([1, 1, 2.5])

    with c1:
        if st.button("üè† Back to Home", key=f"btn_home_{stage}"):
            _go_stage("landing")
            st.stop()

    with c2:
        if st.button("ü§ñ Back to Agents", key=f"btn_agents_{stage}"):
            _go_stage("agents")
            st.stop()

    with c3:
        is_dark = (ss.get("ui_theme", "dark") == "dark")
        new_is_dark = st.toggle("üåô Dark mode", value=is_dark, key="ui_theme_toggle", help="Switch theme")
        new_theme = "dark" if new_is_dark else "light"
        if new_theme != ss["ui_theme"]:
            ss["ui_theme"] = new_theme
            apply_theme(ss["ui_theme"])

    st.markdown("---")



# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # üß≠ GLOBAL NAVBAR + THEME SWITCHER (Fixed Alignment)
# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def render_nav_bar_app():
#     """Top navigation bar with Home, Agents, and Theme Switch."""
#     import streamlit as st

#     # Get active stage
#     active_stage = st.session_state.get("stage", "")
#     home_active = "color:#60a5fa;" if active_stage == "landing" else ""
#     agent_active = "color:#60a5fa;" if active_stage == "agents" else ""

#     # --- Navbar CSS ---
#     st.markdown("""
#     <style>
#     .topnav {
#         display: flex;
#         justify-content: space-between;
#         align-items: center;
#         background: linear-gradient(90deg,#0d1320,#0b1220);
#         border-bottom: 1px solid #1e3a8a;
#         padding: 0.7rem 1.6rem;
#         border-radius: 10px;
#         box-shadow: 0 2px 14px rgba(0,0,0,0.6);
#         position: sticky;
#         top: 0;
#         z-index: 999;
#     }
#     .nav-links a {
#         color: #f1f5f9;
#         font-weight: 600;
#         text-decoration: none;
#         margin-right: 1.5rem;
#         font-family: "Inter", "SF Pro Display", "Segoe UI", sans-serif;
#         transition: color 0.25s ease;
#     }
#     .nav-links a:hover { color: #60a5fa; }
#     .theme-label {
#         font-size: 0.9rem;
#         color: #93a4b8;
#         font-weight: 500;
#         margin-right: 6px;
#     }
#     </style>
#     """, unsafe_allow_html=True)

#     # --- Navbar Layout ---
#     c1, c2, c3 = st.columns([1, 5, 1])
#     with c2:
#         st.markdown(f"""
#         <div class="topnav">
#             <div class="nav-links">
#                 <a href="?stage=landing" style="{home_active}">üè† Home</a>
#                 <a href="?stage=agents" style="{agent_active}">ü§ñ Agents</a>
#             </div>
#             <div class="theme-label">üåó Theme Mode</div>
#         </div>
#         """, unsafe_allow_html=True)

#     # --- Toggle logic (sync with session state) ---
#     toggle = st.toggle(
#         "Dark Mode",
#         value=(st.session_state.get("ui_theme", "dark") == "dark"),
#         key="navbar_theme_toggle"
#     )
#     new_theme = "dark" if toggle else "light"
#     if new_theme != st.session_state.get("ui_theme", "dark"):
#         st.session_state["ui_theme"] = new_theme
#         apply_theme(new_theme)
#         st.rerun()


# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # üöÄ Apply theme & set page config
# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# apply_theme(st.session_state["ui_theme"])
# st.set_page_config(page_title="üè¶ Asset Appraisal Agent", layout="wide")


# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # üß≠ THEME BOOTSTRAP ‚Äî Dynamic Dark/Light Mode
# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# import streamlit as st

# # Initialize default theme
# if "ui_theme" not in st.session_state:
#     st.session_state["ui_theme"] = "dark"

# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # üé® THEME DEFINITIONS (macOS style)
# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def apply_theme(theme: str = "dark"):
#     """Apply macOS-style theme (light/dark) dynamically."""
#     if theme == "light":
#         bg      = "#f8fafc"
#         text    = "#0f172a"
#         subtext = "#334155"
#         card    = "#ffffff"
#         border  = "#cbd5e1"
#         accent  = "#007aff"
#         accent2 = "#22c55e"
#         tab_bg  = "#f1f5f9"
#         table_bg= "#ffffff"
#         table_head_bg = "#e2e8f0"
#         table_head_tx = "#0f172a"
#     else:  # dark
#         bg      = "#0b1220"
#         text    = "#f8fafc"
#         subtext = "#93a4b8"
#         card    = "#0f172a"
#         border  = "#1e3a8a"
#         accent  = "#007aff"
#         accent2 = "#22c55e"
#         tab_bg  = "#111827"
#         table_bg= "#0d1525"
#         table_head_bg = "#1e3a8a"
#         table_head_tx = "#e2e8f0"

#     st.markdown(f"""
#     <style>
#       html, body, [class*="stAppViewContainer"], .stApp {{
#         background: {bg} !important;
#         color: {text} !important;
#       }}

#       /* Buttons */
#       .stButton>button {{
#         background: {accent} !important;
#         color: #fff !important;
#         border-radius: 8px !important;
#         border: 1px solid {border} !important;
#         font-weight: 600 !important;
#         transition: all 0.2s ease-in-out;
#       }}
#       .stButton>button:hover {{
#         filter: brightness(1.1);
#       }}

#       /* Tabs */
#       .stTabs [data-baseweb="tab-list"] button {{
#         background: {tab_bg} !important;
#         color: {text} !important;
#         border-radius: 8px !important;
#         border: 1px solid {border} !important;
#         margin-right: 4px !important;
#       }}
#       .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {{
#         background: {accent} !important;
#         color: #ffffff !important;
#       }}

#       /* DataFrame Table */
#       [data-testid="stDataFrame"] {{
#         background: {table_bg} !important;
#         border: 1px solid {border} !important;
#         border-radius: 10px !important;
#         box-shadow: 0 0 12px rgba(0,0,0,0.3) inset !important;
#         margin-top: 10px !important;
#       }}
#       [data-testid="stDataFrame"] thead tr th {{
#         background: {table_head_bg} !important;
#         color: {table_head_tx} !important;
#         font-weight: 700 !important;
#         border-bottom: 2px solid {accent} !important;
#       }}
#       [data-testid="stDataFrame"] tbody td {{
#         color: {text} !important;
#         border-top: 1px solid {border} !important;
#       }}

#       /* Sidebar */
#       section[data-testid="stSidebar"] {{
#         background: {card} !important;
#         border-right: 1px solid {border} !important;
#       }}
#       section[data-testid="stSidebar"] * {{
#         color: {text} !important;
#       }}

#       /* Headings */
#       h1, h2, h3, h4, h5, h6 {{
#         color: {text} !important;
#       }}

#       /* Rules */
#       hr {{
#         border-color: {border} !important;
#       }}
#     </style>
#     """, unsafe_allow_html=True)






# def apply_theme(theme: str = "dark"):
#     import streamlit as st

#     st.markdown("""
#     <style>
#     /* ===============================================
#        üåô MACOS BLUE DARK THEME ‚Äî GLOBAL BASE
#     =============================================== */
#     html, body, [data-testid="stAppViewContainer"] {
#         background: radial-gradient(circle at 20% 20%, #0b0f16, #060a12 85%) !important;
#         color: #f8fafc !important;
#         font-family: "Inter","SF Pro Display","Segoe UI",system-ui,sans-serif !important;
#     }

#     h1,h2,h3,h4,h5,h6 {
#         color: #f8fafc !important;
#         font-weight: 700 !important;
#         letter-spacing: -0.02em !important;
#     }

#     p, li, label, span, div {
#         color: #e2e8f0 !important;
#     }
#     small, .stCaption { color: #94a3b8 !important; }

#     a, a:link, a:visited { color: #339dff !important; }
#     a:hover { color: #60a5fa !important; text-decoration: underline; }

#     hr {
#         border: none !important;
#         height: 1px !important;
#         background: linear-gradient(90deg,transparent,#007aff,transparent) !important;
#     }

#     /* ===============================================
#        üß± CONTAINERS & CARDS
#     =============================================== */
#     .stMarkdown, .stContainer, .stAlert, [class*="stCard"], [class*="block-container"] {
#         background: #0f172a !important;
#         border: 1px solid #1e3a8a !important;
#         border-radius: 12px !important;
#         box-shadow: 0 4px 16px rgba(0,0,0,0.5) !important;
#     }

#     /* ===============================================
#        üîò BUTTONS ‚Äî macOS BLUE
#     =============================================== */
#     button[kind="primary"], .stButton>button, .stDownloadButton>button, .stDownloadButton button {
#         background: linear-gradient(180deg,#007aff,#005ecb) !important;
#         color: #ffffff !important;
#         border: 1px solid #0051b8 !important;
#         border-radius: 8px !important;
#         font-weight: 600 !important;
#         padding: 0.5rem 1rem !important;
#         box-shadow: 0 4px 10px rgba(0,122,255,0.35),
#                     inset 0 -1px 0 rgba(255,255,255,0.2) !important;
#         transition: all 0.25s ease-in-out !important;
#     }
#     button[kind="primary"]:hover, .stButton>button:hover, .stDownloadButton>button:hover {
#         background: linear-gradient(180deg,#339dff,#006ae6) !important;
#         box-shadow: 0 4px 14px rgba(0,122,255,0.45) !important;
#         transform: translateY(-1px) !important;
#     }
#     button[kind="primary"]:active, .stButton>button:active, .stDownloadButton>button:active {
#         background: linear-gradient(180deg,#004fc4,#0042a8) !important;
#         box-shadow: inset 0 2px 6px rgba(0,122,255,0.3) !important;
#         transform: translateY(0) !important;
#     }
#     .stButton button[disabled], .stDownloadButton button[disabled] {
#         background: #1e293b !important;
#         color: #64748b !important;
#         border: 1px solid #334155 !important;
#     }

#     /* ===============================================
#     üß† INPUTS (Text, Select, Number) & FOCUS STATE
#     =============================================== */
#     .stTextInput>div>div>input,
#     .stSelectbox>div>div>div,
#     .stNumberInput input {
#         background: #111827 !important;
#         color: #f8fafc !important;
#         border: 1px solid #1e3a8a !important;
#         border-radius: 8px !important;
#         padding: 6px 10px !important;
#         transition: all 0.25s ease;
#     }
#     .stTextInput>div>div>input:focus,
#     .stSelectbox>div>div>div:focus-within,
#     .stNumberInput input:focus {
#         outline: none !important;
#         border-color: #007aff !important;
#         box-shadow: 0 0 0 2px rgba(0,122,255,0.4) !important;
#     }
#     ::placeholder {
#         color: #9ca3af !important;
#         opacity: 1 !important;
#     }
#     /* ===============================================
#    üéõ DROPDOWN MENUS
#     =============================================== */
#     [data-baseweb="popover"], [role="listbox"] {
#         background: #0f172a !important;
#         color: #f8fafc !important;
#         border: 1px solid #1e3a8a !important;
#         box-shadow: 0 4px 20px rgba(0,0,0,0.6) !important;
#     }
#     [data-baseweb="menu-item"] {
#         background: #0f172a !important;
#         color: #f8fafc !important;
#     }
#     [data-baseweb="menu-item"]:hover {
#         background: #1e3a8a !important;
#         color: #ffffff !important;
#     }
#     /* ===============================================
#     üß≠ SIDEBAR THEME
#     =============================================== */
#     [data-testid="stSidebar"] {
#         background: linear-gradient(180deg,#0d1320,#060a12) !important;
#         border-right: 1px solid #1e3a8a !important;
#         color: #f8fafc !important;
#     }

    
#     /* ===============================================
#        ‚òëÔ∏è CHECKBOXES / RADIOS / SLIDERS
#     =============================================== */
#     input[type="checkbox"], input[type="radio"] {
#         accent-color: #007aff !important;
#     }
#     .stSlider [role="slider"] {
#         background-color: #007aff !important;
#     }

#     /* ===============================================
#        üóÇÔ∏è TABS
#     =============================================== */
#     .stTabs [data-baseweb="tab-list"] button {
#         color: #e2e8f0 !important;
#         background: #111827 !important;
#         border: 1px solid #1e293b !important;
#         border-radius: 10px !important;
#         font-weight: 500 !important;
#         margin-right: 4px !important;
#     }
#     .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {
#         background: #007aff !important;
#         color: #ffffff !important;
#         box-shadow: 0 0 12px rgba(0,122,255,0.4) !important;
#     }

#     /* ===============================================
#        üß≠ EXPANDERS / ACCORDIONS
#     =============================================== */
#     .streamlit-expanderHeader {
#         background: linear-gradient(90deg,#0d284d,#0a1f3a) !important;
#         color: #dbeafe !important;
#         border: 1px solid #1e3a5f !important;
#         border-radius: 8px !important;
#         font-weight: 600 !important;
#     }
#     .streamlit-expanderContent {
#         background: #0f172a !important;
#         color: #e2e8f0 !important;
#         border: 1px solid #1e3a5f !important;
#         border-radius: 0 0 8px 8px !important;
#     }

#     /* ===============================================
#        üìä METRIC CARDS (st.metric)
#     =============================================== */
#     [data-testid="stMetric"] {
#         background: linear-gradient(180deg,#0b1220,#101a2c) !important;
#         border: 1px solid #1e3a8a !important;
#         border-radius: 10px !important;
#         box-shadow: inset 0 0 10px rgba(255,255,255,0.03),
#                     0 3px 10px rgba(0,0,0,0.6) !important;
#         padding: 10px 14px !important;
#         text-align: center !important;
#     }
#     div[data-testid="stMetricLabel"] {
#         color: #94a3b8 !important;
#         font-size: 0.85rem !important;
#         font-weight: 500 !important;
#     }
#     div[data-testid="stMetricValue"] {
#         color: #ffffff !important;
#         font-size: 1.3rem !important;
#         font-weight: 600 !important;
#     }

#     /* ===============================================
#        üìä METRIC COMPARISON TABLE ‚Äî FINAL
#     =============================================== */
#     [data-testid="stDataFrame"] {
#         background: radial-gradient(circle at 50% 50%, #0b1220, #060a12 90%) !important;
#         border: 1px solid #1e3a8a !important;
#         border-radius: 12px !important;
#         box-shadow:
#             0 0 14px rgba(0,0,0,0.6) inset,
#             0 4px 18px rgba(0,0,0,0.7),
#             0 0 12px rgba(0,122,255,0.15) !important;
#         margin-top: 12px !important;
#         padding: 8px !important;
#     }
#     [data-testid="stDataFrame"] thead tr th {
#         background: linear-gradient(90deg,#004fc4,#007aff) !important;
#         color: #ffffff !important;
#         border-bottom: 2px solid #007aff !important;
#         font-weight: 700 !important;
#         text-transform: uppercase !important;
#         letter-spacing: 0.02em !important;
#         font-size: 0.92rem !important;
#         padding: 10px 14px !important;
#     }
#     [data-testid="stDataFrame"] tbody tr {
#         background-color: #0b1220 !important;
#         color: #ffffff !important;
#         transition: background 0.25s ease;
#     }
#     [data-testid="stDataFrame"] tbody tr:nth-child(even) {
#         background-color: #101a2c !important;
#     }
#     [data-testid="stDataFrame"] tbody tr:hover {
#         background-color: #112a52 !important;
#         box-shadow: 0 0 8px rgba(0,122,255,0.25) inset !important;
#     }
#     [data-testid="stDataFrame"] tbody td {
#         border-top: 1px solid #1e3a8a !important;
#         color: #ffffff !important;
#         padding: 9px 14px !important;
#         font-size: 0.95rem !important;
#         font-weight: 500 !important;
#     }
#     [data-testid="stDataFrame"] tbody td:last-child {
#         color: #60a5fa !important;
#         font-weight: 500 !important;
#     }

#     /* ===============================================
#        üìÅ FILE UPLOADER
#     =============================================== */
#     [data-testid="stFileUploaderDropzone"] {
#         background: rgba(255,255,255,0.03) !important;
#         border: 1px dashed #1e3a8a !important;
#         border-radius: 10px !important;
#         color: #cbd5e1 !important;
#         transition: all 0.25s ease;
#     }
#     [data-testid="stFileUploaderDropzone"]:hover {
#         border-color: #007aff !important;
#         background: rgba(0,122,255,0.1) !important;
#     }

#     /* ===============================================
#        ‚ö†Ô∏è ALERT BOXES
#     =============================================== */
#     [data-testid^="stAlert"] {
#         border-radius: 10px !important;
#         border: 1px solid #1e3a8a !important;
#         color: #e2e8f0 !important;
#         box-shadow: 0 3px 15px rgba(0,0,0,0.4) !important;
#     }
#     [data-testid="stAlertInfo"]    { background: linear-gradient(145deg,#0d1829,#10243d)!important; }
#     [data-testid="stAlertSuccess"] { background: linear-gradient(145deg,#0f2414,#183820)!important; }
#     [data-testid="stAlertError"]   { background: linear-gradient(145deg,#2b1617,#1a0c0d)!important; }
#     [data-testid="stAlertWarning"] { background: linear-gradient(145deg,#2f2a10,#1c1a0a)!important; }

#     </style>
#     """, unsafe_allow_html=True)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CREDIT AGENT ‚Äî HEADER (used in credit flow)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import streamlit as st  # (no-op if already imported)

def render_credit_header():
    ss = st.session_state

    # pick a display name if available
    user = (
        (ss.get("credit_user") or {}).get("name")
        or (ss.get("asset_user") or {}).get("name")
        or (ss.get("user_info") or {}).get("name")
        or "guest"
    )

    # use your theme switch (defaults to dark)
    theme = ss.get("theme", "dark")
    brand = {
        "dark": {"text":"#e2e8f0","muted":"#94a3b8","accent":"#3b82f6"},
        "light":{"text":"#0f172a","muted":"#475569","accent":"#2563eb"},
    }[theme]

    st.title("üè¶ Credit Appraisal Agent")
    st.caption(
        "A‚ÜíH pipeline ‚Äî Intake ‚Üí Privacy ‚Üí Credit Appraisal ‚Üí Human Review ‚Üí "
        "Training ‚Üí Deployment ‚Üí Monitoring ‚Üí Reporting "
        f"| üëã {user}"
    )

# ‚úÖ JSON ‚Üí DataFrame converter (final, unified, safe)
# ============================================================
def json_to_dataframe(payload) -> pd.DataFrame:
    """
    Convert arbitrary API JSON (dict/list/bytes/str) into a DataFrame.
    Prefers server 'artifacts.merged_csv' ‚Üí fallback to json_normalize.
    """

    # -------------------------------
    # Case 1: payload is dict
    # -------------------------------
    if isinstance(payload, dict):

        # ‚úÖ Try artifacts.merged_csv first
        res = payload.get("result") or payload
        artifacts = res.get("artifacts") or {}
        merged_csv = artifacts.get("merged_csv")

        if isinstance(merged_csv, str) and os.path.exists(merged_csv):
            try:
                return pd.read_csv(merged_csv)
            except Exception:
                pass

        # ‚úÖ Embedded merged_df inside the JSON
        if "merged_df" in res:
            try:
                return pd.DataFrame(res["merged_df"])
            except Exception:
                pass

        # ‚úÖ If result is list ‚Üí DF
        if isinstance(res, list):
            try:
                return pd.DataFrame(res)
            except Exception:
                try:
                    return pd.json_normalize(res)
                except Exception:
                    pass

        # ‚úÖ Try keys inside result
        for key in ("rows", "data", "result", "results", "items", "records"):
            if key in res:
                try:
                    return json_to_dataframe(res[key])
                except Exception:
                    pass

    # -------------------------------
    # Case 2: payload is list
    # -------------------------------
    if isinstance(payload, list):
        if len(payload) == 0:
            return pd.DataFrame()
        if all(isinstance(x, dict) for x in payload):
            try:
                return pd.DataFrame(payload)
            except:
                return pd.json_normalize(payload)
        return pd.DataFrame({"value": payload})

    # -------------------------------
    # Case 3: payload is bytes
    # -------------------------------
    if isinstance(payload, bytes):
        try:
            payload = payload.decode("utf-8", errors="ignore")
        except:
            return pd.DataFrame({"value": [repr(payload)]})

    # -------------------------------
    # Case 4: payload is str ‚Üí try JSON parse
    # -------------------------------
    if isinstance(payload, str):
        payload = payload.strip()
        if not payload:
            return pd.DataFrame()
        try:
            j = json.loads(payload)
            return json_to_dataframe(j)
        except:
            # Fallback ‚Üí line-by-line DF
            lines = [ln for ln in payload.splitlines() if ln.strip()]
            return pd.DataFrame({"value": lines}) if lines else pd.DataFrame()

    # -------------------------------
    # Default fallback
    # -------------------------------
    return pd.DataFrame({"value": [payload]})



def _extract_run_fields(raw_json):  # ADD
    """
    Return (run_id, normalized_payload_dict).
    Ensures downstream code always receives a dict-like 'payload'.
    """
    run_id = extract_run_id(raw_json)

    # Normalize to dict payload so later code can access keys safely
    payload = raw_json
    if not isinstance(payload, dict):
        if isinstance(payload, list):
            first_dict = next((x for x in payload if isinstance(x, dict)), None)
            payload = first_dict if first_dict is not None else {"result": raw_json}
        else:
            payload = {"result": raw_json}
    return run_id, payload





# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# PAGE CONFIG & THEME
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.set_page_config(
    page_title="üí≥ Credit Appraisal",
    page_icon="üí≥",
    layout="wide",
    initial_sidebar_state="collapsed",
)
#



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SESSION STATE INIT
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if "stage" not in st.session_state:
    st.session_state.stage = "landing"
if "user_info" not in st.session_state:
    st.session_state.user_info = {"name": "", "email": "", "flagged": False}
if "logged_in" not in st.session_state:
    st.session_state.logged_in = False
if "flagged" not in st.session_state.user_info:
    st.session_state.user_info["flagged"] = False
if "timestamp" not in st.session_state.user_info:
    st.session_state.user_info["timestamp"] = datetime.now(timezone.utc).isoformat()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# HELPERS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _extract_run_fields(res_json):
    """
    Return (run_id, result_dict) from API responses that may be dicts or lists.
    """
    run_id = None
    result_obj = {}

    if isinstance(res_json, dict):
        run_id = (
            res_json.get("run_id")
            or res_json.get("id")
            or (res_json.get("data") or {}).get("run_id")
        )
        result_obj = (
            res_json.get("result")
            or (res_json.get("data") or {}).get("result")
            or {}
        )

    elif isinstance(res_json, list):
        # Find first dict item that contains identifiers/results
        for item in res_json:
            if isinstance(item, dict):
                if not run_id:
                    run_id = item.get("run_id") or item.get("id")
                if not result_obj:
                    result_obj = item.get("result") or {}
                if run_id and result_obj != {}:
                    break
        # If still nothing and list[0] is a dict, use it as best-effort
        if not run_id and res_json and isinstance(res_json[0], dict):
            run_id = res_json[0].get("run_id") or res_json[0].get("id")
            result_obj = res_json[0].get("result") or {}

    # Ensure result is a dict
    if not isinstance(result_obj, dict):
        result_obj = {"value": result_obj}

    return run_id, result_obj


def _clear_qp():
    """Clear query params (modern Streamlit API)."""
    try:
        st.query_params.clear()
    except Exception:
        pass


def load_image(base: str) -> Optional[str]:
    for ext in [".png", ".jpg", ".jpeg", ".webp", ".gif", ".svg"]:
        p = os.path.join(LANDING_IMG_DIR, f"{base}{ext}")
        if os.path.exists(p):
            return p
    return None


def save_uploaded_image(uploaded_file, base: str):
    if not uploaded_file:
        return None
    ext = os.path.splitext(uploaded_file.name)[1].lower() or ".png"
    dest = os.path.join(LANDING_IMG_DIR, f"{base}{ext}")
    with open(dest, "wb") as f:
        f.write(uploaded_file.getvalue())
    return dest


def render_image_tag(agent_id: str, industry: str, emoji_fallback: str) -> str:
    base = agent_id.lower().replace(" ", "_")
    img_path = load_image(base) or load_image(industry.replace(" ", "_"))
    if img_path:
        return (
            f'<img src="file://{img_path}" '
            f'style="width:48px;height:48px;border-radius:10px;object-fit:cover;">'
        )
    return f'<div style="font-size:32px;">{emoji_fallback}</div>'




st.markdown(
    """
    <style>
    [data-testid="stSidebar"], section[data-testid="stSidebar"]{display:none!important}
    [data-testid="stAppViewContainer"]{margin-left:0!important;padding-left:0!important}
    </style>
    """,
    unsafe_allow_html=True,
)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CONSTANTS / PATHS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# You can point this to your FastAPI host
API_URL = os.environ.get("AGENT_API_URL", "http://localhost:8090")

# Base & temp runs folder
BASE_DIR = os.path.abspath(".")
RUNS_DIR = os.path.join(BASE_DIR, ".tmp_runs")
os.makedirs(RUNS_DIR, exist_ok=True)





# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # UNIVERSAL TOP NAVIGATION + THEME TOGGLE (fixed)
# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def render_nav_bar_app():
#     import streamlit as st
#     ss = st.session_state  # ‚úÖ define session alias

#     # --- stage & visibility
#     stage = ss.get("stage", "landing")
#     show_home   = stage in ("agents", "credit_agent", "asset_agent")
#     show_agents = stage not in ("landing", "agents")

#     # nothing to render on pure landing
#     if not (show_home or show_agents):
#         return

#     # --- theme state: migrate old key and set default
#     # Theme state
#     ss.setdefault("theme", "dark")

    
#     # if "theme" not in ss and "ui_theme" in ss:
#     #     ss["theme"] = ss["ui_theme"]
#     # ss.setdefault("theme", "dark")
#     # ss["ui_theme"] = ss.get("theme", "dark")  # keep both in sync

#     # three columns: home, agents, theme toggle
#     c1, c2, c3 = st.columns([1, 1, 2.5])

#     with c1:
#         if show_home and st.button("üè† Back to Home", key=f"btn_home_{stage}"):
#             _go_stage("landing")
#             st.stop()

#     with c2:
#         if show_agents and st.button("ü§ñ Back to Agents", key=f"btn_agents_{stage}"):
#             _go_stage("agents")
#             st.stop()

#     with c3:
#         is_dark = (ss.get("theme", "dark") == "dark")
#         new_is_dark = st.toggle(
#             "üåô Dark mode",
#             value=is_dark,
#             key="ui_theme_toggle",
#             help="Switch theme"
#         )
#         new_theme = "dark" if new_is_dark else "light"
#         if new_theme != ss.get("theme"):
#             ss["theme"] = new_theme      # ‚úÖ primary key
#             ss["ui_theme"] = new_theme   # ‚úÖ legacy key stays in sync
#             apply_theme(ss["theme"])     # your existing helper

#     st.markdown("---")








# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# LOGIN GATE
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚úÖ Always show navbar (Home / Agents / Theme)
render_nav_bar_app()

def login_block():
    st.title("üîê Login to AI Credit Appraisal Platform")
    c1, c2, c3 = st.columns([1, 1, 1])
    with c1:
        user = st.text_input("Username", placeholder="e.g. dzoan")
    with c2:
        email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
    with c3:
        pwd = st.text_input("Password", type="password", placeholder="Enter any password")

    if st.button("Login", key="btn_credit_login", use_container_width=True):
        if (user or "").strip() and (email or "").strip():
            st.session_state["user_info"] = {
                "name": user.strip(),
                "email": email.strip(),
                "flagged": False,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
            st.session_state["credit_logged_in"] = True
            st.session_state["stage"] = "credit_agent"
            st.rerun()
        else:
            st.error("‚ö†Ô∏è Please fill all fields before continuing.")


# ‚úÖ If not logged in ‚Üí show login
if not st.session_state.get("credit_logged_in", False):
    login_block()
    st.stop()




# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # LOGIN GATE
# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def login_block():
#     st.title("üîê Login to AI Credit Appraisal Platform")
#     c1, c2, c3 = st.columns([1, 1, 1])
#     with c1:
#         user = st.text_input("Username", placeholder="e.g. dzoan")
#     with c2:
#         email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
#     with c3:
#         pwd = st.text_input("Password", type="password", placeholder="Enter any password")

#     if st.button("Login", key="btn_credit_login", use_container_width=True):
#         if (user or "").strip() and (email or "").strip():
#             st.session_state["user_info"] = {
#                 "name": user.strip(),
#                 "email": email.strip(),
#                 "flagged": False,
#                 "timestamp": datetime.now(timezone.utc).isoformat(),
#             }
#             st.session_state["credit_logged_in"] = True
#             st.session_state["stage"] = "credit_agent"
#             st.rerun()
#         else:
#             st.error("‚ö†Ô∏è Please fill all fields before continuing.")


# if not st.session_state.get("credit_logged_in", False):
#     login_block()
#     st.stop()





# -----------------------------------------------------------
# CREDIT WORKFLOW ACTIVE ONLY IF CREDIT AGENT SELECTED
# -----------------------------------------------------------
ss = st.session_state
stage = ss.get("stage")

if stage == "credit_agent":

    # Header (from your render_credit_header() defined earlier)
    render_credit_header()


       
# ‚úÖ CREDIT APPRAISAL WORKFLOW TABS (1 ‚Üí 8)

    # ============================================================
    # üåà Colorized Tabs (Matches A‚ÜíH badge palette exactly)
    # ============================================================
    st.markdown("""
    <style>
    /* --- Layout adjustments for clean alignment --- */
    .stTabs [data-baseweb="tab-list"] {
    border-bottom: 2px solid #1e293b;
    justify-content: flex-start;
    flex-wrap: wrap;
    gap: .25rem;
    }

    /* --- Base style for all tabs --- */
    .stTabs [data-baseweb="tab"] {
    border-radius: .6rem;
    padding: .45rem .9rem;
    font-weight: 600;
    color: #fff !important;
    border: none;
    opacity: 0.95;
    transition: all 0.2s ease-in-out;
    }

    /* Hover and active effects */
    .stTabs [data-baseweb="tab"]:hover {
    transform: translateY(-2px);
    filter: brightness(1.1);
    opacity: 1;
    }
    .stTabs [data-baseweb="tab"][aria-selected="true"] {
    box-shadow: 0 0 8px rgba(255,255,255,0.15);
    transform: translateY(-1px);
    filter: brightness(1.1);
    }

    /* --- Tab Colors: A‚ÜíH palette --- */
    .stTabs [data-baseweb="tab"]:nth-child(1) { background: #1d4ed8; }  /* A) Blue */
    .stTabs [data-baseweb="tab"]:nth-child(2) { background: #059669; }  /* B) Green */
    .stTabs [data-baseweb="tab"]:nth-child(3) { background: #d97706; }  /* C) Amber */
    .stTabs [data-baseweb="tab"]:nth-child(4) { background: #7c3aed; }  /* D) Violet */
    .stTabs [data-baseweb="tab"]:nth-child(5) { background: #a16207; }  /* E) Gold */
    .stTabs [data-baseweb="tab"]:nth-child(6) { background: #e11d48; }  /* F) Red */
    .stTabs [data-baseweb="tab"]:nth-child(7) { background: #0ea5e9; }  /* G) Cyan */
    .stTabs [data-baseweb="tab"]:nth-child(8) { background: #64748b; }  /* H) Slate */
    </style>
    """, unsafe_allow_html=True)


# TABSLIST =====================================================
    tab_input, tab_clean, tab_run, tab_review, tab_train, tab_deploy, tab_handoff, tab_feedback = st.tabs([
        "1Ô∏è‚É£ üè¶ Synthetic Data Generator",
        "2Ô∏è‚É£ üßπ Anonymize & Sanitize Data",
        "3Ô∏è‚É£ ü§ñ Credit appraisal by AI assistant",
        "4Ô∏è‚É£ üßë‚Äç‚öñÔ∏è Human Review",
        "5Ô∏è‚É£ üîÅ Training (Feedback ‚Üí Retrain)",
        "6Ô∏è‚É£ üöÄ Deployment of Credit Model",
        "7Ô∏è‚É£ üì¶ Reporting & Handoff",
        "8Ô∏è‚É£ üó£Ô∏è Feedback & Feature Requests"
    ])

else:
    # Safe placeholders when not on the Credit Agent stage
    tab_input = st.container()
    tab_clean = st.container()
    tab_run = st.container()
    tab_review = st.container()
    tab_train = st.container()
    tab_deploy = st.container()
    tab_handoff = st.container()
    tab_feedback = st.container()




# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GLOBAL UTILS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

BANNED_NAMES = {"race", "gender", "religion", "ethnicity", "ssn", "national_id"}
PII_COLS = {"customer_name", "name", "email", "phone", "address", "ssn", "national_id", "dob"}

EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
PHONE_RE = re.compile(r"\+?\d[\d\-\s]{6,}\d")


def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:
    return df.loc[:, ~df.columns.duplicated(keep="last")]


def scrub_text_pii(s):
    if not isinstance(s, str):
        return s
    s = EMAIL_RE.sub("", s)
    s = PHONE_RE.sub("", s)
    return s.strip()


def drop_pii_columns(df: pd.DataFrame):
    original_cols = list(df.columns)
    keep_cols = [c for c in original_cols if all(k not in c.lower() for k in PII_COLS)]
    dropped = [c for c in original_cols if c not in keep_cols]
    out = df[keep_cols].copy()
    for c in out.select_dtypes(include="object"):
        out[c] = out[c].apply(scrub_text_pii)
    return dedupe_columns(out), dropped


def strip_policy_banned(df: pd.DataFrame) -> pd.DataFrame:
    keep = []
    for c in df.columns:
        cl = c.lower()
        if cl in BANNED_NAMES:
            continue
        keep.append(c)
    return df[keep]


def append_user_info(df: pd.DataFrame) -> pd.DataFrame:
    meta = st.session_state["user_info"]
    out = df.copy()
    out["session_user_name"] = meta["name"]
    out["session_user_email"] = meta["email"]
    out["session_flagged"] = meta["flagged"]
    out["created_at"] = meta["timestamp"]
    return dedupe_columns(out)


def save_to_runs(df: pd.DataFrame, prefix: str) -> str:
    #ts = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M")
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M")
    flag_suffix = "_FLAGGED" if st.session_state["user_info"]["flagged"] else ""
    fname = f"{prefix}_{ts}{flag_suffix}.csv"
    fpath = os.path.join(RUNS_DIR, fname)
    dedupe_columns(df).to_csv(fpath, index=False)
    return fpath


def try_json(x):
    if isinstance(x, (dict, list)):
        return x
    if not isinstance(x, str):
        return None
    try:
        return json.loads(x)
    except Exception:
        return None


def _safe_json(x):
    if isinstance(x, dict):
        return x
    if isinstance(x, str) and x.strip():
        try:
            return json.loads(x)
        except Exception:
            return {}
    return {}


def fmt_currency_label(base: str) -> str:
    sym = st.session_state.get("currency_symbol", "")
    return f"{base} ({sym})" if sym else base


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CURRENCY CATALOG
CURRENCY_OPTIONS = {
    # code: (label, symbol, fx to apply on USD-like base generated numbers)
    "USD": ("USD $", "$", 1.0),
    "EUR": ("EUR ‚Ç¨", "‚Ç¨", 0.93),
    "GBP": ("GBP ¬£", "¬£", 0.80),
    "JPY": ("JPY ¬•", "¬•", 150.0),
    "VND": ("VND ‚Ç´", "‚Ç´", 24000.0),
}


def set_currency_defaults():
    if "currency_code" not in st.session_state:
        st.session_state["currency_code"] = "USD"
    label, symbol, fx = CURRENCY_OPTIONS[st.session_state["currency_code"]]
    st.session_state["currency_label"] = label
    st.session_state["currency_symbol"] = symbol
    st.session_state["currency_fx"] = fx


set_currency_defaults()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DASHBOARD HELPERS (Plotly, dark theme)
def _kpi_card(label: str, value: str, sublabel: str | None = None):
    st.markdown(
        f"""
        <div style="background:#0e1117;border:1px solid #2a2f3e;border-radius:12px;padding:14px 16px;margin-bottom:10px;">
          <div style="font-size:12px;color:#9aa4b2;text-transform:uppercase;letter-spacing:.06em;">{label}</div>
          <div style="font-size:28px;font-weight:700;color:#e6edf3;line-height:1.1;margin-top:2px;">{value}</div>
          {f'<div style="font-size:12px;color:#9aa4b2;margin-top:6px;">{sublabel}</div>' if sublabel else ''}
        </div>
        """,
        unsafe_allow_html=True,
    )


def render_credit_dashboard(df: pd.DataFrame, currency_symbol: str = ""):
    """
    Renders the whole dashboard (TOP-10s ‚Üí Opportunities ‚Üí KPIs & pies/bars ‚Üí Mix table).
    Keeps decision filter in the table only.
    """
    if df is None or df.empty:
        st.info("No data to visualize yet.")
        return

    cols = df.columns

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TOP 10s FIRST ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üîù Top 10 Snapshot")

    # Top 10 loans approved
    if {"decision", "loan_amount", "application_id"} <= set(cols):
        top_approved = df[df["decision"].astype(str).str.lower() == "approved"].copy()
        if not top_approved.empty:
            top_approved = top_approved.sort_values("loan_amount", ascending=False).head(10)
            fig = px.bar(
                top_approved,
                x="loan_amount",
                y="application_id",
                orientation="h",
                title="Top 10 Approved Loans",
                labels={"loan_amount": f"Loan Amount {currency_symbol}", "application_id": "Application"},
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No approved loans available to show top 10.")

    # Top 10 collateral types by average value
    if {"collateral_type", "collateral_value"} <= set(cols):
        cprof = df.groupby("collateral_type", dropna=False).agg(
            avg_value=("collateral_value", "mean"),
            cnt=("collateral_type", "count")
        ).reset_index()
        if not cprof.empty:
            cprof = cprof.sort_values("avg_value", ascending=False).head(10)
            fig = px.bar(
                cprof,
                x="avg_value",
                y="collateral_type",
                orientation="h",
                title="Top 10 Collateral Types (Avg Value)",
                labels={"avg_value": f"Avg Value {currency_symbol}", "collateral_type": "Collateral Type"},
                hover_data=["cnt"]
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    # Top 10 reasons for denial (from rule_reasons False flags)
    if "rule_reasons" in cols and "decision" in cols:
        denied = df[df["decision"].astype(str).str.lower() == "denied"].copy()
        reasons_count = {}
        for _, r in denied.iterrows():
            rr = _safe_json(r.get("rule_reasons"))
            if isinstance(rr, dict):
                for k, v in rr.items():
                    if v is False:
                        reasons_count[k] = reasons_count.get(k, 0) + 1
        if reasons_count:
            items = pd.DataFrame(sorted(reasons_count.items(), key=lambda x: x[1], reverse=True),
                                 columns=["reason", "count"]).head(10)
            fig = px.bar(
                items, x="count", y="reason", orientation="h",
                title="Top 10 Reasons for Denial",
                labels={"count": "Count", "reason": "Rule"},
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No denial reasons detected.")

    # Top 10 loan officer performance (approval rate) if officer column present
    officer_col = None
    for guess in ("loan_officer", "officer", "reviewed_by", "session_user_name"):
        if guess in cols:
            officer_col = guess
            break
    if officer_col and "decision" in cols:
        perf = (
            df.assign(is_approved=(df["decision"].astype(str).str.lower() == "approved").astype(int))
              .groupby(officer_col, dropna=False)["is_approved"]
              .agg(approved_rate="mean", n="count")
              .reset_index()
        )
        if not perf.empty:
            perf["approved_rate_pct"] = (perf["approved_rate"] * 100).round(1)
            perf = perf.sort_values(["approved_rate_pct", "n"], ascending=[False, False]).head(10)
            fig = px.bar(
                perf, x="approved_rate_pct", y=officer_col, orientation="h",
                title="Top 10 Loan Officer Approval Rate (this batch)",
                labels={"approved_rate_pct": "Approval Rate (%)", officer_col: "Officer"},
                hover_data=["n"]
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ OPPORTUNITIES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üí° Opportunities")

    # Short-term loan opportunities (simple heuristic)
    opp_rows = []
    if {"income", "loan_amount"}.issubset(cols):
        term_col = "loan_term_months" if "loan_term_months" in cols else ("loan_duration_months" if "loan_duration_months" in cols else None)
        if term_col:
            for _, r in df.iterrows():
                inc = float(r.get("income", 0) or 0)
                amt = float(r.get("loan_amount", 0) or 0)
                term = int(r.get(term_col, 0) or 0)
                dti = float(r.get("DTI", 0) or 0)
                if (term >= 36) and (amt <= inc * 0.8) and (dti <= 0.45):
                    opp_rows.append({
                        "application_id": r.get("application_id"),
                        "suggested_term": 24,
                        "loan_amount": amt,
                        "income": inc,
                        "DTI": dti,
                        "note": "Candidate for short-term plan (<=24m) based on affordability."
                    })
    if opp_rows:
        st.markdown("#### üìé Short-Term Loan Candidates")
        st.dataframe(pd.DataFrame(opp_rows).head(25), use_container_width=True, height=320)
    else:
        st.info("No short-term loan candidates identified in this batch.")

    st.markdown("#### üîÅ Buyback / Consolidation Beneficiaries")
    candidates = []
    need = {"decision", "existing_debt", "loan_amount", "DTI"}
    if need <= set(cols):
        for _, r in df.iterrows():
            dec = str(r.get("decision", "")).lower()
            debt = float(r.get("existing_debt", 0) or 0)
            loan = float(r.get("loan_amount", 0) or 0)
            dti = float(r.get("DTI", 0) or 0)
            proposal = _safe_json(r.get("proposed_consolidation_loan", {}))
            has_bb = bool(proposal)

            if dec == "denied" or dti > 0.45 or debt > loan:
                benefit_score = round((debt / (loan + 1e-6)) * 0.4 + dti * 0.6, 2)
                candidates.append({
                    "application_id": r.get("application_id"),
                    "customer_type": r.get("customer_type"),
                    "existing_debt": debt,
                    "loan_amount": loan,
                    "DTI": dti,
                    "collateral_type": r.get("collateral_type"),
                    "buyback_proposed": has_bb,
                    "buyback_amount": proposal.get("buyback_amount") if has_bb else None,
                    "benefit_score": benefit_score,
                    "note": proposal.get("note") if has_bb else None
                })
    if candidates:
        cand_df = pd.DataFrame(candidates).sort_values("benefit_score", ascending=False)
        st.dataframe(cand_df.head(25), use_container_width=True, height=380)
    else:
        st.info("No additional buyback beneficiaries identified.")

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PORTFOLIO KPIs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üìà Portfolio Snapshot")
    c1, c2, c3, c4 = st.columns(4)

    # Approval rate
    if "decision" in cols:
        total = len(df)
        approved = int((df["decision"].astype(str).str.lower() == "approved").sum())
        rate = (approved / total * 100) if total else 0.0
        with c1: _kpi_card("Approval Rate", f"{rate:.1f}%", f"{approved} of {total}")

    # Avg approved loan amount
    if {"decision", "loan_amount"} <= set(cols):
        ap = df[df["decision"].astype(str).str.lower() == "approved"]["loan_amount"]
        avg_amt = ap.mean() if len(ap) else 0.0
        with c2: _kpi_card("Avg Approved Amount", f"{currency_symbol}{avg_amt:,.0f}")

    # Decision time (if present)
    if {"created_at", "decision_at"} <= set(cols):
        try:
            t = (pd.to_datetime(df["decision_at"]) - pd.to_datetime(df["created_at"])).dt.total_seconds() / 60.0
            avg_min = float(t.mean())
            with c3: _kpi_card("Avg Decision Time", f"{avg_min:.1f} min")
        except Exception:
            with c3: _kpi_card("Avg Decision Time", "‚Äî")

    # Non-bank share
    if "customer_type" in cols:
        nb = int((df["customer_type"].astype(str).str.lower() == "non-bank").sum())
        total = len(df)
        share = (nb / total * 100) if total else 0.0
        with c4: _kpi_card("Non-bank Share", f"{share:.1f}%", f"{nb} of {total}")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ COMPOSITION & RISK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üß≠ Composition & Risk")

    # Approval vs Denial (pie)
    if "decision" in cols:
        pie_df = df["decision"].value_counts().rename_axis("Decision").reset_index(name="Count")
        fig = px.pie(pie_df, names="Decision", values="Count", title="Decision Mix")
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Avg DTI / LTV by decision (grouped bars)
    have_dti = "DTI" in cols
    have_ltv = "LTV" in cols
    if "decision" in cols and (have_dti or have_ltv):
        agg_map = {}
        if have_dti: agg_map["avg_DTI"] = ("DTI", "mean")
        if have_ltv: agg_map["avg_LTV"] = ("LTV", "mean")
        grp = df.groupby("decision").agg(**agg_map).reset_index()
        melted = grp.melt(id_vars=["decision"], var_name="metric", value_name="value")
        fig = px.bar(melted, x="decision", y="value", color="metric",
                     barmode="group", title="Average DTI / LTV by Decision")
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Loan term mix (stacked)
    term_col = "loan_term_months" if "loan_term_months" in cols else ("loan_duration_months" if "loan_duration_months" in cols else None)
    if term_col and "decision" in cols:
        mix = df.groupby([term_col, "decision"]).size().reset_index(name="count")
        fig = px.bar(
            mix, x=term_col, y="count", color="decision", title="Loan Term Mix",
            labels={term_col: "Term (months)", "count": "Count"}, barmode="stack"
        )
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Collateral avg value by type (bar)
    if {"collateral_type", "collateral_value"} <= set(cols):
        cprof = df.groupby("collateral_type").agg(
            avg_col=("collateral_value", "mean"),
            cnt=("collateral_type", "count")
        ).reset_index()
        fig = px.bar(
            cprof.sort_values("avg_col", ascending=False),
            x="collateral_type", y="avg_col",
            title=f"Avg Collateral Value by Type ({currency_symbol})",
            hover_data=["cnt"]
        )
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Top proposed plans (horizontal bar)
    if "proposed_loan_option" in cols:
        plans = df["proposed_loan_option"].dropna().astype(str)
        if len(plans) > 0:
            plan_types = []
            for s in plans:
                p = _safe_json(s)
                plan_types.append(p.get("type") if isinstance(p, dict) and "type" in p else s)
            plan_df = pd.Series(plan_types).value_counts().head(10).rename_axis("plan").reset_index(name="count")
            fig = px.bar(
                plan_df, x="count", y="plan", orientation="h",
                title="Top 10 Proposed Plans"
            )
            fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    # Customer mix table (bank vs non-bank)
    if "customer_type" in cols:
        mix = df["customer_type"].value_counts().rename_axis("Customer Type").reset_index(name="Count")
        mix["Ratio"] = (mix["Count"] / mix["Count"].sum()).round(3)
        st.markdown("### üë• Customer Mix")
        st.dataframe(mix, use_container_width=True, height=220)




# DATA GENERATORS

def generate_raw_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    names = ["Alice Nguyen","Bao Tran","Chris Do","Duy Le","Emma Tran",
             "Felix Nguyen","Giang Ho","Hanh Vo","Ivan Pham","Julia Ngo"]
    emails = [f"{n.split()[0].lower()}.{n.split()[1].lower()}@gmail.com" for n in names]
    addrs = [
        "23 Elm St, Boston, MA","19 Pine Ave, San Jose, CA","14 High St, London, UK",
        "55 Nguyen Hue, Ho Chi Minh","78 Oak St, Chicago, IL","10 Broadway, New York, NY",
        "8 Rue Lafayette, Paris, FR","21 K√∂nigstr, Berlin, DE","44 Maple Dr, Los Angeles, CA","22 Bay St, Toronto, CA"
    ]
    is_non = rng.random(n) < non_bank_ratio
    cust_type = np.where(is_non, "non-bank", "bank")

    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "customer_name": np.random.choice(names, n),
        "email": np.random.choice(emails, n),
        "phone": [f"+1-202-555-{1000+i:04d}" for i in range(n)],
        "address": np.random.choice(addrs, n),
        "national_id": rng.integers(10_000_000, 99_999_999, n),
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["real_estate","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
        "current_loans": rng.integers(0, 5, n),
        "customer_type": cust_type,
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))

    fx = st.session_state["currency_fx"]
    for c in ("income", "loan_amount", "collateral_value", "assets_owned", "existing_debt"):
        df[c] = (df[c] * fx).round(2)
    df["currency_code"] = st.session_state["currency_code"]
    return dedupe_columns(df)


def generate_anon_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    is_non = rng.random(n) < non_bank_ratio
    cust_type = np.where(is_non, "non-bank", "bank")

    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["real_estate","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
        "current_loans": rng.integers(0, 5, n),
        "customer_type": cust_type,
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))

    fx = st.session_state["currency_fx"]
    for c in ("income", "loan_amount", "collateral_value", "assets_owned", "existing_debt"):
        df[c] = (df[c] * fx).round(2)
    df["currency_code"] = st.session_state["currency_code"]
    return dedupe_columns(df)


def to_agent_schema(df: pd.DataFrame) -> pd.DataFrame:
    """
    Harmonize to the server-side agent‚Äôs expected schema.
    """
    out = df.copy()
    n = len(out)
    if "employment_years" not in out.columns:
        out["employment_years"] = out.get("employment_length", 0)
    if "debt_to_income" not in out.columns:
        if "DTI" in out.columns:
            out["debt_to_income"] = out["DTI"].astype(float)
        elif "existing_debt" in out.columns and "income" in out.columns:
            denom = out["income"].replace(0, np.nan)
            dti = (out["existing_debt"] / denom).fillna(0.0)
            out["debt_to_income"] = dti.clip(0, 10)
        else:
            out["debt_to_income"] = 0.0
    rng = np.random.default_rng(12345)
    if "credit_history_length" not in out.columns:
        out["credit_history_length"] = rng.integers(0, 30, n)
    if "num_delinquencies" not in out.columns:
        out["num_delinquencies"] = np.minimum(rng.poisson(0.2, n), 10)
    if "requested_amount" not in out.columns:
        out["requested_amount"] = out.get("loan_amount", 0)
    if "loan_term_months" not in out.columns:
        out["loan_term_months"] = out.get("loan_duration_months", 0)
    return dedupe_columns(out)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üè¶ TAB 1 ‚Äî Synthetic Data Generator
with tab_input:
    st.subheader("üè¶ Synthetic Credit Data Generator")

    # Currency selector (before generation)
    c1, c2 = st.columns([1, 2])
    with c1:
        code = st.selectbox(
            "Currency",
            list(CURRENCY_OPTIONS.keys()),
            index=list(CURRENCY_OPTIONS.keys()).index(st.session_state["currency_code"]),
            help="All monetary fields will be in this local currency."
        )
        if code != st.session_state["currency_code"]:
            st.session_state["currency_code"] = code
            set_currency_defaults()
    with c2:
        st.markdown(
            f"""
            <div style='background-color:#1e293b; padding:12px 16px; border-radius:8px;'>
                <span style='font-weight:600; color:#f8fafc;'>
                    üí∞ Amounts will be generated in
                    <span style='color:#4ade80;'>{st.session_state['currency_label']}</span>.
                </span>
            </div>
            """,
            unsafe_allow_html=True,
        )

    rows = st.slider("Number of rows to generate", 50, 2000, 200, step=50)
    non_bank_ratio = st.slider("Share of non-bank customers", 0.0, 1.0, 0.30, 0.05)

    colA, colB = st.columns(2)
    with colA:
        if st.button("üî¥ Generate RAW Synthetic Data (with PII)", use_container_width=True):
            raw_df = append_user_info(generate_raw_synthetic(rows, non_bank_ratio))
            st.session_state.synthetic_raw_df = raw_df
            raw_path = save_to_runs(raw_df, "synthetic_raw")
            st.success(f"Generated RAW (PII) dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {raw_path}")
            st.dataframe(raw_df.head(10), use_container_width=True)
            st.download_button(
                "‚¨áÔ∏è Download RAW CSV",
                raw_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(raw_path),
                "text/csv"
            )

    with colB:
        if st.button("üü¢ Generate ANON Synthetic Data (ready for agent)", use_container_width=True):
            anon_df = append_user_info(generate_anon_synthetic(rows, non_bank_ratio))
            st.session_state.synthetic_df = anon_df
            anon_path = save_to_runs(anon_df, "synthetic_anon")
            st.success(f"Generated ANON dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {anon_path}")
            st.dataframe(anon_df.head(10), use_container_width=True)
            st.download_button(
                "‚¨áÔ∏è Download ANON CSV",
                anon_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(anon_path),
                "text/csv"
            )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üßπ TAB 2 ‚Äî Anonymize & Sanitize Data
with tab_clean:
    st.subheader("üßπ Upload & Anonymize Customer Data (PII columns will be DROPPED)")
    st.markdown("Upload your **real CSV**. We drop PII columns and scrub emails/phones in text fields.")

    uploaded = st.file_uploader("Upload CSV file", type=["csv"])
    if uploaded:
        try:
            df = pd.read_csv(uploaded)
        except Exception as e:
            st.error(f"Could not read CSV: {e}")
            st.stop()

        st.write("üìä Original Data Preview:")
        st.dataframe(dedupe_columns(df.head(5)), use_container_width=True)

        sanitized, dropped_cols = drop_pii_columns(df)
        sanitized = append_user_info(sanitized)
        sanitized = dedupe_columns(sanitized)
        st.session_state.anonymized_df = sanitized

        st.success(f"Dropped PII columns: {sorted(dropped_cols) if dropped_cols else 'None'}")
        st.write("‚úÖ Sanitized Data Preview:")
        st.dataframe(sanitized.head(5), use_container_width=True)

        fpath = save_to_runs(sanitized, "anonymized")
        st.success(f"Saved anonymized file: {fpath}")
        st.download_button(
            "‚¨áÔ∏è Download Clean Data",
            sanitized.to_csv(index=False).encode("utf-8"),
            os.path.basename(fpath),
            "text/csv"
        )
    else:
        st.info("Choose a CSV to see the sanitize flow.", icon="‚ÑπÔ∏è")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ü§ñ TAB 3 ‚Äî Credit appraisal by AI assistant
with tab_run:
    st.subheader("ü§ñ Credit appraisal by AI assistant")
    # Anchor for loopback link from Training tab
    st.markdown('<a name="credit-appraisal-stage"></a>', unsafe_allow_html=True)

    # Production model banner (optional)
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.status_code == 200:
            meta = resp.json()
            if meta.get("has_production"):
                ver = (meta.get("meta") or {}).get("version", "1.x")
                src = (meta.get("meta") or {}).get("source", "production")
                st.success(f"üü¢ Production model active ‚Äî version: {ver} ‚Ä¢ source: {src}")
            else:
                st.warning("‚ö†Ô∏è No production model promoted yet ‚Äî using baseline.")
        else:
            st.info("‚ÑπÔ∏è Could not fetch production model meta.")
    except Exception:
        st.info("‚ÑπÔ∏è Production meta unavailable.")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß© Model Selection (list all trained models) ‚Äî Hardcoded Stable Version
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    from datetime import datetime
    import os, shutil, streamlit as st

    # Hardcoded absolute paths (your confirmed working setup)
    trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/trained"
    production_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/production"

    st.caption(f"üì¶ Trained dir = `{trained_dir}`")
    st.caption(f"üì¶ Production dir = `{production_dir}`")

    # ‚îÄ‚îÄ Refresh models list
    if st.button("‚Üª Refresh models", key="credit_refresh_models"):
        st.session_state.pop("selected_trained_model", None)
        st.rerun()

    # ‚îÄ‚îÄ Collect models
    models = []
    if os.path.isdir(trained_dir):
        for f in os.listdir(trained_dir):
            if f.endswith(".joblib"):
                fpath = os.path.join(trained_dir, f)
                ctime = os.path.getctime(fpath)
                created = datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
                models.append((f, fpath, created))
    else:
        st.error(f"‚ùå Trained dir not found: {trained_dir}")

    # ‚îÄ‚îÄ Show list if found
    if models:
        models.sort(key=lambda x: os.path.getctime(x[1]), reverse=True)
        display_names = [f"{m[0]} ‚Äî {m[2]}" for m in models]

        selected_display = st.selectbox("üì¶ Select trained model to use", display_names)
        selected_model = models[display_names.index(selected_display)][1]
        st.success(f"‚úÖ Using model: {os.path.basename(selected_model)}")

        st.session_state["selected_trained_model"] = selected_model

        # ‚îÄ‚îÄ Promote model
        if st.button("üöÄ Promote this model to Production"):
            try:
                os.makedirs(production_dir, exist_ok=True)
                prod_path = os.path.join(production_dir, "model.joblib")
                shutil.copy2(selected_model, prod_path)
                st.success(f"‚úÖ Model promoted to production: {prod_path}")
            except Exception as e:
                st.error(f"‚ùå Promotion failed: {e}")
    else:
        st.warning("‚ö†Ô∏è No trained models found ‚Äî train one in Step 5 first.")

    # 1) Model + Hardware selection (UI hints)
    LLM_MODELS = [
        ("Phi-3 Mini (3.8B) ‚Äî CPU OK", "phi3:3.8b", "CPU 8GB RAM (fast)"),
        ("Mistral 7B Instruct ‚Äî CPU slow / GPU OK", "mistral:7b-instruct", "CPU 16GB (slow) or GPU ‚â•8GB"),
        ("Gemma-2 7B ‚Äî CPU slow / GPU OK", "gemma2:7b", "CPU 16GB (slow) or GPU ‚â•8GB"),
        ("LLaMA-3 8B ‚Äî GPU recommended", "llama3:8b-instruct", "GPU ‚â•12GB (CPU very slow)"),
        ("Qwen2 7B ‚Äî GPU recommended", "qwen2:7b-instruct", "GPU ‚â•12GB (CPU very slow)"),
        ("Mixtral 8x7B ‚Äî GPU only (big)", "mixtral:8x7b-instruct", "GPU 24‚Äì48GB"),
    ]
    LLM_LABELS = [l for (l, _, _) in LLM_MODELS]
    LLM_VALUE_BY_LABEL = {l: v for (l, v, _) in LLM_MODELS}
    LLM_HINT_BY_LABEL = {l: h for (l, _, h) in LLM_MODELS}

    OPENSTACK_FLAVORS = {
        "m4.medium": "4 vCPU / 8 GB RAM ‚Äî CPU-only small",
        "m8.large": "8 vCPU / 16 GB RAM ‚Äî CPU-only medium",
        "g1.a10.1": "8 vCPU / 32 GB RAM + 1√óA10 24GB",
        "g1.l40.1": "16 vCPU / 64 GB RAM + 1√óL40 48GB",
        "g2.a100.1": "24 vCPU / 128 GB RAM + 1√óA100 80GB",
    }

    with st.expander("üß† Local LLM & Hardware Profile", expanded=True):
        c1, c2 = st.columns([1.2, 1])
        with c1:
            model_label = st.selectbox("Local LLM (used for narratives/explanations)", LLM_LABELS, index=1)
            llm_value = LLM_VALUE_BY_LABEL[model_label]
            st.caption(f"Hint: {LLM_HINT_BY_LABEL[model_label]}")
        with c2:
            flavor = st.selectbox("OpenStack flavor / host profile", list(OPENSTACK_FLAVORS.keys()), index=0)
            st.caption(OPENSTACK_FLAVORS[flavor])
        st.caption("These are passed to the API as hints; your API can choose Ollama/Flowise backends accordingly.")

    # 2) Data Source
    data_choice = st.selectbox(
        "Select Data Source",
        [
            "Use synthetic (ANON)",
            "Use synthetic (RAW ‚Äì auto-sanitize)",
            "Use anonymized dataset",
            "Upload manually",
        ],
    )
    use_llm = st.checkbox("Use LLM narrative", value=False)
    agent_name = "credit_appraisal"

    if data_choice == "Upload manually":
        up = st.file_uploader("Upload your CSV", type=["csv"], key="manual_upload_run_file")
        if up is not None:
            st.session_state["manual_upload_name"] = up.name
            st.session_state["manual_upload_bytes"] = up.getvalue()
            st.success(f"File staged: {up.name} ({len(st.session_state['manual_upload_bytes'])} bytes)")

    # 3) Rules
    st.markdown("### ‚öôÔ∏è Decision Rule Set")
    rule_mode = st.radio(
        "Choose rule mode",
        ["Classic (bank-style metrics)", "NDI (Net Disposable Income) ‚Äî simple"],
        index=0,
        help="NDI = income - all monthly obligations. Approve if NDI and NDI ratio pass thresholds.",
    )

    CLASSIC_DEFAULTS = {
        "max_dti": 0.45,
        "min_emp_years": 2,
        "min_credit_hist": 3,
        "salary_floor": 3000,
        "max_delinquencies": 2,
        "max_current_loans": 3,
        "req_min": 1000,
        "req_max": 200000,
        "loan_terms": [12, 24, 36, 48, 60],
        "threshold": 0.45,
        "target_rate": None,
        "random_band": True,
        "min_income_debt_ratio": 0.35,
        "compounded_debt_factor": 1.0,
        "monthly_debt_relief": 0.50,
    }
    NDI_DEFAULTS = {"ndi_value": 800.0, "ndi_ratio": 0.50, "threshold": 0.45, "target_rate": None, "random_band": True}

    if "classic_rules" not in st.session_state:
        st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    if "ndi_rules" not in st.session_state:
        st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    def reset_classic(): st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    def reset_ndi():     st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    if rule_mode.startswith("Classic"):
        with st.expander("Classic Metrics (with Reset)", expanded=True):
            rc = st.session_state.classic_rules
            r1, r2, r3 = st.columns(3)
            with r1:
                rc["max_dti"] = st.slider("Max Debt-to-Income (DTI)", 0.0, 1.0, rc["max_dti"], 0.01)
                rc["min_emp_years"] = st.number_input("Min Employment Years", 0, 40, rc["min_emp_years"])
                rc["min_credit_hist"] = st.number_input("Min Credit History (years)", 0, 40, rc["min_credit_hist"])
            with r2:
                rc["salary_floor"] = st.number_input(
                    "Minimum Monthly Salary", 0, 1_000_000_000, rc["salary_floor"], step=1000, help=fmt_currency_label("in local currency")
                )
                rc["max_delinquencies"] = st.number_input("Max Delinquencies", 0, 10, rc["max_delinquencies"])
                rc["max_current_loans"] = st.number_input("Max Current Loans", 0, 10, rc["max_current_loans"])
            with r3:
                rc["req_min"] = st.number_input(fmt_currency_label("Requested Amount Min"), 0, 10_000_000_000, rc["req_min"], step=1000)
                rc["req_max"] = st.number_input(fmt_currency_label("Requested Amount Max"), 0, 10_000_000_000, rc["req_max"], step=1000)
                rc["loan_terms"] = st.multiselect("Allowed Loan Terms (months)", [12, 24, 36, 48, 60, 72], default=rc["loan_terms"])

            st.markdown("#### üßÆ Debt Pressure Controls")
            d1, d2, d3 = st.columns(3)
            with d1:
                rc["min_income_debt_ratio"] = st.slider(
                    "Min Income / (Compounded Debt) Ratio", 0.10, 2.00, rc["min_income_debt_ratio"], 0.01
                )
            with d2:
                rc["compounded_debt_factor"] = st.slider(
                    "Compounded Debt Factor (√ó requested)", 0.5, 3.0, rc["compounded_debt_factor"], 0.1
                )
            with d3:
                rc["monthly_debt_relief"] = st.slider("Monthly Debt Relief Factor", 0.10, 1.00, rc["monthly_debt_relief"], 0.05)

            st.markdown("---")
            c1, c2, c3 = st.columns([1, 1, 1])
            with c1:
                use_target = st.toggle("üéØ Use target approval rate", value=(rc["target_rate"] is not None))
            with c2:
                rc["random_band"] = st.toggle(
                    "üé≤ Randomize approval band (20‚Äì60%) when no target", value=rc["random_band"]
                )
            with c3:
                if st.button("‚Ü©Ô∏è Reset to defaults"):
                    reset_classic()
                    st.rerun()

            if use_target:
                rc["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rc["target_rate"] or 0.40, 0.01)
                rc["threshold"] = None
            else:
                rc["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rc["threshold"], 0.01)
                rc["target_rate"] = None
    else:
        with st.expander("NDI Metrics (with Reset)", expanded=True):
            rn = st.session_state.ndi_rules
            n1, n2 = st.columns(2)
            with n1:
                rn["ndi_value"] = st.number_input(
                    fmt_currency_label("Min NDI (Net Disposable Income) per month"),
                    0.0,
                    1e12,
                    float(rn["ndi_value"]),
                    step=50.0,
                )
            with n2:
                rn["ndi_ratio"] = st.slider("Min NDI / Income ratio", 0.0, 1.0, float(rn["ndi_ratio"]), 0.01)
            st.caption("NDI = income - all monthly obligations (rent, food, loans, cards, etc.).")

            st.markdown("---")
            c1, c2, c3 = st.columns([1, 1, 1])
            with c1:
                use_target = st.toggle("üéØ Use target approval rate", value=(rn["target_rate"] is not None))
            with c2:
                rn["random_band"] = st.toggle(
                    "üé≤ Randomize approval band (20‚Äì60%) when no target", value=rn["random_band"]
                )
            with c3:
                if st.button("‚Ü©Ô∏è Reset to defaults (NDI)"):
                    reset_ndi()
                    st.rerun()

            if use_target:
                rn["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rn["target_rate"] or 0.40, 0.01)
                rn["threshold"] = None
            else:
                rn["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rn["threshold"], 0.01)
                rn["target_rate"] = None

    # Helper used below (your function name later referenced as json_to_dataframe in the draft)
    def json_to_df(obj) -> pd.DataFrame:
        if obj is None:
            return pd.DataFrame()
        if isinstance(obj, pd.DataFrame):
            return obj
        if isinstance(obj, bytes):
            try:
                obj = obj.decode("utf-8", errors="ignore")
            except Exception:
                return pd.DataFrame({"value": [repr(obj)]})
        if isinstance(obj, str):
            obj = obj.strip()
            if not obj:
                return pd.DataFrame()
            try:
                j = json.loads(obj)
                return json_to_df(j)
            except Exception:
                lines = [ln for ln in obj.splitlines() if ln.strip()]
                return pd.DataFrame({"value": lines}) if lines else pd.DataFrame()
        if isinstance(obj, list):
            if len(obj) == 0:
                return pd.DataFrame()
            if all(isinstance(x, dict) for x in obj):
                try:
                    return pd.json_normalize(obj)
                except Exception:
                    return pd.DataFrame(obj)
            if all(isinstance(x, list) for x in obj):
                return pd.DataFrame({"row": obj})
            return pd.DataFrame({"value": obj})
        if isinstance(obj, dict):
            for key in ("rows", "data", "result", "results", "items", "records"):
                if key in obj and isinstance(obj[key], (list, dict)):
                    return json_to_df(obj[key])
            try:
                return pd.json_normalize(obj)
            except Exception:
                return pd.DataFrame([obj])
        return pd.DataFrame({"value": [obj]})

    # 4) Run
    if st.button("üöÄ Run Agent", use_container_width=True):
        try:
            files = None
            data: Dict[str, Any] = {
                "use_llm_narrative": str(use_llm).lower(),
                "llm_model": llm_value,
                "hardware_flavor": flavor,
                "currency_code": st.session_state["currency_code"],
                "currency_symbol": st.session_state["currency_symbol"],
            }
            if rule_mode.startswith("Classic"):
                rc = st.session_state.classic_rules
                data.update(
                    {
                        "min_employment_years": str(rc["min_emp_years"]),
                        "max_debt_to_income": str(rc["max_dti"]),
                        "min_credit_history_length": str(rc["min_credit_hist"]),
                        "max_num_delinquencies": str(rc["max_delinquencies"]),
                        "max_current_loans": str(rc["max_current_loans"]),
                        "requested_amount_min": str(rc["req_min"]),
                        "requested_amount_max": str(rc["req_max"]),
                        "loan_term_months_allowed": ",".join(map(str, rc["loan_terms"])) if rc["loan_terms"] else "",
                        "min_income_debt_ratio": str(rc["min_income_debt_ratio"]),
                        "compounded_debt_factor": str(rc["compounded_debt_factor"]),
                        "monthly_debt_relief": str(rc["monthly_debt_relief"]),
                        "salary_floor": str(rc["salary_floor"]),
                        "threshold": "" if rc["threshold"] is None else str(rc["threshold"]),
                        "target_approval_rate": "" if rc["target_rate"] is None else str(rc["target_rate"]),
                        "random_band": str(rc["random_band"]).lower(),
                        "random_approval_band": str(rc["random_band"]).lower(),
                        "rule_mode": "classic",
                    }
                )
            else:
                rn = st.session_state.ndi_rules
                data.update(
                    {
                        "ndi_value": str(rn["ndi_value"]),
                        "ndi_ratio": str(rn["ndi_ratio"]),
                        "threshold": "" if rn["threshold"] is None else str(rn["threshold"]),
                        "target_approval_rate": "" if rn["target_rate"] is None else str(rn["target_rate"]),
                        "random_band": str(rn["random_band"]).lower(),
                        "random_approval_band": str(rn["random_band"]).lower(),
                        "rule_mode": "ndi",
                    }
                )

            def prep_and_pack(df: pd.DataFrame, filename: str):
                safe = dedupe_columns(df)
                safe, _ = drop_pii_columns(safe)
                safe = strip_policy_banned(safe)
                safe = to_agent_schema(safe)
                buf = io.StringIO()
                safe.to_csv(buf, index=False)
                return {"file": (filename, buf.getvalue().encode("utf-8"), "text/csv")}

            if data_choice == "Use synthetic (ANON)":
                if "synthetic_df" not in st.session_state:
                    st.warning("No ANON synthetic dataset found. Generate it in the first tab.")
                    st.stop()
                files = prep_and_pack(st.session_state.synthetic_df, "synthetic_anon.csv")

            elif data_choice == "Use synthetic (RAW ‚Äì auto-sanitize)":
                if "synthetic_raw_df" not in st.session_state:
                    st.warning("No RAW synthetic dataset found. Generate it in the first tab.")
                    st.stop()
                files = prep_and_pack(st.session_state.synthetic_raw_df, "synthetic_raw_sanitized.csv")

            elif data_choice == "Use anonymized dataset":
                if "anonymized_df" not in st.session_state:
                    st.warning("No anonymized dataset found. Create it in the second tab.")
                    st.stop()
                files = prep_and_pack(st.session_state.anonymized_df, "anonymized.csv")

            elif data_choice == "Upload manually":
                up_name = st.session_state.get("manual_upload_name")
                up_bytes = st.session_state.get("manual_upload_bytes")
                if not up_name or not up_bytes:
                    st.warning("Please upload a CSV first.")
                    st.stop()
                try:
                    tmp_df = pd.read_csv(io.BytesIO(up_bytes))
                    files = prep_and_pack(tmp_df, up_name)
                except Exception:
                    files = {"file": (up_name, up_bytes, "text/csv")}
            else:
                st.error("Unknown data source selection.")
                st.stop()

            # ---- RUN REQUEST ----
            r = requests.post(
                f"{API_URL}/v1/agents/{agent_name}/run",
                data=data,
                files=files,
                timeout=180
            )

            if r.status_code != 200:
                st.error(f"Run failed ({r.status_code}): {r.text}")
                st.stop()

            res = r.json()

            # ---- Robust run_id + data extraction ----
            run_id = None
            payload_rows = None  # fallback rows for rendering

            if isinstance(res, dict):
                run_id = res.get("run_id") or res.get("id")
                payload_rows = res.get("result") or res.get("data") or res.get("results") or res.get("rows")
            elif isinstance(res, list):
                payload_rows = res
            else:
                try:
                    maybe = json.loads(res)
                    if isinstance(maybe, dict):
                        run_id = maybe.get("run_id") or maybe.get("id")
                        payload_rows = maybe.get("result") or maybe.get("data") or maybe.get("results") or maybe.get("rows")
                    elif isinstance(maybe, list):
                        payload_rows = maybe
                except Exception:
                    pass

            # ---- Helper: turn any JSON-like into a DataFrame ----
            def json_to_df(obj) -> pd.DataFrame:
                if obj is None:
                    return pd.DataFrame()
                if isinstance(obj, pd.DataFrame):
                    return obj
                if isinstance(obj, bytes):
                    try:
                        obj = obj.decode("utf-8", errors="ignore")
                    except Exception:
                        return pd.DataFrame({"value": [repr(obj)]})
                if isinstance(obj, str):
                    obj = obj.strip()
                    if not obj:
                        return pd.DataFrame()
                    try:
                        j = json.loads(obj)
                        return json_to_df(j)
                    except Exception:
                        lines = [ln for ln in obj.splitlines() if ln.strip()]
                        return pd.DataFrame({"value": lines}) if lines else pd.DataFrame()
                if isinstance(obj, list):
                    if len(obj) == 0:
                        return pd.DataFrame()
                    if all(isinstance(x, dict) for x in obj):
                        try:
                            return pd.json_normalize(obj)
                        except Exception:
                            return pd.DataFrame(obj)
                    if all(isinstance(x, list) for x in obj):
                        return pd.DataFrame({"row": obj})
                    return pd.DataFrame({"value": obj})
                if isinstance(obj, dict):
                    for key in ("rows", "data", "result", "results", "items", "records"):
                        if key in obj and isinstance(obj[key], (list, dict)):
                            return json_to_df(obj[key])
                    try:
                        return pd.json_normalize(obj)
                    except Exception:
                        return pd.DataFrame([obj])
                return pd.DataFrame({"value": [obj]})

            # ---- Prefer server report via run_id; otherwise fall back to local JSON‚ÜíDF ----
            
            # ============================================================
            # ‚úÖ Prefer server CSV ‚Üí fallback to JSON Parser
            # ============================================================
            if run_id:
                try:
                    rid = run_id
                    merged_url = f"{API_URL}/v1/runs/{rid}/report?format=csv"
                    merged_bytes = requests.get(merged_url, timeout=30).content
                    merged_df = pd.read_csv(io.BytesIO(merged_bytes))
                    st.session_state.last_run_id = rid
                    st.success(f"‚úÖ Run succeeded! Run ID: {rid}")
                except Exception as e:
                    st.warning(f"Could not fetch CSV via run_id ({run_id}): {e}")
                    merged_df = json_to_dataframe(payload_rows)
            else:
                st.warning("‚ö†Ô∏è Backend did not return a run_id. Falling back to JSON.")
                merged_df = json_to_dataframe(payload_rows)

 

            if merged_df is None or merged_df.empty:
                st.error("No data available to render (both report and fallback JSON were empty).")
                st.write("Raw response:", res)
                st.stop()

            # Keep for later tabs
            st.session_state["last_merged_df"] = dedupe_columns(merged_df)
            
            # ‚úÖ Make results available to Stage 7 (Reporting & Handoff)
            try:
                st.session_state["credit_scored_df"] = dedupe_columns(merged_df.copy())
                st.success("‚úÖ Stage C outputs saved for Stage 7 (Reporting & Handoff).")
            except Exception as e:
                st.warning(f"Could not persist scored dataset for Stage 7: {e}")

            # ---- Decisions Table (with filter) ----
            st.markdown("### üìÑ Credit AI Agent Decisions Table (filtered)")
            uniq_dec = sorted([d for d in merged_df.get("decision", pd.Series(dtype=str)).dropna().unique()]) \
                    if "decision" in merged_df.columns else []
            chosen = st.multiselect("Filter decision", options=uniq_dec, default=uniq_dec, key="filter_decisions")
            df_view = merged_df.copy()
            if "decision" in df_view.columns and chosen:
                df_view = df_view[df_view["decision"].isin(chosen)]
            st.dataframe(df_view, use_container_width=True)

            # ---- Dashboard ----
            st.markdown("## üìä Dashboard")
            render_credit_dashboard(merged_df, st.session_state.get("currency_symbol", ""))

            # Add per-row metrics columns if present
            if "rule_reasons" in df_view.columns:
                rr = df_view["rule_reasons"].apply(try_json)
                df_view["metrics_met"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is True])) if isinstance(d, dict) else "")
                df_view["metrics_unmet"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is False])) if isinstance(d, dict) else "")

            cols_show = [c for c in [
                "application_id","customer_type","decision","score","loan_amount","income","metrics_met","metrics_unmet",
                "proposed_loan_option","proposed_consolidation_loan","top_feature","explanation"
            ] if c in df_view.columns]
            if cols_show:
                st.dataframe(df_view[cols_show].head(500), use_container_width=True)

            # ---- Download button (keep your large button style) ----
            ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
            out_name = f"ai-appraisal-outputs-{ts}-{st.session_state['currency_code']}.csv"
            csv_data = merged_df.to_csv(index=False).encode("utf-8")

            st.markdown("""
            <style>
            div[data-testid="stDownloadButton"] button {
                font-size: 90px !important;
                font-weight: 900 !important;
                padding: 28px 48px !important;
                border-radius: 16px !important;
                background: linear-gradient(90deg, #2563eb, #1d4ed8) !important;
                color: white !important;
                border: none !important;
                box-shadow: 0 6px 18px rgba(0,0,0,0.35) !important;
                transition: all 0.3s ease-in-out !important;
            }
            div[data-testid="stDownloadButton"] button:hover {
                background: linear-gradient(90deg, #1e3a8a, #1d4ed8) !important;
                transform: scale(1.03);
            }
            </style>
            """, unsafe_allow_html=True)

            st.download_button(
                "‚¨áÔ∏è Download AI Outputs For Human Review (CSV)",
                csv_data,
                file_name=out_name,
                mime="text/csv",
                use_container_width=True
            )
            
            # ‚úÖ CREATE TRAINING LABEL (Stage C ‚Üí Stage F)
            train_df = merged_df.copy()

            # 1) Default probability ‚Üí binary label
            if "default_probability" in train_df.columns:
                train_df["label"] = (train_df["default_probability"] >= 0.5).astype(int)

            # 2) Fallback: use score column if exists
            elif "score" in train_df.columns:
                train_df["label"] = (train_df["score"] >= 0.5).astype(int)

            # 3) Final fallback to avoid Stage F crash
            else:
                train_df["label"] = 0

            # ‚úÖ SAVE FOR TRAINING PIPELINE
            try:
                st.session_state["credit_train_df"] = train_df.copy()
                st.success("‚úÖ Stage C dataset prepared and saved for Stage F (training).")
            except Exception as e:
                st.error(f"Could not save training dataset for Stage F: {e}")

            
            # ‚úÖ SAVE OUTPUT FOR STAGE F (Training)
            try:
                #st.session_state["credit_train_df"] = scored_df.copy()
                st.session_state["credit_train_df"] = merged_df.copy()

                st.success("‚úÖ Stage C output saved for Stage F (training).")
            except Exception as e:
                st.error(f"Could not save Stage C dataset for training: {e}")

        except Exception as e:
            st.exception(e)

    # Re-download quick section
    if st.session_state.get("last_run_id"):
        st.markdown("---")
        st.subheader("üì• Download Latest Outputs")
        rid = st.session_state.last_run_id
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1: st.markdown(f"[‚¨áÔ∏è PDF]({API_URL}/v1/runs/{rid}/report?format=pdf)")
        with col2: st.markdown(f"[‚¨áÔ∏è Scores CSV]({API_URL}/v1/runs/{rid}/report?format=scores_csv)")
        with col3: st.markdown(f"[‚¨áÔ∏è Explanations CSV]({API_URL}/v1/runs/{rid}/report?format=explanations_csv)")
        with col4: st.markdown(f"[‚¨áÔ∏è Merged CSV]({API_URL}/v1/runs/{rid}/report?format=csv)")
        with col5: st.markdown(f"[‚¨áÔ∏è JSON]({API_URL}/v1/runs/{rid}/report?format=json)")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üßë‚Äç‚öñÔ∏è TAB 4 ‚Äî Human Review
with tab_review:
    st.subheader("üßë‚Äç‚öñÔ∏è Human Review ‚Äî Correct AI Decisions & Score Agreement > Drop your AI appraisal output CSV from previous Stage  below")

    # Allow loading AI output CSV back into review via dropdown upload
    uploaded_review = st.file_uploader("Load AI outputs CSV for review (optional)", type=["csv"], key="review_csv_loader")
    if uploaded_review is not None:
        try:
            st.session_state["last_merged_df"] = pd.read_csv(uploaded_review)
            st.success("Loaded review dataset from uploaded CSV.")
        except Exception as e:
            st.error(f"Could not read uploaded CSV: {e}")

    if "last_merged_df" not in st.session_state:
        st.info("Run the agent (previous tab) or upload an AI outputs CSV to load results for review.")
    else:
        dfm = st.session_state["last_merged_df"].copy()
        st.markdown("#### 1) Select rows to review and correct")

        editable_cols = []
        if "decision" in dfm.columns: editable_cols.append("decision")
        if "rule_reasons" in dfm.columns: editable_cols.append("rule_reasons")
        if "customer_type" in dfm.columns: editable_cols.append("customer_type")

        editable = dfm[["application_id"] + editable_cols].copy()
        editable.rename(columns={"decision": "ai_decision"}, inplace=True)
        editable["human_decision"] = editable.get("ai_decision", "approved")
        editable["human_rule_reasons"] = editable.get("rule_reasons", "")

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # LIGHTER EDITABLE CELL STYLING (improved)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        st.markdown("""
            <style>
            /* Bright background for editable cells */
            [data-testid="stDataFrameCellEditable"] textarea {
                background-color: #fefefe !important;   /* bright white background */
                color: #111 !important;                 /* dark text */
                border: 1px solid #cbd5e1 !important;   /* subtle gray border */
                border-radius: 6px !important;
                padding: 6px 8px !important;
                font-weight: 500 !important;
            }

            /* Hover and focus effect */
            [data-testid="stDataFrameCellEditable"]:focus-within textarea,
            [data-testid="stDataFrameCellEditable"]:hover textarea {
                background-color: #ffffff !important;
                border-color: #22c55e !important;        /* green glow */
                box-shadow: 0 0 0 2px rgba(34,197,94,0.4) !important;
            }

            /* Read-only cells: keep dark */
            [data-testid="stDataFrameCell"] {
                background-color: #1e293b !important;
                color: #e2e8f0 !important;
            }
            </style>
        """, unsafe_allow_html=True)


        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # EDITOR
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        edited = st.data_editor(
            editable,
            num_rows="dynamic",
            use_container_width=True,
            key="review_editor",
            column_config={
                "human_decision": st.column_config.SelectboxColumn(options=["approved", "denied"]),
                "customer_type": st.column_config.SelectboxColumn(options=["bank", "non-bank"], disabled=True)
            }
        )

        st.markdown("#### 2) Compute agreement score")

        if st.button("Compute agreement score"):
            if "ai_decision" in edited.columns and "human_decision" in edited.columns:
                agree = (edited["ai_decision"] == edited["human_decision"]).astype(int)
                score = float(agree.mean()) if len(agree) else 0.0
                st.session_state["last_agreement_score"] = score

                # üå°Ô∏è BEAUTIFUL Gauge
                import plotly.graph_objects as go
                fig = go.Figure(go.Indicator(
                    mode="gauge+number",
                    value=score * 100,
                    number={'suffix': "%", 'font': {'size': 72, 'color': "#f8fafc", 'family': "Arial Black"}},
                    title={'text': "AI ‚Üî Human Agreement", 'font': {'size': 28, 'color': "#93c5fd", 'family': "Arial"}},
                    gauge={
                        'axis': {'range': [0, 100], 'tickwidth': 2, 'tickcolor': "#f8fafc"},
                        'bar': {'color': "#3b82f6", 'thickness': 0.3},
                        'bgcolor': "#1e293b",
                        'borderwidth': 2,
                        'bordercolor': "#334155",
                        'steps': [
                            {'range': [0, 50], 'color': "#ef4444"},
                            {'range': [50, 75], 'color': "#f59e0b"},
                            {'range': [75, 100], 'color': "#22c55e"},
                        ],
                    }
                ))
                fig.update_layout(
                    paper_bgcolor="#0f172a",
                    plot_bgcolor="#0f172a",
                    height=400,
                    margin=dict(t=60, b=20, l=60, r=60)
                )
                st.plotly_chart(fig, use_container_width=True)



            # üí° Detailed disagreement table (AI vs Human + AI metrics explanation)
                mismatched = edited[edited["ai_decision"] != edited["human_decision"]].copy()
                total = len(edited)
                disagree = len(mismatched)

                if disagree > 0:
                    st.markdown(f"### ‚ùå {disagree} loans disagreed out of {total} ({(disagree/total)*100:.1f}% disagreement rate)")

                    import json

                    def parse_ai_reason(r: str):
                        """Parse AI rule_reasons and summarize which metrics passed or failed."""
                        if not isinstance(r, str):
                            return "No metrics available"
                        try:
                            data = json.loads(r.replace("'", "\""))
                            passed = [k for k, v in data.items() if v is True]
                            failed = [k for k, v in data.items() if v is False]
                            result = []
                            if passed:
                                result.append("‚úÖ Pass: " + ", ".join(passed))
                            if failed:
                                result.append("‚ùå Fail: " + ", ".join(failed))
                            return " | ".join(result) if result else "No metrics recorded"
                        except Exception:
                            return "Unreadable metrics"

                    # Extract AI reasoning and Human reason columns
                    mismatched["ai_metrics"] = mismatched["rule_reasons"].apply(parse_ai_reason) if "rule_reasons" in mismatched else "No data"
                    mismatched["human_reason"] = mismatched.get("human_rule_reasons", "Manual review adjustment")

                    # üü©üü• Color styling for AI vs Human
                    def highlight_disagreement(row):
                        ai_color = "background-color: #ef4444; color: white;"      # red for AI decision
                        human_color = "background-color: #22c55e; color: black;"   # green for Human decision
                        return [
                            ai_color if col == "ai_decision" else
                            human_color if col == "human_decision" else
                            ""
                            for col in row.index
                        ]

                    # Columns: ID ‚Üí AI Decision ‚Üí Human Decision ‚Üí AI Metrics ‚Üí Human Reason
                    show_cols = [
                        c for c in ["application_id", "ai_decision", "human_decision", "ai_metrics", "human_reason"]
                        if c in mismatched.columns
                    ]
                    styled_df = mismatched[show_cols].style.apply(highlight_disagreement, axis=1)
                    st.dataframe(styled_df, use_container_width=True, height=420)
                else:
                    st.success("‚úÖ Full agreement ‚Äî no human-AI mismatches found.")



        # Export review CSV (manual loop into training)
        st.markdown("#### 3) Export Human review CSV for Next Step : Training and loopback ")
        model_used = "production"  # if you track specific model names, set it here
        ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
        safe_user = st.session_state["user_info"]["name"].replace(" ", "").lower()
        review_name = f"creditappraisal.{safe_user}.{model_used}.{ts}.csv"
        csv_bytes = edited.to_csv(index=False).encode("utf-8")
        st.download_button("‚¨áÔ∏è Export review CSV", csv_bytes, review_name, "text/csv")
        st.caption(f"Saved file name pattern: **{review_name}**")


# -------------------------------------------------------------
# ‚úÖ STAGE 5 ‚Äî Credit Model Training (Executive Dashboard)
# -------------------------------------------------------------
with tab_train:
    import os, json, glob, shutil, zipfile
    from datetime import datetime, timezone
    from pathlib import Path
    import numpy as np
    import pandas as pd
    import plotly.express as px
    import plotly.graph_objects as go

    from sklearn.model_selection import train_test_split
    from sklearn.metrics import (
        roc_auc_score, accuracy_score, precision_score, 
        recall_score, f1_score, confusion_matrix
    )

    import joblib
    import streamlit as st

    # ---------------------------------------------------------
    # ‚úÖ HEADER
    # ---------------------------------------------------------
    st.markdown("""
    <h2>üß† Stage 5 ‚Äî Credit Model Training</h2>
    <p style='font-size:1.1rem'>
    Train ‚Üí Compare ‚Üí Evaluate ‚Üí Promote<br>
    Build a robust, regulator-friendly credit scoring model.
    </p>
    """, unsafe_allow_html=True)

    # ---------------------------------------------------------
    # ‚úÖ LOAD TRAINING DATA (from Stage C)
    # ---------------------------------------------------------
    train_df = st.session_state.get("credit_train_df")

    if train_df is None or train_df.empty:
        st.error("‚ö†Ô∏è Missing training dataset. Please run Stage C first.")
        st.stop()

    st.success(f"‚úÖ Training dataset detected with {len(train_df):,} rows.")
    st.dataframe(train_df.head(), use_container_width=True)

    st.markdown("---")

    # ---------------------------------------------------------
    # ‚úÖ TRAINING DATA LOADING (Human feedback OR CSV upload)
    # ---------------------------------------------------------
    st.markdown("### üì• Stage 5 Training Data Input")

    train_df = None
    source_label = None

    # ‚úÖ Option A ‚Äî Human Review Stage output is available
    if "credit_human_review_df" in st.session_state:
        df_human = st.session_state.get("credit_human_review_df")
        if isinstance(df_human, pd.DataFrame) and not df_human.empty:
            train_df = df_human.copy()
            source_label = "Human Review Stage (Session State)"

    # ‚úÖ Option B ‚Äî Model Inference Stage C merged_df output (fallback)
    elif "credit_train_df" in st.session_state:
        df_auto = st.session_state.get("credit_train_df")
        if isinstance(df_auto, pd.DataFrame) and not df_auto.empty:
            train_df = df_auto.copy()
            source_label = "Stage C auto-saved dataset"

    
    # ‚úÖ Option C ‚Äî User uploads CSV manually
    uploaded = st.file_uploader("Upload training CSV (optional)", type=["csv"])

    if uploaded is not None:
        try:
            train_df = pd.read_csv(uploaded)
            source_label = f"Uploaded CSV ({len(train_df)} rows)"
        except Exception as e:
            st.error(f"‚ùå Could not read uploaded CSV: {e}")

    # ‚úÖ Hard stop if no dataset is available
    if train_df is None or train_df.empty:
        st.error("""
        ‚ùå No training data found.

        ‚úÖ Provide training dataset by:
        ‚Ä¢ Completing Human Review Stage (Stage D)  
        ‚Ä¢ OR uploading a CSV here  
        ‚Ä¢ OR enabling Stage C to save merged output  
        """)
        st.stop()
    


    # ‚úÖ Show dataset preview + source
    st.success(f"‚úÖ Training dataset loaded from: **{source_label}**")
    st.dataframe(train_df.head(), use_container_width=True)

    st.markdown("---")

 
    # ---------------------------------------------------------
    # ‚úÖ MODEL SELECTION
    # ---------------------------------------------------------
    st.subheader("ü§ñ Choose training model")

    model_choice = st.selectbox(
        "Select model:",
        ["LogisticRegression", "RandomForest", "LightGBM", "XGBoost"]
    )

    # ---------------------------------------------------------
    # ‚úÖ Smart Target Auto-Detection (BEFORE training)
    # ---------------------------------------------------------
    def detect_best_target(df):
        """
        Smart target auto-detection for credit scoring.
        Priority:
        1) AI numeric scores
        2) human decisions
        3) any suitable numeric predictive column
        """

        score_candidates = [
            "score", "default_probability", "risk_score",
            "pd", "probability_default"
        ]

        # ‚úÖ 1. Direct AI numeric score column
        for col in score_candidates:
            if col in df.columns:
                return col, "numeric_score"

        # ‚úÖ 2. Human decision labels
        decision_candidates = ["human_decision", "final_decision", "decision"]

        for col in decision_candidates:
            if col in df.columns:
                vals = df[col].dropna().astype(str).str.lower().unique()
                if any(v in ["approved", "rejected"] for v in vals):
                    return col, "decision_label"

        # ‚úÖ 3. Numeric fallback
        numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()

        # exclude leakage columns
        blacklist = ["loan_amount", "requested_amount", "income", "assets_owned"]
        numeric_cols = [c for c in numeric_cols if c not in blacklist]

        if numeric_cols:
            return numeric_cols[0], "numeric_fallback"

        return None, "none"


    # ---------------------------------------------------------
    # ‚úÖ TRAINING LOGIC
    # ---------------------------------------------------------
    if st.button("üöÄ Train Credit Model Now"):
        with st.spinner("Training model‚Ä¶"):

            # ‚úÖ Smart Target Detection
            TARGET_COL, target_mode = detect_best_target(train_df)

            if TARGET_COL is None:
                st.error("‚ùå No suitable target column found in dataset.")
                st.stop()

            st.success(f"‚úÖ Selected target: **{TARGET_COL}** ({target_mode})")

            # ‚úÖ Clean and prepare target
            y_cont = pd.to_numeric(train_df[TARGET_COL], errors="coerce")
            df_clean = train_df.dropna(subset=[TARGET_COL]).copy()
            y_cont = df_clean[TARGET_COL].astype(float)

            # ---------------------------------------------------------
            # ‚úÖ MODE 1: DECISION LABEL (approved / rejected ‚Üí 1/0)
            # ---------------------------------------------------------
            if target_mode == "decision_label":
                y_bin = df_clean[TARGET_COL].astype(str).str.lower().map({
                    "approved": 1,
                    "rejected": 0
                })
                st.info("‚úÖ Using human decisions converted to binary 0/1")

            # ---------------------------------------------------------
            # ‚úÖ MODE 2 & 3: NUMERIC TARGET ‚Üí BINARIZE USING MEDIAN
            # ---------------------------------------------------------
            else:
                threshold = float(y_cont.median())
                y_bin = (y_cont >= threshold).astype(int)
                st.info(f"‚úÖ Numeric target ‚Üí auto-threshold = {threshold:.4f}")

            # ---------------------------------------------------------
            # ‚úÖ FEATURE SELECTION ‚Äî remove target + leakage
            # ---------------------------------------------------------
            LEAKAGE_COLS = [
                TARGET_COL,
                "decision", "confidence",
                "top_feature", "explanation",
                "proposed_loan_option", "proposed_consolidation_loan",
                "rule_reasons"
            ]

            X = df_clean.drop(columns=[c for c in LEAKAGE_COLS if c in df_clean.columns])
            
            
            # ---------------------------------------------------------
            # ‚úÖ Encode non-numeric columns for ML training
            # ---------------------------------------------------------
            X = X.copy()  # safe copy

            # Detect non-numeric columns
            non_numeric_cols = X.select_dtypes(include=["object"]).columns.tolist()

            if non_numeric_cols:
                st.warning(f"Encoding non-numeric columns: {non_numeric_cols}")

                # ‚úÖ Safe label encoding for LightGBM / RF / XGB / LR
                from sklearn.preprocessing import LabelEncoder

                for col in non_numeric_cols:
                    try:
                        le = LabelEncoder()
                        X[col] = le.fit_transform(X[col].astype(str))
                    except Exception as e:
                        st.error(f"‚ùå Failed to encode column '{col}': {e}")
                        st.stop()


            # ---------------------------------------------------------
            # ‚úÖ TRAIN/TEST SPLIT
            # ---------------------------------------------------------
            from sklearn.model_selection import train_test_split
            Xtr, Xte, ytr, yte = train_test_split(
                X, y_bin, test_size=0.2, random_state=42
            )

            # ---------------------------------------------------------
            # ‚úÖ MODEL SELECTION AND TRAINING
            # ---------------------------------------------------------
            if model_choice == "LogisticRegression":
                from sklearn.linear_model import LogisticRegression
                model = LogisticRegression(max_iter=2000)

                model = LogisticRegression(max_iter=2000)
            elif model_choice == "RandomForest":
                from sklearn.ensemble import RandomForestClassifier
                model = RandomForestClassifier(n_estimators=300)

                model = RandomForestClassifier(n_estimators=300)
            elif model_choice == "LightGBM":
                from lightgbm import LGBMClassifier
                model = LGBMClassifier()

                model = LGBMClassifier()
            else:
                from xgboost import XGBClassifier

                model = XGBClassifier()

            model.fit(Xtr, ytr)

            # ---------------------------------------------------------
            # ‚úÖ PREDICTIONS & METRICS
            # ---------------------------------------------------------
            preds_proba = model.predict_proba(Xte)[:, 1]
            preds = (preds_proba >= 0.5).astype(int)

            from sklearn.metrics import (
                roc_auc_score, accuracy_score, precision_score,
                recall_score, f1_score
            )

            metrics = {
                "AUC": roc_auc_score(yte, preds_proba),
                "Accuracy": accuracy_score(yte, preds),
                "Precision": precision_score(yte, preds),
                "Recall": recall_score(yte, preds),
                "F1": f1_score(yte, preds),
            }

            st.success("‚úÖ Model trained successfully!")
            st.json(metrics)

        
    
            # -----------------------------------------------------
            # ‚úÖ LOAD PRODUCTION BASELINE IF EXISTS
            # -----------------------------------------------------
            PROD_DIR = Path("./agents/credit_appraisal/models/production")
            prod_meta_path = PROD_DIR / "production_meta.json"
            if prod_meta_path.exists():
                prod_m = json.load(open(prod_meta_path))["metrics"]
            else:
                prod_m = None

            st.markdown("---")
            st.subheader("üìä A/B Model Comparison")

            # ‚úÖ COMPARISON TABLE
            cmp_df = pd.DataFrame({
                "Metric": list(metrics.keys()),
                "New Model": [f"{v:.4f}" for v in metrics.values()],
                "Production": [
                    f"{prod_m[k]:.4f}" if prod_m else "‚Äî"
                    for k in metrics.keys()
                ]
            })
            st.table(cmp_df)

            # -----------------------------------------------------
            # ‚úÖ EXECUTIVE SUMMARY (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)
            # -----------------------------------------------------
            st.markdown("## üß≠ Executive Summary (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)")

            if prod_m:
                auc_delta = metrics["AUC"] - prod_m["AUC"]
                if auc_delta > 0:
                    st.success(f"‚úÖ Model improves **AUC by {auc_delta:.4f}** ‚Äî better discrimination.")
                else:
                    st.warning(f"‚ö†Ô∏è AUC dropped by {auc_delta:.4f} ‚Äî further tuning required.")
            else:
                st.info("üü¢ First model ‚Äî will become baseline.")

            # -----------------------------------------------------
            # ‚úÖ CONFUSION MATRIX
            # -----------------------------------------------------
            cm = confusion_matrix(yte, preds)
            cm_fig = px.imshow(
                cm, text_auto=True,
                title="Confusion Matrix",
                labels={"x": "Predicted", "y": "Actual"}
            )
            st.plotly_chart(cm_fig, use_container_width=True)

            # -----------------------------------------------------
            # ‚úÖ FEATURE IMPORTANCE
            # -----------------------------------------------------
            st.subheader("üß† Feature Importance")
            if hasattr(model, "feature_importances_"):
                imp = pd.DataFrame({"feature": X.columns, "importance": model.feature_importances_}).sort_values(
                    "importance", ascending=False
                )
                st.bar_chart(imp.set_index("feature"))
            elif hasattr(model, "coef_"):
                coef = pd.DataFrame({"feature": X.columns, "coef": np.ravel(model.coef_)}).sort_values(
                    "coef", key=np.abs, ascending=False
                )
                st.bar_chart(coef.set_index("feature"))
            else:
                st.info("This model does not expose importance metrics.")

            # -----------------------------------------------------
            # ‚úÖ SAVE MODEL
            # -----------------------------------------------------
            TRAINED_DIR = Path("./agents/credit_appraisal/models/trained")
            TRAINED_DIR.mkdir(parents=True, exist_ok=True)

            ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
            model_path = TRAINED_DIR / f"{model_choice}_{ts}.joblib"
            joblib.dump(model, model_path)
            st.success(f"‚úÖ Model saved ‚Üí `{model_path}`")

            # ‚úÖ SAVE REPORT
            RUNS_DIR = Path("./.tmp_runs")
            RUNS_DIR.mkdir(exist_ok=True)

            report = {
                "timestamp": ts,
                "model_choice": model_choice,
                "metrics": metrics,
                "model_path": str(model_path),
                "features": list(X.columns),
                "threshold": threshold,
            }

            rep_path = RUNS_DIR / f"credit_training_report_{ts}.json"
            json.dump(report, open(rep_path, "w"), indent=2)

            # store for next stage
            st.session_state["credit_last_model_path"] = str(model_path)
            st.session_state["credit_last_metrics"] = metrics
            st.session_state["credit_last_report"] = report

            st.caption(f"üìÑ Report saved ‚Üí `{rep_path}`")

            # -----------------------------------------------------
            # ‚úÖ PROMOTION BLOCK
            # -----------------------------------------------------
            st.markdown("## üì§ Promote This Model to Production")
            if st.button("‚úÖ Promote to Production"):
                try:
                    PROD_DIR.mkdir(parents=True, exist_ok=True)
                    shutil.copy(model_path, PROD_DIR / "model.joblib")
                    meta = {
                        "promoted_at": datetime.now(timezone.utc).isoformat(),
                        "metrics": metrics,
                        "model_path": str(model_path),
                        "model_choice": model_choice,
                    }
                    json.dump(meta, open(PROD_DIR / "production_meta.json", "w"), indent=2)
                    st.balloons()
                    st.success("‚úÖ Model promoted successfully!")
                except Exception as e:
                    st.error(f"‚ùå Promotion failed: {e}")

            # -----------------------------------------------------
            # ‚úÖ EXPORT ZIP
            # -----------------------------------------------------
            st.markdown("## üì¶ Export Project ZIP")
            EXPORT_DIR = Path("./exports")
            EXPORT_DIR.mkdir(exist_ok=True)
            zip_name = f"credit_project_bundle_{ts}.zip"
            zip_path = EXPORT_DIR / zip_name

            if st.button("‚¨áÔ∏è Build ZIP Bundle"):
                try:
                    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:

                        # Runs
                        for root, dirs, files in os.walk(RUNS_DIR):
                            for f in files:
                                full = os.path.join(root, f)
                                arc = os.path.relpath(full, RUNS_PATH)
                                zf.write(full, f"runs/{arc}")

                        # Production models
                        if PROD_DIR.exists():
                            for f in PROD_DIR.glob("*"):
                                zf.write(f, f"production/{f.name}")

                        # Trained models
                        for f in TRAINED_DIR.glob("*.joblib"):
                            zf.write(f, f"trained/{f.name}")

                        # Training report
                        zf.write(rep_path, "training_report.json")

                    st.success("‚úÖ ZIP created!")
                    with open(zip_path, "rb") as fp:
                        st.download_button(
                            "‚¨áÔ∏è Download ZIP",
                            data=fp,
                            file_name=zip_name,
                            mime="application/zip",
                            use_container_width=True
                        )
                except Exception as e:
                    st.error(f"‚ùå ZIP creation failed: {e}")


# -------------------------------------------------------------
# ‚úÖ STAGE 6 ‚Äî Deployment of Credit Scoring Model
# -------------------------------------------------------------
with tab_deploy:
    import os, json, shutil, zipfile
    from pathlib import Path
    from datetime import datetime, timezone

    st.markdown("## üöÄ Stage G ‚Äî Model Deployment")
    st.caption("Promote trained model ‚Üí publish ‚Üí export deployment bundle")

    last_model = st.session_state.get("credit_last_model_path")
    metrics = st.session_state.get("credit_last_metrics")
    report = st.session_state.get("credit_last_report")

    if not last_model:
        st.warning("‚ö†Ô∏è Train a model in Stage F before deploying.")
        st.stop()

    st.success(f"‚úÖ Latest trained model detected:\n`{last_model}`")
    st.json(metrics)

    # ---------------------------------------------------------
    # ‚úÖ Promote to production
    # ---------------------------------------------------------
    if st.button("‚úÖ Promote This Model to Production"):
        prod_dir = Path("./agents/credit_appraisal/models/production")
        prod_dir.mkdir(parents=True, exist_ok=True)

        shutil.copy(last_model, prod_dir / "model.joblib")

        prod_meta = {
            "model_path": last_model,
            "promoted_at": datetime.now(timezone.utc).isoformat(),
            "metrics": metrics,
            "report": report,
        }
        json.dump(prod_meta, open(prod_dir / "production_meta.json", "w"), indent=2)

        st.balloons()
        st.success("‚úÖ Model promoted to production successfully!")

    # ---------------------------------------------------------
    # ‚úÖ Export deployment ZIP
    # ---------------------------------------------------------
    EXPORT_DIR = Path("./exports")
    EXPORT_DIR.mkdir(exist_ok=True)
    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    if st.button("‚¨áÔ∏è Export Deployment Bundle"):
        zip_path = EXPORT_DIR / f"credit_deployment_{ts}.zip"
        with zipfile.ZipFile(zip_path, "w") as zf:
            zf.write(last_model, arcname="trained_model.joblib")
            zf.write("./agents/credit_appraisal/models/production/production_meta.json",
                     arcname="production_meta.json")

        with open(zip_path, "rb") as f:
            st.download_button(
                "‚¨áÔ∏è Download Deployment ZIP",
                data=f,
                file_name=zip_path.name,
                mime="application/zip",
            )
        st.success("‚úÖ Deployment bundle ready!")


# -------------------------------------------------------------
# ‚úÖ STAGE 7 ‚Äî Reporting & Handoff
# -------------------------------------------------------------
with tab_handoff:
    import os, json, zipfile
    import numpy as np
    import pandas as pd
    from pathlib import Path
    from datetime import datetime, timezone
    import streamlit as st
    import plotly.express as px

    st.markdown("## üìä Stage 7 ‚Äî Portfolio Reporting & Handoff")

    # 1) Primary: dataset saved by Stage C/E
    df = st.session_state.get("credit_scored_df")

    # 2) Fallback: Stage C merged output
    if df is None or df.empty:
        df = st.session_state.get("last_merged_df")

    # 3) Optional: user upload
    uploaded_scored = st.file_uploader(
        "‚¨ÜÔ∏è (Optional) Load scored CSV for reporting",
        type=["csv"], key="stage7_upload"
    )
    if uploaded_scored is not None:
        try:
            df = pd.read_csv(uploaded_scored)
            st.success(f"Loaded scored dataset from upload ({len(df)} rows).")
        except Exception as e:
            st.error(f"Could not read uploaded CSV: {e}")

    # Final guard
    if df is None or df.empty:
        st.warning("‚ö†Ô∏è Missing scored dataset. Run Stage 3 (Credit appraisal) or upload a scored CSV above.")
        st.stop()

    st.session_state["credit_scored_df"] = df.copy()

    st.success("‚úÖ Portfolio loaded.")
    st.dataframe(df.head(), use_container_width=True)

    # ‚Ä¶ (keep the rest of Stage 7: metrics, charts, handoff CSV/ZIP) ‚Ä¶


    # ---------------------------------------------------------
    # ‚úÖ Executive dashboard
    # ---------------------------------------------------------
    st.markdown("### üß≠ Executive Summary")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Applications", len(df))
    with col2:
        st.metric("Approved", (df["decision"] == "approve").sum())
    with col3:
        st.metric("Rejected", (df["decision"] == "reject").sum())

    # # ---------------------------------------------------------
    # # ‚úÖ Approval distribution
    # # ---------------------------------------------------------
    # # st.markdown("### üìà Approval Distribution")
    # # fig = px.histogram(df, x="decision", color="decision", title="Approval vs Rejection")
    # # st.plotly_chart(fig, use_container_width=True)
    # fig = px.histogram(
    # df, x="decision", color="decision",
    # color_discrete_sequence=PALETTE,
    # title="Approval vs Rejection"
    # )
    # fig = apply_dark(fig)
    # fig.update_traces(marker_line_width=0.5)
    # fig.update_xaxes(title=None, gridcolor="rgba(255,255,255,0.08)")
    # fig.update_yaxes(gridcolor="rgba(255,255,255,0.08)")
    # st.plotly_chart(fig, use_container_width=True)

    # ---------------------------------------------------------
    # ‚úÖ Approval distribution (robust to 0/1, strings, themes)
    # ---------------------------------------------------------
    st.markdown("### üìà Approval Distribution")

    # 1) Normalize labels
    if "decision" not in df.columns:
        st.info("No 'decision' column found; skipping approval chart.")
    else:
        vals = df["decision"]

        def to_label(v):
            if isinstance(v, str):
                s = v.strip().lower()
                if s in ("approve", "approved", "yes", "y", "1", "true"):
                    return "approve"
                if s in ("reject", "rejected", "no", "n", "0", "false"):
                    return "reject"
                return s or "unknown"
            try:
                return "approve" if float(v) >= 1 else "reject"
            except Exception:
                return "unknown"

        df = df.copy()
        df["decision_label"] = vals.map(to_label).fillna("unknown")

        # 2) Safe color map (must be a LIST ‚Üí map to dict)
        palette = px.colors.qualitative.Set2  # e.g., ['#66c2a5', '#fc8d62', ...]
        color_map = {
            "approve": palette[0] if len(palette) > 0 else "#22c55e",
            "reject":  palette[1] if len(palette) > 1 else "#ef4444",
            "unknown": palette[2] if len(palette) > 2 else "#94a3b8",
        }

        # 3) Fixed category order for readability
        categories = ["approve", "reject", "unknown"]

        fig = px.histogram(
            df,
            x="decision_label",
            color="decision_label",
            category_orders={"decision_label": categories},
            color_discrete_map=color_map,
            title="Approval vs Rejection",
        )
        fig.update_layout(
            legend_title_text="Decision",
            bargap=0.2,
            paper_bgcolor="rgba(0,0,0,0)",
            plot_bgcolor="rgba(0,0,0,0)",
            font_color=("#e2e8f0" if st.session_state.get("theme", "dark") == "dark" else "#0f172a"),
        )
        st.plotly_chart(fig, use_container_width=True)


    # ---------------------------------------------------------
    # ‚úÖ Department Handoff: Credit / Risk / Compliance / CS
    # ---------------------------------------------------------
    st.markdown("## üè¶ Department Handoff Packages")
    
    # ---------- helpers ----------
    def pick(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
        """Return df with only columns that actually exist (no KeyError)."""
        keep = [c for c in cols if c in df.columns]
        return df[keep].copy()

    # ---------- ensure 'reason' exists ----------
    if "reason" not in df.columns:
        if "explanation" in df.columns:
            df["reason"] = df["explanation"].astype(str).str.slice(0, 200)
        elif {"pd", "dti", "ltv"}.issubset(df.columns) or "score" in df.columns:
            def infer_reason(row):
                try:
                    if float(row.get("pd", 0)) >= 0.15:
                        return "High probability of default"
                    if float(row.get("dti", 0)) >= 0.5:
                        return "High debt-to-income"
                    if float(row.get("ltv", 0)) >= 0.8:
                        return "High loan-to-value"
                    if float(row.get("score", 999)) < 600:
                        return "Low credit score"
                except Exception:
                    pass
                return "Policy/Other"
            df["reason"] = df.apply(infer_reason, axis=1)
        else:
            df["reason"] = ""

    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
    HANDOFF_DIR = Path("./credit_handoff")
    ZIP_DIR = HANDOFF_DIR / "zips"
    HANDOFF_DIR.mkdir(exist_ok=True)
    ZIP_DIR.mkdir(exist_ok=True)
    
    credit = pick(df, ["application_id","score","decision","reason","income","loan_amount"])
    risk = pick(df, ["application_id","score","pd","ltv","dti","decision"])
    compliance = pick(df, ["application_id","account_age","delinquencies","fraud_flag","decision"])
    customer = pick(df, ["application_id","score","decision","explanation","reason"])


    # credit = df[["application_id","score","decision","reason","income","loan_amount"]]
    # risk = df[["application_id","score","pd","ltv","dti","decision"]]
    # compliance = df[["application_id","account_age","delinquencies","fraud_flag","decision"]]
    # customer = df[["application_id","score","decision","explanation"]]

    paths = {
        "credit": HANDOFF_DIR / f"credit_{ts}.csv",
        "risk": HANDOFF_DIR / f"risk_{ts}.csv",
        "compliance": HANDOFF_DIR / f"compliance_{ts}.csv",
        "customer": HANDOFF_DIR / f"customer_service_{ts}.csv",
    }

    # Save all
    credit.to_csv(paths["credit"], index=False)
    risk.to_csv(paths["risk"], index=False)
    compliance.to_csv(paths["compliance"], index=False)
    customer.to_csv(paths["customer"], index=False)

    # ---------------------------------------------------------
    # ‚úÖ ZIP bundle
    # ---------------------------------------------------------
    zip_path = ZIP_DIR / f"credit_handoff_{ts}.zip"
    with zipfile.ZipFile(zip_path, "w") as zf:
        for p in paths.values():
            zf.write(p, arcname=os.path.basename(p))

    st.download_button(
        "‚¨áÔ∏è Download Full Handoff ZIP",
        data=open(zip_path, "rb").read(),
        file_name=os.path.basename(zip_path),
        mime="application/zip",
        use_container_width=True,
    )

    # st.markdown("### üß© Department Package Map")
    # st.json({k: list(df[list(credit.columns)].columns)})
    st.markdown("### üß© Department Package Map")
    st.json({
        "credit":   list(credit.columns),
        "risk":     list(risk.columns),
        "compliance": list(compliance.columns),
        "customer_service": list(customer.columns),
    })

    # Optional: tell user if something was missing
    expected = {
        "credit": ["application_id","score","decision","reason","income","loan_amount"],
        "risk": ["application_id","score","pd","ltv","dti","decision"],
        "compliance": ["application_id","account_age","delinquencies","fraud_flag","decision"],
        "customer_service": ["application_id","score","decision","explanation","reason"],
    }
    missing_report = {
        pkg: [c for c in expected[pkg] if c not in cols]
        for pkg, cols in {
            "credit": credit.columns,
            "risk": risk.columns,
            "compliance": compliance.columns,
            "customer_service": customer.columns,
        }.items()
    }
    if any(missing_report.values()):
        st.info(f"Some expected columns were not present and were skipped: {missing_report}")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üó£Ô∏è TAB 8 ‚Äî Feedback & Feature Requests
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tab_feedback:
    st.subheader("üó£Ô∏è Share Your Feedback and Feature Ideas")

    FEEDBACK_FILE = os.path.join(BASE_DIR, "agents_feedback.json")

    def load_feedback() -> dict:
        try:
            with open(FEEDBACK_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    def save_feedback(data: dict):
        try:
            with open(FEEDBACK_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            st.error(f"Could not save feedback: {e}")

    feedback_data = load_feedback()
    if not feedback_data:
        # bootstrap section names to avoid empty selectbox
        feedback_data = {
            "üí≥ Credit Appraisal Agent": {"rating": 5, "users": 1, "comments": ["Great starting point!"]},
            "üè¶ Asset Appraisal Agent": {"rating": 5, "users": 1, "comments": ["Loving the dashboard."]},
        }

    st.markdown("### üí¨ Current Agent Reviews & Ratings")
    for agent, fb in feedback_data.items():
        with st.expander(f"‚≠ê {agent} ‚Äî {fb.get('rating', 0)}/5  |  üë• {fb.get('users', 0)} users"):
            st.markdown("#### Recent Comments:")
            for cmt in reversed(fb.get("comments", [])):
                st.markdown(f"- {cmt}")
            st.markdown("---")

    st.markdown("### ‚úçÔ∏è Submit Your Own Feedback or Feature Request")
    agent_choice = st.selectbox("Select Agent", list(feedback_data.keys()))
    new_comment = st.text_area("Your Comment or Feature Suggestion", placeholder="e.g. Add multi-language support for reports...")
    new_rating = st.slider("Your Rating", 1, 5, 5)

    if st.button("üì® Submit Feedback"):
        if new_comment.strip():
            fb = feedback_data.get(agent_choice, {"rating": 0, "users": 0, "comments": []})
            fb["comments"].append(new_comment.strip())
            fb["rating"] = round((fb.get("rating", 0) + new_rating) / 2, 2)
            fb["users"] = fb.get("users", 0) + 1
            feedback_data[agent_choice] = fb
            save_feedback(feedback_data)
            st.session_state["feedback_data"] = feedback_data
            st.success("‚úÖ Feedback submitted successfully!")
            st.rerun()
        else:
            st.warning("Please enter a comment before submitting.")




==================== ./chatbot.py ====================
from __future__ import annotations

import os
import requests
import streamlit as st

from services.ui.theme_manager import apply_theme, render_theme_toggle
from services.common.personas import list_personas
from services.ui.data.chatbot_faqs import get_agent_faqs

API_URL = os.getenv("AGENT_API_URL", "http://localhost:8090")

st.set_page_config(page_title="ü§ñ Gemma Chatbot", layout="wide")
apply_theme()

st.title("ü§ñ Gemma CPU Chatbot")
st.caption("External Gemma (Ollama) + Chroma RAG powered by recent agent CSV outputs.")
render_theme_toggle()

if "chatbot_history" not in st.session_state:
    st.session_state["chatbot_history"] = []
if "chatbot_sources" not in st.session_state:
    st.session_state["chatbot_sources"] = []
st.session_state.setdefault("chatbot_pending_prompt", None)
st.session_state.setdefault("chatbot_pending_agent", None)


def _post_json(path: str, payload: dict, *, params: dict | None = None, timeout: int = 120):
    resp = requests.post(
        f"{API_URL.rstrip('/')}{path}",
        json=payload,
        params=params,
        timeout=timeout,
    )
    resp.raise_for_status()
    return resp.json()


def _post_file(path: str, file_name: str, data: bytes, *, params: dict | None = None):
    files = {"file": (file_name, data, "application/octet-stream")}
    resp = requests.post(
        f"{API_URL.rstrip('/')}{path}",
        files=files,
        params=params,
        timeout=120,
    )
    resp.raise_for_status()
    return resp.json()


personas = list_personas()
persona_options = ["auto"] + [p["id"] for p in personas]
persona_lookup = {p["id"]: p for p in personas}

with st.sidebar:
    st.subheader("RAG Controls")
    selected_persona = st.selectbox(
        "Tag uploaded content to an agent persona",
        persona_options,
        format_func=lambda pid: "Auto-detect" if pid == "auto" else f"{pid}",
    )
    agent_param = None if selected_persona == "auto" else {"agent_id": selected_persona}

    uploaded = st.file_uploader("Upload CSV to ingest", type=["csv"])
    if uploaded is not None and st.button("Ingest uploaded CSV"):
        try:
            meta = _post_file(
                "/chatbot/ingest",
                uploaded.name,
                uploaded.getvalue(),
                params=agent_param,
            )
            st.success(f"Ingested {meta.get('rows_indexed', 0)} rows.")
        except requests.RequestException as exc:
            st.error(f"Upload failed: {exc}")

    upload_any = st.file_uploader(
        "Upload document (csv/txt/md/html/pdf/json/py)",
        type=["csv", "txt", "md", "html", "htm", "json", "pdf", "py", "log", "doc", "docx"],
        accept_multiple_files=False,
        help="Any file will be chunked into text and embedded for the selected persona.",
    )
    if upload_any is not None and st.button("Embed & ingest file"):
        try:
            meta = _post_file(
                "/chatbot/ingest/file",
                upload_any.name,
                upload_any.getvalue(),
                params=agent_param,
            )
            st.success(
                f"Ingested {meta.get('rows_indexed', 0)} chunks from {upload_any.name} "
                f"under persona '{agent_param.get('agent_id', 'auto') if agent_param else 'auto'}'."
            )
        except requests.RequestException as exc:
            st.error(f"Document upload failed: {exc}")

    if st.button("üîÑ Refresh RAG DB"):
        try:
            meta = _post_json("/chatbot/refresh", {}, params=None)
            st.success(
                "CSV rows: "
                f"{meta['csv'].get('rows_indexed', 0)} | Agent chunks: {meta['agent_ui'].get('rows_indexed', 0)} | "
                f"Doc chunks: {meta['docs'].get('rows_indexed', 0)} | Pruned runs: {meta['pruned_runs'].get('entries_removed', 0)}"
            )
        except requests.RequestException as exc:
            st.error(f"Refresh failed: {exc}")

    if st.button("‚ôªÔ∏è Hard Reset & Rebuild RAG"):
        try:
            meta = _post_json("/chatbot/refresh", {}, params={"reset": "true"})
            st.success(
                "Recreated store. CSV rows: "
                f"{meta['csv'].get('rows_indexed', 0)}, agent chunks: {meta['agent_ui'].get('rows_indexed', 0)}, "
                f"doc chunks: {meta['docs'].get('rows_indexed', 0)}, pruned runs: {meta['pruned_runs'].get('entries_removed', 0)}"
            )
        except requests.RequestException as exc:
            st.error(f"Rebuild failed: {exc}")

    st.markdown("**Current Model:** `Gemma (Ollama)`")
    st.caption("Ensure `ollama run gemma2:2b` (or similar) is active locally.")

    if personas:
        st.divider()
        st.subheader("Agent FAQs")
        default_persona_id = personas[0]["id"]
        faq_persona = st.selectbox(
            "Choose persona",
            [p["id"] for p in personas],
            format_func=lambda pid: f"{persona_lookup[pid]['emoji']} {persona_lookup[pid]['name']}",
            key="chatbot_faq_persona",
        )
        faqs = get_agent_faqs(faq_persona) or get_agent_faqs(default_persona_id)
        for idx, question in enumerate(faqs[:10]):
            if st.button(
                question,
                key=f"chatbot_faq_{faq_persona}_{idx}",
                help="Ask this FAQ using the selected persona namespace",
            ):
                st.session_state["chatbot_pending_prompt"] = question
                st.session_state["chatbot_pending_agent"] = faq_persona


def _handle_prompt(prompt_text: str, agent_override: str | None = None):
    """Send prompt to chatbot and stream response within the chat UI."""
    if not prompt_text:
        return
    target_agent = agent_override or (None if selected_persona == "auto" else selected_persona)
    agent_params = {"agent_id": target_agent} if target_agent else None
    st.session_state["chatbot_history"].append({"role": "user", "content": prompt_text})
    with st.chat_message("user"):
        st.markdown(prompt_text)
        if target_agent:
            persona = persona_lookup.get(target_agent)
            label = persona["name"] if persona else target_agent
            st.caption(f"Persona: {label}")
    try:
        response = _post_json("/chatbot/chat", {"question": prompt_text}, params=agent_params)
    except requests.RequestException as exc:
        st.error(f"Chat backend error: {exc}")
        return

    answer = response.get("answer", "No answer returned.")
    sources = response.get("sources", [])
    st.session_state["chatbot_history"].append(
        {"role": "assistant", "content": answer, "sources": sources}
    )
    st.session_state["chatbot_sources"] = sources
    with st.chat_message("assistant"):
        st.markdown(answer)
        if sources:
            st.caption("Sources:")
            for src in sources:
                st.write(f"- `{src['file']}` (score={src['score']})")


for message in st.session_state["chatbot_history"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and message.get("sources"):
            st.caption("Sources:")
            for src in message["sources"]:
                st.write(f"- `{src['file']}` (score={src['score']})")


pending_prompt = st.session_state.pop("chatbot_pending_prompt", None)
pending_agent = st.session_state.pop("chatbot_pending_agent", None)
prompt = st.chat_input("Ask about any agent pipeline‚Ä¶")
if prompt:
    _handle_prompt(prompt)
elif pending_prompt:
    _handle_prompt(pending_prompt, agent_override=pending_agent)


if st.session_state.get("chatbot_sources"):
    with st.expander("Sources used in last answer", expanded=True):
        for src in st.session_state["chatbot_sources"]:
            st.write(f"- `{src['file']}` ¬∑ score {src['score']}")



==================== ./_old/credit_appraisal.py.0711.py ====================
# services/ui/app.py
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üåê OpenSource AI Agent Library + Credit Appraisal PoC by Dzoan
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from __future__ import annotations
import os
import re
import io
import json
import datetime
from typing import Optional, Dict, List, Any

import pandas as pd
import numpy as np
import streamlit as st
import requests
import plotly.express as px
import plotly.graph_objects as go


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üîÅ Manage active tab navigation manually
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if "active_tab" not in st.session_state:
    st.session_state.active_tab = "tab_gen"  # or whichever tab should open by default

def switch_tab(tab_name: str):
    """Programmatically switch between workflow tabs."""
    st.session_state.active_tab = tab_name
    st.rerun()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üåå GLOBAL DARK THEME + BLUE GLOW ENHANCED UI
st.markdown("""
<style>
/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   GLOBAL BACKGROUND + TEXT
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
body, .stApp {
    background-color: #0f172a !important;   /* very dark navy */
    color: #e5e7eb !important;
    font-family: 'Inter', sans-serif;
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   HEADER + TITLE AREA
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
[data-testid="stHeader"], .stApp header {
    background: #0f172a !important;
    color: #f8fafc !important;
    border-bottom: 1px solid #1e293b !important;
}

/* Main App Title ‚Äî add neon blue glow */
h1, h2, h3 {
    color: #f8fafc !important;
    font-weight: 800 !important;
    text-shadow: 0 0 8px rgba(37,99,235,0.6),
                 0 0 16px rgba(59,130,246,0.3);
}

/* Subheaders (like Human Review, Credit Appraisal, etc.) */
.stSubheader, h4, h5, h6 {
    color: #f1f5f9 !important;
    font-weight: 700 !important;
    text-shadow: 0 0 4px rgba(37,99,235,0.4);
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   TAB BAR
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
div[data-baseweb="tab"] > button {
    color: #cbd5e1 !important;
    background-color: #1e293b !important;
    border: 1px solid #334155 !important;
    border-radius: 10px !important;
    font-weight: 600 !important;
}
div[data-baseweb="tab"] > button[aria-selected="true"] {
    background: linear-gradient(90deg, #2563eb, #1d4ed8);
    color: white !important;
    border: none !important;
    box-shadow: 0 0 12px rgba(37,99,235,0.5);
    transform: scale(1.05);
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   BOXES, EXPANDERS, AND CONTAINERS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.stExpander, .stFileUploader, .stDataFrame, .stJson, .stMetric {
    background-color: #1e293b !important;
    color: #f1f5f9 !important;
    border-radius: 10px !important;
}

/* DataFrames */
[data-testid="stDataFrame"] thead tr th {
    color: #f1f5f9 !important;
    background-color: #1e293b !important;
    border-bottom: 1px solid #334155 !important;
}
[data-testid="stDataFrame"] tbody tr td {
    color: #e2e8f0 !important;
}

/* Inputs & Selectors */
input, select, textarea {
    background-color: #1e293b !important;
    color: #f8fafc !important;
    border: 1px solid #334155 !important;
    border-radius: 8px !important;
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   BUTTONS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.stButton > button {
    background: linear-gradient(90deg, #2563eb, #1d4ed8) !important;
    color: white !important;
    font-weight: 700 !important;
    border-radius: 10px !important;
    padding: 10px 26px !important;
    border: none !important;
    box-shadow: 0 4px 14px rgba(0,0,0,0.3),
                0 0 10px rgba(37,99,235,0.4);
    transition: all 0.25s ease-in-out;
}
.stButton > button:hover {
    transform: translateY(-3px);
    background: linear-gradient(90deg, #1e40af, #1e3a8a) !important;
    box-shadow: 0 6px 18px rgba(0,0,0,0.4),
                0 0 18px rgba(37,99,235,0.6);
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   LINK STYLES
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
a {
    color: #60a5fa !important;
    text-decoration: none !important;
}
a:hover {
    color: #93c5fd !important;
    text-shadow: 0 0 6px rgba(96,165,250,0.6);
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   FILE UPLOADER + PROGRESS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.stFileUploader {
    border: 1px solid #334155 !important;
    background-color: #1e293b !important;
    border-radius: 10px !important;
    color: #e2e8f0 !important;
    box-shadow: inset 0 0 8px rgba(37,99,235,0.25);
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   SCROLLBARS (for aesthetics)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
::-webkit-scrollbar {
    width: 10px;
}
::-webkit-scrollbar-track {
    background: #1e293b;
}
::-webkit-scrollbar-thumb {
    background: #3b82f6;
    border-radius: 10px;
}
::-webkit-scrollbar-thumb:hover {
    background: #60a5fa;
}
</style>
""", unsafe_allow_html=True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üåü READABILITY & INPUT FIELD FIX PATCH

st.markdown("""
<style>
/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   FIX: Input Fields + Dropdowns Too Dark
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
input, select, textarea, .stTextInput, .stSelectbox, .stNumberInput {
    background-color: #1e293b !important;
    color: #ffffff !important;
    border: 1px solid #3b82f6 !important;
    border-radius: 8px !important;
    font-size: 18px !important;
    font-weight: 600 !important;
    padding: 8px 12px !important;
}

/* Dropdown menu items */
div[data-baseweb="select"] > div {
    background-color: #1e293b !important;
    color: #f8fafc !important;
    font-size: 18px !important;
}

/* Dropdown list options */
ul[role="listbox"] li {
    background-color: #1e293b !important;
    color: #f8fafc !important;
    font-size: 18px !important;
}
ul[role="listbox"] li:hover {
    background-color: #2563eb !important;
    color: white !important;
}

/* Sliders ‚Äî brighter and thicker */
[data-baseweb="slider"] div[role="slider"] {
    background-color: #3b82f6 !important;
}
[data-baseweb="slider"] div {
    height: 8px !important;
}
[data-testid="stSliderTickBar"] {
    background-color: #334155 !important;
}

/* Labels + Captions */
label, .stMarkdown p, .stCaption, .stText {
    font-size: 18px !important;
    color: #f1f5f9 !important;
    font-weight: 500 !important;
}

/* Subheaders and Section Titles */
h2, h3, h4, .stSubheader {
    font-size: 26px !important;
    color: #f8fafc !important;
    text-shadow: 0 0 8px rgba(59,130,246,0.4);
}

/* Decision Rule Section Styling */
[data-testid="stExpander"] > div:first-child {
    background-color: #111827 !important;
    color: #f8fafc !important;
    font-size: 20px !important;
    font-weight: 700 !important;
}

/* Tabs ‚Äî make them more visible and larger text */
div[data-baseweb="tab"] > button {
    font-size: 18px !important;
    padding: 10px 18px !important;
}

/* Global font size bump */
.stMarkdown, .stText, p, span, div {
    font-size: 18px !important;
}
</style>
""", unsafe_allow_html=True)
st.markdown("""
<style>
/* Brighten all radio + checkbox labels */
div[role="radio"], div[role="checkbox"] label, label[data-baseweb="radio"], label[data-baseweb="checkbox"] {
    color: #f8fafc !important;
    font-size: 18px !important;
    font-weight: 600 !important;
}

/* Fix small sub-labels near checkboxes */
div[role="radio"] p, div[role="checkbox"] p {
    color: #e2e8f0 !important;
    font-size: 16px !important;
}

/* Make rule mode label visible */
.stRadio label {
    color: #f1f5f9 !important;
    font-weight: 600 !important;
}

/* "Use LLM narrative" checkbox label */
[data-testid="stCheckbox"] label {
    color: #f8fafc !important;
    font-size: 18px !important;
    font-weight: 700 !important;
}
</style>
""", unsafe_allow_html=True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üîÅ Manage active tab navigation manually (INSERT HERE)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if "active_tab" not in st.session_state:
    st.session_state.active_tab = "tab_gen"  # or tab_train if you want to start from training

def switch_tab(tab_name: str):
    """Programmatically switch between workflow tabs."""
    st.session_state.active_tab = tab_name
    st.rerun()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GLOBAL CONFIG
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
BASE_DIR = os.path.expanduser("~/credit-appraisal-agent-poc/services/ui")
LANDING_IMG_DIR = os.path.join(BASE_DIR, "landing_images")
RUNS_DIR = os.path.join(BASE_DIR, ".runs")
TMP_FEEDBACK_DIR = os.path.join(BASE_DIR, ".tmp_feedback")

for d in (LANDING_IMG_DIR, RUNS_DIR, TMP_FEEDBACK_DIR):
    os.makedirs(d, exist_ok=True)

API_URL = os.getenv("API_URL", "http://localhost:8090")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SESSION STATE INIT
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if "stage" not in st.session_state:
    st.session_state.stage = "landing"
if "user_info" not in st.session_state:
    st.session_state.user_info = {"name": "", "email": "", "flagged": False}
if "logged_in" not in st.session_state:
    st.session_state.logged_in = False
if "flagged" not in st.session_state.user_info:
    st.session_state.user_info["flagged"] = False
if "timestamp" not in st.session_state.user_info:
    st.session_state.user_info["timestamp"] = datetime.datetime.utcnow().isoformat()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# PAGE CONFIG
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.set_page_config(
    page_title="AI Agent Sandbox ‚Äî By the People, For the People",
    layout="wide",
)



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# HELPERS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _clear_qp():
    """Clear query params (modern Streamlit API)."""
    try:
        st.query_params.clear()
    except Exception:
        pass


def load_image(base: str) -> Optional[str]:
    for ext in [".png", ".jpg", ".jpeg", ".webp", ".gif", ".svg"]:
        p = os.path.join(LANDING_IMG_DIR, f"{base}{ext}")
        if os.path.exists(p):
            return p
    return None


def save_uploaded_image(uploaded_file, base: str):
    if not uploaded_file:
        return None
    ext = os.path.splitext(uploaded_file.name)[1].lower() or ".png"
    dest = os.path.join(LANDING_IMG_DIR, f"{base}{ext}")
    with open(dest, "wb") as f:
        f.write(uploaded_file.getvalue())
    return dest


def render_image_tag(agent_id: str, industry: str, emoji_fallback: str) -> str:
    base = agent_id.lower().replace(" ", "_")
    img_path = load_image(base) or load_image(industry.replace(" ", "_"))
    if img_path:
        return (
            f'<img src="file://{img_path}" '
            f'style="width:48px;height:48px;border-radius:10px;object-fit:cover;">'
        )
    return f'<div style="font-size:32px;">{emoji_fallback}</div>'


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DATA
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
AGENTS = [
    ("üè¶ Banking & Finance", "üí∞ Retail Banking", "üí≥ Credit Appraisal Agent",
     "Explainable AI for loan decisioning", "Available", "üí≥"),
    ("üè¶ Banking & Finance", "üí∞ Retail Banking", "üè¶ Asset Appraisal Agent",
     "Market-driven collateral valuation", "Available", "üè¶"),
    ("üè¶ Banking & Finance", "ü©∫ Insurance", "ü©∫ Claims Triage Agent",
     "Automated claims prioritization", "Coming Soon", "ü©∫"),
    ("‚ö° Energy & Sustainability", "üîã EV & Charging", "‚ö° EV Charger Optimizer",
     "Optimize charger deployment via AI", "Coming Soon", "‚ö°"),
    ("‚ö° Energy & Sustainability", "‚òÄÔ∏è Solar", "‚òÄÔ∏è Solar Yield Estimator",
     "Estimate solar ROI and efficiency", "Coming Soon", "‚òÄÔ∏è"),
    ("üöó Automobile & Transport", "üöô Automobile", "üöó Predictive Maintenance",
     "Prevent downtime via sensor analytics", "Coming Soon", "üöó"),
    ("üöó Automobile & Transport", "üîã EV", "üîã EV Battery Health Agent",
     "Monitor EV battery health cycles", "Coming Soon", "üîã"),
    ("üöó Automobile & Transport", "üöö Ride-hailing / Logistics", "üõª Fleet Route Optimizer",
     "Dynamic route optimization for fleets", "Coming Soon", "üõª"),
    ("üíª Information Technology", "üß∞ Support & Security", "üß© IT Ticket Triage",
     "Auto-prioritize support tickets", "Coming Soon", "üß©"),
    ("üíª Information Technology", "üõ°Ô∏è Security", "üîê SecOps Log Triage",
     "Detect anomalies & summarize alerts", "Coming Soon", "üîê"),
    ("‚öñÔ∏è Legal & Government", "‚öñÔ∏è Law Firms", "‚öñÔ∏è Contract Analyzer",
     "Extract clauses and compliance risks", "Coming Soon", "‚öñÔ∏è"),
    ("‚öñÔ∏è Legal & Government", "üèõÔ∏è Public Services", "üèõÔ∏è Citizen Service Agent",
     "Smart assistant for citizen services", "Coming Soon", "üèõÔ∏è"),
    ("üõçÔ∏è Retail / SMB / Creative", "üè¨ Retail & eCommerce", "üìà Sales Forecast Agent",
     "Predict demand & inventory trends", "Coming Soon", "üìà"),
    ("üé¨ Retail / SMB / Creative", "üé® Media & Film", "üé¨ Budget Cost Assistant",
     "Estimate, optimize, and track film & production costs using AI", "Coming Soon", "üé¨"),
]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# STYLES
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.markdown(
    """
    <style>
    html, body, .block-container { background-color:#0f172a !important; color:#e2e8f0 !important; }
    footer { text-align:center; padding:2rem; color:#aab3c2; font-size:1.2rem; font-weight:600; margin-top:2rem; }
    .left-box {
        background: radial-gradient(circle at top left, #0f172a, #1e293b);
        border-radius:20px; padding:3rem 2rem; color:#f1f5f9; box-shadow:6px 0 24px rgba(0,0,0,0.4);
    }
    .right-box {
        background:linear-gradient(180deg,#1e293b,#0f172a);
        border-radius:20px; padding:2rem; box-shadow:-6px 0 24px rgba(0,0,0,0.35);
    }
    .stButton > button {
        border:none !important; cursor:pointer;
        padding:14px 28px !important; font-size:18px !important; font-weight:700 !important;
        border-radius:14px !important; color:#fff !important;
        background:linear-gradient(180deg,#4ea3ff 0%,#2f86ff 60%,#0f6fff 100%) !important;
        box-shadow:0 8px 24px rgba(15,111,255,0.35);
    }
    a.macbtn {
        display:inline-block; text-decoration:none !important; color:#fff !important;
        padding:10px 22px; font-weight:700; border-radius:12px;
        background:linear-gradient(180deg,#4ea3ff 0%,#2f86ff 60%,#0f6fff 100%);
    }
    /* Larger workflow tabs */
    [data-testid="stTabs"] [data-baseweb="tab"] {
        font-size: 28px !important;
        font-weight: 800 !important;
        padding: 20px 40px !important;
        border-radius: 12px !important;
        background-color: #1e293b !important;
        color: #f8fafc !important;
    }
    [data-testid="stTabs"] [data-baseweb="tab"][aria-selected="true"] {
        background: linear-gradient(90deg, #2563eb, #1d4ed8) !important;
        color: white !important;
        border-bottom: 6px solid #60a5fa !important;
        box-shadow: 0 4px 14px rgba(37,99,235,0.5);
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# QUERY PARAM ROUTING (modern API)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    qp = st.query_params
except Exception:
    qp = {}

if "stage" in qp:
    target = qp["stage"]
    if target in {"landing", "agents", "login", "credit_agent"} and st.session_state.stage != target:
        st.session_state.stage = target
        _clear_qp()
        st.rerun()

if "launch" in qp or ("agent" in qp and qp.get("agent") == ["credit"]):
    st.session_state.stage = "login"
    _clear_qp()
    st.rerun()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# STAGE: LANDING
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if st.session_state.stage == "landing":
    c1, c2 = st.columns([1.1, 1.9], gap="large")
    with c1:
        st.markdown("<div class='left-box'>", unsafe_allow_html=True)
        logo_path = load_image("people_logo")
        if logo_path:
            st.image(logo_path, width=160)
        else:
            up = st.file_uploader("Upload People Logo", type=["jpg", "png", "webp"], key="upload_logo")
            if up:
                save_uploaded_image(up, "people_logo")
                st.success("‚úÖ Logo uploaded, refresh to view.")
        st.markdown(
            """
            <h1 style="font-size:38px; font-weight:800;">üöÄ Together, Let‚Äôs Build an AI Foundry ‚Äî by the People, for the People</h1>
            <h3 style="font-size:28px; font-weight:700; color:#38bdf8;">‚öôÔ∏è Open AI Agent Sandbox ‚Äî From Idea to Production</h3>

            <p style="font-size:18px; line-height:1.8;">
            <span style="font-size:26px; font-weight:800; color:#60a5fa;">What:</span><br>
            The <b>Open AI Agent Sandbox</b> is a <b>Foundry</b> where your AI ideas become reality ‚Äî
            turning imagination into explainable, open, and living agents.
            </p>

            <p style="font-size:18px; line-height:1.8;">
            <span style="font-size:26px; font-weight:800; color:#60a5fa;">So What:</span><br>
            No CAPEX. No gatekeepers. Just <b>GPU-for-Rent power</b>, <b>open-source models</b>, and <b>privacy-first design</b>.
            Build faster, own your data, and innovate without limits.
            </p>

            <p style="font-size:18px; line-height:1.8;">
            <span style="font-size:26px; font-weight:800; color:#60a5fa;">How:</span><br>
            Start with a <b>ready-to-use AI Agent Template</b> ‚Äî customize, test, improve,
            and export when it‚Äôs production-ready.
            </p>

            <p style="font-size:18px; line-height:1.8;">
            <span style="font-size:26px; font-weight:800; color:#60a5fa;">Where:</span><br>
            All inside your <b>GPU-for-Rent Cloud Sandbox</b> ‚Äî
            a secure, sovereign forge where ideas ignite and models evolve.
            </p>

            <p style="font-size:18px; line-height:1.8;">
            <span style="font-size:26px; font-weight:800; color:#60a5fa;">For Who:</span><br>
            For builders, dreamers, educators, and enterprises who believe
            AI should empower the many, not the few.
            </p>

            <p style="font-size:18px; line-height:1.8;">
            <span style="font-size:26px; font-weight:800; color:#60a5fa;">What Now:</span><br>
            Bring your spark. Shape your agent. Forge your legacy.<br>
            <b>Your AI idea ‚Üí Production-ready Reality.</b>
            </p>
            """,
            unsafe_allow_html=True,
        )
        if st.button("üöÄ Start Building Now", key="btn_start_build_now"):
            st.session_state.stage = "agents"
            st.rerun()
        st.markdown("</div>", unsafe_allow_html=True)
    with c2:
        st.markdown("<div class='right-box'>", unsafe_allow_html=True)
        st.markdown("<h2>üìä Global AI Agent Library</h2>", unsafe_allow_html=True)
        rows = []
        for sector, industry, agent, desc, status, emoji in AGENTS:
            rows.append({
                "üñºÔ∏è": render_image_tag(agent, industry, emoji),
                "üè≠ Sector": sector,
                "üß© Industry": industry,
                "ü§ñ Agent": agent,
                "üß† Description": desc,
                "üì∂ Status": f'<span style="color:{"#22c55e" if status=="Available" else "#f59e0b"};">{status}</span>'
            })
        st.write(pd.DataFrame(rows).to_html(escape=False, index=False), unsafe_allow_html=True)
        st.markdown("</div>", unsafe_allow_html=True)
    st.markdown("<footer>Made with ‚ù§Ô∏è by DzoanNguyenTran@gmail.com ‚Äî Open AIgents Sandbox Initiative</footer>", unsafe_allow_html=True)
    st.stop()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# STAGE: AGENTS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if st.session_state.stage == "agents":
    top = st.columns([1, 4, 1])
    with top[0]:
        if st.button("‚¨ÖÔ∏è Back to Home", key="btn_back_home_from_agents"):
            st.session_state.stage = "landing"
            st.rerun()
    with top[1]:
        st.title("ü§ñ Available AI Agents")

    df = pd.DataFrame([
        {"Agent": "üí≥ Credit Appraisal Agent",
         "Description": "Explainable AI for retail loan decisioning",
         "Status": "‚úÖ Available",
         "Action": '<a class="macbtn" href="?agent=credit&stage=login">üöÄ Launch</a>'},
        {"Agent": "üè¶ Asset Appraisal Agent",
         "Description": "Market-driven collateral valuation",
	 "Status": "‚úÖ Available",
	 "Action": '<a class="macbtn" href="?agent=asset&stage=login">üöÄ Launch</a>'},
         
	 
	
    ])
    st.write(df.to_html(escape=False, index=False), unsafe_allow_html=True)
    st.markdown("<footer>Made with ‚ù§Ô∏è by DzoanNguyenTran@gmail.com ‚Äî Open AIgents Sandbox Initiative</footer>", unsafe_allow_html=True)
    st.stop()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# STAGE: LOGIN
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if st.session_state.stage == "login":
    top = st.columns([1, 4, 1])
    with top[0]:
        if st.button("‚¨ÖÔ∏è Back to Agents", key="btn_back_agents_from_login"):
            st.session_state.stage = "agents"
            st.rerun()
    with top[1]:
        st.title("üîê Login to AI Credit Appraisal Platform")
    c1, c2, c3 = st.columns([1, 1, 1])
    with c1:
        user = st.text_input("Username", placeholder="e.g. dzoan")
    with c2:
        email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
    with c3:
        pwd = st.text_input("Password", type="password", placeholder="Enter any password")
    if st.button("Login", key="btn_login_submit", use_container_width=True):
        if user.strip() and email.strip():
            st.session_state.user_info = {
                "name": user.strip(),
                "email": email.strip(),
                "flagged": False,
                "timestamp": datetime.datetime.utcnow().isoformat()
            }
            st.session_state.logged_in = True
            st.session_state.stage = "credit_agent"
            st.rerun()
        else:
            st.error("‚ö†Ô∏è Please fill all fields before continuing.")
    st.markdown("<footer>Made with ‚ù§Ô∏è by DzoanNguyenTran@gmail.com ‚Äî Open AIgents Sandbox Initiative</footer>", unsafe_allow_html=True)
    st.stop()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# STAGE: CREDIT WORKFLOW
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if st.session_state.stage == "credit_agent":
    top = st.columns([1, 4, 1])
    with top[0]:
        if st.button("‚¨ÖÔ∏è Back to Agents", key="btn_back_agents_from_pipeline"):
            st.session_state.stage = "agents"
            st.rerun()
    with top[1]:
        st.title("üí≥ AI Credit Appraisal Platform")
        st.caption("Generate, sanitize, and appraise credit with AI agent power and human insight.")

# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # WORKFLOW TABS ‚Äî full 6 steps
# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# tab_gen, tab_clean, tab_run, tab_review, tab_train, tab_feedback = st.tabs([
#     "üè¶ Synthetic Data Generator",
#     "üßπ Anonymize & Sanitize Data",
#     "ü§ñ Credit appraisal by AI assistant",
#     "üßë‚Äç‚öñÔ∏è Human Review",
#     "üîÅ Training (Feedback ‚Üí Retrain)",
#     "üó£Ô∏è Feedback & Feature Requests"
# ])





# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GLOBAL UTILS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


BANNED_NAMES = {"race", "gender", "religion", "ethnicity", "ssn", "national_id"}
PII_COLS = {"customer_name", "name", "email", "phone", "address", "ssn", "national_id", "dob"}

EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
PHONE_RE = re.compile(r"\+?\d[\d\-\s]{6,}\d")

def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:
    return df.loc[:, ~df.columns.duplicated(keep="last")]

def scrub_text_pii(s):
    if not isinstance(s, str):
        return s
    s = EMAIL_RE.sub("", s)
    s = PHONE_RE.sub("", s)
    return s.strip()

def drop_pii_columns(df: pd.DataFrame):
    original_cols = list(df.columns)
    keep_cols = [c for c in original_cols if all(k not in c.lower() for k in PII_COLS)]
    dropped = [c for c in original_cols if c not in keep_cols]
    out = df[keep_cols].copy()
    for c in out.select_dtypes(include="object"):
        out[c] = out[c].apply(scrub_text_pii)
    return dedupe_columns(out), dropped

def strip_policy_banned(df: pd.DataFrame) -> pd.DataFrame:
    keep = []
    for c in df.columns:
        cl = c.lower()
        if cl in BANNED_NAMES:
            continue
        keep.append(c)
    return df[keep]

def append_user_info(df: pd.DataFrame) -> pd.DataFrame:
    meta = st.session_state["user_info"]
    out = df.copy()
    out["session_user_name"] = meta["name"]
    out["session_user_email"] = meta["email"]
    out["session_flagged"] = meta["flagged"]
    out["created_at"] = meta["timestamp"]
    return dedupe_columns(out)

def save_to_runs(df: pd.DataFrame, prefix: str) -> str:
    ts = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
    flag_suffix = "_FLAGGED" if st.session_state["user_info"]["flagged"] else ""
    fname = f"{prefix}_{ts}{flag_suffix}.csv"
    fpath = os.path.join(RUNS_DIR, fname)
    dedupe_columns(df).to_csv(fpath, index=False)
    return fpath

def try_json(x):
    if isinstance(x, (dict, list)):
        return x
    if not isinstance(x, str):
        return None
    try:
        return json.loads(x)
    except Exception:
        return None

def _safe_json(x):
    if isinstance(x, dict):
        return x
    if isinstance(x, str) and x.strip():
        try:
            return json.loads(x)
        except Exception:
            return {}
    return {}

def fmt_currency_label(base: str) -> str:
    sym = st.session_state.get("currency_symbol", "")
    return f"{base} ({sym})" if sym else base

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CURRENCY CATALOG

CURRENCY_OPTIONS = {
    # code: (label, symbol, fx to apply on USD-like base generated numbers)
    "USD": ("USD $", "$", 1.0),
    "EUR": ("EUR ‚Ç¨", "‚Ç¨", 0.93),
    "GBP": ("GBP ¬£", "¬£", 0.80),
    "JPY": ("JPY ¬•", "¬•", 150.0),
    "VND": ("VND ‚Ç´", "‚Ç´", 24000.0),
}

def set_currency_defaults():
    if "currency_code" not in st.session_state:
        st.session_state["currency_code"] = "USD"
    label, symbol, fx = CURRENCY_OPTIONS[st.session_state["currency_code"]]
    st.session_state["currency_label"] = label
    st.session_state["currency_symbol"] = symbol
    st.session_state["currency_fx"] = fx

set_currency_defaults()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DASHBOARD HELPERS (Plotly, dark theme)

def _kpi_card(label: str, value: str, sublabel: str | None = None):
    st.markdown(
        f"""
        <div style="background:#0e1117;border:1px solid #2a2f3e;border-radius:12px;padding:14px 16px;margin-bottom:10px;">
          <div style="font-size:12px;color:#9aa4b2;text-transform:uppercase;letter-spacing:.06em;">{label}</div>
          <div style="font-size:28px;font-weight:700;color:#e6edf3;line-height:1.1;margin-top:2px;">{value}</div>
          {f'<div style="font-size:12px;color:#9aa4b2;margin-top:6px;">{sublabel}</div>' if sublabel else ''}
        </div>
        """,
        unsafe_allow_html=True,
    )

def render_credit_dashboard(df: pd.DataFrame, currency_symbol: str = ""):
    """
    Renders the whole dashboard (TOP-10s ‚Üí Opportunities ‚Üí KPIs & pies/bars ‚Üí Mix table).
    Keeps decision filter in the table only.
    """
    if df is None or df.empty:
        st.info("No data to visualize yet.")
        return

    cols = df.columns

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TOP 10s FIRST ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üîù Top 10 Snapshot")

    # Top 10 loans approved
    if {"decision", "loan_amount", "application_id"} <= set(cols):
        top_approved = df[df["decision"].astype(str).str.lower() == "approved"].copy()
        if not top_approved.empty:
            top_approved = top_approved.sort_values("loan_amount", ascending=False).head(10)
            fig = px.bar(
                top_approved,
                x="loan_amount",
                y="application_id",
                orientation="h",
                title="Top 10 Approved Loans",
                labels={"loan_amount": f"Loan Amount {currency_symbol}", "application_id": "Application"},
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No approved loans available to show top 10.")

    # Top 10 collateral types by average value
    if {"collateral_type", "collateral_value"} <= set(cols):
        cprof = df.groupby("collateral_type", dropna=False).agg(
            avg_value=("collateral_value", "mean"),
            cnt=("collateral_type", "count")
        ).reset_index()
        if not cprof.empty:
            cprof = cprof.sort_values("avg_value", ascending=False).head(10)
            fig = px.bar(
                cprof,
                x="avg_value",
                y="collateral_type",
                orientation="h",
                title="Top 10 Collateral Types (Avg Value)",
                labels={"avg_value": f"Avg Value {currency_symbol}", "collateral_type": "Collateral Type"},
                hover_data=["cnt"]
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    # Top 10 reasons for denial (from rule_reasons False flags)
    if "rule_reasons" in cols and "decision" in cols:
        denied = df[df["decision"].astype(str).str.lower() == "denied"].copy()
        reasons_count = {}
        for _, r in denied.iterrows():
            rr = _safe_json(r.get("rule_reasons"))
            if isinstance(rr, dict):
                for k, v in rr.items():
                    if v is False:
                        reasons_count[k] = reasons_count.get(k, 0) + 1
        if reasons_count:
            items = pd.DataFrame(sorted(reasons_count.items(), key=lambda x: x[1], reverse=True),
                                 columns=["reason", "count"]).head(10)
            fig = px.bar(
                items, x="count", y="reason", orientation="h",
                title="Top 10 Reasons for Denial",
                labels={"count": "Count", "reason": "Rule"},
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No denial reasons detected.")

    # Top 10 loan officer performance (approval rate) if officer column present
    officer_col = None
    for guess in ("loan_officer", "officer", "reviewed_by", "session_user_name"):
        if guess in cols:
            officer_col = guess
            break
    if officer_col and "decision" in cols:
        perf = (
            df.assign(is_approved=(df["decision"].astype(str).str.lower() == "approved").astype(int))
              .groupby(officer_col, dropna=False)["is_approved"]
              .agg(approved_rate="mean", n="count")
              .reset_index()
        )
        if not perf.empty:
            perf["approved_rate_pct"] = (perf["approved_rate"] * 100).round(1)
            perf = perf.sort_values(["approved_rate_pct", "n"], ascending=[False, False]).head(10)
            fig = px.bar(
                perf, x="approved_rate_pct", y=officer_col, orientation="h",
                title="Top 10 Loan Officer Approval Rate (this batch)",
                labels={"approved_rate_pct": "Approval Rate (%)", officer_col: "Officer"},
                hover_data=["n"]
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ OPPORTUNITIES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üí° Opportunities")

    # Short-term loan opportunities (simple heuristic)
    opp_rows = []
    if {"income", "loan_amount"}.issubset(cols):
        term_col = "loan_term_months" if "loan_term_months" in cols else ("loan_duration_months" if "loan_duration_months" in cols else None)
        if term_col:
            for _, r in df.iterrows():
                inc = float(r.get("income", 0) or 0)
                amt = float(r.get("loan_amount", 0) or 0)
                term = int(r.get(term_col, 0) or 0)
                dti = float(r.get("DTI", 0) or 0)
                if (term >= 36) and (amt <= inc * 0.8) and (dti <= 0.45):
                    opp_rows.append({
                        "application_id": r.get("application_id"),
                        "suggested_term": 24,
                        "loan_amount": amt,
                        "income": inc,
                        "DTI": dti,
                        "note": "Candidate for short-term plan (<=24m) based on affordability."
                    })
    if opp_rows:
        st.markdown("#### üìé Short-Term Loan Candidates")
        st.dataframe(pd.DataFrame(opp_rows).head(25), use_container_width=True, height=320)
    else:
        st.info("No short-term loan candidates identified in this batch.")

    st.markdown("#### üîÅ Buyback / Consolidation Beneficiaries")
    candidates = []
    need = {"decision", "existing_debt", "loan_amount", "DTI"}
    if need <= set(cols):
        for _, r in df.iterrows():
            dec = str(r.get("decision", "")).lower()
            debt = float(r.get("existing_debt", 0) or 0)
            loan = float(r.get("loan_amount", 0) or 0)
            dti = float(r.get("DTI", 0) or 0)
            proposal = _safe_json(r.get("proposed_consolidation_loan", {}))
            has_bb = bool(proposal)

            if dec == "denied" or dti > 0.45 or debt > loan:
                benefit_score = round((debt / (loan + 1e-6)) * 0.4 + dti * 0.6, 2)
                candidates.append({
                    "application_id": r.get("application_id"),
                    "customer_type": r.get("customer_type"),
                    "existing_debt": debt,
                    "loan_amount": loan,
                    "DTI": dti,
                    "collateral_type": r.get("collateral_type"),
                    "buyback_proposed": has_bb,
                    "buyback_amount": proposal.get("buyback_amount") if has_bb else None,
                    "benefit_score": benefit_score,
                    "note": proposal.get("note") if has_bb else None
                })
    if candidates:
        cand_df = pd.DataFrame(candidates).sort_values("benefit_score", ascending=False)
        st.dataframe(cand_df.head(25), use_container_width=True, height=380)
    else:
        st.info("No additional buyback beneficiaries identified.")

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PORTFOLIO KPIs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üìà Portfolio Snapshot")
    c1, c2, c3, c4 = st.columns(4)

    # Approval rate
    if "decision" in cols:
        total = len(df)
        approved = int((df["decision"].astype(str).str.lower() == "approved").sum())
        rate = (approved / total * 100) if total else 0.0
        with c1: _kpi_card("Approval Rate", f"{rate:.1f}%", f"{approved} of {total}")

    # Avg approved loan amount
    if {"decision", "loan_amount"} <= set(cols):
        ap = df[df["decision"].astype(str).str.lower() == "approved"]["loan_amount"]
        avg_amt = ap.mean() if len(ap) else 0.0
        with c2: _kpi_card("Avg Approved Amount", f"{currency_symbol}{avg_amt:,.0f}")

    # Decision time (if present)
    if {"created_at", "decision_at"} <= set(cols):
        try:
            t = (pd.to_datetime(df["decision_at"]) - pd.to_datetime(df["created_at"])).dt.total_seconds() / 60.0
            avg_min = float(t.mean())
            with c3: _kpi_card("Avg Decision Time", f"{avg_min:.1f} min")
        except Exception:
            with c3: _kpi_card("Avg Decision Time", "‚Äî")

    # Non-bank share
    if "customer_type" in cols:
        nb = int((df["customer_type"].astype(str).str.lower() == "non-bank").sum())
        total = len(df)
        share = (nb / total * 100) if total else 0.0
        with c4: _kpi_card("Non-bank Share", f"{share:.1f}%", f"{nb} of {total}")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ COMPOSITION & RISK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üß≠ Composition & Risk")

    # Approval vs Denial (pie)
    if "decision" in cols:
        pie_df = df["decision"].value_counts().rename_axis("Decision").reset_index(name="Count")
        fig = px.pie(pie_df, names="Decision", values="Count", title="Decision Mix")
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Avg DTI / LTV by decision (grouped bars)
    have_dti = "DTI" in cols
    have_ltv = "LTV" in cols
    if "decision" in cols and (have_dti or have_ltv):
        agg_map = {}
        if have_dti: agg_map["avg_DTI"] = ("DTI", "mean")
        if have_ltv: agg_map["avg_LTV"] = ("LTV", "mean")
        grp = df.groupby("decision").agg(**agg_map).reset_index()
        melted = grp.melt(id_vars=["decision"], var_name="metric", value_name="value")
        fig = px.bar(melted, x="decision", y="value", color="metric",
                     barmode="group", title="Average DTI / LTV by Decision")
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Loan term mix (stacked)
    term_col = "loan_term_months" if "loan_term_months" in cols else ("loan_duration_months" if "loan_duration_months" in cols else None)
    if term_col and "decision" in cols:
        mix = df.groupby([term_col, "decision"]).size().reset_index(name="count")
        fig = px.bar(
            mix, x=term_col, y="count", color="decision", title="Loan Term Mix",
            labels={term_col: "Term (months)", "count": "Count"}, barmode="stack"
        )
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Collateral avg value by type (bar)
    if {"collateral_type", "collateral_value"} <= set(cols):
        cprof = df.groupby("collateral_type").agg(
            avg_col=("collateral_value", "mean"),
            cnt=("collateral_type", "count")
        ).reset_index()
        fig = px.bar(
            cprof.sort_values("avg_col", ascending=False),
            x="collateral_type", y="avg_col",
            title=f"Avg Collateral Value by Type ({currency_symbol})",
            hover_data=["cnt"]
        )
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Top proposed plans (horizontal bar)
    if "proposed_loan_option" in cols:
        plans = df["proposed_loan_option"].dropna().astype(str)
        if len(plans) > 0:
            plan_types = []
            for s in plans:
                p = _safe_json(s)
                plan_types.append(p.get("type") if isinstance(p, dict) and "type" in p else s)
            plan_df = pd.Series(plan_types).value_counts().head(10).rename_axis("plan").reset_index(name="count")
            fig = px.bar(
                plan_df, x="count", y="plan", orientation="h",
                title="Top 10 Proposed Plans"
            )
            fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    # Customer mix table (bank vs non-bank)
    if "customer_type" in cols:
        mix = df["customer_type"].value_counts().rename_axis("Customer Type").reset_index(name="Count")
        mix["Ratio"] = (mix["Count"] / mix["Count"].sum()).round(3)
        st.markdown("### üë• Customer Mix")
        st.dataframe(mix, use_container_width=True, height=220)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# WORKFLOW TABS ‚Äî full 6 steps
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
tab_gen, tab_clean, tab_run, tab_review, tab_train, tab_feedback = st.tabs([
    "üè¶ Synthetic Data Generator",
    "üßπ Anonymize & Sanitize Data",
    "ü§ñ Credit appraisal by AI assistant",
    "üßë‚Äç‚öñÔ∏è Human Review",
    "üîÅ Training (Feedback ‚Üí Retrain)",
    "üó£Ô∏è Feedback & Feature Requests"
])

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DATA GENERATORS

def generate_raw_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    names = ["Alice Nguyen","Bao Tran","Chris Do","Duy Le","Emma Tran",
             "Felix Nguyen","Giang Ho","Hanh Vo","Ivan Pham","Julia Ngo"]
    emails = [f"{n.split()[0].lower()}.{n.split()[1].lower()}@gmail.com" for n in names]
    addrs = [
        "23 Elm St, Boston, MA","19 Pine Ave, San Jose, CA","14 High St, London, UK",
        "55 Nguyen Hue, Ho Chi Minh","78 Oak St, Chicago, IL","10 Broadway, New York, NY",
        "8 Rue Lafayette, Paris, FR","21 K√∂nigstr, Berlin, DE","44 Maple Dr, Los Angeles, CA","22 Bay St, Toronto, CA"
    ]
    is_non = rng.random(n) < non_bank_ratio
    cust_type = np.where(is_non, "non-bank", "bank")

    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "customer_name": np.random.choice(names, n),
        "email": np.random.choice(emails, n),
        "phone": [f"+1-202-555-{1000+i:04d}" for i in range(n)],
        "address": np.random.choice(addrs, n),
        "national_id": rng.integers(10_000_000, 99_999_999, n),
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["real_estate","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
        "current_loans": rng.integers(0, 5, n),
        "customer_type": cust_type,
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))

    fx = st.session_state["currency_fx"]
    for c in ("income", "loan_amount", "collateral_value", "assets_owned", "existing_debt"):
        df[c] = (df[c] * fx).round(2)
    df["currency_code"] = st.session_state["currency_code"]
    return dedupe_columns(df)

def generate_anon_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    is_non = rng.random(n) < non_bank_ratio
    cust_type = np.where(is_non, "non-bank", "bank")

    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["real_estate","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
        "current_loans": rng.integers(0, 5, n),
        "customer_type": cust_type,
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))

    fx = st.session_state["currency_fx"]
    for c in ("income", "loan_amount", "collateral_value", "assets_owned", "existing_debt"):
        df[c] = (df[c] * fx).round(2)
    df["currency_code"] = st.session_state["currency_code"]
    return dedupe_columns(df)

def to_agent_schema(df: pd.DataFrame) -> pd.DataFrame:
    """
    Harmonize to the server-side agent‚Äôs expected schema.
    """
    out = df.copy()
    n = len(out)
    if "employment_years" not in out.columns:
        out["employment_years"] = out.get("employment_length", 0)
    if "debt_to_income" not in out.columns:
        if "DTI" in out.columns:
            out["debt_to_income"] = out["DTI"].astype(float)
        elif "existing_debt" in out.columns and "income" in out.columns:
            denom = out["income"].replace(0, np.nan)
            dti = (out["existing_debt"] / denom).fillna(0.0)
            out["debt_to_income"] = dti.clip(0, 10)
        else:
            out["debt_to_income"] = 0.0
    rng = np.random.default_rng(12345)
    if "credit_history_length" not in out.columns:
        out["credit_history_length"] = rng.integers(0, 30, n)
    if "num_delinquencies" not in out.columns:
        out["num_delinquencies"] = np.minimum(rng.poisson(0.2, n), 10)
    if "requested_amount" not in out.columns:
        out["requested_amount"] = out.get("loan_amount", 0)
    if "loan_term_months" not in out.columns:
        out["loan_term_months"] = out.get("loan_duration_months", 0)
    return dedupe_columns(out)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üè¶ TAB 1 ‚Äî Synthetic Data Generator
with tab_gen:
    st.subheader("üè¶ Synthetic Credit Data Generator")

    # Currency selector (before generation)
    c1, c2 = st.columns([1, 2])
    with c1:
        code = st.selectbox(
            "Currency",
            list(CURRENCY_OPTIONS.keys()),
            index=list(CURRENCY_OPTIONS.keys()).index(st.session_state["currency_code"]),
            help="All monetary fields will be in this local currency."
        )
        if code != st.session_state["currency_code"]:
            st.session_state["currency_code"] = code
            set_currency_defaults()
    with c2:
        st.markdown(
            f"""
            <div style='background-color:#1e293b; padding:12px 16px; border-radius:8px;'>
                <span style='font-weight:600; color:#f8fafc;'>
                    üí∞ Amounts will be generated in
                    <span style='color:#4ade80;'>{st.session_state['currency_label']}</span>.
                </span>
            </div>
            """,
            unsafe_allow_html=True,
        )


    rows = st.slider("Number of rows to generate", 50, 2000, 200, step=50)
    non_bank_ratio = st.slider("Share of non-bank customers", 0.0, 1.0, 0.30, 0.05)

    colA, colB = st.columns(2)
    with colA:
        if st.button("üî¥ Generate RAW Synthetic Data (with PII)", use_container_width=True):
            raw_df = append_user_info(generate_raw_synthetic(rows, non_bank_ratio))
            st.session_state.synthetic_raw_df = raw_df
            raw_path = save_to_runs(raw_df, "synthetic_raw")
            st.success(f"Generated RAW (PII) dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {raw_path}")
            st.dataframe(raw_df.head(10), use_container_width=True)
            st.download_button(
                "‚¨áÔ∏è Download RAW CSV",
                raw_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(raw_path),
                "text/csv"
            )

    with colB:
        if st.button("üü¢ Generate ANON Synthetic Data (ready for agent)", use_container_width=True):
            anon_df = append_user_info(generate_anon_synthetic(rows, non_bank_ratio))
            st.session_state.synthetic_df = anon_df
            anon_path = save_to_runs(anon_df, "synthetic_anon")
            st.success(f"Generated ANON dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {anon_path}")
            st.dataframe(anon_df.head(10), use_container_width=True)
            st.download_button(
                "‚¨áÔ∏è Download ANON CSV",
                anon_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(anon_path),
                "text/csv"
            )

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üßπ TAB 2 ‚Äî Anonymize & Sanitize Data
with tab_clean:
    st.subheader("üßπ Upload & Anonymize Customer Data (PII columns will be DROPPED)")
    st.markdown("Upload your **real CSV**. We drop PII columns and scrub emails/phones in text fields.")

    uploaded = st.file_uploader("Upload CSV file", type=["csv"])
    if uploaded:
        try:
            df = pd.read_csv(uploaded)
        except Exception as e:
            st.error(f"Could not read CSV: {e}")
            st.stop()

        st.write("üìä Original Data Preview:")
        st.dataframe(dedupe_columns(df.head(5)), use_container_width=True)

        sanitized, dropped_cols = drop_pii_columns(df)
        sanitized = append_user_info(sanitized)
        sanitized = dedupe_columns(sanitized)
        st.session_state.anonymized_df = sanitized

        st.success(f"Dropped PII columns: {sorted(dropped_cols) if dropped_cols else 'None'}")
        st.write("‚úÖ Sanitized Data Preview:")
        st.dataframe(sanitized.head(5), use_container_width=True)

        fpath = save_to_runs(sanitized, "anonymized")
        st.success(f"Saved anonymized file: {fpath}")
        st.download_button(
            "‚¨áÔ∏è Download Clean Data",
            sanitized.to_csv(index=False).encode("utf-8"),
            os.path.basename(fpath),
            "text/csv"
        )
    else:
        st.info("Choose a CSV to see the sanitize flow.", icon="‚ÑπÔ∏è")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ü§ñ TAB 3 ‚Äî Credit appraisal by AI assistant
with tab_run:
    st.subheader("ü§ñ Credit appraisal by AI assistant")
    # Anchor for loopback link from Training tab
    st.markdown('<a name="credit-appraisal-stage"></a>', unsafe_allow_html=True)

    # Production model banner (optional)
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.status_code == 200:
            meta = resp.json()
            if meta.get("has_production"):
                ver = (meta.get("meta") or {}).get("version", "1.x")
                src = (meta.get("meta") or {}).get("source", "production")
                st.success(f"üü¢ Production model active ‚Äî version: {ver} ‚Ä¢ source: {src}")
            else:
                st.warning("‚ö†Ô∏è No production model promoted yet ‚Äî using baseline.")
        else:
            st.info("‚ÑπÔ∏è Could not fetch production model meta.")
    except Exception:
        st.info("‚ÑπÔ∏è Production meta unavailable.")

    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß© Model Selection (list all trained models) ‚Äî HARD-CODED TEST
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    from datetime import datetime
    import os, shutil, streamlit as st

    # Hardcoded absolute paths for your environment
    trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/trained"
    production_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/production"

    # Debug info
    st.caption(f"üìÇ Trained dir: `{trained_dir}`")
    st.caption(f"üì¶ Production dir: `{production_dir}`")

    # Refresh button
    if st.button("‚Üª Refresh models", key="credit_refresh_models"):
        st.session_state.pop("selected_trained_model", None)
        st.rerun()

    models = []
    if os.path.isdir(trained_dir):
        for f in os.listdir(trained_dir):
            if f.endswith(".joblib"):
                fpath = os.path.join(trained_dir, f)
                ctime = os.path.getctime(fpath)
                created = datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
                models.append((f, fpath, created))
    else:
        st.error(f"‚ùå Trained dir not found: {trained_dir}")

    if models:
        # Sort by creation time (latest first)
        models.sort(key=lambda x: os.path.getctime(x[1]), reverse=True)
        display_names = [f"{m[0]} ‚Äî {m[2]}" for m in models]

        selected_display = st.selectbox("üì¶ Select trained model to use", display_names)
        selected_model = models[display_names.index(selected_display)][1]
        st.success(f"‚úÖ Using model: {os.path.basename(selected_model)}")

        st.session_state["selected_trained_model"] = selected_model

        if st.button("üöÄ Promote this model to Production"):
            try:
                os.makedirs(production_dir, exist_ok=True)
                prod_path = os.path.join(production_dir, "model.joblib")
                shutil.copy2(selected_model, prod_path)
                st.success(f"‚úÖ Model promoted to production: {prod_path}")
            except Exception as e:
                st.error(f"‚ùå Promotion failed: {e}")
    else:
        st.warning("‚ö†Ô∏è No trained models found ‚Äî train one in Step 5 first.")

    # # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # # üß© Model Selection (list all trained models)
    # # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # trained_dir = os.path.expanduser(
    #     "~/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/trained"
    # )
    # models = []
    # if os.path.exists(trained_dir):
    #     for f in os.listdir(trained_dir):
    #         if f.endswith(".joblib"):
    #             fpath = os.path.join(trained_dir, f)
    #             ctime = os.path.getctime(fpath)
    #             created = datetime.datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
    #             models.append((f, fpath, created))

    # if models:
    #     models.sort(key=lambda x: x[2], reverse=True)
    #     display_names = [f"{m[0]} ‚Äî {m[2]}" for m in models]

    #     selected_display = st.selectbox("üì¶ Select trained model to use", display_names)
    #     selected_model = models[display_names.index(selected_display)][1]
    #     st.success(f"‚úÖ Using model: {os.path.basename(selected_model)}")

    #     # Store for later use by /run API
    #     st.session_state["selected_trained_model"] = selected_model

    #     # Optional promote button
    #     if st.button("üöÄ Promote this model to Production"):
    #         try:
    #             prod_path = os.path.expanduser(
    #                 "~/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/production/model.joblib"
    #             )
    #             os.makedirs(os.path.dirname(prod_path), exist_ok=True)
    #             import shutil
    #             shutil.copy2(selected_model, prod_path)
    #             st.success(f"‚úÖ Model promoted to production: {os.path.basename(prod_path)}")
    #         except Exception as e:
    #             st.error(f"‚ùå Promotion failed: {e}")
    # else:
    #     st.warning("‚ö†Ô∏è No trained models found ‚Äî train one in Step 5 first.")

    # 1) Model + Hardware selection (UI hints)
    LLM_MODELS = [
        ("Phi-3 Mini (3.8B) ‚Äî CPU OK", "phi3:3.8b", "CPU 8GB RAM (fast)"),
        ("Mistral 7B Instruct ‚Äî CPU slow / GPU OK", "mistral:7b-instruct", "CPU 16GB (slow) or GPU ‚â•8GB"),
        ("Gemma-2 7B ‚Äî CPU slow / GPU OK", "gemma2:7b", "CPU 16GB (slow) or GPU ‚â•8GB"),
        ("LLaMA-3 8B ‚Äî GPU recommended", "llama3:8b-instruct", "GPU ‚â•12GB (CPU very slow)"),
        ("Qwen2 7B ‚Äî GPU recommended", "qwen2:7b-instruct", "GPU ‚â•12GB (CPU very slow)"),
        ("Mixtral 8x7B ‚Äî GPU only (big)", "mixtral:8x7b-instruct", "GPU 24‚Äì48GB"),
    ]
    LLM_LABELS = [l for (l, _, _) in LLM_MODELS]
    LLM_VALUE_BY_LABEL = {l: v for (l, v, _) in LLM_MODELS}
    LLM_HINT_BY_LABEL = {l: h for (l, _, h) in LLM_MODELS}

    OPENSTACK_FLAVORS = {
        "m4.medium":  "4 vCPU / 8 GB RAM ‚Äî CPU-only small",
        "m8.large":   "8 vCPU / 16 GB RAM ‚Äî CPU-only medium",
        "g1.a10.1":   "8 vCPU / 32 GB RAM + 1√óA10 24GB",
        "g1.l40.1":   "16 vCPU / 64 GB RAM + 1√óL40 48GB",
        "g2.a100.1":  "24 vCPU / 128 GB RAM + 1√óA100 80GB",
    }

    with st.expander("üß† Local LLM & Hardware Profile", expanded=True):
        c1, c2 = st.columns([1.2, 1])
        with c1:
            model_label = st.selectbox("Local LLM (used for narratives/explanations)", LLM_LABELS, index=1)
            llm_value = LLM_VALUE_BY_LABEL[model_label]
            st.caption(f"Hint: {LLM_HINT_BY_LABEL[model_label]}")
        with c2:
            flavor = st.selectbox("OpenStack flavor / host profile", list(OPENSTACK_FLAVORS.keys()), index=0)
            st.caption(OPENSTACK_FLAVORS[flavor])
        st.caption("These are passed to the API as hints; your API can choose Ollama/Flowise backends accordingly.")

    # 2) Data Source
    data_choice = st.selectbox(
        "Select Data Source",
        [
            "Use synthetic (ANON)",
            "Use synthetic (RAW ‚Äì auto-sanitize)",
            "Use anonymized dataset",
            "Upload manually",
        ]
    )
    use_llm = st.checkbox("Use LLM narrative", value=False)
    agent_name = "credit_appraisal"

    if data_choice == "Upload manually":
        up = st.file_uploader("Upload your CSV", type=["csv"], key="manual_upload_run_file")
        if up is not None:
            st.session_state["manual_upload_name"] = up.name
            st.session_state["manual_upload_bytes"] = up.getvalue()
            st.success(f"File staged: {up.name} ({len(st.session_state['manual_upload_bytes'])} bytes)")

    # 3) Rules
    st.markdown("### ‚öôÔ∏è Decision Rule Set")
    rule_mode = st.radio(
        "Choose rule mode",
        ["Classic (bank-style metrics)", "NDI (Net Disposable Income) ‚Äî simple"],
        index=0,
        help="NDI = income - all monthly obligations. Approve if NDI and NDI ratio pass thresholds."
    )

    CLASSIC_DEFAULTS = {
        "max_dti": 0.45, "min_emp_years": 2, "min_credit_hist": 3, "salary_floor": 3000,
        "max_delinquencies": 2, "max_current_loans": 3, "req_min": 1000, "req_max": 200000,
        "loan_terms": [12, 24, 36, 48, 60], "threshold": 0.45, "target_rate": None, "random_band": True,
        "min_income_debt_ratio": 0.35, "compounded_debt_factor": 1.0, "monthly_debt_relief": 0.50,
    }
    NDI_DEFAULTS = {"ndi_value": 800.0, "ndi_ratio": 0.50, "threshold": 0.45, "target_rate": None, "random_band": True}

    if "classic_rules" not in st.session_state:
        st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    if "ndi_rules" not in st.session_state:
        st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    def reset_classic(): st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    def reset_ndi():     st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    if rule_mode.startswith("Classic"):
        with st.expander("Classic Metrics (with Reset)", expanded=True):
            rc = st.session_state.classic_rules
            r1, r2, r3 = st.columns(3)
            with r1:
                rc["max_dti"] = st.slider("Max Debt-to-Income (DTI)", 0.0, 1.0, rc["max_dti"], 0.01)
                rc["min_emp_years"] = st.number_input("Min Employment Years", 0, 40, rc["min_emp_years"])
                rc["min_credit_hist"] = st.number_input("Min Credit History (years)", 0, 40, rc["min_credit_hist"])
            with r2:
                rc["salary_floor"] = st.number_input("Minimum Monthly Salary", 0, 1_000_000_000, rc["salary_floor"], step=1000, help=fmt_currency_label("in local currency"))
                rc["max_delinquencies"] = st.number_input("Max Delinquencies", 0, 10, rc["max_delinquencies"])
                rc["max_current_loans"] = st.number_input("Max Current Loans", 0, 10, rc["max_current_loans"])
            with r3:
                rc["req_min"] = st.number_input(fmt_currency_label("Requested Amount Min"), 0, 10_000_000_000, rc["req_min"], step=1000)
                rc["req_max"] = st.number_input(fmt_currency_label("Requested Amount Max"), 0, 10_000_000_000, rc["req_max"], step=1000)
                rc["loan_terms"] = st.multiselect("Allowed Loan Terms (months)", [12,24,36,48,60,72], default=rc["loan_terms"])

            st.markdown("#### üßÆ Debt Pressure Controls")
            d1, d2, d3 = st.columns(3)
            with d1:
                rc["min_income_debt_ratio"] = st.slider("Min Income / (Compounded Debt) Ratio", 0.10, 2.00, rc["min_income_debt_ratio"], 0.01)
            with d2:
                rc["compounded_debt_factor"] = st.slider("Compounded Debt Factor (√ó requested)", 0.5, 3.0, rc["compounded_debt_factor"], 0.1)
            with d3:
                rc["monthly_debt_relief"] = st.slider("Monthly Debt Relief Factor", 0.10, 1.00, rc["monthly_debt_relief"], 0.05)

            st.markdown("---")
            c1, c2, c3 = st.columns([1,1,1])
            with c1:
                use_target = st.toggle("üéØ Use target approval rate", value=(rc["target_rate"] is not None))
            with c2:
                rc["random_band"] = st.toggle("üé≤ Randomize approval band (20‚Äì60%) when no target", value=rc["random_band"])
            with c3:
                if st.button("‚Ü©Ô∏è Reset to defaults"):
                    reset_classic()
                    st.rerun()

            if use_target:
                rc["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rc["target_rate"] or 0.40, 0.01)
                rc["threshold"] = None
            else:
                rc["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rc["threshold"], 0.01)
                rc["target_rate"] = None
    else:
        with st.expander("NDI Metrics (with Reset)", expanded=True):
            rn = st.session_state.ndi_rules
            n1, n2 = st.columns(2)
            with n1:
                rn["ndi_value"] = st.number_input(fmt_currency_label("Min NDI (Net Disposable Income) per month"), 0.0, 1e12, float(rn["ndi_value"]), step=50.0)
            with n2:
                rn["ndi_ratio"] = st.slider("Min NDI / Income ratio", 0.0, 1.0, float(rn["ndi_ratio"]), 0.01)
            st.caption("NDI = income - all monthly obligations (rent, food, loans, cards, etc.).")

            st.markdown("---")
            c1, c2, c3 = st.columns([1,1,1])
            with c1:
                use_target = st.toggle("üéØ Use target approval rate", value=(rn["target_rate"] is not None))
            with c2:
                rn["random_band"] = st.toggle("üé≤ Randomize approval band (20‚Äì60%) when no target", value=rn["random_band"])
            with c3:
                if st.button("‚Ü©Ô∏è Reset to defaults (NDI)"):
                    reset_ndi()
                    st.rerun()

            if use_target:
                rn["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rn["target_rate"] or 0.40, 0.01)
                rn["threshold"] = None
            else:
                rn["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rn["threshold"], 0.01)
                rn["target_rate"] = None

    # 4) Run
    if st.button("üöÄ Run Agent", use_container_width=True):
        try:
            files = None
            data: Dict[str, Any] = {
                "use_llm_narrative": str(use_llm).lower(),
                "llm_model": llm_value,
                "hardware_flavor": flavor,
                "currency_code": st.session_state["currency_code"],
                "currency_symbol": st.session_state["currency_symbol"],
            }
            if rule_mode.startswith("Classic"):
                rc = st.session_state.classic_rules
                data.update({
                    "min_employment_years": str(rc["min_emp_years"]),
                    "max_debt_to_income": str(rc["max_dti"]),
                    "min_credit_history_length": str(rc["min_credit_hist"]),
                    "max_num_delinquencies": str(rc["max_delinquencies"]),
                    "max_current_loans": str(rc["max_current_loans"]),
                    "requested_amount_min": str(rc["req_min"]),
                    "requested_amount_max": str(rc["req_max"]),
                    "loan_term_months_allowed": ",".join(map(str, rc["loan_terms"])) if rc["loan_terms"] else "",
                    "min_income_debt_ratio": str(rc["min_income_debt_ratio"]),
                    "compounded_debt_factor": str(rc["compounded_debt_factor"]),
                    "monthly_debt_relief": str(rc["monthly_debt_relief"]),
                    "salary_floor": str(rc["salary_floor"]),
                    "threshold": "" if rc["threshold"] is None else str(rc["threshold"]),
                    "target_approval_rate": "" if rc["target_rate"] is None else str(rc["target_rate"]),
                    "random_band": str(rc["random_band"]).lower(),
                    "random_approval_band": str(rc["random_band"]).lower(),
                    "rule_mode": "classic",
                })
            else:
                rn = st.session_state.ndi_rules
                data.update({
                    "ndi_value": str(rn["ndi_value"]),
                    "ndi_ratio": str(rn["ndi_ratio"]),
                    "threshold": "" if rn["threshold"] is None else str(rn["threshold"]),
                    "target_approval_rate": "" if rn["target_rate"] is None else str(rn["target_rate"]),
                    "random_band": str(rn["random_band"]).lower(),
                    "random_approval_band": str(rn["random_band"]).lower(),
                    "rule_mode": "ndi",
                })

            def prep_and_pack(df: pd.DataFrame, filename: str):
                safe = dedupe_columns(df)
                safe, _ = drop_pii_columns(safe)
                safe = strip_policy_banned(safe)
                safe = to_agent_schema(safe)
                buf = io.StringIO()
                safe.to_csv(buf, index=False)
                return {"file": (filename, buf.getvalue().encode("utf-8"), "text/csv")}

            if data_choice == "Use synthetic (ANON)":
                if "synthetic_df" not in st.session_state:
                    st.warning("No ANON synthetic dataset found. Generate it in the first tab."); st.stop()
                files = prep_and_pack(st.session_state.synthetic_df, "synthetic_anon.csv")

            elif data_choice == "Use synthetic (RAW ‚Äì auto-sanitize)":
                if "synthetic_raw_df" not in st.session_state:
                    st.warning("No RAW synthetic dataset found. Generate it in the first tab."); st.stop()
                files = prep_and_pack(st.session_state.synthetic_raw_df, "synthetic_raw_sanitized.csv")

            elif data_choice == "Use anonymized dataset":
                if "anonymized_df" not in st.session_state:
                    st.warning("No anonymized dataset found. Create it in the second tab."); st.stop()
                files = prep_and_pack(st.session_state.anonymized_df, "anonymized.csv")

            elif data_choice == "Upload manually":
                up_name = st.session_state.get("manual_upload_name")
                up_bytes = st.session_state.get("manual_upload_bytes")
                if not up_name or not up_bytes:
                    st.warning("Please upload a CSV first."); st.stop()
                try:
                    tmp_df = pd.read_csv(io.BytesIO(up_bytes))
                    files = prep_and_pack(tmp_df, up_name)
                except Exception:
                    files = {"file": (up_name, up_bytes, "text/csv")}
            else:
                st.error("Unknown data source selection."); st.stop()

            r = requests.post(f"{API_URL}/v1/agents/{agent_name}/run", data=data, files=files, timeout=180)
            if r.status_code != 200:
                st.error(f"Run failed ({r.status_code}): {r.text}"); st.stop()

            res = r.json()
            st.session_state.last_run_id = res.get("run_id")
            result = res.get("result", {}) or {}
            st.success(f"‚úÖ Run succeeded! Run ID: {st.session_state.last_run_id}")

            # Pull merged.csv for dashboards/review
            rid = st.session_state.last_run_id
            merged_url = f"{API_URL}/v1/runs/{rid}/report?format=csv"
            merged_bytes = requests.get(merged_url, timeout=30).content
            merged_df = pd.read_csv(io.BytesIO(merged_bytes))
            st.session_state["last_merged_df"] = merged_df

            # # Export AI outputs as csv with currency code (for Human Review dropdown)
            # ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            # out_name = f"ai-appraisal-outputs-{ts}-{st.session_state['currency_code']}.csv"
            # st.download_button("‚¨áÔ∏è Download AI outputs (CSV)", merged_df.to_csv(index=False).encode("utf-8"), out_name, "text/csv")

            # Decision filter IN TABLE (not hiding dashboard)
            st.markdown("### üìÑ Credit Ai Agent  Decisions Table (filtered)")
            uniq_dec = sorted([d for d in merged_df.get("decision", pd.Series(dtype=str)).dropna().unique()])
            chosen = st.multiselect("Filter decision", options=uniq_dec, default=uniq_dec, key="filter_decisions")
            df_view = merged_df.copy()
            if "decision" in df_view.columns and chosen:
                df_view = df_view[df_view["decision"].isin(chosen)]
            st.dataframe(df_view, use_container_width=True)

            # ‚îÄ‚îÄ DASHBOARD (always visible; filters apply in table below)
            st.markdown("## üìä Dashboard")
            render_credit_dashboard(merged_df, st.session_state.get("currency_symbol", ""))

            # Per-row metrics met/not met
            if "rule_reasons" in df_view.columns:
                rr = df_view["rule_reasons"].apply(try_json)
                df_view["metrics_met"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is True])) if isinstance(d, dict) else "")
                df_view["metrics_unmet"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is False])) if isinstance(d, dict) else "")
            cols_show = [c for c in [
                "application_id","customer_type","decision","score","loan_amount","income","metrics_met","metrics_unmet",
                "proposed_loan_option","proposed_consolidation_loan","top_feature","explanation"
            ] if c in df_view.columns]
            st.dataframe(df_view[cols_show].head(500), use_container_width=True)

            #  # Export AI outputs as csv with currency code (for Human Review dropdown)
            # ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            # out_name = f"ai-appraisal-outputs-{ts}-{st.session_state['currency_code']}.csv"
            # st.download_button("‚¨áÔ∏è Download AI outputs (CSV)", merged_df.to_csv(index=False).encode("utf-8"), out_name, "text/csv")
            # Export AI outputs as CSV with currency code (for Human Review dropdown)

                        # Export AI outputs as CSV with currency code (for Human Review dropdown)
            ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            out_name = f"ai-appraisal-outputs-{ts}-{st.session_state['currency_code']}.csv"
            csv_data = merged_df.to_csv(index=False).encode("utf-8")

            # Correct CSS selector for Streamlit's download button
            st.markdown("""
            <style>
            div[data-testid="stDownloadButton"] button {
                font-size: 90px !important;
                font-weight: 900 !important;
                padding: 28px 48px !important;
                border-radius: 16px !important;
                background: linear-gradient(90deg, #2563eb, #1d4ed8) !important;
                color: white !important;
                border: none !important;
                box-shadow: 0 6px 18px rgba(0,0,0,0.35) !important;
                transition: all 0.3s ease-in-out !important;
            }
            div[data-testid="stDownloadButton"] button:hover {
                background: linear-gradient(90deg, #1e3a8a, #1d4ed8) !important;
                transform: scale(1.03);
            }
            </style>
            """, unsafe_allow_html=True)

            # Styled large download button
            st.download_button(
                "‚¨áÔ∏è Download AI Outputs For Human Review (CSV)",
                csv_data,
                file_name=out_name,
                mime="text/csv",
                use_container_width=True
            )

        except Exception as e:
            st.exception(e)



    # Re-download quick section
    if st.session_state.get("last_run_id"):
        st.markdown("---")
        st.subheader("üì• Download Latest Outputs")
        rid = st.session_state.last_run_id
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1: st.markdown(f"[‚¨áÔ∏è PDF]({API_URL}/v1/runs/{rid}/report?format=pdf)")
        with col2: st.markdown(f"[‚¨áÔ∏è Scores CSV]({API_URL}/v1/runs/{rid}/report?format=scores_csv)")
        with col3: st.markdown(f"[‚¨áÔ∏è Explanations CSV]({API_URL}/v1/runs/{rid}/report?format=explanations_csv)")
        with col4: st.markdown(f"[‚¨áÔ∏è Merged CSV]({API_URL}/v1/runs/{rid}/report?format=csv)")
        with col5: st.markdown(f"[‚¨áÔ∏è JSON]({API_URL}/v1/runs/{rid}/report?format=json)")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üßë‚Äç‚öñÔ∏è TAB 4 ‚Äî Human Review
with tab_review:
    st.subheader("üßë‚Äç‚öñÔ∏è Human Review ‚Äî Correct AI Decisions & Score Agreement > Drop your AI appraisal output CSV from previous Stage  below")

    # Allow loading AI output CSV back into review via dropdown upload
    uploaded_review = st.file_uploader("Load AI outputs CSV for review (optional)", type=["csv"], key="review_csv_loader")
    if uploaded_review is not None:
        try:
            st.session_state["last_merged_df"] = pd.read_csv(uploaded_review)
            st.success("Loaded review dataset from uploaded CSV.")
        except Exception as e:
            st.error(f"Could not read uploaded CSV: {e}")

    if "last_merged_df" not in st.session_state:
        st.info("Run the agent (previous tab) or upload an AI outputs CSV to load results for review.")
    else:
        dfm = st.session_state["last_merged_df"].copy()
        st.markdown("#### 1) Select rows to review and correct")

        editable_cols = []
        if "decision" in dfm.columns: editable_cols.append("decision")
        if "rule_reasons" in dfm.columns: editable_cols.append("rule_reasons")
        if "customer_type" in dfm.columns: editable_cols.append("customer_type")

        editable = dfm[["application_id"] + editable_cols].copy()
        editable.rename(columns={"decision": "ai_decision"}, inplace=True)
        editable["human_decision"] = editable.get("ai_decision", "approved")
        editable["human_rule_reasons"] = editable.get("rule_reasons", "")

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # LIGHTER EDITABLE CELL STYLING (improved)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        st.markdown("""
            <style>
            /* Bright background for editable cells */
            [data-testid="stDataFrameCellEditable"] textarea {
                background-color: #fefefe !important;   /* bright white background */
                color: #111 !important;                 /* dark text */
                border: 1px solid #cbd5e1 !important;   /* subtle gray border */
                border-radius: 6px !important;
                padding: 6px 8px !important;
                font-weight: 500 !important;
            }

            /* Hover and focus effect */
            [data-testid="stDataFrameCellEditable"]:focus-within textarea,
            [data-testid="stDataFrameCellEditable"]:hover textarea {
                background-color: #ffffff !important;
                border-color: #22c55e !important;        /* green glow */
                box-shadow: 0 0 0 2px rgba(34,197,94,0.4) !important;
            }

            /* Read-only cells: keep dark */
            [data-testid="stDataFrameCell"] {
                background-color: #1e293b !important;
                color: #e2e8f0 !important;
            }
            </style>
        """, unsafe_allow_html=True)


        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # EDITOR
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        edited = st.data_editor(
            editable,
            num_rows="dynamic",
            use_container_width=True,
            key="review_editor",
            column_config={
                "human_decision": st.column_config.SelectboxColumn(options=["approved", "denied"]),
                "customer_type": st.column_config.SelectboxColumn(options=["bank", "non-bank"], disabled=True)
            }
        )

        st.markdown("#### 2) Compute agreement score")

        if st.button("Compute agreement score"):
            if "ai_decision" in edited.columns and "human_decision" in edited.columns:
                agree = (edited["ai_decision"] == edited["human_decision"]).astype(int)
                score = float(agree.mean()) if len(agree) else 0.0
                st.session_state["last_agreement_score"] = score

                # üå°Ô∏è BEAUTIFUL Gauge
                import plotly.graph_objects as go
                fig = go.Figure(go.Indicator(
                    mode="gauge+number",
                    value=score * 100,
                    number={'suffix': "%", 'font': {'size': 72, 'color': "#f8fafc", 'family': "Arial Black"}},
                    title={'text': "AI ‚Üî Human Agreement", 'font': {'size': 28, 'color': "#93c5fd", 'family': "Arial"}},
                    gauge={
                        'axis': {'range': [0, 100], 'tickwidth': 2, 'tickcolor': "#f8fafc"},
                        'bar': {'color': "#3b82f6", 'thickness': 0.3},
                        'bgcolor': "#1e293b",
                        'borderwidth': 2,
                        'bordercolor': "#334155",
                        'steps': [
                            {'range': [0, 50], 'color': "#ef4444"},
                            {'range': [50, 75], 'color': "#f59e0b"},
                            {'range': [75, 100], 'color': "#22c55e"},
                        ],
                    }
                ))
                fig.update_layout(
                    paper_bgcolor="#0f172a",
                    plot_bgcolor="#0f172a",
                    height=400,
                    margin=dict(t=60, b=20, l=60, r=60)
                )
                st.plotly_chart(fig, use_container_width=True)



            # üí° Detailed disagreement table (AI vs Human + AI metrics explanation)
                mismatched = edited[edited["ai_decision"] != edited["human_decision"]].copy()
                total = len(edited)
                disagree = len(mismatched)

                if disagree > 0:
                    st.markdown(f"### ‚ùå {disagree} loans disagreed out of {total} ({(disagree/total)*100:.1f}% disagreement rate)")

                    import json

                    def parse_ai_reason(r: str):
                        """Parse AI rule_reasons and summarize which metrics passed or failed."""
                        if not isinstance(r, str):
                            return "No metrics available"
                        try:
                            data = json.loads(r.replace("'", "\""))
                            passed = [k for k, v in data.items() if v is True]
                            failed = [k for k, v in data.items() if v is False]
                            result = []
                            if passed:
                                result.append("‚úÖ Pass: " + ", ".join(passed))
                            if failed:
                                result.append("‚ùå Fail: " + ", ".join(failed))
                            return " | ".join(result) if result else "No metrics recorded"
                        except Exception:
                            return "Unreadable metrics"

                    # Extract AI reasoning and Human reason columns
                    mismatched["ai_metrics"] = mismatched["rule_reasons"].apply(parse_ai_reason) if "rule_reasons" in mismatched else "No data"
                    mismatched["human_reason"] = mismatched.get("human_rule_reasons", "Manual review adjustment")

                    # üü©üü• Color styling for AI vs Human
                    def highlight_disagreement(row):
                        ai_color = "background-color: #ef4444; color: white;"      # red for AI decision
                        human_color = "background-color: #22c55e; color: black;"   # green for Human decision
                        return [
                            ai_color if col == "ai_decision" else
                            human_color if col == "human_decision" else
                            ""
                            for col in row.index
                        ]

                    # Columns: ID ‚Üí AI Decision ‚Üí Human Decision ‚Üí AI Metrics ‚Üí Human Reason
                    show_cols = [
                        c for c in ["application_id", "ai_decision", "human_decision", "ai_metrics", "human_reason"]
                        if c in mismatched.columns
                    ]
                    styled_df = mismatched[show_cols].style.apply(highlight_disagreement, axis=1)
                    st.dataframe(styled_df, use_container_width=True, height=420)

                else:
                    st.success("‚úÖ Full agreement ‚Äî no human-AI mismatches found.")



        # Export review CSV (manual loop into training)
        st.markdown("#### 3) Export Human review CSV for Next Step : Training and loopback ")
        model_used = "production"  # if you track specific model names, set it here
        ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        safe_user = st.session_state["user_info"]["name"].replace(" ", "").lower()
        review_name = f"creditappraisal.{safe_user}.{model_used}.{ts}.csv"
        csv_bytes = edited.to_csv(index=False).encode("utf-8")
        st.download_button("‚¨áÔ∏è Export review CSV", csv_bytes, review_name, "text/csv")
        st.caption(f"Saved file name pattern: **{review_name}**")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üîÅ TAB 5 ‚Äî Training (Feedback ‚Üí Retrain)
with tab_train:
    st.subheader("üîÅ From Human Feedback CSV ‚Üí Train and Promote Trained Model to Production Model ")

    st.markdown("**Drag & drop** one or more review CSVs exported from the Human Review tab.")
    up_list = st.file_uploader("Upload feedback CSV(s)", type=["csv"], accept_multiple_files=True, key="train_feedback_uploader")

    staged_paths: List[str] = []
    if up_list:
        for up in up_list:
            # stage to tmp_feedback dir
            dest = os.path.join(TMP_FEEDBACK_DIR, up.name)
            with open(dest, "wb") as f:
                f.write(up.getvalue())
            staged_paths.append(dest)
        st.success(f"Staged {len(staged_paths)} feedback file(s) to {TMP_FEEDBACK_DIR}")
        st.write(staged_paths)

    st.markdown("#### Launch Retrain")
    payload = {
        "feedback_csvs": staged_paths,
        "user_name": st.session_state["user_info"]["name"],
        "agent_name": "credit_appraisal",
        "algo_name": "credit_lr",
    }
    st.code(json.dumps(payload, indent=2), language="json")

    colA, colB = st.columns([1,1])
    with colA:
        if st.button("üöÄ Train candidate model"):
            try:
                r = requests.post(f"{API_URL}/v1/training/train", json=payload, timeout=90)
                if r.ok:
                    st.success(r.json())
                    st.session_state["last_train_job"] = r.json().get("job_id")
                else:
                    st.error(r.text)
            except Exception as e:
                st.error(f"Train failed: {e}")
    with colB:
        if st.button("‚¨ÜÔ∏è Promote last candidate to PRODUCTION"):
            try:
                r = requests.post(f"{API_URL}/v1/training/promote", timeout=30)
                st.write(r.json() if r.ok else r.text)
            except Exception as e:
                st.error(f"Promote failed: {e}")

    st.markdown("---")
    st.markdown("#### Production Model")
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.ok:
            st.json(resp.json())
        else:
            st.info("No production model yet.")
    except Exception as e:
        st.warning(f"Could not load production meta: {e}")


      # üîÅ Loopback Section (Real functional button)

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üîÅ Loopback Section ‚Äî Go back to Step 3
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("---")
    st.markdown("### üí≥ Loop back to Step 3 ‚Äî Credit Appraisal Agent")
    st.caption("After retraining, return to the Credit Appraisal tab and use your new production model.")

    st.markdown("""
    <a href="#credit-appraisal-stage" target="_self">
        <button style="
            background-color:#2563eb;
            color:white;
            border:none;
            border-radius:8px;
            padding:12px 24px;
            font-size:16px;
            font-weight:600;
            cursor:pointer;
            width:100%;
            box-shadow:0px 0px 6px rgba(37,99,235,0.5);
        ">‚¨ÖÔ∏è Go Back to Step 3 and Use New Model</button>
    </a>
    """, unsafe_allow_html=True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üó£Ô∏è TAB 6 ‚Äî Feedback & Feature Requests
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tab_feedback:
    st.subheader("üó£Ô∏è Share Your Feedback and Feature Ideas")

    FEEDBACK_FILE = os.path.join(BASE_DIR, "agents_feedback.json")

    def load_feedback() -> dict:
        try:
            with open(FEEDBACK_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    def save_feedback(data: dict):
        try:
            with open(FEEDBACK_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            st.error(f"Could not save feedback: {e}")

    feedback_data = load_feedback()

    # View all current agent feedback
    st.markdown("### üí¨ Current Agent Reviews & Ratings")
    for agent, fb in feedback_data.items():
        with st.expander(f"‚≠ê {agent} ‚Äî {fb.get('rating', 0)}/5  |  üë• {fb.get('users', 0)} users"):
            st.markdown("#### Recent Comments:")
            for cmt in reversed(fb.get("comments", [])):
                st.markdown(f"- {cmt}")
            st.markdown("---")

    st.markdown("### ‚úçÔ∏è Submit Your Own Feedback or Feature Request")

    agent_choice = st.selectbox("Select Agent", list(feedback_data.keys()))
    new_comment = st.text_area("Your Comment or Feature Suggestion", placeholder="e.g. Add multi-language support for reports...")
    new_rating = st.slider("Your Rating", 1, 5, 5)

    # if st.button("üì® Submit Feedback"):
    #     if new_comment.strip():
    #         fb = feedback_data.get(agent_choice, {"rating": 0, "users": 0, "comments": []})
    #         fb["comments"].append(new_comment.strip())
    #         fb["rating"] = round((fb.get("rating", 0) + new_rating) / 2, 2)
    #         fb["users"] = fb.get("users", 0) + 1
    #         feedback_data[agent_choice] = fb
    #         save_feedback(feedback_data)
    #         st.success("‚úÖ Feedback submitted successfully!")
    #         st.rerun()
    #     else:
    #         st.warning("Please enter a comment before submitting.")
    if st.button("üì® Submit Feedback"):
        if new_comment.strip():
            fb = feedback_data.get(agent_choice, {"rating": 0, "users": 0, "comments": []})
            fb["comments"].append(new_comment.strip())
            fb["rating"] = round((fb.get("rating", 0) + new_rating) / 2, 2)
            fb["users"] = fb.get("users", 0) + 1
            feedback_data[agent_choice] = fb
            save_feedback(feedback_data)

            # ‚úÖ Sync latest feedback globally
            st.session_state["feedback_data"] = feedback_data

            # ‚úÖ Force full reload so Landing updates instantly
            st.success("‚úÖ Feedback submitted successfully!")
            st.rerun()
        else:
            st.warning("Please enter a comment before submitting.")




==================== ./_old/newcredit.py ====================
# services/ui/app.py
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üåê OpenSource AI Agent Library + Credit Appraisal PoC by Dzoan
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from __future__ import annotations
import os
import re
import io
import json
from datetime import datetime, timezone
from typing import Optional, Dict, List, Any
import pandas as pd
import numpy as np
import streamlit as st
import requests
import plotly.express as px
import plotly.graph_objects as go
import logging
import sys
import streamlit as st
import pandas as pd

from services.ui.utils.style import apply_theme, ensure_keys, render_nav_bar_app

# ---- Page config & theme
st.set_page_config(page_title="üí≥ Credit Appraisal", page_icon="üí≥",
                   layout="wide", initial_sidebar_state="collapsed")
ensure_keys()
apply_theme(st.session_state.get("theme", "dark"))

# ---- Hide the left sidebar
st.markdown("""
<style>
[data-testid="stSidebar"], section[data-testid="stSidebar"]{display:none!important}
[data-testid="stAppViewContainer"]{margin-left:0!important;padding-left:0!important}
</style>""", unsafe_allow_html=True)

# ---- Navbar (identical to landing)
render_nav_bar_app()

# ---- Login gate
def login_block():
    st.title("üîê Login to AI Credit Appraisal Platform")
    c1, c2, c3 = st.columns([1,1,1])
    with c1: user  = st.text_input("Username", placeholder="e.g. dzoan")
    with c2: email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
    with c3: pwd   = st.text_input("Password", type="password", placeholder="Enter any password")
    if st.button("Login", key="btn_credit_login", use_container_width=True):
        if (user or "").strip() and (email or "").strip():
            st.session_state["user_info"] = {
                "name": user.strip(), "email": email.strip(),
                "flagged": False, "timestamp": datetime.now(timezone.utc).isoformat()
            }
            st.session_state["credit_logged_in"] = True
            st.session_state["stage"] = "credit_agent"
            st.rerun()
        else:
            st.error("‚ö†Ô∏è Please fill all fields before continuing.")

# ---- Header (moved from app.py)
def render_credit_header():
    ss = st.session_state
    user = (ss.get("user_info") or {}).get("name") or "guest"
    st.title("üè¶ Credit Appraisal Agent")
    st.caption(
        "A‚ÜíF pipeline ‚Äî Intake ‚Üí Privacy ‚Üí Modelling ‚Üí Policy ‚Üí Review ‚Üí Reporting "
        f"| üëã {user}"
    )
    st.markdown("""
    <div style="display:flex;gap:.6rem;flex-wrap:wrap;margin:.35rem 0 1rem 0;">
      <span style="padding:.28rem .6rem;border-radius:.6rem;background:#1d4ed8;color:#fff;font-weight:600;">A) Intake & Evidence</span>
      <span style="padding:.28rem .6rem;border-radius:.6rem;background:#059669;color:#fff;font-weight:600;">B) Privacy & Features</span>
      <span style="padding:.28rem .6rem;border-radius:.6rem;background:#d97706;color:#fff;font-weight:600;">C) Modelling & Scoring</span>
      <span style="padding:.28rem .6rem;border-radius:.6rem;background:#7c3aed;color:#fff;font-weight:600;">D) Policy & Decision</span>
      <span style="padding:.28rem .6rem;border-radius:.6rem;background:#a16207;color:#fff;font-weight:600;">E) Review</span>
      <span style="padding:.28rem .6rem;border-radius:.6rem;background:#64748b;color:#fff;font-weight:600;">F) Reporting & Handoff</span>
    </div>
    """, unsafe_allow_html=True)



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SESSION STATE INIT
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if "stage" not in st.session_state:
    st.session_state.stage = "landing"
if "user_info" not in st.session_state:
    st.session_state.user_info = {"name": "", "email": "", "flagged": False}
if "logged_in" not in st.session_state:
    st.session_state.logged_in = False
if "flagged" not in st.session_state.user_info:
    st.session_state.user_info["flagged"] = False
if "timestamp" not in st.session_state.user_info:
    st.session_state.user_info["timestamp"] = datetime.now(timezone.utc).isoformat()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GLOBAL UTILS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


BANNED_NAMES = {"race", "gender", "religion", "ethnicity", "ssn", "national_id"}
PII_COLS = {"customer_name", "name", "email", "phone", "address", "ssn", "national_id", "dob"}

EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
PHONE_RE = re.compile(r"\+?\d[\d\-\s]{6,}\d")

def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:
    return df.loc[:, ~df.columns.duplicated(keep="last")]

def scrub_text_pii(s):
    if not isinstance(s, str):
        return s
    s = EMAIL_RE.sub("", s)
    s = PHONE_RE.sub("", s)
    return s.strip()

def drop_pii_columns(df: pd.DataFrame):
    original_cols = list(df.columns)
    keep_cols = [c for c in original_cols if all(k not in c.lower() for k in PII_COLS)]
    dropped = [c for c in original_cols if c not in keep_cols]
    out = df[keep_cols].copy()
    for c in out.select_dtypes(include="object"):
        out[c] = out[c].apply(scrub_text_pii)
    return dedupe_columns(out), dropped

def strip_policy_banned(df: pd.DataFrame) -> pd.DataFrame:
    keep = []
    for c in df.columns:
        cl = c.lower()
        if cl in BANNED_NAMES:
            continue
        keep.append(c)
    return df[keep]

def append_user_info(df: pd.DataFrame) -> pd.DataFrame:
    meta = st.session_state["user_info"]
    out = df.copy()
    out["session_user_name"] = meta["name"]
    out["session_user_email"] = meta["email"]
    out["session_flagged"] = meta["flagged"]
    out["created_at"] = meta["timestamp"]
    return dedupe_columns(out)

def save_to_runs(df: pd.DataFrame, prefix: str) -> str:
    #ts = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M")
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M")
    flag_suffix = "_FLAGGED" if st.session_state["user_info"]["flagged"] else ""
    fname = f"{prefix}_{ts}{flag_suffix}.csv"
    fpath = os.path.join(RUNS_DIR, fname)
    dedupe_columns(df).to_csv(fpath, index=False)
    return fpath

def try_json(x):
    if isinstance(x, (dict, list)):
        return x
    if not isinstance(x, str):
        return None
    try:
        return json.loads(x)
    except Exception:
        return None

def _safe_json(x):
    if isinstance(x, dict):
        return x
    if isinstance(x, str) and x.strip():
        try:
            return json.loads(x)
        except Exception:
            return {}
    return {}

def fmt_currency_label(base: str) -> str:
    sym = st.session_state.get("currency_symbol", "")
    return f"{base} ({sym})" if sym else base

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CURRENCY CATALOG

CURRENCY_OPTIONS = {
    # code: (label, symbol, fx to apply on USD-like base generated numbers)
    "USD": ("USD $", "$", 1.0),
    "EUR": ("EUR ‚Ç¨", "‚Ç¨", 0.93),
    "GBP": ("GBP ¬£", "¬£", 0.80),
    "JPY": ("JPY ¬•", "¬•", 150.0),
    "VND": ("VND ‚Ç´", "‚Ç´", 24000.0),
}

def set_currency_defaults():
    if "currency_code" not in st.session_state:
        st.session_state["currency_code"] = "USD"
    label, symbol, fx = CURRENCY_OPTIONS[st.session_state["currency_code"]]
    st.session_state["currency_label"] = label
    st.session_state["currency_symbol"] = symbol
    st.session_state["currency_fx"] = fx

set_currency_defaults()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DASHBOARD HELPERS (Plotly, dark theme)

def _kpi_card(label: str, value: str, sublabel: str | None = None):
    st.markdown(
        f"""
        <div style="background:#0e1117;border:1px solid #2a2f3e;border-radius:12px;padding:14px 16px;margin-bottom:10px;">
          <div style="font-size:12px;color:#9aa4b2;text-transform:uppercase;letter-spacing:.06em;">{label}</div>
          <div style="font-size:28px;font-weight:700;color:#e6edf3;line-height:1.1;margin-top:2px;">{value}</div>
          {f'<div style="font-size:12px;color:#9aa4b2;margin-top:6px;">{sublabel}</div>' if sublabel else ''}
        </div>
        """,
        unsafe_allow_html=True,
    )

def render_credit_dashboard(df: pd.DataFrame, currency_symbol: str = ""):
    """
    Renders the whole dashboard (TOP-10s ‚Üí Opportunities ‚Üí KPIs & pies/bars ‚Üí Mix table).
    Keeps decision filter in the table only.
    """
    if df is None or df.empty:
        st.info("No data to visualize yet.")
        return

    cols = df.columns

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TOP 10s FIRST ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üîù Top 10 Snapshot")

    # Top 10 loans approved
    if {"decision", "loan_amount", "application_id"} <= set(cols):
        top_approved = df[df["decision"].astype(str).str.lower() == "approved"].copy()
        if not top_approved.empty:
            top_approved = top_approved.sort_values("loan_amount", ascending=False).head(10)
            fig = px.bar(
                top_approved,
                x="loan_amount",
                y="application_id",
                orientation="h",
                title="Top 10 Approved Loans",
                labels={"loan_amount": f"Loan Amount {currency_symbol}", "application_id": "Application"},
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No approved loans available to show top 10.")

    # Top 10 collateral types by average value
    if {"collateral_type", "collateral_value"} <= set(cols):
        cprof = df.groupby("collateral_type", dropna=False).agg(
            avg_value=("collateral_value", "mean"),
            cnt=("collateral_type", "count")
        ).reset_index()
        if not cprof.empty:
            cprof = cprof.sort_values("avg_value", ascending=False).head(10)
            fig = px.bar(
                cprof,
                x="avg_value",
                y="collateral_type",
                orientation="h",
                title="Top 10 Collateral Types (Avg Value)",
                labels={"avg_value": f"Avg Value {currency_symbol}", "collateral_type": "Collateral Type"},
                hover_data=["cnt"]
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    # Top 10 reasons for denial (from rule_reasons False flags)
    if "rule_reasons" in cols and "decision" in cols:
        denied = df[df["decision"].astype(str).str.lower() == "denied"].copy()
        reasons_count = {}
        for _, r in denied.iterrows():
            rr = _safe_json(r.get("rule_reasons"))
            if isinstance(rr, dict):
                for k, v in rr.items():
                    if v is False:
                        reasons_count[k] = reasons_count.get(k, 0) + 1
        if reasons_count:
            items = pd.DataFrame(sorted(reasons_count.items(), key=lambda x: x[1], reverse=True),
                                 columns=["reason", "count"]).head(10)
            fig = px.bar(
                items, x="count", y="reason", orientation="h",
                title="Top 10 Reasons for Denial",
                labels={"count": "Count", "reason": "Rule"},
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No denial reasons detected.")

    # Top 10 loan officer performance (approval rate) if officer column present
    officer_col = None
    for guess in ("loan_officer", "officer", "reviewed_by", "session_user_name"):
        if guess in cols:
            officer_col = guess
            break
    if officer_col and "decision" in cols:
        perf = (
            df.assign(is_approved=(df["decision"].astype(str).str.lower() == "approved").astype(int))
              .groupby(officer_col, dropna=False)["is_approved"]
              .agg(approved_rate="mean", n="count")
              .reset_index()
        )
        if not perf.empty:
            perf["approved_rate_pct"] = (perf["approved_rate"] * 100).round(1)
            perf = perf.sort_values(["approved_rate_pct", "n"], ascending=[False, False]).head(10)
            fig = px.bar(
                perf, x="approved_rate_pct", y=officer_col, orientation="h",
                title="Top 10 Loan Officer Approval Rate (this batch)",
                labels={"approved_rate_pct": "Approval Rate (%)", officer_col: "Officer"},
                hover_data=["n"]
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ OPPORTUNITIES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üí° Opportunities")

    # Short-term loan opportunities (simple heuristic)
    opp_rows = []
    if {"income", "loan_amount"}.issubset(cols):
        term_col = "loan_term_months" if "loan_term_months" in cols else ("loan_duration_months" if "loan_duration_months" in cols else None)
        if term_col:
            for _, r in df.iterrows():
                inc = float(r.get("income", 0) or 0)
                amt = float(r.get("loan_amount", 0) or 0)
                term = int(r.get(term_col, 0) or 0)
                dti = float(r.get("DTI", 0) or 0)
                if (term >= 36) and (amt <= inc * 0.8) and (dti <= 0.45):
                    opp_rows.append({
                        "application_id": r.get("application_id"),
                        "suggested_term": 24,
                        "loan_amount": amt,
                        "income": inc,
                        "DTI": dti,
                        "note": "Candidate for short-term plan (<=24m) based on affordability."
                    })
    if opp_rows:
        st.markdown("#### üìé Short-Term Loan Candidates")
        st.dataframe(pd.DataFrame(opp_rows).head(25), use_container_width=True, height=320)
    else:
        st.info("No short-term loan candidates identified in this batch.")

    st.markdown("#### üîÅ Buyback / Consolidation Beneficiaries")
    candidates = []
    need = {"decision", "existing_debt", "loan_amount", "DTI"}
    if need <= set(cols):
        for _, r in df.iterrows():
            dec = str(r.get("decision", "")).lower()
            debt = float(r.get("existing_debt", 0) or 0)
            loan = float(r.get("loan_amount", 0) or 0)
            dti = float(r.get("DTI", 0) or 0)
            proposal = _safe_json(r.get("proposed_consolidation_loan", {}))
            has_bb = bool(proposal)

            if dec == "denied" or dti > 0.45 or debt > loan:
                benefit_score = round((debt / (loan + 1e-6)) * 0.4 + dti * 0.6, 2)
                candidates.append({
                    "application_id": r.get("application_id"),
                    "customer_type": r.get("customer_type"),
                    "existing_debt": debt,
                    "loan_amount": loan,
                    "DTI": dti,
                    "collateral_type": r.get("collateral_type"),
                    "buyback_proposed": has_bb,
                    "buyback_amount": proposal.get("buyback_amount") if has_bb else None,
                    "benefit_score": benefit_score,
                    "note": proposal.get("note") if has_bb else None
                })
    if candidates:
        cand_df = pd.DataFrame(candidates).sort_values("benefit_score", ascending=False)
        st.dataframe(cand_df.head(25), use_container_width=True, height=380)
    else:
        st.info("No additional buyback beneficiaries identified.")

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PORTFOLIO KPIs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üìà Portfolio Snapshot")
    c1, c2, c3, c4 = st.columns(4)

    # Approval rate
    if "decision" in cols:
        total = len(df)
        approved = int((df["decision"].astype(str).str.lower() == "approved").sum())
        rate = (approved / total * 100) if total else 0.0
        with c1: _kpi_card("Approval Rate", f"{rate:.1f}%", f"{approved} of {total}")

    # Avg approved loan amount
    if {"decision", "loan_amount"} <= set(cols):
        ap = df[df["decision"].astype(str).str.lower() == "approved"]["loan_amount"]
        avg_amt = ap.mean() if len(ap) else 0.0
        with c2: _kpi_card("Avg Approved Amount", f"{currency_symbol}{avg_amt:,.0f}")

    # Decision time (if present)
    if {"created_at", "decision_at"} <= set(cols):
        try:
            t = (pd.to_datetime(df["decision_at"]) - pd.to_datetime(df["created_at"])).dt.total_seconds() / 60.0
            avg_min = float(t.mean())
            with c3: _kpi_card("Avg Decision Time", f"{avg_min:.1f} min")
        except Exception:
            with c3: _kpi_card("Avg Decision Time", "‚Äî")

    # Non-bank share
    if "customer_type" in cols:
        nb = int((df["customer_type"].astype(str).str.lower() == "non-bank").sum())
        total = len(df)
        share = (nb / total * 100) if total else 0.0
        with c4: _kpi_card("Non-bank Share", f"{share:.1f}%", f"{nb} of {total}")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ COMPOSITION & RISK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üß≠ Composition & Risk")

    # Approval vs Denial (pie)
    if "decision" in cols:
        pie_df = df["decision"].value_counts().rename_axis("Decision").reset_index(name="Count")
        fig = px.pie(pie_df, names="Decision", values="Count", title="Decision Mix")
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Avg DTI / LTV by decision (grouped bars)
    have_dti = "DTI" in cols
    have_ltv = "LTV" in cols
    if "decision" in cols and (have_dti or have_ltv):
        agg_map = {}
        if have_dti: agg_map["avg_DTI"] = ("DTI", "mean")
        if have_ltv: agg_map["avg_LTV"] = ("LTV", "mean")
        grp = df.groupby("decision").agg(**agg_map).reset_index()
        melted = grp.melt(id_vars=["decision"], var_name="metric", value_name="value")
        fig = px.bar(melted, x="decision", y="value", color="metric",
                     barmode="group", title="Average DTI / LTV by Decision")
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Loan term mix (stacked)
    term_col = "loan_term_months" if "loan_term_months" in cols else ("loan_duration_months" if "loan_duration_months" in cols else None)
    if term_col and "decision" in cols:
        mix = df.groupby([term_col, "decision"]).size().reset_index(name="count")
        fig = px.bar(
            mix, x=term_col, y="count", color="decision", title="Loan Term Mix",
            labels={term_col: "Term (months)", "count": "Count"}, barmode="stack"
        )
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Collateral avg value by type (bar)
    if {"collateral_type", "collateral_value"} <= set(cols):
        cprof = df.groupby("collateral_type").agg(
            avg_col=("collateral_value", "mean"),
            cnt=("collateral_type", "count")
        ).reset_index()
        fig = px.bar(
            cprof.sort_values("avg_col", ascending=False),
            x="collateral_type", y="avg_col",
            title=f"Avg Collateral Value by Type ({currency_symbol})",
            hover_data=["cnt"]
        )
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Top proposed plans (horizontal bar)
    if "proposed_loan_option" in cols:
        plans = df["proposed_loan_option"].dropna().astype(str)
        if len(plans) > 0:
            plan_types = []
            for s in plans:
                p = _safe_json(s)
                plan_types.append(p.get("type") if isinstance(p, dict) and "type" in p else s)
            plan_df = pd.Series(plan_types).value_counts().head(10).rename_axis("plan").reset_index(name="count")
            fig = px.bar(
                plan_df, x="count", y="plan", orientation="h",
                title="Top 10 Proposed Plans"
            )
            fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    # Customer mix table (bank vs non-bank)
    if "customer_type" in cols:
        mix = df["customer_type"].value_counts().rename_axis("Customer Type").reset_index(name="Count")
        mix["Ratio"] = (mix["Count"] / mix["Count"].sum()).round(3)
        st.markdown("### üë• Customer Mix")
        st.dataframe(mix, use_container_width=True, height=220)




# DATA GENERATORS

def generate_raw_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    names = ["Alice Nguyen","Bao Tran","Chris Do","Duy Le","Emma Tran",
             "Felix Nguyen","Giang Ho","Hanh Vo","Ivan Pham","Julia Ngo"]
    emails = [f"{n.split()[0].lower()}.{n.split()[1].lower()}@gmail.com" for n in names]
    addrs = [
        "23 Elm St, Boston, MA","19 Pine Ave, San Jose, CA","14 High St, London, UK",
        "55 Nguyen Hue, Ho Chi Minh","78 Oak St, Chicago, IL","10 Broadway, New York, NY",
        "8 Rue Lafayette, Paris, FR","21 K√∂nigstr, Berlin, DE","44 Maple Dr, Los Angeles, CA","22 Bay St, Toronto, CA"
    ]
    is_non = rng.random(n) < non_bank_ratio
    cust_type = np.where(is_non, "non-bank", "bank")

    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "customer_name": np.random.choice(names, n),
        "email": np.random.choice(emails, n),
        "phone": [f"+1-202-555-{1000+i:04d}" for i in range(n)],
        "address": np.random.choice(addrs, n),
        "national_id": rng.integers(10_000_000, 99_999_999, n),
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["real_estate","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
        "current_loans": rng.integers(0, 5, n),
        "customer_type": cust_type,
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))

    fx = st.session_state["currency_fx"]
    for c in ("income", "loan_amount", "collateral_value", "assets_owned", "existing_debt"):
        df[c] = (df[c] * fx).round(2)
    df["currency_code"] = st.session_state["currency_code"]
    return dedupe_columns(df)

def generate_anon_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    is_non = rng.random(n) < non_bank_ratio
    cust_type = np.where(is_non, "non-bank", "bank")

    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["real_estate","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
        "current_loans": rng.integers(0, 5, n),
        "customer_type": cust_type,
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))

    fx = st.session_state["currency_fx"]
    for c in ("income", "loan_amount", "collateral_value", "assets_owned", "existing_debt"):
        df[c] = (df[c] * fx).round(2)
    df["currency_code"] = st.session_state["currency_code"]
    return dedupe_columns(df)

def to_agent_schema(df: pd.DataFrame) -> pd.DataFrame:
    """
    Harmonize to the server-side agent‚Äôs expected schema.
    """
    out = df.copy()
    n = len(out)
    if "employment_years" not in out.columns:
        out["employment_years"] = out.get("employment_length", 0)
    if "debt_to_income" not in out.columns:
        if "DTI" in out.columns:
            out["debt_to_income"] = out["DTI"].astype(float)
        elif "existing_debt" in out.columns and "income" in out.columns:
            denom = out["income"].replace(0, np.nan)
            dti = (out["existing_debt"] / denom).fillna(0.0)
            out["debt_to_income"] = dti.clip(0, 10)
        else:
            out["debt_to_income"] = 0.0
    rng = np.random.default_rng(12345)
    if "credit_history_length" not in out.columns:
        out["credit_history_length"] = rng.integers(0, 30, n)
    if "num_delinquencies" not in out.columns:
        out["num_delinquencies"] = np.minimum(rng.poisson(0.2, n), 10)
    if "requested_amount" not in out.columns:
        out["requested_amount"] = out.get("loan_amount", 0)
    if "loan_term_months" not in out.columns:
        out["loan_term_months"] = out.get("loan_duration_months", 0)
    return dedupe_columns(out)






# ---- Tabs scaffold (drop your code per stage)
def tab_A():
    st.subheader("A) Intake & Evidence")
    st.info("TODO: paste your Credit Intake code here (uploads, schema normalization, validation).")

def tab_B():
    st.subheader("B) Privacy & Features")
    st.info("TODO: PII anonymization + feature engineering.")

def tab_C():
    st.subheader("C) Modelling & Scoring")
    st.info("TODO: Train/Load model, infer, SHAP visualizations.")

def tab_D():
    st.subheader("D) Policy & Decision")
    st.info("TODO: Policy thresholds, LTV/DTI caps, final decision.")

def tab_E():
    st.subheader("E) Human Review & Feedback")
    st.info("TODO: Overrides, notes, export feedback CSV.")

def tab_F():
    st.subheader("F) Reporting & Handoff")
    st.info("TODO: Audit JSON/PDF, create credit handoff zip.")

# ---- Page flow
if not st.session_state.get("credit_logged_in", False):
    login_block()
    st.stop()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üè¶ TAB 1 ‚Äî Synthetic Data Generator

def tab_A():
    st.subheader("üè¶ Synthetic Credit Data Generator")

    # Currency selector (before generation)
    c1, c2 = st.columns([1, 2])
    with c1:
        code = st.selectbox(
            "Currency",
            list(CURRENCY_OPTIONS.keys()),
            index=list(CURRENCY_OPTIONS.keys()).index(st.session_state["currency_code"]),
            help="All monetary fields will be in this local currency."
        )
        if code != st.session_state["currency_code"]:
            st.session_state["currency_code"] = code
            set_currency_defaults()
    #with c2:
        #st.info(f"Amounts will be generated in **{st.session_state['currency_label']}**.", icon="üí∞")
    with c2:
        st.markdown(
            f"""
            <div style='background-color:#1e293b; padding:12px 16px; border-radius:8px;'>
                <span style='font-weight:600; color:#f8fafc;'>
                    üí∞ Amounts will be generated in
                    <span style='color:#4ade80;'>{st.session_state['currency_label']}</span>.
                </span>
            </div>
            """,
            unsafe_allow_html=True,
        )


    rows = st.slider("Number of rows to generate", 50, 2000, 200, step=50)
    non_bank_ratio = st.slider("Share of non-bank customers", 0.0, 1.0, 0.30, 0.05)

    colA, colB = st.columns(2)
    with colA:
        if st.button("üî¥ Generate RAW Synthetic Data (with PII)", use_container_width=True):
            raw_df = append_user_info(generate_raw_synthetic(rows, non_bank_ratio))
            st.session_state.synthetic_raw_df = raw_df
            raw_path = save_to_runs(raw_df, "synthetic_raw")
            st.success(f"Generated RAW (PII) dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {raw_path}")
            st.dataframe(raw_df.head(10), use_container_width=True)
            st.download_button(
                "‚¨áÔ∏è Download RAW CSV",
                raw_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(raw_path),
                "text/csv"
            )

    with colB:
        if st.button("üü¢ Generate ANON Synthetic Data (ready for agent)", use_container_width=True):
            anon_df = append_user_info(generate_anon_synthetic(rows, non_bank_ratio))
            st.session_state.synthetic_df = anon_df
            anon_path = save_to_runs(anon_df, "synthetic_anon")
            st.success(f"Generated ANON dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {anon_path}")
            st.dataframe(anon_df.head(10), use_container_width=True)
            st.download_button(
                "‚¨áÔ∏è Download ANON CSV",
                anon_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(anon_path),
                "text/csv"
            )

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üßπ TAB 2 ‚Äî Anonymize & Sanitize Data
def tab_B():
    st.subheader("üßπ Upload & Anonymize Customer Data (PII columns will be DROPPED)")
    st.markdown("Upload your **real CSV**. We drop PII columns and scrub emails/phones in text fields.")

    uploaded = st.file_uploader("Upload CSV file", type=["csv"])
    if uploaded:
        try:
            df = pd.read_csv(uploaded)
        except Exception as e:
            st.error(f"Could not read CSV: {e}")
            st.stop()

        st.write("üìä Original Data Preview:")
        st.dataframe(dedupe_columns(df.head(5)), use_container_width=True)

        sanitized, dropped_cols = drop_pii_columns(df)
        sanitized = append_user_info(sanitized)
        sanitized = dedupe_columns(sanitized)
        st.session_state.anonymized_df = sanitized

        st.success(f"Dropped PII columns: {sorted(dropped_cols) if dropped_cols else 'None'}")
        st.write("‚úÖ Sanitized Data Preview:")
        st.dataframe(sanitized.head(5), use_container_width=True)

        fpath = save_to_runs(sanitized, "anonymized")
        st.success(f"Saved anonymized file: {fpath}")
        st.download_button(
            "‚¨áÔ∏è Download Clean Data",
            sanitized.to_csv(index=False).encode("utf-8"),
            os.path.basename(fpath),
            "text/csv"
        )
    else:
        st.info("Choose a CSV to see the sanitize flow.", icon="‚ÑπÔ∏è")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ü§ñ TAB 3 ‚Äî Credit appraisal by AI assistant

def tab_C():
    st.subheader("ü§ñ Credit appraisal by AI assistant")
    # Anchor for loopback link from Training tab
    st.markdown('<a name="credit-appraisal-stage"></a>', unsafe_allow_html=True)

    # Production model banner (optional)
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.status_code == 200:
            meta = resp.json()
            if meta.get("has_production"):
                ver = (meta.get("meta") or {}).get("version", "1.x")
                src = (meta.get("meta") or {}).get("source", "production")
                st.success(f"üü¢ Production model active ‚Äî version: {ver} ‚Ä¢ source: {src}")
            else:
                st.warning("‚ö†Ô∏è No production model promoted yet ‚Äî using baseline.")
        else:
            st.info("‚ÑπÔ∏è Could not fetch production model meta.")
    except Exception:
        st.info("‚ÑπÔ∏è Production meta unavailable.")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß© Model Selection (list all trained models) ‚Äî Hardcoded Stable Version
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    from datetime import datetime
    import os, shutil, streamlit as st

    # Hardcoded absolute paths (your confirmed working setup)
    trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/trained"
    production_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/production"

    st.caption(f"üì¶ Trained dir = `{trained_dir}`")
    st.caption(f"üì¶ Production dir = `{production_dir}`")

    # ‚îÄ‚îÄ Refresh models list
    if st.button("‚Üª Refresh models", key="credit_refresh_models"):
        st.session_state.pop("selected_trained_model", None)
        st.rerun()

    # ‚îÄ‚îÄ Collect models
    models = []
    if os.path.isdir(trained_dir):
        for f in os.listdir(trained_dir):
            if f.endswith(".joblib"):
                fpath = os.path.join(trained_dir, f)
                ctime = os.path.getctime(fpath)
                created = datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
                models.append((f, fpath, created))
    else:
        st.error(f"‚ùå Trained dir not found: {trained_dir}")

    # ‚îÄ‚îÄ Show list if found
    if models:
        models.sort(key=lambda x: os.path.getctime(x[1]), reverse=True)
        display_names = [f"{m[0]} ‚Äî {m[2]}" for m in models]

        selected_display = st.selectbox("üì¶ Select trained model to use", display_names)
        selected_model = models[display_names.index(selected_display)][1]
        st.success(f"‚úÖ Using model: {os.path.basename(selected_model)}")

        st.session_state["selected_trained_model"] = selected_model

        # ‚îÄ‚îÄ Promote model
        if st.button("üöÄ Promote this model to Production"):
            try:
                os.makedirs(production_dir, exist_ok=True)
                prod_path = os.path.join(production_dir, "model.joblib")
                shutil.copy2(selected_model, prod_path)
                st.success(f"‚úÖ Model promoted to production: {prod_path}")
            except Exception as e:
                st.error(f"‚ùå Promotion failed: {e}")
    else:
        st.warning("‚ö†Ô∏è No trained models found ‚Äî train one in Step 5 first.")



    # 1) Model + Hardware selection (UI hints)
    LLM_MODELS = [
        ("Phi-3 Mini (3.8B) ‚Äî CPU OK", "phi3:3.8b", "CPU 8GB RAM (fast)"),
        ("Mistral 7B Instruct ‚Äî CPU slow / GPU OK", "mistral:7b-instruct", "CPU 16GB (slow) or GPU ‚â•8GB"),
        ("Gemma-2 7B ‚Äî CPU slow / GPU OK", "gemma2:7b", "CPU 16GB (slow) or GPU ‚â•8GB"),
        ("LLaMA-3 8B ‚Äî GPU recommended", "llama3:8b-instruct", "GPU ‚â•12GB (CPU very slow)"),
        ("Qwen2 7B ‚Äî GPU recommended", "qwen2:7b-instruct", "GPU ‚â•12GB (CPU very slow)"),
        ("Mixtral 8x7B ‚Äî GPU only (big)", "mixtral:8x7b-instruct", "GPU 24‚Äì48GB"),
    ]
    LLM_LABELS = [l for (l, _, _) in LLM_MODELS]
    LLM_VALUE_BY_LABEL = {l: v for (l, v, _) in LLM_MODELS}
    LLM_HINT_BY_LABEL = {l: h for (l, _, h) in LLM_MODELS}

    OPENSTACK_FLAVORS = {
        "m4.medium":  "4 vCPU / 8 GB RAM ‚Äî CPU-only small",
        "m8.large":   "8 vCPU / 16 GB RAM ‚Äî CPU-only medium",
        "g1.a10.1":   "8 vCPU / 32 GB RAM + 1√óA10 24GB",
        "g1.l40.1":   "16 vCPU / 64 GB RAM + 1√óL40 48GB",
        "g2.a100.1":  "24 vCPU / 128 GB RAM + 1√óA100 80GB",
    }

    with st.expander("üß† Local LLM & Hardware Profile", expanded=True):
        c1, c2 = st.columns([1.2, 1])
        with c1:
            model_label = st.selectbox("Local LLM (used for narratives/explanations)", LLM_LABELS, index=1)
            llm_value = LLM_VALUE_BY_LABEL[model_label]
            st.caption(f"Hint: {LLM_HINT_BY_LABEL[model_label]}")
        with c2:
            flavor = st.selectbox("OpenStack flavor / host profile", list(OPENSTACK_FLAVORS.keys()), index=0)
            st.caption(OPENSTACK_FLAVORS[flavor])
        st.caption("These are passed to the API as hints; your API can choose Ollama/Flowise backends accordingly.")

    # 2) Data Source
    data_choice = st.selectbox(
        "Select Data Source",
        [
            "Use synthetic (ANON)",
            "Use synthetic (RAW ‚Äì auto-sanitize)",
            "Use anonymized dataset",
            "Upload manually",
        ]
    )
    use_llm = st.checkbox("Use LLM narrative", value=False)
    agent_name = "credit_appraisal"

    if data_choice == "Upload manually":
        up = st.file_uploader("Upload your CSV", type=["csv"], key="manual_upload_run_file")
        if up is not None:
            st.session_state["manual_upload_name"] = up.name
            st.session_state["manual_upload_bytes"] = up.getvalue()
            st.success(f"File staged: {up.name} ({len(st.session_state['manual_upload_bytes'])} bytes)")

    # 3) Rules
    st.markdown("### ‚öôÔ∏è Decision Rule Set")
    rule_mode = st.radio(
        "Choose rule mode",
        ["Classic (bank-style metrics)", "NDI (Net Disposable Income) ‚Äî simple"],
        index=0,
        help="NDI = income - all monthly obligations. Approve if NDI and NDI ratio pass thresholds."
    )

    CLASSIC_DEFAULTS = {
        "max_dti": 0.45, "min_emp_years": 2, "min_credit_hist": 3, "salary_floor": 3000,
        "max_delinquencies": 2, "max_current_loans": 3, "req_min": 1000, "req_max": 200000,
        "loan_terms": [12, 24, 36, 48, 60], "threshold": 0.45, "target_rate": None, "random_band": True,
        "min_income_debt_ratio": 0.35, "compounded_debt_factor": 1.0, "monthly_debt_relief": 0.50,
    }
    NDI_DEFAULTS = {"ndi_value": 800.0, "ndi_ratio": 0.50, "threshold": 0.45, "target_rate": None, "random_band": True}

    if "classic_rules" not in st.session_state:
        st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    if "ndi_rules" not in st.session_state:
        st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    def reset_classic(): st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    def reset_ndi():     st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    if rule_mode.startswith("Classic"):
        with st.expander("Classic Metrics (with Reset)", expanded=True):
            rc = st.session_state.classic_rules
            r1, r2, r3 = st.columns(3)
            with r1:
                rc["max_dti"] = st.slider("Max Debt-to-Income (DTI)", 0.0, 1.0, rc["max_dti"], 0.01)
                rc["min_emp_years"] = st.number_input("Min Employment Years", 0, 40, rc["min_emp_years"])
                rc["min_credit_hist"] = st.number_input("Min Credit History (years)", 0, 40, rc["min_credit_hist"])
            with r2:
                rc["salary_floor"] = st.number_input("Minimum Monthly Salary", 0, 1_000_000_000, rc["salary_floor"], step=1000, help=fmt_currency_label("in local currency"))
                rc["max_delinquencies"] = st.number_input("Max Delinquencies", 0, 10, rc["max_delinquencies"])
                rc["max_current_loans"] = st.number_input("Max Current Loans", 0, 10, rc["max_current_loans"])
            with r3:
                rc["req_min"] = st.number_input(fmt_currency_label("Requested Amount Min"), 0, 10_000_000_000, rc["req_min"], step=1000)
                rc["req_max"] = st.number_input(fmt_currency_label("Requested Amount Max"), 0, 10_000_000_000, rc["req_max"], step=1000)
                rc["loan_terms"] = st.multiselect("Allowed Loan Terms (months)", [12,24,36,48,60,72], default=rc["loan_terms"])

            st.markdown("#### üßÆ Debt Pressure Controls")
            d1, d2, d3 = st.columns(3)
            with d1:
                rc["min_income_debt_ratio"] = st.slider("Min Income / (Compounded Debt) Ratio", 0.10, 2.00, rc["min_income_debt_ratio"], 0.01)
            with d2:
                rc["compounded_debt_factor"] = st.slider("Compounded Debt Factor (√ó requested)", 0.5, 3.0, rc["compounded_debt_factor"], 0.1)
            with d3:
                rc["monthly_debt_relief"] = st.slider("Monthly Debt Relief Factor", 0.10, 1.00, rc["monthly_debt_relief"], 0.05)

            st.markdown("---")
            c1, c2, c3 = st.columns([1,1,1])
            with c1:
                use_target = st.toggle("üéØ Use target approval rate", value=(rc["target_rate"] is not None))
            with c2:
                rc["random_band"] = st.toggle("üé≤ Randomize approval band (20‚Äì60%) when no target", value=rc["random_band"])
            with c3:
                if st.button("‚Ü©Ô∏è Reset to defaults"):
                    reset_classic()
                    st.rerun()

            if use_target:
                rc["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rc["target_rate"] or 0.40, 0.01)
                rc["threshold"] = None
            else:
                rc["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rc["threshold"], 0.01)
                rc["target_rate"] = None
    else:
        with st.expander("NDI Metrics (with Reset)", expanded=True):
            rn = st.session_state.ndi_rules
            n1, n2 = st.columns(2)
            with n1:
                rn["ndi_value"] = st.number_input(fmt_currency_label("Min NDI (Net Disposable Income) per month"), 0.0, 1e12, float(rn["ndi_value"]), step=50.0)
            with n2:
                rn["ndi_ratio"] = st.slider("Min NDI / Income ratio", 0.0, 1.0, float(rn["ndi_ratio"]), 0.01)
            st.caption("NDI = income - all monthly obligations (rent, food, loans, cards, etc.).")

            st.markdown("---")
            c1, c2, c3 = st.columns([1,1,1])
            with c1:
                use_target = st.toggle("üéØ Use target approval rate", value=(rn["target_rate"] is not None))
            with c2:
                rn["random_band"] = st.toggle("üé≤ Randomize approval band (20‚Äì60%) when no target", value=rn["random_band"])
            with c3:
                if st.button("‚Ü©Ô∏è Reset to defaults (NDI)"):
                    reset_ndi()
                    st.rerun()

            if use_target:
                rn["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rn["target_rate"] or 0.40, 0.01)
                rn["threshold"] = None
            else:
                rn["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rn["threshold"], 0.01)
                rn["target_rate"] = None

    # 4) Run
    if st.button("üöÄ Run Agent", use_container_width=True):
        try:
            files = None
            data: Dict[str, Any] = {
                "use_llm_narrative": str(use_llm).lower(),
                "llm_model": llm_value,
                "hardware_flavor": flavor,
                "currency_code": st.session_state["currency_code"],
                "currency_symbol": st.session_state["currency_symbol"],
            }
            if rule_mode.startswith("Classic"):
                rc = st.session_state.classic_rules
                data.update({
                    "min_employment_years": str(rc["min_emp_years"]),
                    "max_debt_to_income": str(rc["max_dti"]),
                    "min_credit_history_length": str(rc["min_credit_hist"]),
                    "max_num_delinquencies": str(rc["max_delinquencies"]),
                    "max_current_loans": str(rc["max_current_loans"]),
                    "requested_amount_min": str(rc["req_min"]),
                    "requested_amount_max": str(rc["req_max"]),
                    "loan_term_months_allowed": ",".join(map(str, rc["loan_terms"])) if rc["loan_terms"] else "",
                    "min_income_debt_ratio": str(rc["min_income_debt_ratio"]),
                    "compounded_debt_factor": str(rc["compounded_debt_factor"]),
                    "monthly_debt_relief": str(rc["monthly_debt_relief"]),
                    "salary_floor": str(rc["salary_floor"]),
                    "threshold": "" if rc["threshold"] is None else str(rc["threshold"]),
                    "target_approval_rate": "" if rc["target_rate"] is None else str(rc["target_rate"]),
                    "random_band": str(rc["random_band"]).lower(),
                    "random_approval_band": str(rc["random_band"]).lower(),
                    "rule_mode": "classic",
                })
            else:
                rn = st.session_state.ndi_rules
                data.update({
                    "ndi_value": str(rn["ndi_value"]),
                    "ndi_ratio": str(rn["ndi_ratio"]),
                    "threshold": "" if rn["threshold"] is None else str(rn["threshold"]),
                    "target_approval_rate": "" if rn["target_rate"] is None else str(rn["target_rate"]),
                    "random_band": str(rn["random_band"]).lower(),
                    "random_approval_band": str(rn["random_band"]).lower(),
                    "rule_mode": "ndi",
                })

            def prep_and_pack(df: pd.DataFrame, filename: str):
                safe = dedupe_columns(df)
                safe, _ = drop_pii_columns(safe)
                safe = strip_policy_banned(safe)
                safe = to_agent_schema(safe)
                buf = io.StringIO()
                safe.to_csv(buf, index=False)
                return {"file": (filename, buf.getvalue().encode("utf-8"), "text/csv")}

            if data_choice == "Use synthetic (ANON)":
                if "synthetic_df" not in st.session_state:
                    st.warning("No ANON synthetic dataset found. Generate it in the first tab."); st.stop()
                files = prep_and_pack(st.session_state.synthetic_df, "synthetic_anon.csv")

            elif data_choice == "Use synthetic (RAW ‚Äì auto-sanitize)":
                if "synthetic_raw_df" not in st.session_state:
                    st.warning("No RAW synthetic dataset found. Generate it in the first tab."); st.stop()
                files = prep_and_pack(st.session_state.synthetic_raw_df, "synthetic_raw_sanitized.csv")

            elif data_choice == "Use anonymized dataset":
                if "anonymized_df" not in st.session_state:
                    st.warning("No anonymized dataset found. Create it in the second tab."); st.stop()
                files = prep_and_pack(st.session_state.anonymized_df, "anonymized.csv")

            elif data_choice == "Upload manually":
                up_name = st.session_state.get("manual_upload_name")
                up_bytes = st.session_state.get("manual_upload_bytes")
                if not up_name or not up_bytes:
                    st.warning("Please upload a CSV first."); st.stop()
                try:
                    tmp_df = pd.read_csv(io.BytesIO(up_bytes))
                    files = prep_and_pack(tmp_df, up_name)
                except Exception:
                    files = {"file": (up_name, up_bytes, "text/csv")}
            else:
                st.error("Unknown data source selection."); st.stop()

            # ---- RUN REQUEST ----
            r = requests.post(
                f"{API_URL}/v1/agents/{agent_name}/run",
                data=data,
                files=files,
                timeout=180
            )

            if r.status_code != 200:
                st.error(f"Run failed ({r.status_code}): {r.text}")
                st.stop()

            res = r.json()

            # ---- Robust run_id + data extraction ----
            run_id = None
            payload_rows = None  # fallback rows for rendering

            if isinstance(res, dict):
                run_id = res.get("run_id") or res.get("id")
                payload_rows = res.get("result") or res.get("data") or res.get("results") or res.get("rows")
            elif isinstance(res, list):
                payload_rows = res
            else:
                try:
                    maybe = json.loads(res)
                    if isinstance(maybe, dict):
                        run_id = maybe.get("run_id") or maybe.get("id")
                        payload_rows = maybe.get("result") or maybe.get("data") or maybe.get("results") or maybe.get("rows")
                    elif isinstance(maybe, list):
                        payload_rows = maybe
                except Exception:
                    pass

            # ---- Helper: turn any JSON-like into a DataFrame ----
            def json_to_df(obj) -> pd.DataFrame:
                if obj is None:
                    return pd.DataFrame()
                if isinstance(obj, pd.DataFrame):
                    return obj
                if isinstance(obj, bytes):
                    try:
                        obj = obj.decode("utf-8", errors="ignore")
                    except Exception:
                        return pd.DataFrame({"value": [repr(obj)]})
                if isinstance(obj, str):
                    obj = obj.strip()
                    if not obj:
                        return pd.DataFrame()
                    try:
                        j = json.loads(obj)
                        return json_to_df(j)
                    except Exception:
                        lines = [ln for ln in obj.splitlines() if ln.strip()]
                        return pd.DataFrame({"value": lines}) if lines else pd.DataFrame()
                if isinstance(obj, list):
                    if len(obj) == 0:
                        return pd.DataFrame()
                    if all(isinstance(x, dict) for x in obj):
                        try:
                            return pd.json_normalize(obj)
                        except Exception:
                            return pd.DataFrame(obj)
                    if all(isinstance(x, list) for x in obj):
                        return pd.DataFrame({"row": obj})
                    return pd.DataFrame({"value": obj})
                if isinstance(obj, dict):
                    for key in ("rows", "data", "result", "results", "items", "records"):
                        if key in obj and isinstance(obj[key], (list, dict)):
                            return json_to_df(obj[key])
                    try:
                        return pd.json_normalize(obj)
                    except Exception:
                        return pd.DataFrame([obj])
                return pd.DataFrame({"value": [obj]})

            # ---- Prefer server report via run_id; otherwise fall back to local JSON‚ÜíDF ----
            
            # ============================================================
            # ‚úÖ Prefer server CSV ‚Üí fallback to JSON Parser
            # ============================================================
            if run_id:
                try:
                    rid = run_id
                    merged_url = f"{API_URL}/v1/runs/{rid}/report?format=csv"
                    merged_bytes = requests.get(merged_url, timeout=30).content
                    merged_df = pd.read_csv(io.BytesIO(merged_bytes))
                    st.session_state.last_run_id = rid
                    st.success(f"‚úÖ Run succeeded! Run ID: {rid}")
                except Exception as e:
                    st.warning(f"Could not fetch CSV via run_id ({run_id}): {e}")
                    merged_df = json_to_dataframe(payload_rows)
            else:
                st.warning("‚ö†Ô∏è Backend did not return a run_id. Falling back to JSON.")
                merged_df = json_to_dataframe(payload_rows)

 

            if merged_df is None or merged_df.empty:
                st.error("No data available to render (both report and fallback JSON were empty).")
                st.write("Raw response:", res)
                st.stop()

            # Keep for later tabs
            st.session_state["last_merged_df"] = dedupe_columns(merged_df)
            
            # ‚úÖ Make results available to Stage 7 (Reporting & Handoff)
            try:
                st.session_state["credit_scored_df"] = dedupe_columns(merged_df.copy())
                st.success("‚úÖ Stage C outputs saved for Stage 7 (Reporting & Handoff).")
            except Exception as e:
                st.warning(f"Could not persist scored dataset for Stage 7: {e}")


            # ---- Decisions Table (with filter) ----
            st.markdown("### üìÑ Credit AI Agent Decisions Table (filtered)")
            uniq_dec = sorted([d for d in merged_df.get("decision", pd.Series(dtype=str)).dropna().unique()]) \
                    if "decision" in merged_df.columns else []
            chosen = st.multiselect("Filter decision", options=uniq_dec, default=uniq_dec, key="filter_decisions")
            df_view = merged_df.copy()
            if "decision" in df_view.columns and chosen:
                df_view = df_view[df_view["decision"].isin(chosen)]
            st.dataframe(df_view, use_container_width=True)

            # ---- Dashboard ----
            st.markdown("## üìä Dashboard")
            render_credit_dashboard(merged_df, st.session_state.get("currency_symbol", ""))

            # Add per-row metrics columns if present
            if "rule_reasons" in df_view.columns:
                rr = df_view["rule_reasons"].apply(try_json)
                df_view["metrics_met"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is True])) if isinstance(d, dict) else "")
                df_view["metrics_unmet"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is False])) if isinstance(d, dict) else "")

            cols_show = [c for c in [
                "application_id","customer_type","decision","score","loan_amount","income","metrics_met","metrics_unmet",
                "proposed_loan_option","proposed_consolidation_loan","top_feature","explanation"
            ] if c in df_view.columns]
            if cols_show:
                st.dataframe(df_view[cols_show].head(500), use_container_width=True)

            # ---- Download button (keep your large button style) ----
            ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
            out_name = f"ai-appraisal-outputs-{ts}-{st.session_state['currency_code']}.csv"
            csv_data = merged_df.to_csv(index=False).encode("utf-8")

            st.markdown("""
            <style>
            div[data-testid="stDownloadButton"] button {
                font-size: 90px !important;
                font-weight: 900 !important;
                padding: 28px 48px !important;
                border-radius: 16px !important;
                background: linear-gradient(90deg, #2563eb, #1d4ed8) !important;
                color: white !important;
                border: none !important;
                box-shadow: 0 6px 18px rgba(0,0,0,0.35) !important;
                transition: all 0.3s ease-in-out !important;
            }
            div[data-testid="stDownloadButton"] button:hover {
                background: linear-gradient(90deg, #1e3a8a, #1d4ed8) !important;
                transform: scale(1.03);
            }
            </style>
            """, unsafe_allow_html=True)

            st.download_button(
                "‚¨áÔ∏è Download AI Outputs For Human Review (CSV)",
                csv_data,
                file_name=out_name,
                mime="text/csv",
                use_container_width=True
            )
            
            # ‚úÖ CREATE TRAINING LABEL (Stage C ‚Üí Stage F)
            train_df = merged_df.copy()

            # 1) Default probability ‚Üí binary label
            if "default_probability" in train_df.columns:
                train_df["label"] = (train_df["default_probability"] >= 0.5).astype(int)

            # 2) Fallback: use score column if exists
            elif "score" in train_df.columns:
                train_df["label"] = (train_df["score"] >= 0.5).astype(int)

            # 3) Final fallback to avoid Stage F crash
            else:
                train_df["label"] = 0

            # ‚úÖ SAVE FOR TRAINING PIPELINE
            try:
                st.session_state["credit_train_df"] = train_df.copy()
                st.success("‚úÖ Stage C dataset prepared and saved for Stage F (training).")
            except Exception as e:
                st.error(f"Could not save training dataset for Stage F: {e}")

            
            # ‚úÖ SAVE OUTPUT FOR STAGE F (Training)
            try:
                #st.session_state["credit_train_df"] = scored_df.copy()
                st.session_state["credit_train_df"] = merged_df.copy()

                st.success("‚úÖ Stage C output saved for Stage F (training).")
            except Exception as e:
                st.error(f"Could not save Stage C dataset for training: {e}")

        except Exception as e:
            st.exception(e)



    # Re-download quick section
    if st.session_state.get("last_run_id"):
        st.markdown("---")
        st.subheader("üì• Download Latest Outputs")
        rid = st.session_state.last_run_id
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1: st.markdown(f"[‚¨áÔ∏è PDF]({API_URL}/v1/runs/{rid}/report?format=pdf)")
        with col2: st.markdown(f"[‚¨áÔ∏è Scores CSV]({API_URL}/v1/runs/{rid}/report?format=scores_csv)")
        with col3: st.markdown(f"[‚¨áÔ∏è Explanations CSV]({API_URL}/v1/runs/{rid}/report?format=explanations_csv)")
        with col4: st.markdown(f"[‚¨áÔ∏è Merged CSV]({API_URL}/v1/runs/{rid}/report?format=csv)")
        with col5: st.markdown(f"[‚¨áÔ∏è JSON]({API_URL}/v1/runs/{rid}/report?format=json)")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üßë‚Äç‚öñÔ∏è TAB 4 ‚Äî Human Review
def tab_D()
    st.subheader("üßë‚Äç‚öñÔ∏è Human Review ‚Äî Correct AI Decisions & Score Agreement > Drop your AI appraisal output CSV from previous Stage  below")

    # Allow loading AI output CSV back into review via dropdown upload
    uploaded_review = st.file_uploader("Load AI outputs CSV for review (optional)", type=["csv"], key="review_csv_loader")
    if uploaded_review is not None:
        try:
            st.session_state["last_merged_df"] = pd.read_csv(uploaded_review)
            st.success("Loaded review dataset from uploaded CSV.")
        except Exception as e:
            st.error(f"Could not read uploaded CSV: {e}")

    if "last_merged_df" not in st.session_state:
        st.info("Run the agent (previous tab) or upload an AI outputs CSV to load results for review.")
    else:
        dfm = st.session_state["last_merged_df"].copy()
        st.markdown("#### 1) Select rows to review and correct")

        editable_cols = []
        if "decision" in dfm.columns: editable_cols.append("decision")
        if "rule_reasons" in dfm.columns: editable_cols.append("rule_reasons")
        if "customer_type" in dfm.columns: editable_cols.append("customer_type")

        editable = dfm[["application_id"] + editable_cols].copy()
        editable.rename(columns={"decision": "ai_decision"}, inplace=True)
        editable["human_decision"] = editable.get("ai_decision", "approved")
        editable["human_rule_reasons"] = editable.get("rule_reasons", "")

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # LIGHTER EDITABLE CELL STYLING (improved)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        st.markdown("""
            <style>
            /* Bright background for editable cells */
            [data-testid="stDataFrameCellEditable"] textarea {
                background-color: #fefefe !important;   /* bright white background */
                color: #111 !important;                 /* dark text */
                border: 1px solid #cbd5e1 !important;   /* subtle gray border */
                border-radius: 6px !important;
                padding: 6px 8px !important;
                font-weight: 500 !important;
            }

            /* Hover and focus effect */
            [data-testid="stDataFrameCellEditable"]:focus-within textarea,
            [data-testid="stDataFrameCellEditable"]:hover textarea {
                background-color: #ffffff !important;
                border-color: #22c55e !important;        /* green glow */
                box-shadow: 0 0 0 2px rgba(34,197,94,0.4) !important;
            }

            /* Read-only cells: keep dark */
            [data-testid="stDataFrameCell"] {
                background-color: #1e293b !important;
                color: #e2e8f0 !important;
            }
            </style>
        """, unsafe_allow_html=True)


        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # EDITOR
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        edited = st.data_editor(
            editable,
            num_rows="dynamic",
            use_container_width=True,
            key="review_editor",
            column_config={
                "human_decision": st.column_config.SelectboxColumn(options=["approved", "denied"]),
                "customer_type": st.column_config.SelectboxColumn(options=["bank", "non-bank"], disabled=True)
            }
        )

        st.markdown("#### 2) Compute agreement score")

        if st.button("Compute agreement score"):
            if "ai_decision" in edited.columns and "human_decision" in edited.columns:
                agree = (edited["ai_decision"] == edited["human_decision"]).astype(int)
                score = float(agree.mean()) if len(agree) else 0.0
                st.session_state["last_agreement_score"] = score

                # üå°Ô∏è BEAUTIFUL Gauge
                import plotly.graph_objects as go
                fig = go.Figure(go.Indicator(
                    mode="gauge+number",
                    value=score * 100,
                    number={'suffix': "%", 'font': {'size': 72, 'color': "#f8fafc", 'family': "Arial Black"}},
                    title={'text': "AI ‚Üî Human Agreement", 'font': {'size': 28, 'color': "#93c5fd", 'family': "Arial"}},
                    gauge={
                        'axis': {'range': [0, 100], 'tickwidth': 2, 'tickcolor': "#f8fafc"},
                        'bar': {'color': "#3b82f6", 'thickness': 0.3},
                        'bgcolor': "#1e293b",
                        'borderwidth': 2,
                        'bordercolor': "#334155",
                        'steps': [
                            {'range': [0, 50], 'color': "#ef4444"},
                            {'range': [50, 75], 'color': "#f59e0b"},
                            {'range': [75, 100], 'color': "#22c55e"},
                        ],
                    }
                ))
                fig.update_layout(
                    paper_bgcolor="#0f172a",
                    plot_bgcolor="#0f172a",
                    height=400,
                    margin=dict(t=60, b=20, l=60, r=60)
                )
                st.plotly_chart(fig, use_container_width=True)



            # üí° Detailed disagreement table (AI vs Human + AI metrics explanation)
                mismatched = edited[edited["ai_decision"] != edited["human_decision"]].copy()
                total = len(edited)
                disagree = len(mismatched)

                if disagree > 0:
                    st.markdown(f"### ‚ùå {disagree} loans disagreed out of {total} ({(disagree/total)*100:.1f}% disagreement rate)")

                    import json

                    def parse_ai_reason(r: str):
                        """Parse AI rule_reasons and summarize which metrics passed or failed."""
                        if not isinstance(r, str):
                            return "No metrics available"
                        try:
                            data = json.loads(r.replace("'", "\""))
                            passed = [k for k, v in data.items() if v is True]
                            failed = [k for k, v in data.items() if v is False]
                            result = []
                            if passed:
                                result.append("‚úÖ Pass: " + ", ".join(passed))
                            if failed:
                                result.append("‚ùå Fail: " + ", ".join(failed))
                            return " | ".join(result) if result else "No metrics recorded"
                        except Exception:
                            return "Unreadable metrics"

                    # Extract AI reasoning and Human reason columns
                    mismatched["ai_metrics"] = mismatched["rule_reasons"].apply(parse_ai_reason) if "rule_reasons" in mismatched else "No data"
                    mismatched["human_reason"] = mismatched.get("human_rule_reasons", "Manual review adjustment")

                    # üü©üü• Color styling for AI vs Human
                    def highlight_disagreement(row):
                        ai_color = "background-color: #ef4444; color: white;"      # red for AI decision
                        human_color = "background-color: #22c55e; color: black;"   # green for Human decision
                        return [
                            ai_color if col == "ai_decision" else
                            human_color if col == "human_decision" else
                            ""
                            for col in row.index
                        ]

                    # Columns: ID ‚Üí AI Decision ‚Üí Human Decision ‚Üí AI Metrics ‚Üí Human Reason
                    show_cols = [
                        c for c in ["application_id", "ai_decision", "human_decision", "ai_metrics", "human_reason"]
                        if c in mismatched.columns
                    ]
                    styled_df = mismatched[show_cols].style.apply(highlight_disagreement, axis=1)
                    st.dataframe(styled_df, use_container_width=True, height=420)

                else:
                    st.success("‚úÖ Full agreement ‚Äî no human-AI mismatches found.")



        # Export review CSV (manual loop into training)
        st.markdown("#### 3) Export Human review CSV for Next Step : Training and loopback ")
        model_used = "production"  # if you track specific model names, set it here
        ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
        safe_user = st.session_state["user_info"]["name"].replace(" ", "").lower()
        review_name = f"creditappraisal.{safe_user}.{model_used}.{ts}.csv"
        csv_bytes = edited.to_csv(index=False).encode("utf-8")
        st.download_button("‚¨áÔ∏è Export review CSV", csv_bytes, review_name, "text/csv")
        st.caption(f"Saved file name pattern: **{review_name}**")


# -------------------------------------------------------------
# ‚úÖ STAGE 5 ‚Äî Credit Model Training (Executive Dashboard)
# -------------------------------------------------------------
def tab_E()
    import os, json, glob, shutil, zipfile
    from datetime import datetime, timezone
    from pathlib import Path
    import numpy as np
    import pandas as pd
    import plotly.express as px
    import plotly.graph_objects as go

    from sklearn.model_selection import train_test_split
    from sklearn.metrics import (
        roc_auc_score, accuracy_score, precision_score, 
        recall_score, f1_score, confusion_matrix
    )

    import joblib
    import streamlit as st

    # ---------------------------------------------------------
    # ‚úÖ HEADER
    # ---------------------------------------------------------
    st.markdown("""
    <h2>üß† Stage 5 ‚Äî Credit Model Training</h2>
    <p style='font-size:1.1rem'>
    Train ‚Üí Compare ‚Üí Evaluate ‚Üí Promote<br>
    Build a robust, regulator-friendly credit scoring model.
    </p>
    """, unsafe_allow_html=True)

    # ---------------------------------------------------------
    # ‚úÖ LOAD TRAINING DATA (from Stage C)
    # ---------------------------------------------------------
    train_df = st.session_state.get("credit_train_df")

    if train_df is None or train_df.empty:
        st.error("‚ö†Ô∏è Missing training dataset. Please run Stage C first.")
        st.stop()

    st.success(f"‚úÖ Training dataset detected with {len(train_df):,} rows.")
    st.dataframe(train_df.head(), use_container_width=True)

    st.markdown("---")

    # ---------------------------------------------------------
    # ‚úÖ TRAINING DATA LOADING (Human feedback OR CSV upload)
    # ---------------------------------------------------------
    st.markdown("### üì• Stage 5 Training Data Input")

    train_df = None
    source_label = None

    # ‚úÖ Option A ‚Äî Human Review Stage output is available
    if "credit_human_review_df" in st.session_state:
        df_human = st.session_state.get("credit_human_review_df")
        if isinstance(df_human, pd.DataFrame) and not df_human.empty:
            train_df = df_human.copy()
            source_label = "Human Review Stage (Session State)"

    # ‚úÖ Option B ‚Äî Model Inference Stage C merged_df output (fallback)
    elif "credit_train_df" in st.session_state:
        df_auto = st.session_state.get("credit_train_df")
        if isinstance(df_auto, pd.DataFrame) and not df_auto.empty:
            train_df = df_auto.copy()
            source_label = "Stage C auto-saved dataset"

    # ‚úÖ Option C ‚Äî User uploads CSV manually
    uploaded = st.file_uploader("Upload training CSV (optional)", type=["csv"])

    if uploaded is not None:
        try:
            train_df = pd.read_csv(uploaded)
            source_label = f"Uploaded CSV ({len(train_df)} rows)"
        except Exception as e:
            st.error(f"‚ùå Could not read uploaded CSV: {e}")

    # ‚úÖ Hard stop if no dataset is available
    if train_df is None or train_df.empty:
        st.error("""
        ‚ùå No training data found.

        ‚úÖ Provide training dataset by:
        ‚Ä¢ Completing Human Review Stage (Stage D)  
        ‚Ä¢ OR uploading a CSV here  
        ‚Ä¢ OR enabling Stage C to save merged output  
        """)
        st.stop()

    # ‚úÖ Show dataset preview + source
    st.success(f"‚úÖ Training dataset loaded from: **{source_label}**")
    st.dataframe(train_df.head(), use_container_width=True)

    st.markdown("---")

    
    
    # ---------------------------------------------------------
    # ‚úÖ MODEL SELECTION
    # ---------------------------------------------------------
    st.subheader("ü§ñ Choose training model")

    model_choice = st.selectbox(
        "Select model:",
        ["LogisticRegression", "RandomForest", "LightGBM", "XGBoost"]
    )

    # ---------------------------------------------------------
    # ‚úÖ TRAINING LOGIC
    # ---------------------------------------------------------
    if st.button("üöÄ Train Credit Model Now"):
        with st.spinner("Training model‚Ä¶"):

            # ‚úÖ TARGET COLUMN = score
            TARGET_COL = "score"

            if TARGET_COL not in train_df.columns:
                st.error(f"‚ùå Target '{TARGET_COL}' not found.")
                st.stop()

            # ‚úÖ Clean target
            y_cont = pd.to_numeric(train_df[TARGET_COL], errors="coerce")
            df_clean = train_df.dropna(subset=[TARGET_COL]).copy()
            y_cont = df_clean[TARGET_COL].astype(float)

            # ‚úÖ Binarize score
            threshold = float(y_cont.median())
            y_bin = (y_cont >= threshold).astype(int)

            st.info(f"‚úÖ Auto-threshold for binarization = {threshold:.4f}")

            # ‚úÖ Feature selection (remove target + leakage)
            LEAKAGE_COLS = [
                TARGET_COL,
                "decision", "confidence",
                "top_feature", "explanation",
                "proposed_loan_option", "proposed_consolidation_loan",
                "rule_reasons"
            ]

            X = df_clean.drop(columns=[c for c in LEAKAGE_COLS if c in df_clean.columns])

            
            # ---------------------------------------------------------
            # ‚úÖ FEATURE SELECTION ‚Äî DROP ALL NON-NUMERIC COLUMNS
            # ---------------------------------------------------------

            # Columns that MUST NOT be used as features
            BAD_COLS = [
                TARGET_COL,                   # remove score
                "application_id",
                "asset_id",
                "customer_id",
                "currency_code",
                "collateral_type",
                "customer_type",
                "created_at",
                "session_flagged",
                "decision", "confidence",
                "top_feature", "explanation",
                "proposed_loan_option", "proposed_consolidation_loan",
                "rule_reasons"
            ]

            # Drop all known non-features
            df_fe = df_clean.drop(columns=[c for c in BAD_COLS if c in df_clean.columns])

            # ‚úÖ Keep ONLY numeric columns
            X = df_fe.select_dtypes(include=["number"]).copy()

            # ‚úÖ Safety check
            if X.empty:
                st.error("‚ùå No numeric features available for training. Check your dataset.")
                st.stop()

            st.success(f"‚úÖ Using {X.shape[1]} numeric features for training.")
            st.dataframe(X.head())

            
            # ‚úÖ Split
            Xtr, Xte, ytr, yte = train_test_split(
                X, y_bin, test_size=0.2, random_state=42
            )

            # ‚úÖ Model selection
            if model_choice == "LogisticRegression":
                from sklearn.linear_model import LogisticRegression
                model = LogisticRegression(max_iter=2000)
            elif model_choice == "RandomForest":
                from sklearn.ensemble import RandomForestClassifier
                model = RandomForestClassifier(n_estimators=300)
            elif model_choice == "LightGBM":
                from lightgbm import LGBMClassifier
                model = LGBMClassifier()
            else:
                from xgboost import XGBClassifier
                model = XGBClassifier()

            # ‚úÖ Train
            model.fit(Xtr, ytr)

            # ‚úÖ Predictions
            preds_proba = model.predict_proba(Xte)[:, 1]
            preds = (preds_proba >= 0.5).astype(int)

            # ‚úÖ Metrics
            new_m = {
                "AUC": roc_auc_score(yte, preds_proba),
                "Accuracy": accuracy_score(yte, preds),
                "Precision": precision_score(yte, preds),
                "Recall": recall_score(yte, preds),
                "F1": f1_score(yte, preds),
            }

            # -----------------------------------------------------
            # ‚úÖ LOAD PRODUCTION BASELINE IF EXISTS
            # -----------------------------------------------------
            PROD_DIR = Path("./agents/credit_appraisal/models/production")
            prod_meta_path = PROD_DIR / "production_meta.json"

            if prod_meta_path.exists():
                prod_m = json.load(open(prod_meta_path))["metrics"]
            else:
                prod_m = None

            st.markdown("---")
            st.subheader("üìä A/B Model Comparison")

            # ‚úÖ COMPARISON TABLE
            cmp_df = pd.DataFrame({
                "Metric": list(new_m.keys()),
                "New Model": [f"{v:.4f}" for v in new_m.values()],
                "Production": [
                    f"{prod_m[k]:.4f}" if prod_m else "‚Äî"
                    for k in new_m.keys()
                ]
            })
            st.table(cmp_df)

            # -----------------------------------------------------
            # ‚úÖ EXECUTIVE SUMMARY (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)
            # -----------------------------------------------------
            st.markdown("## üß≠ Executive Summary (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)")

            if prod_m:
                auc_delta = new_m["AUC"] - prod_m["AUC"]
                if auc_delta > 0:
                    st.success(f"‚úÖ Model improves **AUC by {auc_delta:.4f}** ‚Äî better discrimination.")
                else:
                    st.warning(f"‚ö†Ô∏è AUC dropped by {auc_delta:.4f} ‚Äî further tuning required.")
            else:
                st.info("üü¢ First model ‚Äî will become baseline.")

            # -----------------------------------------------------
            # ‚úÖ CONFUSION MATRIX
            # -----------------------------------------------------
            cm = confusion_matrix(yte, preds)
            cm_fig = px.imshow(
                cm, text_auto=True,
                title="Confusion Matrix",
                labels={"x": "Predicted", "y": "Actual"}
            )
            st.plotly_chart(cm_fig, use_container_width=True)

            # -----------------------------------------------------
            # ‚úÖ FEATURE IMPORTANCE
            # -----------------------------------------------------
            st.subheader("üß† Feature Importance")

            if hasattr(model, "feature_importances_"):
                imp = pd.DataFrame({
                    "feature": X.columns,
                    "importance": model.feature_importances_
                }).sort_values("importance", ascending=False)
                st.bar_chart(imp.set_index("feature"))
            elif hasattr(model, "coef_"):
                coef = pd.DataFrame({
                    "feature": X.columns,
                    "coef": np.ravel(model.coef_)
                }).sort_values("coef", key=np.abs, ascending=False)
                st.bar_chart(coef.set_index("feature"))
            else:
                st.info("This model does not expose importance metrics.")

            # -----------------------------------------------------
            # ‚úÖ SAVE MODEL
            # -----------------------------------------------------
            TRAINED_DIR = Path("./agents/credit_appraisal/models/trained")
            TRAINED_DIR.mkdir(parents=True, exist_ok=True)

            ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
            model_path = TRAINED_DIR / f"{model_choice}_{ts}.joblib"
            joblib.dump(model, model_path)

            st.success(f"‚úÖ Model saved ‚Üí `{model_path}`")

            # ‚úÖ SAVE REPORT
            RUNS_DIR = Path("./.tmp_runs")
            RUNS_DIR.mkdir(exist_ok=True)

            report = {
                "timestamp": ts,
                "model_choice": model_choice,
                "metrics": new_m,
                "model_path": str(model_path),
                "features": list(X.columns),
                "threshold": threshold,
            }

            rep_path = RUNS_DIR / f"credit_training_report_{ts}.json"
            json.dump(report, open(rep_path, "w"), indent=2)

            # store for next stage
            st.session_state["credit_last_model_path"] = str(model_path)
            st.session_state["credit_last_metrics"] = new_m
            st.session_state["credit_last_report"] = report

            st.caption(f"üìÑ Report saved ‚Üí `{rep_path}`")

            # -----------------------------------------------------
            # ‚úÖ PROMOTION BLOCK
            # -----------------------------------------------------
            st.markdown("## üì§ Promote This Model to Production")

            if st.button("‚úÖ Promote to Production"):
                try:
                    PROD_DIR.mkdir(parents=True, exist_ok=True)

                    shutil.copy(model_path, PROD_DIR / "model.joblib")

                    meta = {
                        "promoted_at": datetime.now(timezone.utc).isoformat(),
                        "metrics": new_m,
                        "model_path": str(model_path),
                        "model_choice": model_choice
                    }
                    json.dump(meta, open(PROD_DIR / "production_meta.json", "w"), indent=2)

                    st.balloons()
                    st.success("‚úÖ Model promoted successfully!")

                except Exception as e:
                    st.error(f"‚ùå Promotion failed: {e}")

            # -----------------------------------------------------
            # ‚úÖ EXPORT ZIP
            # -----------------------------------------------------
            st.markdown("## üì¶ Export Project ZIP")

            EXPORT_DIR = Path("./exports")
            EXPORT_DIR.mkdir(exist_ok=True)

            zip_name = f"credit_project_bundle_{ts}.zip"
            zip_path = EXPORT_DIR / zip_name

            if st.button("‚¨áÔ∏è Build ZIP Bundle"):
                try:
                    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:

                        # Runs
                        for root, dirs, files in os.walk(RUNS_DIR):
                            for f in files:
                                full = os.path.join(root, f)
                                arc = os.path.relpath(full, RUNS_DIR)
                                zf.write(full, f"runs/{arc}")

                        # Production models
                        if PROD_DIR.exists():
                            for f in PROD_DIR.glob("*"):
                                zf.write(f, f"production/{f.name}")

                        # Trained models
                        for f in TRAINED_DIR.glob("*.joblib"):
                            zf.write(f, f"trained/{f.name}")

                        # Training report
                        zf.write(rep_path, "training_report.json")

                    st.success("‚úÖ ZIP created!")
                    with open(zip_path, "rb") as fp:
                        st.download_button(
                            "‚¨áÔ∏è Download ZIP",
                            data=fp,
                            file_name=zip_name,
                            mime="application/zip",
                            use_container_width=True
                        )
                except Exception as e:
                    st.error(f"‚ùå ZIP creation failed: {e}")




# -------------------------------------------------------------
# ‚úÖ STAGE 6 ‚Äî Deployment of Credit Scoring Model
# -------------------------------------------------------------
def tab_B()
    import os, json, shutil, zipfile
    from pathlib import Path
    from datetime import datetime, timezone

    st.markdown("## üöÄ Stage G ‚Äî Model Deployment")
    st.caption("Promote trained model ‚Üí publish ‚Üí export deployment bundle")

    last_model = st.session_state.get("credit_last_model_path")
    metrics = st.session_state.get("credit_last_metrics")
    report = st.session_state.get("credit_last_report")

    if not last_model:
        st.warning("‚ö†Ô∏è Train a model in Stage F before deploying.")
        st.stop()

    st.success(f"‚úÖ Latest trained model detected:\n`{last_model}`")
    st.json(metrics)

    # ---------------------------------------------------------
    # ‚úÖ Promote to production
    # ---------------------------------------------------------
    if st.button("‚úÖ Promote This Model to Production"):
        prod_dir = Path("./agents/credit_appraisal/models/production")
        prod_dir.mkdir(parents=True, exist_ok=True)

        shutil.copy(last_model, prod_dir / "model.joblib")

        prod_meta = {
            "model_path": last_model,
            "promoted_at": datetime.now(timezone.utc).isoformat(),
            "metrics": metrics,
            "report": report,
        }
        json.dump(prod_meta, open(prod_dir / "production_meta.json", "w"), indent=2)

        st.balloons()
        st.success("‚úÖ Model promoted to production successfully!")

    # ---------------------------------------------------------
    # ‚úÖ Export deployment ZIP
    # ---------------------------------------------------------
    EXPORT_DIR = Path("./exports")
    EXPORT_DIR.mkdir(exist_ok=True)
    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    if st.button("‚¨áÔ∏è Export Deployment Bundle"):
        zip_path = EXPORT_DIR / f"credit_deployment_{ts}.zip"

        with zipfile.ZipFile(zip_path, "w") as zf:
            zf.write(last_model, arcname="trained_model.joblib")
            zf.write("./agents/credit_appraisal/models/production/production_meta.json",
                     arcname="production_meta.json")

        with open(zip_path, "rb") as f:
            st.download_button(
                "‚¨áÔ∏è Download Deployment ZIP",
                data=f,
                file_name=zip_path.name,
                mime="application/zip",
            )

        st.success("‚úÖ Deployment bundle ready!")


# -------------------------------------------------------------
# ‚úÖ STAGE 7 ‚Äî Reporting & Handoff
# -------------------------------------------------------------

with tab_handoff:
    import os, json, zipfile
    import numpy as np
    import pandas as pd
    from pathlib import Path
    from datetime import datetime, timezone
    import streamlit as st
    import plotly.express as px

    st.markdown("## üìä Stage 7 ‚Äî Portfolio Reporting & Handoff")

    # 1) Primary: dataset saved by Stage C/E
    df = st.session_state.get("credit_scored_df")

    # 2) Fallback: Stage C merged output
    if df is None or df.empty:
        df = st.session_state.get("last_merged_df")

    # 3) Optional: user upload
    uploaded_scored = st.file_uploader(
        "‚¨ÜÔ∏è (Optional) Load scored CSV for reporting",
        type=["csv"], key="stage7_upload"
    )
    if uploaded_scored is not None:
        try:
            df = pd.read_csv(uploaded_scored)
            st.success(f"Loaded scored dataset from upload ({len(df)} rows).")
        except Exception as e:
            st.error(f"Could not read uploaded CSV: {e}")

    # Final guard
    if df is None or df.empty:
        st.warning("‚ö†Ô∏è Missing scored dataset. Run Stage 3 (Credit appraisal) or upload a scored CSV above.")
        st.stop()

    # Persist for other tabs
    st.session_state["credit_scored_df"] = df.copy()

    st.success("‚úÖ Portfolio loaded.")
    st.dataframe(df.head(), use_container_width=True)

    # ‚Ä¶ (keep the rest of Stage 7: metrics, charts, handoff CSV/ZIP) ‚Ä¶


    # ---------------------------------------------------------
    # ‚úÖ Executive dashboard
    # ---------------------------------------------------------
    st.markdown("### üß≠ Executive Summary")

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Applications", len(df))
    with col2:
        st.metric("Approved", (df["decision"] == "approve").sum())
    with col3:
        st.metric("Rejected", (df["decision"] == "reject").sum())

    # # ---------------------------------------------------------
    # # ‚úÖ Approval distribution
    # # ---------------------------------------------------------
    # # st.markdown("### üìà Approval Distribution")
    # # fig = px.histogram(df, x="decision", color="decision", title="Approval vs Rejection")
    # # st.plotly_chart(fig, use_container_width=True)
    # fig = px.histogram(
    # df, x="decision", color="decision",
    # color_discrete_sequence=PALETTE,
    # title="Approval vs Rejection"
    # )
    # fig = apply_dark(fig)
    # fig.update_traces(marker_line_width=0.5)
    # fig.update_xaxes(title=None, gridcolor="rgba(255,255,255,0.08)")
    # fig.update_yaxes(gridcolor="rgba(255,255,255,0.08)")
    # st.plotly_chart(fig, use_container_width=True)

    # ---------------------------------------------------------
    # ‚úÖ Approval distribution (robust to 0/1, strings, themes)
    # ---------------------------------------------------------
    st.markdown("### üìà Approval Distribution")

    # 1) Normalize labels
    if "decision" not in df.columns:
        st.info("No 'decision' column found; skipping approval chart.")
    else:
        vals = df["decision"]

        def to_label(v):
            if isinstance(v, str):
                s = v.strip().lower()
                if s in ("approve", "approved", "yes", "y", "1", "true"):
                    return "approve"
                if s in ("reject", "rejected", "no", "n", "0", "false"):
                    return "reject"
                return s or "unknown"
            # numeric/bool
            try:
                return "approve" if float(v) >= 1 else "reject"
            except Exception:
                return "unknown"

        df["decision_label"] = vals.map(to_label).fillna("unknown")

        # 2) Safe color map (must be a LIST ‚Üí map to dict)
        palette = px.colors.qualitative.Set2  # e.g., ['#66c2a5', '#fc8d62', ...]
        color_map = {
            "approve": palette[0] if len(palette) > 0 else "#22c55e",
            "reject":  palette[1] if len(palette) > 1 else "#ef4444",
            "unknown": palette[2] if len(palette) > 2 else "#94a3b8",
        }

        # 3) Fixed category order for readability
        categories = ["approve", "reject", "unknown"]

        fig = px.histogram(
            df,
            x="decision_label",
            color="decision_label",
            category_orders={"decision_label": categories},
            color_discrete_map=color_map,
            title="Approval vs Rejection",
        )
        fig.update_layout(
            legend_title_text="Decision",
            bargap=0.2,
            paper_bgcolor="rgba(0,0,0,0)",
            plot_bgcolor="rgba(0,0,0,0)",
            font_color=("#e2e8f0" if st.session_state.get("theme", "dark") == "dark" else "#0f172a"),
        )
        st.plotly_chart(fig, use_container_width=True)


    # ---------------------------------------------------------
    # ‚úÖ Department Handoff: Credit / Risk / Compliance / CS
    # ---------------------------------------------------------
    st.markdown("## üè¶ Department Handoff Packages")
    
    # ---------- helpers ----------
    def pick(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
        """Return df with only columns that actually exist (no KeyError)."""
        keep = [c for c in cols if c in df.columns]
        return df[keep].copy()

    # ---------- ensure 'reason' exists ----------
    if "reason" not in df.columns:
        if "explanation" in df.columns:
            df["reason"] = df["explanation"].astype(str).str.slice(0, 200)
        elif {"pd", "dti", "ltv"}.issubset(df.columns) or "score" in df.columns:
            def infer_reason(row):
                try:
                    if float(row.get("pd", 0)) >= 0.15:
                        return "High probability of default"
                    if float(row.get("dti", 0)) >= 0.5:
                        return "High debt-to-income"
                    if float(row.get("ltv", 0)) >= 0.8:
                        return "High loan-to-value"
                    if float(row.get("score", 999)) < 600:
                        return "Low credit score"
                except Exception:
                    pass
                return "Policy/Other"
            df["reason"] = df.apply(infer_reason, axis=1)
        else:
            df["reason"] = ""


    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
    HANDOFF_DIR = Path("./credit_handoff")
    ZIP_DIR = HANDOFF_DIR / "zips"
    HANDOFF_DIR.mkdir(exist_ok=True)
    ZIP_DIR.mkdir(exist_ok=True)
    
    credit = pick(df, ["application_id","score","decision","reason","income","loan_amount"])
    risk = pick(df, ["application_id","score","pd","ltv","dti","decision"])
    compliance = pick(df, ["application_id","account_age","delinquencies","fraud_flag","decision"])
    customer = pick(df, ["application_id","score","decision","explanation","reason"])


    # credit = df[["application_id","score","decision","reason","income","loan_amount"]]
    # risk = df[["application_id","score","pd","ltv","dti","decision"]]
    # compliance = df[["application_id","account_age","delinquencies","fraud_flag","decision"]]
    # customer = df[["application_id","score","decision","explanation"]]

    paths = {
        "credit": HANDOFF_DIR / f"credit_{ts}.csv",
        "risk": HANDOFF_DIR / f"risk_{ts}.csv",
        "compliance": HANDOFF_DIR / f"compliance_{ts}.csv",
        "customer": HANDOFF_DIR / f"customer_service_{ts}.csv",
    }

    # Save all
    credit.to_csv(paths["credit"], index=False)
    risk.to_csv(paths["risk"], index=False)
    compliance.to_csv(paths["compliance"], index=False)
    customer.to_csv(paths["customer"], index=False)

    # ---------------------------------------------------------
    # ‚úÖ ZIP bundle
    # ---------------------------------------------------------
    zip_path = ZIP_DIR / f"credit_handoff_{ts}.zip"
    with zipfile.ZipFile(zip_path, "w") as zf:
        for p in paths.values():
            zf.write(p, arcname=os.path.basename(p))

    st.download_button(
        "‚¨áÔ∏è Download Full Handoff ZIP",
        data=open(zip_path, "rb").read(),
        file_name=os.path.basename(zip_path),
        mime="application/zip",
        use_container_width=True
    )

    # st.markdown("### üß© Department Package Map")
    # st.json({k: list(df[list(credit.columns)].columns)})
    st.markdown("### üß© Department Package Map")
    st.json({
        "credit":   list(credit.columns),
        "risk":     list(risk.columns),
        "compliance": list(compliance.columns),
        "customer_service": list(customer.columns),
    })

    # Optional: tell user if something was missing
    expected = {
        "credit": ["application_id","score","decision","reason","income","loan_amount"],
        "risk": ["application_id","score","pd","ltv","dti","decision"],
        "compliance": ["application_id","account_age","delinquencies","fraud_flag","decision"],
        "customer_service": ["application_id","score","decision","explanation","reason"],
    }
    missing_report = {
        pkg: [c for c in expected[pkg] if c not in cols]
        for pkg, cols in {
            "credit": credit.columns,
            "risk": risk.columns,
            "compliance": compliance.columns,
            "customer_service": customer.columns,
        }.items()
    }
    if any(missing_report.values()):
        st.info(f"Some expected columns were not present and were skipped: {missing_report}")



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üó£Ô∏è TAB 8 ‚Äî Feedback & Feature Requests
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tab_feedback:
    st.subheader("üó£Ô∏è Share Your Feedback and Feature Ideas")

    FEEDBACK_FILE = os.path.join(BASE_DIR, "agents_feedback.json")

    def load_feedback() -> dict:
        try:
            with open(FEEDBACK_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    def save_feedback(data: dict):
        try:
            with open(FEEDBACK_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            st.error(f"Could not save feedback: {e}")

    feedback_data = load_feedback()

    # View all current agent feedback
    st.markdown("### üí¨ Current Agent Reviews & Ratings")
    for agent, fb in feedback_data.items():
        with st.expander(f"‚≠ê {agent} ‚Äî {fb.get('rating', 0)}/5  |  üë• {fb.get('users', 0)} users"):
            st.markdown("#### Recent Comments:")
            for cmt in reversed(fb.get("comments", [])):
                st.markdown(f"- {cmt}")
            st.markdown("---")

    st.markdown("### ‚úçÔ∏è Submit Your Own Feedback or Feature Request")

    agent_choice = st.selectbox("Select Agent", list(feedback_data.keys()))
    new_comment = st.text_area("Your Comment or Feature Suggestion", placeholder="e.g. Add multi-language support for reports...")
    new_rating = st.slider("Your Rating", 1, 5, 5)


    if st.button("üì® Submit Feedback"):
        if new_comment.strip():
            fb = feedback_data.get(agent_choice, {"rating": 0, "users": 0, "comments": []})
            fb["comments"].append(new_comment.strip())
            fb["rating"] = round((fb.get("rating", 0) + new_rating) / 2, 2)
            fb["users"] = fb.get("users", 0) + 1
            feedback_data[agent_choice] = fb
            save_feedback(feedback_data)

            # ‚úÖ Sync latest feedback globally
            st.session_state["feedback_data"] = feedback_data

            # ‚úÖ Force full reload so Landing updates instantly
            st.success("‚úÖ Feedback submitted successfully!")
            st.rerun()
        else:
            st.warning("Please enter a comment before submitting.")





render_credit_header()
tA, tB, tC, tD, tE, tF = st.tabs([
    "üü¶ A) Intake", "üü© B) Privacy & Features", "üü® C) Modelling",
    "üüß D) Policy & Decision", "üü™ E) Review", "üü´ F) Reporting"
])
with tA: tab_A()
with tB: tab_B()
with tC: tab_C()
with tD: tab_D()
with tE: tab_E()
with tF: tab_F()



==================== ./_old/credit_appraisalgood.py ====================
# services/ui/app.py
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üåê OpenSource AI Agent Library + Credit Appraisal PoC by Dzoan
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from __future__ import annotations
import os
import re
import io
import json
import datetime
from typing import Optional, Dict, List, Any

import pandas as pd
import numpy as np
import streamlit as st
import requests
import plotly.express as px
import plotly.graph_objects as go


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üîÅ Manage active tab navigation manually
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if "active_tab" not in st.session_state:
    st.session_state.active_tab = "tab_gen"  # or whichever tab should open by default

def switch_tab(tab_name: str):
    """Programmatically switch between workflow tabs."""
    st.session_state.active_tab = tab_name
    st.rerun()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üåå GLOBAL DARK THEME + BLUE GLOW ENHANCED UI
st.markdown("""
<style>
/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   GLOBAL BACKGROUND + TEXT
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
body, .stApp {
    background-color: #0f172a !important;   /* very dark navy */
    color: #e5e7eb !important;
    font-family: 'Inter', sans-serif;
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   HEADER + TITLE AREA
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
[data-testid="stHeader"], .stApp header {
    background: #0f172a !important;
    color: #f8fafc !important;
    border-bottom: 1px solid #1e293b !important;
}

/* Main App Title ‚Äî add neon blue glow */
h1, h2, h3 {
    color: #f8fafc !important;
    font-weight: 800 !important;
    text-shadow: 0 0 8px rgba(37,99,235,0.6),
                 0 0 16px rgba(59,130,246,0.3);
}

/* Subheaders (like Human Review, Credit Appraisal, etc.) */
.stSubheader, h4, h5, h6 {
    color: #f1f5f9 !important;
    font-weight: 700 !important;
    text-shadow: 0 0 4px rgba(37,99,235,0.4);
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   TAB BAR
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
div[data-baseweb="tab"] > button {
    color: #cbd5e1 !important;
    background-color: #1e293b !important;
    border: 1px solid #334155 !important;
    border-radius: 10px !important;
    font-weight: 600 !important;
}
div[data-baseweb="tab"] > button[aria-selected="true"] {
    background: linear-gradient(90deg, #2563eb, #1d4ed8);
    color: white !important;
    border: none !important;
    box-shadow: 0 0 12px rgba(37,99,235,0.5);
    transform: scale(1.05);
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   BOXES, EXPANDERS, AND CONTAINERS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.stExpander, .stFileUploader, .stDataFrame, .stJson, .stMetric {
    background-color: #1e293b !important;
    color: #f1f5f9 !important;
    border-radius: 10px !important;
}

/* DataFrames */
[data-testid="stDataFrame"] thead tr th {
    color: #f1f5f9 !important;
    background-color: #1e293b !important;
    border-bottom: 1px solid #334155 !important;
}
[data-testid="stDataFrame"] tbody tr td {
    color: #e2e8f0 !important;
}

/* Inputs & Selectors */
input, select, textarea {
    background-color: #1e293b !important;
    color: #f8fafc !important;
    border: 1px solid #334155 !important;
    border-radius: 8px !important;
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   BUTTONS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.stButton > button {
    background: linear-gradient(90deg, #2563eb, #1d4ed8) !important;
    color: white !important;
    font-weight: 700 !important;
    border-radius: 10px !important;
    padding: 10px 26px !important;
    border: none !important;
    box-shadow: 0 4px 14px rgba(0,0,0,0.3),
                0 0 10px rgba(37,99,235,0.4);
    transition: all 0.25s ease-in-out;
}
.stButton > button:hover {
    transform: translateY(-3px);
    background: linear-gradient(90deg, #1e40af, #1e3a8a) !important;
    box-shadow: 0 6px 18px rgba(0,0,0,0.4),
                0 0 18px rgba(37,99,235,0.6);
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   LINK STYLES
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
a {
    color: #60a5fa !important;
    text-decoration: none !important;
}
a:hover {
    color: #93c5fd !important;
    text-shadow: 0 0 6px rgba(96,165,250,0.6);
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   FILE UPLOADER + PROGRESS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.stFileUploader {
    border: 1px solid #334155 !important;
    background-color: #1e293b !important;
    border-radius: 10px !important;
    color: #e2e8f0 !important;
    box-shadow: inset 0 0 8px rgba(37,99,235,0.25);
}

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   SCROLLBARS (for aesthetics)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
::-webkit-scrollbar {
    width: 10px;
}
::-webkit-scrollbar-track {
    background: #1e293b;
}
::-webkit-scrollbar-thumb {
    background: #3b82f6;
    border-radius: 10px;
}
::-webkit-scrollbar-thumb:hover {
    background: #60a5fa;
}
</style>
""", unsafe_allow_html=True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üåü READABILITY & INPUT FIELD FIX PATCH

st.markdown("""
<style>
/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   FIX: Input Fields + Dropdowns Too Dark
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
input, select, textarea, .stTextInput, .stSelectbox, .stNumberInput {
    background-color: #1e293b !important;
    color: #ffffff !important;
    border: 1px solid #3b82f6 !important;
    border-radius: 8px !important;
    font-size: 18px !important;
    font-weight: 600 !important;
    padding: 8px 12px !important;
}

/* Dropdown menu items */
div[data-baseweb="select"] > div {
    background-color: #1e293b !important;
    color: #f8fafc !important;
    font-size: 18px !important;
}

/* Dropdown list options */
ul[role="listbox"] li {
    background-color: #1e293b !important;
    color: #f8fafc !important;
    font-size: 18px !important;
}
ul[role="listbox"] li:hover {
    background-color: #2563eb !important;
    color: white !important;
}

/* Sliders ‚Äî brighter and thicker */
[data-baseweb="slider"] div[role="slider"] {
    background-color: #3b82f6 !important;
}
[data-baseweb="slider"] div {
    height: 8px !important;
}
[data-testid="stSliderTickBar"] {
    background-color: #334155 !important;
}

/* Labels + Captions */
label, .stMarkdown p, .stCaption, .stText {
    font-size: 18px !important;
    color: #f1f5f9 !important;
    font-weight: 500 !important;
}

/* Subheaders and Section Titles */
h2, h3, h4, .stSubheader {
    font-size: 26px !important;
    color: #f8fafc !important;
    text-shadow: 0 0 8px rgba(59,130,246,0.4);
}

/* Decision Rule Section Styling */
[data-testid="stExpander"] > div:first-child {
    background-color: #111827 !important;
    color: #f8fafc !important;
    font-size: 20px !important;
    font-weight: 700 !important;
}

/* Tabs ‚Äî make them more visible and larger text */
div[data-baseweb="tab"] > button {
    font-size: 18px !important;
    padding: 10px 18px !important;
}

/* Global font size bump */
.stMarkdown, .stText, p, span, div {
    font-size: 18px !important;
}
</style>
""", unsafe_allow_html=True)
st.markdown("""
<style>
/* Brighten all radio + checkbox labels */
div[role="radio"], div[role="checkbox"] label, label[data-baseweb="radio"], label[data-baseweb="checkbox"] {
    color: #f8fafc !important;
    font-size: 18px !important;
    font-weight: 600 !important;
}

/* Fix small sub-labels near checkboxes */
div[role="radio"] p, div[role="checkbox"] p {
    color: #e2e8f0 !important;
    font-size: 16px !important;
}

/* Make rule mode label visible */
.stRadio label {
    color: #f1f5f9 !important;
    font-weight: 600 !important;
}

/* "Use LLM narrative" checkbox label */
[data-testid="stCheckbox"] label {
    color: #f8fafc !important;
    font-size: 18px !important;
    font-weight: 700 !important;
}
</style>
""", unsafe_allow_html=True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üîÅ Manage active tab navigation manually (INSERT HERE)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if "active_tab" not in st.session_state:
    st.session_state.active_tab = "tab_gen"  # or tab_train if you want to start from training

def switch_tab(tab_name: str):
    """Programmatically switch between workflow tabs."""
    st.session_state.active_tab = tab_name
    st.rerun()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GLOBAL CONFIG
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
BASE_DIR = os.path.expanduser("~/AI-AIGENTbythePeoplesANDBOX/HUGKAG/services/ui")
LANDING_IMG_DIR = os.path.join(BASE_DIR, "landing_images")
RUNS_DIR = os.path.join(BASE_DIR, ".runs")
TMP_FEEDBACK_DIR = os.path.join(BASE_DIR, ".tmp_feedback")

for d in (LANDING_IMG_DIR, RUNS_DIR, TMP_FEEDBACK_DIR):
    os.makedirs(d, exist_ok=True)

API_URL = os.getenv("API_URL", "http://localhost:8090")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SESSION STATE INIT
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if "stage" not in st.session_state:
    st.session_state.stage = "landing"
if "user_info" not in st.session_state:
    st.session_state.user_info = {"name": "", "email": "", "flagged": False}
if "logged_in" not in st.session_state:
    st.session_state.logged_in = False
if "flagged" not in st.session_state.user_info:
    st.session_state.user_info["flagged"] = False
if "timestamp" not in st.session_state.user_info:
    st.session_state.user_info["timestamp"] = datetime.datetime.utcnow().isoformat()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# PAGE CONFIG
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.set_page_config(
    page_title="AI Agent Sandbox ‚Äî By the People, For the People",
    layout="wide",
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# HELPERS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _clear_qp():
    """Clear query params (modern Streamlit API)."""
    try:
        st.query_params.clear()
    except Exception:
        pass


def load_image(base: str) -> Optional[str]:
    for ext in [".png", ".jpg", ".jpeg", ".webp", ".gif", ".svg"]:
        p = os.path.join(LANDING_IMG_DIR, f"{base}{ext}")
        if os.path.exists(p):
            return p
    return None


def save_uploaded_image(uploaded_file, base: str):
    if not uploaded_file:
        return None
    ext = os.path.splitext(uploaded_file.name)[1].lower() or ".png"
    dest = os.path.join(LANDING_IMG_DIR, f"{base}{ext}")
    with open(dest, "wb") as f:
        f.write(uploaded_file.getvalue())
    return dest


def render_image_tag(agent_id: str, industry: str, emoji_fallback: str) -> str:
    base = agent_id.lower().replace(" ", "_")
    img_path = load_image(base) or load_image(industry.replace(" ", "_"))
    if img_path:
        return (
            f'<img src="file://{img_path}" '
            f'style="width:48px;height:48px;border-radius:10px;object-fit:cover;">'
        )
    return f'<div style="font-size:32px;">{emoji_fallback}</div>'


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DATA
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
AGENTS = [
    ("üè¶ Banking & Finance", "üí∞ Retail Banking", "üí≥ Credit Appraisal Agent",
     "Explainable AI for loan decisioning", "Available", "üí≥"),
    ("üè¶ Banking & Finance", "üí∞ Retail Banking", "üè¶ Asset Appraisal Agent",
     "Market-driven collateral valuation", "Coming Soon", "üè¶"),
    ("üè¶ Banking & Finance", "ü©∫ Insurance", "ü©∫ Claims Triage Agent",
     "Automated claims prioritization", "Coming Soon", "ü©∫"),
    ("‚ö° Energy & Sustainability", "üîã EV & Charging", "‚ö° EV Charger Optimizer",
     "Optimize charger deployment via AI", "Coming Soon", "‚ö°"),
    ("‚ö° Energy & Sustainability", "‚òÄÔ∏è Solar", "‚òÄÔ∏è Solar Yield Estimator",
     "Estimate solar ROI and efficiency", "Coming Soon", "‚òÄÔ∏è"),
    ("üöó Automobile & Transport", "üöô Automobile", "üöó Predictive Maintenance",
     "Prevent downtime via sensor analytics", "Coming Soon", "üöó"),
    ("üöó Automobile & Transport", "üîã EV", "üîã EV Battery Health Agent",
     "Monitor EV battery health cycles", "Coming Soon", "üîã"),
    ("üöó Automobile & Transport", "üöö Ride-hailing / Logistics", "üõª Fleet Route Optimizer",
     "Dynamic route optimization for fleets", "Coming Soon", "üõª"),
    ("üíª Information Technology", "üß∞ Support & Security", "üß© IT Ticket Triage",
     "Auto-prioritize support tickets", "Coming Soon", "üß©"),
    ("üíª Information Technology", "üõ°Ô∏è Security", "üîê SecOps Log Triage",
     "Detect anomalies & summarize alerts", "Coming Soon", "üîê"),
    ("‚öñÔ∏è Legal & Government", "‚öñÔ∏è Law Firms", "‚öñÔ∏è Contract Analyzer",
     "Extract clauses and compliance risks", "Coming Soon", "‚öñÔ∏è"),
    ("‚öñÔ∏è Legal & Government", "üèõÔ∏è Public Services", "üèõÔ∏è Citizen Service Agent",
     "Smart assistant for citizen services", "Coming Soon", "üèõÔ∏è"),
    ("üõçÔ∏è Retail / SMB / Creative", "üè¨ Retail & eCommerce", "üìà Sales Forecast Agent",
     "Predict demand & inventory trends", "Coming Soon", "üìà"),
    ("üé¨ Retail / SMB / Creative", "üé® Media & Film", "üé¨ Budget Cost Assistant",
     "Estimate, optimize, and track film & production costs using AI", "Coming Soon", "üé¨"),
]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# STYLES
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.markdown(
    """
    <style>
    html, body, .block-container { background-color:#0f172a !important; color:#e2e8f0 !important; }
    footer { text-align:center; padding:2rem; color:#aab3c2; font-size:1.2rem; font-weight:600; margin-top:2rem; }
    .left-box {
        background: radial-gradient(circle at top left, #0f172a, #1e293b);
        border-radius:20px; padding:3rem 2rem; color:#f1f5f9; box-shadow:6px 0 24px rgba(0,0,0,0.4);
    }
    .right-box {
        background:linear-gradient(180deg,#1e293b,#0f172a);
        border-radius:20px; padding:2rem; box-shadow:-6px 0 24px rgba(0,0,0,0.35);
    }
    .stButton > button {
        border:none !important; cursor:pointer;
        padding:14px 28px !important; font-size:18px !important; font-weight:700 !important;
        border-radius:14px !important; color:#fff !important;
        background:linear-gradient(180deg,#4ea3ff 0%,#2f86ff 60%,#0f6fff 100%) !important;
        box-shadow:0 8px 24px rgba(15,111,255,0.35);
    }
    a.macbtn {
        display:inline-block; text-decoration:none !important; color:#fff !important;
        padding:10px 22px; font-weight:700; border-radius:12px;
        background:linear-gradient(180deg,#4ea3ff 0%,#2f86ff 60%,#0f6fff 100%);
    }
    /* Larger workflow tabs */
    [data-testid="stTabs"] [data-baseweb="tab"] {
        font-size: 28px !important;
        font-weight: 800 !important;
        padding: 20px 40px !important;
        border-radius: 12px !important;
        background-color: #1e293b !important;
        color: #f8fafc !important;
    }
    [data-testid="stTabs"] [data-baseweb="tab"][aria-selected="true"] {
        background: linear-gradient(90deg, #2563eb, #1d4ed8) !important;
        color: white !important;
        border-bottom: 6px solid #60a5fa !important;
        box-shadow: 0 4px 14px rgba(37,99,235,0.5);
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# QUERY PARAM ROUTING (modern API)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    qp = st.query_params
except Exception:
    qp = {}

if "stage" in qp:
    target = qp["stage"]
    if target in {"landing", "agents", "login", "credit_agent"} and st.session_state.stage != target:
        st.session_state.stage = target
        _clear_qp()
        st.rerun()

if "launch" in qp or ("agent" in qp and qp.get("agent") == ["credit"]):
    st.session_state.stage = "login"
    _clear_qp()
    st.rerun()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# STAGE: LANDING
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if st.session_state.stage == "landing":
    c1, c2 = st.columns([1.1, 1.9], gap="large")
    with c1:
        st.markdown("<div class='left-box'>", unsafe_allow_html=True)
        logo_path = load_image("people_logo")
        if logo_path:
            st.image(logo_path, width=160)
        else:
            up = st.file_uploader("Upload People Logo", type=["jpg", "png", "webp"], key="upload_logo")
            if up:
                save_uploaded_image(up, "people_logo")
                st.success("‚úÖ Logo uploaded, refresh to view.")
        st.markdown(
            """
            <h1>‚úä Let‚Äôs Build an AI by the People, for the People</h1>
            <h3>‚öôÔ∏è Ready-to-Use AI Agent Sandbox ‚Äî From Sandbox to Production</h3>
            <p>Build, test, and deploy AI agents using open-source explainable models.<br><br>
            <b>Privacy:</b> Synthetic & anonymized data only.<br>
            <b>Deployment:</b> GPU-as-a-Service Cloud, zero CAPEX.</p>
            """,
            unsafe_allow_html=True,
        )
        if st.button("üöÄ Start Building Now", key="btn_start_build_now"):
            st.session_state.stage = "agents"
            st.rerun()
        st.markdown("</div>", unsafe_allow_html=True)
    with c2:
        st.markdown("<div class='right-box'>", unsafe_allow_html=True)
        st.markdown("<h2>üìä Global AI Agent Library</h2>", unsafe_allow_html=True)
        rows = []
        for sector, industry, agent, desc, status, emoji in AGENTS:
            rows.append({
                "üñºÔ∏è": render_image_tag(agent, industry, emoji),
                "üè≠ Sector": sector,
                "üß© Industry": industry,
                "ü§ñ Agent": agent,
                "üß† Description": desc,
                "üì∂ Status": f'<span style="color:{"#22c55e" if status=="Available" else "#f59e0b"};">{status}</span>'
            })
        st.write(pd.DataFrame(rows).to_html(escape=False, index=False), unsafe_allow_html=True)
        st.markdown("</div>", unsafe_allow_html=True)
    st.markdown("<footer>Made with ‚ù§Ô∏è by DzoanNguyenTran@gmail.com ‚Äî Open AIgents Sandbox Initiative</footer>", unsafe_allow_html=True)
    st.stop()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# STAGE: AGENTS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if st.session_state.stage == "agents":
    top = st.columns([1, 4, 1])
    with top[0]:
        if st.button("‚¨ÖÔ∏è Back to Home", key="btn_back_home_from_agents"):
            st.session_state.stage = "landing"
            st.rerun()
    with top[1]:
        st.title("ü§ñ Available AI Agents")

    df = pd.DataFrame([
        {"Agent": "üí≥ Credit Appraisal Agent",
         "Description": "Explainable AI for retail loan decisioning",
         "Status": "‚úÖ Available",
         "Action": '<a class="macbtn" href="?agent=credit&stage=login">üöÄ Launch</a>'},
        {"Agent": "üè¶ Asset Appraisal Agent",
         "Description": "Market-driven collateral valuation",
         "Status": "üïì Coming Soon", "Action": "‚Äî"},
    ])
    st.write(df.to_html(escape=False, index=False), unsafe_allow_html=True)
    st.markdown("<footer>Made with ‚ù§Ô∏è by DzoanNguyenTran@gmail.com ‚Äî Open AIgents Sandbox Initiative</footer>", unsafe_allow_html=True)
    st.stop()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# STAGE: LOGIN
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if st.session_state.stage == "login":
    top = st.columns([1, 4, 1])
    with top[0]:
        if st.button("‚¨ÖÔ∏è Back to Agents", key="btn_back_agents_from_login"):
            st.session_state.stage = "agents"
            st.rerun()
    with top[1]:
        st.title("üîê Login to AI Credit Appraisal Platform")
    c1, c2, c3 = st.columns([1, 1, 1])
    with c1:
        user = st.text_input("Username", placeholder="e.g. dzoan")
    with c2:
        email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
    with c3:
        pwd = st.text_input("Password", type="password", placeholder="Enter any password")
    if st.button("Login", key="btn_login_submit", use_container_width=True):
        if user.strip() and email.strip():
            st.session_state.user_info = {
                "name": user.strip(),
                "email": email.strip(),
                "flagged": False,
                "timestamp": datetime.datetime.utcnow().isoformat()
            }
            st.session_state.logged_in = True
            st.session_state.stage = "credit_agent"
            st.rerun()
        else:
            st.error("‚ö†Ô∏è Please fill all fields before continuing.")
    st.markdown("<footer>Made with ‚ù§Ô∏è by DzoanNguyenTran@gmail.com ‚Äî Open AIgents Sandbox Initiative</footer>", unsafe_allow_html=True)
    st.stop()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# STAGE: CREDIT WORKFLOW
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if st.session_state.stage == "credit_agent":
    top = st.columns([1, 4, 1])
    with top[0]:
        if st.button("‚¨ÖÔ∏è Back to Agents", key="btn_back_agents_from_pipeline"):
            st.session_state.stage = "agents"
            st.rerun()
    with top[1]:
        st.title("üí≥ AI Credit Appraisal Platform")
        st.caption("Generate, sanitize, and appraise credit with AI agent power and human insight.")




# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GLOBAL UTILS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


BANNED_NAMES = {"race", "gender", "religion", "ethnicity", "ssn", "national_id"}
PII_COLS = {"customer_name", "name", "email", "phone", "address", "ssn", "national_id", "dob"}

EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
PHONE_RE = re.compile(r"\+?\d[\d\-\s]{6,}\d")

def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:
    return df.loc[:, ~df.columns.duplicated(keep="last")]

def scrub_text_pii(s):
    if not isinstance(s, str):
        return s
    s = EMAIL_RE.sub("", s)
    s = PHONE_RE.sub("", s)
    return s.strip()

def drop_pii_columns(df: pd.DataFrame):
    original_cols = list(df.columns)
    keep_cols = [c for c in original_cols if all(k not in c.lower() for k in PII_COLS)]
    dropped = [c for c in original_cols if c not in keep_cols]
    out = df[keep_cols].copy()
    for c in out.select_dtypes(include="object"):
        out[c] = out[c].apply(scrub_text_pii)
    return dedupe_columns(out), dropped

def strip_policy_banned(df: pd.DataFrame) -> pd.DataFrame:
    keep = []
    for c in df.columns:
        cl = c.lower()
        if cl in BANNED_NAMES:
            continue
        keep.append(c)
    return df[keep]

def append_user_info(df: pd.DataFrame) -> pd.DataFrame:
    meta = st.session_state["user_info"]
    out = df.copy()
    out["session_user_name"] = meta["name"]
    out["session_user_email"] = meta["email"]
    out["session_flagged"] = meta["flagged"]
    out["created_at"] = meta["timestamp"]
    return dedupe_columns(out)

def save_to_runs(df: pd.DataFrame, prefix: str) -> str:
    ts = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
    flag_suffix = "_FLAGGED" if st.session_state["user_info"]["flagged"] else ""
    fname = f"{prefix}_{ts}{flag_suffix}.csv"
    fpath = os.path.join(RUNS_DIR, fname)
    dedupe_columns(df).to_csv(fpath, index=False)
    return fpath

def try_json(x):
    if isinstance(x, (dict, list)):
        return x
    if not isinstance(x, str):
        return None
    try:
        return json.loads(x)
    except Exception:
        return None

def _safe_json(x):
    if isinstance(x, dict):
        return x
    if isinstance(x, str) and x.strip():
        try:
            return json.loads(x)
        except Exception:
            return {}
    return {}

def fmt_currency_label(base: str) -> str:
    sym = st.session_state.get("currency_symbol", "")
    return f"{base} ({sym})" if sym else base

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CURRENCY CATALOG

CURRENCY_OPTIONS = {
    # code: (label, symbol, fx to apply on USD-like base generated numbers)
    "USD": ("USD $", "$", 1.0),
    "EUR": ("EUR ‚Ç¨", "‚Ç¨", 0.93),
    "GBP": ("GBP ¬£", "¬£", 0.80),
    "JPY": ("JPY ¬•", "¬•", 150.0),
    "VND": ("VND ‚Ç´", "‚Ç´", 24000.0),
}

def set_currency_defaults():
    if "currency_code" not in st.session_state:
        st.session_state["currency_code"] = "USD"
    label, symbol, fx = CURRENCY_OPTIONS[st.session_state["currency_code"]]
    st.session_state["currency_label"] = label
    st.session_state["currency_symbol"] = symbol
    st.session_state["currency_fx"] = fx

set_currency_defaults()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DASHBOARD HELPERS (Plotly, dark theme)

def _kpi_card(label: str, value: str, sublabel: str | None = None):
    st.markdown(
        f"""
        <div style="background:#0e1117;border:1px solid #2a2f3e;border-radius:12px;padding:14px 16px;margin-bottom:10px;">
          <div style="font-size:12px;color:#9aa4b2;text-transform:uppercase;letter-spacing:.06em;">{label}</div>
          <div style="font-size:28px;font-weight:700;color:#e6edf3;line-height:1.1;margin-top:2px;">{value}</div>
          {f'<div style="font-size:12px;color:#9aa4b2;margin-top:6px;">{sublabel}</div>' if sublabel else ''}
        </div>
        """,
        unsafe_allow_html=True,
    )

def render_credit_dashboard(df: pd.DataFrame, currency_symbol: str = ""):
    """
    Renders the whole dashboard (TOP-10s ‚Üí Opportunities ‚Üí KPIs & pies/bars ‚Üí Mix table).
    Keeps decision filter in the table only.
    """
    if df is None or df.empty:
        st.info("No data to visualize yet.")
        return

    cols = df.columns

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TOP 10s FIRST ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üîù Top 10 Snapshot")

    # Top 10 loans approved
    if {"decision", "loan_amount", "application_id"} <= set(cols):
        top_approved = df[df["decision"].astype(str).str.lower() == "approved"].copy()
        if not top_approved.empty:
            top_approved = top_approved.sort_values("loan_amount", ascending=False).head(10)
            fig = px.bar(
                top_approved,
                x="loan_amount",
                y="application_id",
                orientation="h",
                title="Top 10 Approved Loans",
                labels={"loan_amount": f"Loan Amount {currency_symbol}", "application_id": "Application"},
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No approved loans available to show top 10.")

    # Top 10 collateral types by average value
    if {"collateral_type", "collateral_value"} <= set(cols):
        cprof = df.groupby("collateral_type", dropna=False).agg(
            avg_value=("collateral_value", "mean"),
            cnt=("collateral_type", "count")
        ).reset_index()
        if not cprof.empty:
            cprof = cprof.sort_values("avg_value", ascending=False).head(10)
            fig = px.bar(
                cprof,
                x="avg_value",
                y="collateral_type",
                orientation="h",
                title="Top 10 Collateral Types (Avg Value)",
                labels={"avg_value": f"Avg Value {currency_symbol}", "collateral_type": "Collateral Type"},
                hover_data=["cnt"]
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    # Top 10 reasons for denial (from rule_reasons False flags)
    if "rule_reasons" in cols and "decision" in cols:
        denied = df[df["decision"].astype(str).str.lower() == "denied"].copy()
        reasons_count = {}
        for _, r in denied.iterrows():
            rr = _safe_json(r.get("rule_reasons"))
            if isinstance(rr, dict):
                for k, v in rr.items():
                    if v is False:
                        reasons_count[k] = reasons_count.get(k, 0) + 1
        if reasons_count:
            items = pd.DataFrame(sorted(reasons_count.items(), key=lambda x: x[1], reverse=True),
                                 columns=["reason", "count"]).head(10)
            fig = px.bar(
                items, x="count", y="reason", orientation="h",
                title="Top 10 Reasons for Denial",
                labels={"count": "Count", "reason": "Rule"},
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No denial reasons detected.")

    # Top 10 loan officer performance (approval rate) if officer column present
    officer_col = None
    for guess in ("loan_officer", "officer", "reviewed_by", "session_user_name"):
        if guess in cols:
            officer_col = guess
            break
    if officer_col and "decision" in cols:
        perf = (
            df.assign(is_approved=(df["decision"].astype(str).str.lower() == "approved").astype(int))
              .groupby(officer_col, dropna=False)["is_approved"]
              .agg(approved_rate="mean", n="count")
              .reset_index()
        )
        if not perf.empty:
            perf["approved_rate_pct"] = (perf["approved_rate"] * 100).round(1)
            perf = perf.sort_values(["approved_rate_pct", "n"], ascending=[False, False]).head(10)
            fig = px.bar(
                perf, x="approved_rate_pct", y=officer_col, orientation="h",
                title="Top 10 Loan Officer Approval Rate (this batch)",
                labels={"approved_rate_pct": "Approval Rate (%)", officer_col: "Officer"},
                hover_data=["n"]
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ OPPORTUNITIES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üí° Opportunities")

    # Short-term loan opportunities (simple heuristic)
    opp_rows = []
    if {"income", "loan_amount"}.issubset(cols):
        term_col = "loan_term_months" if "loan_term_months" in cols else ("loan_duration_months" if "loan_duration_months" in cols else None)
        if term_col:
            for _, r in df.iterrows():
                inc = float(r.get("income", 0) or 0)
                amt = float(r.get("loan_amount", 0) or 0)
                term = int(r.get(term_col, 0) or 0)
                dti = float(r.get("DTI", 0) or 0)
                if (term >= 36) and (amt <= inc * 0.8) and (dti <= 0.45):
                    opp_rows.append({
                        "application_id": r.get("application_id"),
                        "suggested_term": 24,
                        "loan_amount": amt,
                        "income": inc,
                        "DTI": dti,
                        "note": "Candidate for short-term plan (<=24m) based on affordability."
                    })
    if opp_rows:
        st.markdown("#### üìé Short-Term Loan Candidates")
        st.dataframe(pd.DataFrame(opp_rows).head(25), use_container_width=True, height=320)
    else:
        st.info("No short-term loan candidates identified in this batch.")

    st.markdown("#### üîÅ Buyback / Consolidation Beneficiaries")
    candidates = []
    need = {"decision", "existing_debt", "loan_amount", "DTI"}
    if need <= set(cols):
        for _, r in df.iterrows():
            dec = str(r.get("decision", "")).lower()
            debt = float(r.get("existing_debt", 0) or 0)
            loan = float(r.get("loan_amount", 0) or 0)
            dti = float(r.get("DTI", 0) or 0)
            proposal = _safe_json(r.get("proposed_consolidation_loan", {}))
            has_bb = bool(proposal)

            if dec == "denied" or dti > 0.45 or debt > loan:
                benefit_score = round((debt / (loan + 1e-6)) * 0.4 + dti * 0.6, 2)
                candidates.append({
                    "application_id": r.get("application_id"),
                    "customer_type": r.get("customer_type"),
                    "existing_debt": debt,
                    "loan_amount": loan,
                    "DTI": dti,
                    "collateral_type": r.get("collateral_type"),
                    "buyback_proposed": has_bb,
                    "buyback_amount": proposal.get("buyback_amount") if has_bb else None,
                    "benefit_score": benefit_score,
                    "note": proposal.get("note") if has_bb else None
                })
    if candidates:
        cand_df = pd.DataFrame(candidates).sort_values("benefit_score", ascending=False)
        st.dataframe(cand_df.head(25), use_container_width=True, height=380)
    else:
        st.info("No additional buyback beneficiaries identified.")

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PORTFOLIO KPIs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üìà Portfolio Snapshot")
    c1, c2, c3, c4 = st.columns(4)

    # Approval rate
    if "decision" in cols:
        total = len(df)
        approved = int((df["decision"].astype(str).str.lower() == "approved").sum())
        rate = (approved / total * 100) if total else 0.0
        with c1: _kpi_card("Approval Rate", f"{rate:.1f}%", f"{approved} of {total}")

    # Avg approved loan amount
    if {"decision", "loan_amount"} <= set(cols):
        ap = df[df["decision"].astype(str).str.lower() == "approved"]["loan_amount"]
        avg_amt = ap.mean() if len(ap) else 0.0
        with c2: _kpi_card("Avg Approved Amount", f"{currency_symbol}{avg_amt:,.0f}")

    # Decision time (if present)
    if {"created_at", "decision_at"} <= set(cols):
        try:
            t = (pd.to_datetime(df["decision_at"]) - pd.to_datetime(df["created_at"])).dt.total_seconds() / 60.0
            avg_min = float(t.mean())
            with c3: _kpi_card("Avg Decision Time", f"{avg_min:.1f} min")
        except Exception:
            with c3: _kpi_card("Avg Decision Time", "‚Äî")

    # Non-bank share
    if "customer_type" in cols:
        nb = int((df["customer_type"].astype(str).str.lower() == "non-bank").sum())
        total = len(df)
        share = (nb / total * 100) if total else 0.0
        with c4: _kpi_card("Non-bank Share", f"{share:.1f}%", f"{nb} of {total}")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ COMPOSITION & RISK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üß≠ Composition & Risk")

    # Approval vs Denial (pie)
    if "decision" in cols:
        pie_df = df["decision"].value_counts().rename_axis("Decision").reset_index(name="Count")
        fig = px.pie(pie_df, names="Decision", values="Count", title="Decision Mix")
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Avg DTI / LTV by decision (grouped bars)
    have_dti = "DTI" in cols
    have_ltv = "LTV" in cols
    if "decision" in cols and (have_dti or have_ltv):
        agg_map = {}
        if have_dti: agg_map["avg_DTI"] = ("DTI", "mean")
        if have_ltv: agg_map["avg_LTV"] = ("LTV", "mean")
        grp = df.groupby("decision").agg(**agg_map).reset_index()
        melted = grp.melt(id_vars=["decision"], var_name="metric", value_name="value")
        fig = px.bar(melted, x="decision", y="value", color="metric",
                     barmode="group", title="Average DTI / LTV by Decision")
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Loan term mix (stacked)
    term_col = "loan_term_months" if "loan_term_months" in cols else ("loan_duration_months" if "loan_duration_months" in cols else None)
    if term_col and "decision" in cols:
        mix = df.groupby([term_col, "decision"]).size().reset_index(name="count")
        fig = px.bar(
            mix, x=term_col, y="count", color="decision", title="Loan Term Mix",
            labels={term_col: "Term (months)", "count": "Count"}, barmode="stack"
        )
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Collateral avg value by type (bar)
    if {"collateral_type", "collateral_value"} <= set(cols):
        cprof = df.groupby("collateral_type").agg(
            avg_col=("collateral_value", "mean"),
            cnt=("collateral_type", "count")
        ).reset_index()
        fig = px.bar(
            cprof.sort_values("avg_col", ascending=False),
            x="collateral_type", y="avg_col",
            title=f"Avg Collateral Value by Type ({currency_symbol})",
            hover_data=["cnt"]
        )
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Top proposed plans (horizontal bar)
    if "proposed_loan_option" in cols:
        plans = df["proposed_loan_option"].dropna().astype(str)
        if len(plans) > 0:
            plan_types = []
            for s in plans:
                p = _safe_json(s)
                plan_types.append(p.get("type") if isinstance(p, dict) and "type" in p else s)
            plan_df = pd.Series(plan_types).value_counts().head(10).rename_axis("plan").reset_index(name="count")
            fig = px.bar(
                plan_df, x="count", y="plan", orientation="h",
                title="Top 10 Proposed Plans"
            )
            fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    # Customer mix table (bank vs non-bank)
    if "customer_type" in cols:
        mix = df["customer_type"].value_counts().rename_axis("Customer Type").reset_index(name="Count")
        mix["Ratio"] = (mix["Count"] / mix["Count"].sum()).round(3)
        st.markdown("### üë• Customer Mix")
        st.dataframe(mix, use_container_width=True, height=220)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# TABS
tab_gen, tab_clean, tab_run, tab_review, tab_train = st.tabs([
    "üè¶ Synthetic Data Generator",
    "üßπ Anonymize & Sanitize Data",
    "ü§ñ Credit appraisal by AI assistant",
    "üßë‚Äç‚öñÔ∏è Human Review",
    "üîÅ Training (Feedback ‚Üí Retrain)"
])

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DATA GENERATORS

def generate_raw_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    names = ["Alice Nguyen","Bao Tran","Chris Do","Duy Le","Emma Tran",
             "Felix Nguyen","Giang Ho","Hanh Vo","Ivan Pham","Julia Ngo"]
    emails = [f"{n.split()[0].lower()}.{n.split()[1].lower()}@gmail.com" for n in names]
    addrs = [
        "23 Elm St, Boston, MA","19 Pine Ave, San Jose, CA","14 High St, London, UK",
        "55 Nguyen Hue, Ho Chi Minh","78 Oak St, Chicago, IL","10 Broadway, New York, NY",
        "8 Rue Lafayette, Paris, FR","21 K√∂nigstr, Berlin, DE","44 Maple Dr, Los Angeles, CA","22 Bay St, Toronto, CA"
    ]
    is_non = rng.random(n) < non_bank_ratio
    cust_type = np.where(is_non, "non-bank", "bank")

    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "customer_name": np.random.choice(names, n),
        "email": np.random.choice(emails, n),
        "phone": [f"+1-202-555-{1000+i:04d}" for i in range(n)],
        "address": np.random.choice(addrs, n),
        "national_id": rng.integers(10_000_000, 99_999_999, n),
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["real_estate","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
        "current_loans": rng.integers(0, 5, n),
        "customer_type": cust_type,
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))

    fx = st.session_state["currency_fx"]
    for c in ("income", "loan_amount", "collateral_value", "assets_owned", "existing_debt"):
        df[c] = (df[c] * fx).round(2)
    df["currency_code"] = st.session_state["currency_code"]
    return dedupe_columns(df)

def generate_anon_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    is_non = rng.random(n) < non_bank_ratio
    cust_type = np.where(is_non, "non-bank", "bank")

    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["real_estate","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
        "current_loans": rng.integers(0, 5, n),
        "customer_type": cust_type,
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))

    fx = st.session_state["currency_fx"]
    for c in ("income", "loan_amount", "collateral_value", "assets_owned", "existing_debt"):
        df[c] = (df[c] * fx).round(2)
    df["currency_code"] = st.session_state["currency_code"]
    return dedupe_columns(df)

def to_agent_schema(df: pd.DataFrame) -> pd.DataFrame:
    """
    Harmonize to the server-side agent‚Äôs expected schema.
    """
    out = df.copy()
    n = len(out)
    if "employment_years" not in out.columns:
        out["employment_years"] = out.get("employment_length", 0)
    if "debt_to_income" not in out.columns:
        if "DTI" in out.columns:
            out["debt_to_income"] = out["DTI"].astype(float)
        elif "existing_debt" in out.columns and "income" in out.columns:
            denom = out["income"].replace(0, np.nan)
            dti = (out["existing_debt"] / denom).fillna(0.0)
            out["debt_to_income"] = dti.clip(0, 10)
        else:
            out["debt_to_income"] = 0.0
    rng = np.random.default_rng(12345)
    if "credit_history_length" not in out.columns:
        out["credit_history_length"] = rng.integers(0, 30, n)
    if "num_delinquencies" not in out.columns:
        out["num_delinquencies"] = np.minimum(rng.poisson(0.2, n), 10)
    if "requested_amount" not in out.columns:
        out["requested_amount"] = out.get("loan_amount", 0)
    if "loan_term_months" not in out.columns:
        out["loan_term_months"] = out.get("loan_duration_months", 0)
    return dedupe_columns(out)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üè¶ TAB 1 ‚Äî Synthetic Data Generator
with tab_gen:
    st.subheader("üè¶ Synthetic Credit Data Generator")

    # Currency selector (before generation)
    c1, c2 = st.columns([1, 2])
    with c1:
        code = st.selectbox(
            "Currency",
            list(CURRENCY_OPTIONS.keys()),
            index=list(CURRENCY_OPTIONS.keys()).index(st.session_state["currency_code"]),
            help="All monetary fields will be in this local currency."
        )
        if code != st.session_state["currency_code"]:
            st.session_state["currency_code"] = code
            set_currency_defaults()
    with c2:
        st.info(f"Amounts will be generated in **{st.session_state['currency_label']}**.", icon="üí∞")

    rows = st.slider("Number of rows to generate", 50, 2000, 200, step=50)
    non_bank_ratio = st.slider("Share of non-bank customers", 0.0, 1.0, 0.30, 0.05)

    colA, colB = st.columns(2)
    with colA:
        if st.button("üî¥ Generate RAW Synthetic Data (with PII)", use_container_width=True):
            raw_df = append_user_info(generate_raw_synthetic(rows, non_bank_ratio))
            st.session_state.synthetic_raw_df = raw_df
            raw_path = save_to_runs(raw_df, "synthetic_raw")
            st.success(f"Generated RAW (PII) dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {raw_path}")
            st.dataframe(raw_df.head(10), use_container_width=True)
            st.download_button(
                "‚¨áÔ∏è Download RAW CSV",
                raw_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(raw_path),
                "text/csv"
            )

    with colB:
        if st.button("üü¢ Generate ANON Synthetic Data (ready for agent)", use_container_width=True):
            anon_df = append_user_info(generate_anon_synthetic(rows, non_bank_ratio))
            st.session_state.synthetic_df = anon_df
            anon_path = save_to_runs(anon_df, "synthetic_anon")
            st.success(f"Generated ANON dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {anon_path}")
            st.dataframe(anon_df.head(10), use_container_width=True)
            st.download_button(
                "‚¨áÔ∏è Download ANON CSV",
                anon_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(anon_path),
                "text/csv"
            )

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üßπ TAB 2 ‚Äî Anonymize & Sanitize Data
with tab_clean:
    st.subheader("üßπ Upload & Anonymize Customer Data (PII columns will be DROPPED)")
    st.markdown("Upload your **real CSV**. We drop PII columns and scrub emails/phones in text fields.")

    uploaded = st.file_uploader("Upload CSV file", type=["csv"])
    if uploaded:
        try:
            df = pd.read_csv(uploaded)
        except Exception as e:
            st.error(f"Could not read CSV: {e}")
            st.stop()

        st.write("üìä Original Data Preview:")
        st.dataframe(dedupe_columns(df.head(5)), use_container_width=True)

        sanitized, dropped_cols = drop_pii_columns(df)
        sanitized = append_user_info(sanitized)
        sanitized = dedupe_columns(sanitized)
        st.session_state.anonymized_df = sanitized

        st.success(f"Dropped PII columns: {sorted(dropped_cols) if dropped_cols else 'None'}")
        st.write("‚úÖ Sanitized Data Preview:")
        st.dataframe(sanitized.head(5), use_container_width=True)

        fpath = save_to_runs(sanitized, "anonymized")
        st.success(f"Saved anonymized file: {fpath}")
        st.download_button(
            "‚¨áÔ∏è Download Clean Data",
            sanitized.to_csv(index=False).encode("utf-8"),
            os.path.basename(fpath),
            "text/csv"
        )
    else:
        st.info("Choose a CSV to see the sanitize flow.", icon="‚ÑπÔ∏è")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ü§ñ TAB 3 ‚Äî Credit appraisal by AI assistant
with tab_run:
    st.subheader("ü§ñ Credit appraisal by AI assistant")
    # Anchor for loopback link from Training tab
    st.markdown('<a name="credit-appraisal-stage"></a>', unsafe_allow_html=True)

    st.markdown("### üè† Collateral Asset Bridge (External Agent)")
    st.markdown(
        f"Asset appraisal runs from a separate repo ‚Äî [open agent repo]({ASSET_AGENT_REPO_URL})."
    )

    if "asset_bridge_info" not in st.session_state:
        st.session_state["asset_bridge_info"] = None
    if "asset_bridge_preview" not in st.session_state:
        st.session_state["asset_bridge_preview"] = []
    if "asset_join_key" not in st.session_state:
        st.session_state["asset_join_key"] = "application_id"

    def _set_asset_bridge(payload: Dict[str, Any], join_key: str) -> None:
        st.session_state["asset_bridge_info"] = payload
        st.session_state["asset_bridge_preview"] = payload.get("preview", [])
        st.session_state["asset_join_key"] = join_key

    with st.expander("üì• Upload Asset Appraisal Results (CSV)", expanded=False):
        join_key_input = st.text_input(
            "Join column (matches credit dataset column)",
            value=st.session_state.get("asset_join_key", "application_id"),
            help="This column must be present in both the asset export and the credit dataset.",
        )
        upload_file = st.file_uploader(
            "Asset appraisal export (.csv)",
            type=["csv"],
            key="asset_bridge_file",
        )
        colA, colB, colC = st.columns([1, 1, 1])
        with colA:
            if st.button("Upload asset CSV", use_container_width=True):
                if not upload_file:
                    st.warning("Select a CSV file first.")
                else:
                    try:
                        resp = requests.post(
                            f"{API_URL}/v1/asset-bridge/upload",
                            files={"file": (upload_file.name, upload_file.getvalue(), "text/csv")},
                            timeout=30,
                        )
                        resp.raise_for_status()
                        payload = resp.json()
                        _set_asset_bridge(payload, join_key_input or "application_id")
                        st.success("Asset collateral export uploaded.")
                    except Exception as exc:
                        st.error(f"Failed to upload asset export: {exc}")
        with colB:
            if st.button("Use sample asset export", use_container_width=True):
                if not os.path.exists(ASSET_SAMPLE_PATH):
                    st.error("Sample asset appraisal export not found.")
                else:
                    try:
                        with open(ASSET_SAMPLE_PATH, "rb") as f:
                            sample_bytes = f.read()
                        resp = requests.post(
                            f"{API_URL}/v1/asset-bridge/upload",
                            files={"file": ("sample_asset_appraisals.csv", sample_bytes, "text/csv")},
                            timeout=30,
                        )
                        resp.raise_for_status()
                        payload = resp.json()
                        _set_asset_bridge(payload, join_key_input or "application_id")
                        st.success("Loaded sample asset appraisal export.")
                    except Exception as exc:
                        st.error(f"Failed to load sample asset export: {exc}")
        with colC:
            if st.button("Clear asset bridge", use_container_width=True):
                st.session_state["asset_bridge_info"] = None
                st.session_state["asset_bridge_preview"] = []
                st.success("Cleared asset bridge context.")

    asset_bridge_info = st.session_state.get("asset_bridge_info")
    if asset_bridge_info:
        st.info(
            f"Asset bridge ready ‚Äî ID `{asset_bridge_info.get('bridge_id')}` using join column `{st.session_state.get('asset_join_key')}`.",
            icon="üîó",
        )
        preview_records = st.session_state.get("asset_bridge_preview") or []
        if preview_records:
            st.caption("Preview of uploaded collateral dataset (first 10 rows).")
            st.dataframe(pd.DataFrame(preview_records), use_container_width=True)
    else:
        st.caption(
            "Upload the collateral CSV generated by the asset appraisal agent. Decisions like `denied_fraud` will auto-adjust the credit appraisal."
        )

    # Production model banner (optional)
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.status_code == 200:
            meta = resp.json()
            if meta.get("has_production"):
                ver = (meta.get("meta") or {}).get("version", "1.x")
                src = (meta.get("meta") or {}).get("source", "production")
                st.success(f"üü¢ Production model active ‚Äî version: {ver} ‚Ä¢ source: {src}")
            else:
                st.warning("‚ö†Ô∏è No production model promoted yet ‚Äî using baseline.")
        else:
            st.info("‚ÑπÔ∏è Could not fetch production model meta.")
    except Exception:
        st.info("‚ÑπÔ∏è Production meta unavailable.")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß© Model Selection (list all trained models)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    trained_dir = os.path.expanduser(
     
        "~/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/trained"
        
    )
    models = []
    if os.path.exists(trained_dir):
        for f in os.listdir(trained_dir):
            if f.endswith(".joblib"):
                fpath = os.path.join(trained_dir, f)
                ctime = os.path.getctime(fpath)
                created = datetime.datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
                models.append((f, fpath, created))

    if models:
        models.sort(key=lambda x: x[2], reverse=True)
        display_names = [f"{m[0]} ‚Äî {m[2]}" for m in models]

        selected_display = st.selectbox("üì¶ Select trained model to use", display_names)
        selected_model = models[display_names.index(selected_display)][1]
        st.success(f"‚úÖ Using model: {os.path.basename(selected_model)}")

        # Store for later use by /run API
        st.session_state["selected_trained_model"] = selected_model

        # Optional promote button
        if st.button("üöÄ Promote this model to Production"):
            try:
                prod_path = os.path.expanduser(
                    "~/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/production/model.joblib"
                )
                os.makedirs(os.path.dirname(prod_path), exist_ok=True)
                import shutil
                shutil.copy2(selected_model, prod_path)
                st.success(f"‚úÖ Model promoted to production: {os.path.basename(prod_path)}")
            except Exception as e:
                st.error(f"‚ùå Promotion failed: {e}")
    else:
        st.warning("‚ö†Ô∏è No trained models found ‚Äî train one in Step 5 first.")

    
    
    
    # 1) Model + Hardware selection (UI hints)
    LLM_MODELS = [
        ("Phi-3 Mini (3.8B) ‚Äî CPU OK", "phi3:3.8b", "CPU 8GB RAM (fast)"),
        ("Mistral 7B Instruct ‚Äî CPU slow / GPU OK", "mistral:7b-instruct", "CPU 16GB (slow) or GPU ‚â•8GB"),
        ("Gemma-2 7B ‚Äî CPU slow / GPU OK", "gemma2:7b", "CPU 16GB (slow) or GPU ‚â•8GB"),
        ("LLaMA-3 8B ‚Äî GPU recommended", "llama3:8b-instruct", "GPU ‚â•12GB (CPU very slow)"),
        ("Qwen2 7B ‚Äî GPU recommended", "qwen2:7b-instruct", "GPU ‚â•12GB (CPU very slow)"),
        ("Mixtral 8x7B ‚Äî GPU only (big)", "mixtral:8x7b-instruct", "GPU 24‚Äì48GB"),
    ]
    LLM_LABELS = [l for (l, _, _) in LLM_MODELS]
    LLM_VALUE_BY_LABEL = {l: v for (l, v, _) in LLM_MODELS}
    LLM_HINT_BY_LABEL = {l: h for (l, _, h) in LLM_MODELS}

    OPENSTACK_FLAVORS = {
        "m4.medium":  "4 vCPU / 8 GB RAM ‚Äî CPU-only small",
        "m8.large":   "8 vCPU / 16 GB RAM ‚Äî CPU-only medium",
        "g1.a10.1":   "8 vCPU / 32 GB RAM + 1√óA10 24GB",
        "g1.l40.1":   "16 vCPU / 64 GB RAM + 1√óL40 48GB",
        "g2.a100.1":  "24 vCPU / 128 GB RAM + 1√óA100 80GB",
    }

    with st.expander("üß† Local LLM & Hardware Profile", expanded=True):
        c1, c2 = st.columns([1.2, 1])
        with c1:
            model_label = st.selectbox("Local LLM (used for narratives/explanations)", LLM_LABELS, index=1)
            llm_value = LLM_VALUE_BY_LABEL[model_label]
            st.caption(f"Hint: {LLM_HINT_BY_LABEL[model_label]}")
        with c2:
            flavor = st.selectbox("OpenStack flavor / host profile", list(OPENSTACK_FLAVORS.keys()), index=0)
            st.caption(OPENSTACK_FLAVORS[flavor])
        st.caption("These are passed to the API as hints; your API can choose Ollama/Flowise backends accordingly.")

    # 2) Data Source
    data_choice = st.selectbox(
        "Select Data Source",
        [
            "Use synthetic (ANON)",
            "Use synthetic (RAW ‚Äì auto-sanitize)",
            "Use anonymized dataset",
            "Upload manually",
        ]
    )
    use_llm = st.checkbox("Use LLM narrative", value=False)
    agent_name = "credit_appraisal"

    if data_choice == "Upload manually":
        up = st.file_uploader("Upload your CSV", type=["csv"], key="manual_upload_run_file")
        if up is not None:
            st.session_state["manual_upload_name"] = up.name
            st.session_state["manual_upload_bytes"] = up.getvalue()
            st.success(f"File staged: {up.name} ({len(st.session_state['manual_upload_bytes'])} bytes)")

    # 3) Rules
    st.markdown("### ‚öôÔ∏è Decision Rule Set")
    rule_mode = st.radio(
        "Choose rule mode",
        ["Classic (bank-style metrics)", "NDI (Net Disposable Income) ‚Äî simple"],
        index=0,
        help="NDI = income - all monthly obligations. Approve if NDI and NDI ratio pass thresholds."
    )

    CLASSIC_DEFAULTS = {
        "max_dti": 0.45, "min_emp_years": 2, "min_credit_hist": 3, "salary_floor": 3000,
        "max_delinquencies": 2, "max_current_loans": 3, "req_min": 1000, "req_max": 200000,
        "loan_terms": [12, 24, 36, 48, 60], "threshold": 0.45, "target_rate": None, "random_band": True,
        "min_income_debt_ratio": 0.35, "compounded_debt_factor": 1.0, "monthly_debt_relief": 0.50,
    }
    NDI_DEFAULTS = {"ndi_value": 800.0, "ndi_ratio": 0.50, "threshold": 0.45, "target_rate": None, "random_band": True}

    if "classic_rules" not in st.session_state:
        st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    if "ndi_rules" not in st.session_state:
        st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    def reset_classic(): st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    def reset_ndi():     st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    if rule_mode.startswith("Classic"):
        with st.expander("Classic Metrics (with Reset)", expanded=True):
            rc = st.session_state.classic_rules
            r1, r2, r3 = st.columns(3)
            with r1:
                rc["max_dti"] = st.slider("Max Debt-to-Income (DTI)", 0.0, 1.0, rc["max_dti"], 0.01)
                rc["min_emp_years"] = st.number_input("Min Employment Years", 0, 40, rc["min_emp_years"])
                rc["min_credit_hist"] = st.number_input("Min Credit History (years)", 0, 40, rc["min_credit_hist"])
            with r2:
                rc["salary_floor"] = st.number_input("Minimum Monthly Salary", 0, 1_000_000_000, rc["salary_floor"], step=1000, help=fmt_currency_label("in local currency"))
                rc["max_delinquencies"] = st.number_input("Max Delinquencies", 0, 10, rc["max_delinquencies"])
                rc["max_current_loans"] = st.number_input("Max Current Loans", 0, 10, rc["max_current_loans"])
            with r3:
                rc["req_min"] = st.number_input(fmt_currency_label("Requested Amount Min"), 0, 10_000_000_000, rc["req_min"], step=1000)
                rc["req_max"] = st.number_input(fmt_currency_label("Requested Amount Max"), 0, 10_000_000_000, rc["req_max"], step=1000)
                rc["loan_terms"] = st.multiselect("Allowed Loan Terms (months)", [12,24,36,48,60,72], default=rc["loan_terms"])

            st.markdown("#### üßÆ Debt Pressure Controls")
            d1, d2, d3 = st.columns(3)
            with d1:
                rc["min_income_debt_ratio"] = st.slider("Min Income / (Compounded Debt) Ratio", 0.10, 2.00, rc["min_income_debt_ratio"], 0.01)
            with d2:
                rc["compounded_debt_factor"] = st.slider("Compounded Debt Factor (√ó requested)", 0.5, 3.0, rc["compounded_debt_factor"], 0.1)
            with d3:
                rc["monthly_debt_relief"] = st.slider("Monthly Debt Relief Factor", 0.10, 1.00, rc["monthly_debt_relief"], 0.05)

            st.markdown("---")
            c1, c2, c3 = st.columns([1,1,1])
            with c1:
                use_target = st.toggle("üéØ Use target approval rate", value=(rc["target_rate"] is not None))
            with c2:
                rc["random_band"] = st.toggle("üé≤ Randomize approval band (20‚Äì60%) when no target", value=rc["random_band"])
            with c3:
                if st.button("‚Ü©Ô∏è Reset to defaults"):
                    reset_classic()
                    st.rerun()

            if use_target:
                rc["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rc["target_rate"] or 0.40, 0.01)
                rc["threshold"] = None
            else:
                rc["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rc["threshold"], 0.01)
                rc["target_rate"] = None
    else:
        with st.expander("NDI Metrics (with Reset)", expanded=True):
            rn = st.session_state.ndi_rules
            n1, n2 = st.columns(2)
            with n1:
                rn["ndi_value"] = st.number_input(fmt_currency_label("Min NDI (Net Disposable Income) per month"), 0.0, 1e12, float(rn["ndi_value"]), step=50.0)
            with n2:
                rn["ndi_ratio"] = st.slider("Min NDI / Income ratio", 0.0, 1.0, float(rn["ndi_ratio"]), 0.01)
            st.caption("NDI = income - all monthly obligations (rent, food, loans, cards, etc.).")

            st.markdown("---")
            c1, c2, c3 = st.columns([1,1,1])
            with c1:
                use_target = st.toggle("üéØ Use target approval rate", value=(rn["target_rate"] is not None))
            with c2:
                rn["random_band"] = st.toggle("üé≤ Randomize approval band (20‚Äì60%) when no target", value=rn["random_band"])
            with c3:
                if st.button("‚Ü©Ô∏è Reset to defaults (NDI)"):
                    reset_ndi()
                    st.rerun()

            if use_target:
                rn["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rn["target_rate"] or 0.40, 0.01)
                rn["threshold"] = None
            else:
                rn["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rn["threshold"], 0.01)
                rn["target_rate"] = None

    # 4) Run
    if st.button("üöÄ Run Agent", use_container_width=True):
        try:
            files = None
            data: Dict[str, Any] = {
                "use_llm_narrative": str(use_llm).lower(),
                "llm_model": llm_value,
                "hardware_flavor": flavor,
                "currency_code": st.session_state["currency_code"],
                "currency_symbol": st.session_state["currency_symbol"],
            }
            bridge_info = st.session_state.get("asset_bridge_info")
            if bridge_info:
                data["asset_bridge_id"] = bridge_info.get("bridge_id")
                data["asset_join_key"] = st.session_state.get("asset_join_key", "application_id")
            if rule_mode.startswith("Classic"):
                rc = st.session_state.classic_rules
                data.update({
                    "min_employment_years": str(rc["min_emp_years"]),
                    "max_debt_to_income": str(rc["max_dti"]),
                    "min_credit_history_length": str(rc["min_credit_hist"]),
                    "max_num_delinquencies": str(rc["max_delinquencies"]),
                    "max_current_loans": str(rc["max_current_loans"]),
                    "requested_amount_min": str(rc["req_min"]),
                    "requested_amount_max": str(rc["req_max"]),
                    "loan_term_months_allowed": ",".join(map(str, rc["loan_terms"])) if rc["loan_terms"] else "",
                    "min_income_debt_ratio": str(rc["min_income_debt_ratio"]),
                    "compounded_debt_factor": str(rc["compounded_debt_factor"]),
                    "monthly_debt_relief": str(rc["monthly_debt_relief"]),
                    "salary_floor": str(rc["salary_floor"]),
                    "threshold": "" if rc["threshold"] is None else str(rc["threshold"]),
                    "target_approval_rate": "" if rc["target_rate"] is None else str(rc["target_rate"]),
                    "random_band": str(rc["random_band"]).lower(),
                    "random_approval_band": str(rc["random_band"]).lower(),
                    "rule_mode": "classic",
                })
            else:
                rn = st.session_state.ndi_rules
                data.update({
                    "ndi_value": str(rn["ndi_value"]),
                    "ndi_ratio": str(rn["ndi_ratio"]),
                    "threshold": "" if rn["threshold"] is None else str(rn["threshold"]),
                    "target_approval_rate": "" if rn["target_rate"] is None else str(rn["target_rate"]),
                    "random_band": str(rn["random_band"]).lower(),
                    "random_approval_band": str(rn["random_band"]).lower(),
                    "rule_mode": "ndi",
                })

            def prep_and_pack(df: pd.DataFrame, filename: str):
                safe = dedupe_columns(df)
                safe, _ = drop_pii_columns(safe)
                safe = strip_policy_banned(safe)
                safe = to_agent_schema(safe)
                buf = io.StringIO()
                safe.to_csv(buf, index=False)
                return {"file": (filename, buf.getvalue().encode("utf-8"), "text/csv")}

            if data_choice == "Use synthetic (ANON)":
                if "synthetic_df" not in st.session_state:
                    st.warning("No ANON synthetic dataset found. Generate it in the first tab."); st.stop()
                files = prep_and_pack(st.session_state.synthetic_df, "synthetic_anon.csv")

            elif data_choice == "Use synthetic (RAW ‚Äì auto-sanitize)":
                if "synthetic_raw_df" not in st.session_state:
                    st.warning("No RAW synthetic dataset found. Generate it in the first tab."); st.stop()
                files = prep_and_pack(st.session_state.synthetic_raw_df, "synthetic_raw_sanitized.csv")

            elif data_choice == "Use anonymized dataset":
                if "anonymized_df" not in st.session_state:
                    st.warning("No anonymized dataset found. Create it in the second tab."); st.stop()
                files = prep_and_pack(st.session_state.anonymized_df, "anonymized.csv")

            elif data_choice == "Upload manually":
                up_name = st.session_state.get("manual_upload_name")
                up_bytes = st.session_state.get("manual_upload_bytes")
                if not up_name or not up_bytes:
                    st.warning("Please upload a CSV first."); st.stop()
                try:
                    tmp_df = pd.read_csv(io.BytesIO(up_bytes))
                    files = prep_and_pack(tmp_df, up_name)
                except Exception:
                    files = {"file": (up_name, up_bytes, "text/csv")}
            else:
                st.error("Unknown data source selection."); st.stop()

            r = requests.post(f"{API_URL}/v1/agents/{agent_name}/run", data=data, files=files, timeout=180)
            if r.status_code != 200:
                st.error(f"Run failed ({r.status_code}): {r.text}"); st.stop()

            res = r.json()
            st.session_state.last_run_id = res.get("run_id")
            result = res.get("result", {}) or {}
            st.success(f"‚úÖ Run succeeded! Run ID: {st.session_state.last_run_id}")

            # Pull merged.csv for dashboards/review
            rid = st.session_state.last_run_id
            merged_url = f"{API_URL}/v1/runs/{rid}/report?format=csv"
            merged_bytes = requests.get(merged_url, timeout=30).content
            merged_df = pd.read_csv(io.BytesIO(merged_bytes))
            st.session_state["last_merged_df"] = merged_df

            # # Export AI outputs as csv with currency code (for Human Review dropdown)
            # ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            # out_name = f"ai-appraisal-outputs-{ts}-{st.session_state['currency_code']}.csv"
            # st.download_button("‚¨áÔ∏è Download AI outputs (CSV)", merged_df.to_csv(index=False).encode("utf-8"), out_name, "text/csv")

            # Decision filter IN TABLE (not hiding dashboard)
            st.markdown("### üìÑ Credit Ai Agent  Decisions Table (filtered)")
            uniq_dec = sorted([d for d in merged_df.get("decision", pd.Series(dtype=str)).dropna().unique()])
            chosen = st.multiselect("Filter decision", options=uniq_dec, default=uniq_dec, key="filter_decisions")
            df_view = merged_df.copy()
            if "decision" in df_view.columns and chosen:
                df_view = df_view[df_view["decision"].isin(chosen)]
            st.dataframe(df_view, use_container_width=True)

            # ‚îÄ‚îÄ DASHBOARD (always visible; filters apply in table below)
            st.markdown("## üìä Dashboard")
            render_credit_dashboard(merged_df, st.session_state.get("currency_symbol", ""))

            # Per-row metrics met/not met
            if "rule_reasons" in df_view.columns:
                rr = df_view["rule_reasons"].apply(try_json)
                df_view["metrics_met"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is True])) if isinstance(d, dict) else "")
                df_view["metrics_unmet"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is False])) if isinstance(d, dict) else "")
            cols_show = [c for c in [
                "application_id","customer_type","decision","score","loan_amount","income","metrics_met","metrics_unmet",
                "proposed_loan_option","proposed_consolidation_loan","top_feature","explanation"
            ] if c in df_view.columns]
            st.dataframe(df_view[cols_show].head(500), use_container_width=True)

            #  # Export AI outputs as csv with currency code (for Human Review dropdown)
            # ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            # out_name = f"ai-appraisal-outputs-{ts}-{st.session_state['currency_code']}.csv"
            # st.download_button("‚¨áÔ∏è Download AI outputs (CSV)", merged_df.to_csv(index=False).encode("utf-8"), out_name, "text/csv")
            # Export AI outputs as CSV with currency code (for Human Review dropdown)

                        # Export AI outputs as CSV with currency code (for Human Review dropdown)
            ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            out_name = f"ai-appraisal-outputs-{ts}-{st.session_state['currency_code']}.csv"
            csv_data = merged_df.to_csv(index=False).encode("utf-8")

            # Correct CSS selector for Streamlit's download button
            st.markdown("""
            <style>
            div[data-testid="stDownloadButton"] button {
                font-size: 90px !important;
                font-weight: 900 !important;
                padding: 28px 48px !important;
                border-radius: 16px !important;
                background: linear-gradient(90deg, #2563eb, #1d4ed8) !important;
                color: white !important;
                border: none !important;
                box-shadow: 0 6px 18px rgba(0,0,0,0.35) !important;
                transition: all 0.3s ease-in-out !important;
            }
            div[data-testid="stDownloadButton"] button:hover {
                background: linear-gradient(90deg, #1e3a8a, #1d4ed8) !important;
                transform: scale(1.03);
            }
            </style>
            """, unsafe_allow_html=True)

            # Styled large download button
            st.download_button(
                "‚¨áÔ∏è Download AI Outputs For Human Review (CSV)",
                csv_data,
                file_name=out_name,
                mime="text/csv",
                use_container_width=True
            )

        except Exception as e:
            st.exception(e)



    # Re-download quick section
    if st.session_state.get("last_run_id"):
        st.markdown("---")
        st.subheader("üì• Download Latest Outputs")
        rid = st.session_state.last_run_id
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1: st.markdown(f"[‚¨áÔ∏è PDF]({API_URL}/v1/runs/{rid}/report?format=pdf)")
        with col2: st.markdown(f"[‚¨áÔ∏è Scores CSV]({API_URL}/v1/runs/{rid}/report?format=scores_csv)")
        with col3: st.markdown(f"[‚¨áÔ∏è Explanations CSV]({API_URL}/v1/runs/{rid}/report?format=explanations_csv)")
        with col4: st.markdown(f"[‚¨áÔ∏è Merged CSV]({API_URL}/v1/runs/{rid}/report?format=csv)")
        with col5: st.markdown(f"[‚¨áÔ∏è JSON]({API_URL}/v1/runs/{rid}/report?format=json)")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üßë‚Äç‚öñÔ∏è TAB 4 ‚Äî Human Review
with tab_review:
    st.subheader("üßë‚Äç‚öñÔ∏è Human Review ‚Äî Correct AI Decisions & Score Agreement > Drop your AI appraisal output CSV from previous Stage  below")

    # Allow loading AI output CSV back into review via dropdown upload
    uploaded_review = st.file_uploader("Load AI outputs CSV for review (optional)", type=["csv"], key="review_csv_loader")
    if uploaded_review is not None:
        try:
            st.session_state["last_merged_df"] = pd.read_csv(uploaded_review)
            st.success("Loaded review dataset from uploaded CSV.")
        except Exception as e:
            st.error(f"Could not read uploaded CSV: {e}")

    if "last_merged_df" not in st.session_state:
        st.info("Run the agent (previous tab) or upload an AI outputs CSV to load results for review.")
    else:
        dfm = st.session_state["last_merged_df"].copy()
        st.markdown("#### 1) Select rows to review and correct")

        editable_cols = []
        if "decision" in dfm.columns: editable_cols.append("decision")
        if "rule_reasons" in dfm.columns: editable_cols.append("rule_reasons")
        if "customer_type" in dfm.columns: editable_cols.append("customer_type")

        editable = dfm[["application_id"] + editable_cols].copy()
        editable.rename(columns={"decision": "ai_decision"}, inplace=True)
        editable["human_decision"] = editable.get("ai_decision", "approved")
        editable["human_rule_reasons"] = editable.get("rule_reasons", "")

        edited = st.data_editor(
            editable,
            num_rows="dynamic",
            use_container_width=True,
            key="review_editor",
            column_config={
                "human_decision": st.column_config.SelectboxColumn(options=["approved", "denied"]),
                "customer_type": st.column_config.SelectboxColumn(options=["bank", "non-bank"], disabled=True)
            }
        )

        st.markdown("#### 2) Compute agreement score")
        # if st.button("Compute agreement score"):
        #     if "ai_decision" in edited.columns and "human_decision" in edited.columns:
        #         agree = (edited["ai_decision"] == edited["human_decision"]).astype(int)
        #         score = float(agree.mean()) if len(agree) else 0.0
        #         st.success(f"Agreement score (AI vs human): {score:.3f}")
        #         st.session_state["last_agreement_score"] = score
        #     else:
        #         st.warning("Missing decision columns to compute score.")
        if st.button("Compute agreement score"):
            if "ai_decision" in edited.columns and "human_decision" in edited.columns:
                agree = (edited["ai_decision"] == edited["human_decision"]).astype(int)
                score = float(agree.mean()) if len(agree) else 0.0
                st.session_state["last_agreement_score"] = score

                # üå°Ô∏è BEAUTIFUL Gauge
                import plotly.graph_objects as go
                fig = go.Figure(go.Indicator(
                    mode="gauge+number",
                    value=score * 100,
                    number={'suffix': "%", 'font': {'size': 72, 'color': "#f8fafc", 'family': "Arial Black"}},
                    title={'text': "AI ‚Üî Human Agreement", 'font': {'size': 28, 'color': "#93c5fd", 'family': "Arial"}},
                    gauge={
                        'axis': {'range': [0, 100], 'tickwidth': 2, 'tickcolor': "#f8fafc"},
                        'bar': {'color': "#3b82f6", 'thickness': 0.3},
                        'bgcolor': "#1e293b",
                        'borderwidth': 2,
                        'bordercolor': "#334155",
                        'steps': [
                            {'range': [0, 50], 'color': "#ef4444"},
                            {'range': [50, 75], 'color': "#f59e0b"},
                            {'range': [75, 100], 'color': "#22c55e"},
                        ],
                    }
                ))
                fig.update_layout(
                    paper_bgcolor="#0f172a",
                    plot_bgcolor="#0f172a",
                    height=400,
                    margin=dict(t=60, b=20, l=60, r=60)
                )
                st.plotly_chart(fig, use_container_width=True)

            #     # üí° Detailed disagreement table (safe + color)
            #     mismatched = edited[edited["ai_decision"] != edited["human_decision"]].copy()
            #     total = len(edited)
            #     disagree = len(mismatched)
            #     if disagree > 0:
            #         st.markdown(f"### ‚ùå {disagree} loans disagreed out of {total} ({(disagree/total)*100:.1f}% disagreement rate)")

            #         import json

            #         def extract_disagreement_reasons(r: str):
            #             """Parse rule_reasons safely and extract disagreement causes."""
            #             if not isinstance(r, str):
            #                 return "Manual review adjustment"
            #             try:
            #                 data = json.loads(r.replace("'", "\""))
            #                 failed = [k for k, v in data.items() if v is False]
            #                 if failed:
            #                     return "Human override on: " + ", ".join(failed)
            #                 return "Manual review adjustment"
            #             except Exception:
            #                 return "Manual review adjustment"

            #         if "rule_reasons" in mismatched.columns:
            #             mismatched["why_disagree"] = mismatched["rule_reasons"].apply(extract_disagreement_reasons)
            #         else:
            #             mismatched["why_disagree"] = "Manual review adjustment"

            #         # üü©üü• Color styling for AI vs Human
            #         def highlight_disagreement(row):
            #             ai_color = "background-color: #ef4444; color: white;"      # red for AI
            #             human_color = "background-color: #22c55e; color: black;"   # green for Human
            #             return [
            #                 ai_color if col == "ai_decision" else
            #                 human_color if col == "human_decision" else
            #                 ""
            #                 for col in row.index
            #             ]

            #         show_cols = [c for c in ["application_id", "ai_decision", "human_decision", "why_disagree"] if c in mismatched.columns]
            #         styled_df = mismatched[show_cols].style.apply(highlight_disagreement, axis=1)
            #         st.dataframe(styled_df, use_container_width=True, height=400)
            #     else:
            #         st.success("‚úÖ Full agreement ‚Äî no human-AI mismatches found.")
            # else:
            #     st.warning("Missing decision columns to compute score.")

            # üí° Detailed disagreement table (AI vs Human + AI metrics explanation)
                mismatched = edited[edited["ai_decision"] != edited["human_decision"]].copy()
                total = len(edited)
                disagree = len(mismatched)

                if disagree > 0:
                    st.markdown(f"### ‚ùå {disagree} loans disagreed out of {total} ({(disagree/total)*100:.1f}% disagreement rate)")

                    import json

                    def parse_ai_reason(r: str):
                        """Parse AI rule_reasons and summarize which metrics passed or failed."""
                        if not isinstance(r, str):
                            return "No metrics available"
                        try:
                            data = json.loads(r.replace("'", "\""))
                            passed = [k for k, v in data.items() if v is True]
                            failed = [k for k, v in data.items() if v is False]
                            result = []
                            if passed:
                                result.append("‚úÖ Pass: " + ", ".join(passed))
                            if failed:
                                result.append("‚ùå Fail: " + ", ".join(failed))
                            return " | ".join(result) if result else "No metrics recorded"
                        except Exception:
                            return "Unreadable metrics"

                    # Extract AI reasoning and Human reason columns
                    mismatched["ai_metrics"] = mismatched["rule_reasons"].apply(parse_ai_reason) if "rule_reasons" in mismatched else "No data"
                    mismatched["human_reason"] = mismatched.get("human_rule_reasons", "Manual review adjustment")

                    # üü©üü• Color styling for AI vs Human
                    def highlight_disagreement(row):
                        ai_color = "background-color: #ef4444; color: white;"      # red for AI decision
                        human_color = "background-color: #22c55e; color: black;"   # green for Human decision
                        return [
                            ai_color if col == "ai_decision" else
                            human_color if col == "human_decision" else
                            ""
                            for col in row.index
                        ]

                    # Columns: ID ‚Üí AI Decision ‚Üí Human Decision ‚Üí AI Metrics ‚Üí Human Reason
                    show_cols = [
                        c for c in ["application_id", "ai_decision", "human_decision", "ai_metrics", "human_reason"]
                        if c in mismatched.columns
                    ]
                    styled_df = mismatched[show_cols].style.apply(highlight_disagreement, axis=1)
                    st.dataframe(styled_df, use_container_width=True, height=420)

                else:
                    st.success("‚úÖ Full agreement ‚Äî no human-AI mismatches found.")



        # Export review CSV (manual loop into training)
        st.markdown("#### 3) Export Human review CSV for Next Step : Training and loopback ")
        model_used = "production"  # if you track specific model names, set it here
        ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        safe_user = st.session_state["user_info"]["name"].replace(" ", "").lower()
        review_name = f"creditappraisal.{safe_user}.{model_used}.{ts}.csv"
        csv_bytes = edited.to_csv(index=False).encode("utf-8")
        st.download_button("‚¨áÔ∏è Export review CSV", csv_bytes, review_name, "text/csv")
        st.caption(f"Saved file name pattern: **{review_name}**")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üîÅ TAB 5 ‚Äî Training (Feedback ‚Üí Retrain)
with tab_train:
    st.subheader("üîÅ From Human Feedback CSV ‚Üí Train and Promote Trained Model to Production Model ")

    st.markdown("**Drag & drop** one or more review CSVs exported from the Human Review tab.")
    up_list = st.file_uploader("Upload feedback CSV(s)", type=["csv"], accept_multiple_files=True, key="train_feedback_uploader")

    staged_paths: List[str] = []
    if up_list:
        for up in up_list:
            # stage to tmp_feedback dir
            dest = os.path.join(TMP_FEEDBACK_DIR, up.name)
            with open(dest, "wb") as f:
                f.write(up.getvalue())
            staged_paths.append(dest)
        st.success(f"Staged {len(staged_paths)} feedback file(s) to {TMP_FEEDBACK_DIR}")
        st.write(staged_paths)

    st.markdown("#### Launch Retrain")
    payload = {
        "feedback_csvs": staged_paths,
        "user_name": st.session_state["user_info"]["name"],
        "agent_name": "credit_appraisal",
        "algo_name": "credit_lr",
    }
    st.code(json.dumps(payload, indent=2), language="json")

    colA, colB = st.columns([1,1])
    with colA:
        if st.button("üöÄ Train candidate model"):
            try:
                r = requests.post(f"{API_URL}/v1/training/train", json=payload, timeout=90)
                if r.ok:
                    st.success(r.json())
                    st.session_state["last_train_job"] = r.json().get("job_id")
                else:
                    st.error(r.text)
            except Exception as e:
                st.error(f"Train failed: {e}")
    with colB:
        if st.button("‚¨ÜÔ∏è Promote last candidate to PRODUCTION"):
            try:
                r = requests.post(f"{API_URL}/v1/training/promote", timeout=30)
                st.write(r.json() if r.ok else r.text)
            except Exception as e:
                st.error(f"Promote failed: {e}")

    st.markdown("---")
    st.markdown("#### Production Model")
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.ok:
            st.json(resp.json())
        else:
            st.info("No production model yet.")
    except Exception as e:
        st.warning(f"Could not load production meta: {e}")


      # üîÅ Loopback Section (Real functional button)

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üîÅ Loopback Section ‚Äî Go back to Step 3
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("---")
    st.markdown("### üí≥ Loop back to Step 3 ‚Äî Credit Appraisal Agent")
    st.caption("After retraining, return to the Credit Appraisal tab and use your new production model.")

    st.markdown("""
    <a href="#credit-appraisal-stage" target="_self">
        <button style="
            background-color:#2563eb;
            color:white;
            border:none;
            border-radius:8px;
            padding:12px 24px;
            font-size:16px;
            font-weight:600;
            cursor:pointer;
            width:100%;
            box-shadow:0px 0px 6px rgba(37,99,235,0.5);
        ">‚¨ÖÔ∏è Go Back to Step 3 and Use New Model</button>
    </a>
    """, unsafe_allow_html=True)



==================== ./_old/deploy_export.py ====================
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# G ‚Äî DEPLOYMENT & EXPORT STAGE
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os, glob, hashlib, json, zipfile, requests
import streamlit as st
from datetime import datetime, timezone
from pathlib import Path

st.set_page_config(page_title="Stage G ‚Äî Deployment & Export", page_icon="üöÄ")

st.title("üöÄ Stage G ‚Äî Deployment & Export")
st.caption("Package, verify, and publish production bundles for external deployment.")

EXPORT_DIR = Path("./exports")
os.makedirs(EXPORT_DIR, exist_ok=True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1Ô∏è‚É£ Detect the latest ZIP archive
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.subheader("üì¶ Detected Export Packages")
zip_files = sorted(EXPORT_DIR.glob("export_production_*.zip"), reverse=True)
if not zip_files:
    st.warning("‚ö†Ô∏è No export ZIP found. Run Stage F and export first.")
    st.stop()

latest_zip = zip_files[0]
st.success(f"‚úÖ Latest package: `{latest_zip.name}` ({latest_zip.stat().st_size/1e6:.2f} MB)")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2Ô∏è‚É£ Compute checksum + digital signature
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.subheader("üîê Checksum & Integrity Verification")

sha256 = hashlib.sha256(latest_zip.read_bytes()).hexdigest()
checksum_file = latest_zip.with_suffix(".sha256")
checksum_file.write_text(sha256)
st.code(sha256, language="text")
st.caption(f"Checksum saved ‚Üí `{checksum_file.name}`")

# Optional: simple signature stub (replace with real key-based signing if needed)
signature_file = latest_zip.with_suffix(".sig")
signature_file.write_text(f"SIGNED BY AI-AGENT-HUB @ {datetime.now(timezone.utc).isoformat()}")
st.caption(f"Signature stub saved ‚Üí `{signature_file.name}`")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3Ô∏è‚É£ Upload options (S3 / Swift / GitHub)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.subheader("‚òÅÔ∏è Upload Options")

dest = st.radio("Choose target destination", ["AWS S3", "OpenStack Swift", "GitHub Release"])

if dest == "AWS S3":
    st.info("Upload via boto3 (requires AWS credentials).")
    bucket = st.text_input("S3 Bucket Name", "my-ai-models")
    key = st.text_input("S3 Object Key", latest_zip.name)
    if st.button("‚¨ÜÔ∏è Upload to S3"):
        import boto3
        try:
            s3 = boto3.client("s3")
            s3.upload_file(str(latest_zip), bucket, key)
            st.success(f"‚úÖ Uploaded to s3://{bucket}/{key}")
        except Exception as e:
            st.error(f"‚ùå Upload failed: {e}")

elif dest == "OpenStack Swift":
    st.info("Upload via python-swiftclient (requires Swift credentials).")
    container = st.text_input("Swift Container", "models")
    if st.button("‚¨ÜÔ∏è Upload to Swift"):
        try:
            from swiftclient.service import SwiftService, SwiftUploadObject
            with SwiftService() as swift:
                swift.upload(container, [SwiftUploadObject(str(latest_zip))])
            st.success(f"‚úÖ Uploaded to Swift container `{container}`")
        except Exception as e:
            st.error(f"‚ùå Swift upload failed: {e}")

elif dest == "GitHub Release":
    st.info("Upload via GitHub API (requires token).")
    repo = st.text_input("GitHub Repo (e.g. username/repo)", "RackspaceAI/asset-appraisal-agent")
    token = st.text_input("Personal Access Token", type="password")
    tag = datetime.now().strftime("v%Y%m%d-%H%M%S")
    if st.button("‚¨ÜÔ∏è Upload to GitHub Release"):
        headers = {"Authorization": f"token {token}", "Accept": "application/vnd.github+json"}
        try:
            # Create release
            r = requests.post(f"https://api.github.com/repos/{repo}/releases",
                              headers=headers,
                              json={"tag_name": tag, "name": f"Model Release {tag}",
                                    "body": "Automated deployment from Stage G"})
            r.raise_for_status()
            upload_url = r.json()["upload_url"].split("{")[0]
            with open(latest_zip, "rb") as f:
                ur = requests.post(f"{upload_url}?name={latest_zip.name}",
                                   headers={**headers, "Content-Type": "application/zip"}, data=f)
            ur.raise_for_status()
            st.success(f"‚úÖ Uploaded to GitHub Release `{tag}`")
        except Exception as e:
            st.error(f"‚ùå GitHub upload failed: {e}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4Ô∏è‚É£ Deployment Audit Record
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.subheader("üßæ Deployment Audit")
audit_record = {
    "timestamp": datetime.now(timezone.utc).isoformat(),
    "export_file": latest_zip.name,
    "checksum": sha256,
    "destination": dest,
}
with open(EXPORT_DIR / "deployment_audit.json", "a", encoding="utf-8") as f:
    f.write(json.dumps(audit_record) + "\n")

st.success("Deployment audit updated ‚úÖ")



==================== ./_old/open_source_models.py ====================
import os, io, subprocess, pandas as pd, streamlit as st
from datasets import load_dataset

RUNS_DIR = os.path.join(os.getcwd(), "services", "ui", ".tmp_runs")
os.makedirs(RUNS_DIR, exist_ok=True)

def render_open_source_tab():
    st.subheader("üåç Import Public Datasets (Kaggle / Hugging Face / OpenML / Portals)")

    src = st.radio("Select source", ["Kaggle (API)", "Hugging Face Datasets", "OpenML"], horizontal=True)
    keyword = st.text_input("üîé Search keywords", placeholder="e.g. house prices real estate valuation")

    outdir = os.path.join(RUNS_DIR, "kaggle")
    os.makedirs(outdir, exist_ok=True)

    # -----------------------------
    # KAGGLE SEARCH
    # -----------------------------
    if src == "Kaggle (API)" and st.button("üîç Search dataset", use_container_width=True):
        try:
            cmd = ["kaggle", "datasets", "list", "-s", keyword, "--csv"]
            result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            df = pd.read_csv(io.StringIO(result.stdout))
            st.session_state["kaggle_results"] = df
            st.success("‚úÖ Kaggle API results shown.")
        except Exception as e:
            st.error(f"Kaggle CLI failed: {e}")
            return

    # -----------------------------
    # SHOW RESULTS + IMPORT UI
    # -----------------------------
    df = st.session_state.get("kaggle_results")
    if df is not None and not df.empty:
        st.dataframe(df, use_container_width=True, height=400)

        # We use an expander to hold download options persistently
        with st.expander("‚¨áÔ∏è Import Selected Dataset", expanded=True):
            selected = st.selectbox(
                "Select a Kaggle dataset to download", 
                df["ref"].tolist(),
                key="selected_kaggle_dataset"
            )
            if st.button("üì• Download & Import Selected", use_container_width=True):
                try:
                    st.info(f"Downloading {selected} ‚Ä¶ please wait ‚è≥")
                    cmd = ["kaggle", "datasets", "download", "-d", selected, "-p", outdir, "--unzip"]
                    subprocess.run(cmd, check=True)

                    csvs = [f for f in os.listdir(outdir) if f.endswith(".csv")]
                    if csvs:
                        csv_path = os.path.join(outdir, csvs[0])
                        df_loaded = pd.read_csv(csv_path)
                        st.session_state["asset_intake_df"] = df_loaded
                        st.session_state["asset_intake_path"] = csv_path
                        st.session_state["asset_stage"] = "asset_flow"

                        st.success(f"‚úÖ Imported {csvs[0]} ({len(df_loaded)} rows)")
                        st.dataframe(df_loaded.head())
                        st.toast("üöÄ Moving to Normalize & Combine stage‚Ä¶", icon="‚úÖ")
                        st.rerun()
                    else:
                        st.warning("‚ö†Ô∏è No CSV file found after download.")
                except Exception as e:
                    st.error(f"‚ùå Download failed: {e}")

    # -----------------------------
    # HUGGING FACE FALLBACK
    # -----------------------------
    elif src == "Hugging Face Datasets" and st.button("ü§ó Load from Hugging Face", use_container_width=True):
        try:
            ds = load_dataset("uciml/real-estate-valuation")
            df = ds["train"].to_pandas()
            st.session_state["asset_intake_df"] = df
            st.dataframe(df.head())
            st.toast("ü§ó Hugging Face dataset loaded!", icon="üéØ")
            st.session_state["asset_stage"] = "asset_flow"
            st.rerun()
        except Exception as e:
            st.error(f"Hugging Face load failed: {e}")



==================== ./credit_appraisal.py ====================
# services/ui/pages/credit_appraisal.py
from __future__ import annotations

import os
import io
import re
import json
import shutil
import threading
import time
from pathlib import Path
from datetime import datetime, timezone
from typing import Optional, Dict, List, Any
import pandas as pd
import numpy as np
import streamlit as st
import requests
import plotly.express as px
import plotly.graph_objects as go
import logging
import sys

from services.ui.utils.pandas_compat import ensure_json_normalize

json_normalize = ensure_json_normalize()

from services.ui.theme_manager import (
    apply_theme as apply_global_theme,
    get_theme,
    render_theme_toggle,
)
from services.ui.components.operator_banner import render_operator_banner
from services.ui.components.telemetry_dashboard import render_telemetry_dashboard
from services.ui.components.feedback import render_feedback_tab
from services.ui.components.chat_assistant import render_chat_assistant
from services.ui.utils.llm_selector import render_llm_selector






def apply_theme(theme: str | None = None):
    """Proxy to shared theme manager for credit page."""
    theme = theme or get_theme()
    apply_global_theme(theme)



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CREDIT AGENT ‚Äî HEADER (used in credit flow)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import streamlit as st  # (no-op if already imported)

def render_credit_header():
    ss = st.session_state

    # pick a display name if available
    user = (
        (ss.get("credit_user") or {}).get("name")
        or (ss.get("asset_user") or {}).get("name")
        or (ss.get("user_info") or {}).get("name")
        or "guest"
    )

    # use your theme switch (defaults to dark)
    theme = ss.get("theme", "dark")
    brand = {
        "dark": {"text":"#e2e8f0","muted":"#94a3b8","accent":"#3b82f6"},
        "light":{"text":"#0f172a","muted":"#475569","accent":"#2563eb"},
    }[theme]

    st.title("üè¶ Credit Appraisal Agent")
    st.caption(
        "A‚ÜíH pipeline ‚Äî Intake ‚Üí Privacy ‚Üí Credit Appraisal ‚Üí Human Review ‚Üí "
        "Training ‚Üí Deployment ‚Üí Monitoring ‚Üí Reporting "
        f"| üëã {user}"
    )

# ‚úÖ JSON ‚Üí DataFrame converter (final, unified, safe)
# ============================================================
def json_to_dataframe(payload) -> pd.DataFrame:
    """
    Convert arbitrary API JSON (dict/list/bytes/str) into a DataFrame.
    Prefers server 'artifacts.merged_csv' ‚Üí fallback to json_normalize.
    """

    # -------------------------------
    # Case 1: payload is dict
    # -------------------------------
    if isinstance(payload, dict):

        # ‚úÖ Try artifacts.merged_csv first
        res = payload.get("result") or payload
        artifacts = res.get("artifacts") or {}
        merged_csv = artifacts.get("merged_csv")

        if isinstance(merged_csv, str) and os.path.exists(merged_csv):
            try:
                return pd.read_csv(merged_csv)
            except Exception:
                pass

        # ‚úÖ Embedded merged_df inside the JSON
        if "merged_df" in res:
            try:
                return pd.DataFrame(res["merged_df"])
            except Exception:
                pass

        # ‚úÖ If result is list ‚Üí DF
        if isinstance(res, list):
            try:
                return pd.DataFrame(res)
            except Exception:
                try:
                    return pd.json_normalize(res)
                except Exception:
                    pass

        # ‚úÖ Try keys inside result
        for key in ("rows", "data", "result", "results", "items", "records"):
            if key in res:
                try:
                    return json_to_dataframe(res[key])
                except Exception:
                    pass

    # -------------------------------
    # Case 2: payload is list
    # -------------------------------
    if isinstance(payload, list):
        if len(payload) == 0:
            return pd.DataFrame()
        if all(isinstance(x, dict) for x in payload):
            try:
                return pd.DataFrame(payload)
            except:
                return pd.json_normalize(payload)
        return pd.DataFrame({"value": payload})

    # -------------------------------
    # Case 3: payload is bytes
    # -------------------------------
    if isinstance(payload, bytes):
        try:
            payload = payload.decode("utf-8", errors="ignore")
        except:
            return pd.DataFrame({"value": [repr(payload)]})

    # -------------------------------
    # Case 4: payload is str ‚Üí try JSON parse
    # -------------------------------
    if isinstance(payload, str):
        payload = payload.strip()
        if not payload:
            return pd.DataFrame()
        try:
            j = json.loads(payload)
            return json_to_dataframe(j)
        except:
            # Fallback ‚Üí line-by-line DF
            lines = [ln for ln in payload.splitlines() if ln.strip()]
            return pd.DataFrame({"value": lines}) if lines else pd.DataFrame()

    # -------------------------------
    # Default fallback
    # -------------------------------
    return pd.DataFrame({"value": [payload]})



def _extract_run_fields(raw_json):  # ADD
    """
    Return (run_id, normalized_payload_dict).
    Ensures downstream code always receives a dict-like 'payload'.
    """
    run_id = extract_run_id(raw_json)

    # Normalize to dict payload so later code can access keys safely
    payload = raw_json
    if not isinstance(payload, dict):
        if isinstance(payload, list):
            first_dict = next((x for x in payload if isinstance(x, dict)), None)
            payload = first_dict if first_dict is not None else {"result": raw_json}
        else:
            payload = {"result": raw_json}
    return run_id, payload


def _coerce_minutes(value, fallback: float = 0.0) -> float:
    """Best-effort conversion of strings like '22 min' to minute floats."""
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        cleaned = "".join(ch for ch in value if ch.isdigit() or ch == ".")
        try:
            return float(cleaned)
        except (TypeError, ValueError):
            pass
    return float(fallback)





# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# PAGE CONFIG & THEME
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.set_page_config(
    page_title="üí≥ Credit Appraisal",
    page_icon="üí≥",
    layout="wide",
    initial_sidebar_state="collapsed",
)
#

apply_theme()

st.markdown(
    """
    > **Unified Risk Checklist**  
    > ‚úÖ Is the borrower real & safe? (Fraud/KYC)  
    > ‚úÖ Is the collateral worth enough? (Asset)  
    > ‚úÖ Can they afford the loan? (this agent)  
    > ‚úÖ Should the bank approve overall? (Unified agent)
    """
)



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SESSION STATE INIT
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if "stage" not in st.session_state:
    st.session_state.stage = "credit_agent"
if "user_info" not in st.session_state:
    st.session_state.user_info = {"name": "", "email": "", "flagged": False}
if "logged_in" not in st.session_state:
    st.session_state.logged_in = False
if "flagged" not in st.session_state.user_info:
    st.session_state.user_info["flagged"] = False
if "timestamp" not in st.session_state.user_info:
    st.session_state.user_info["timestamp"] = datetime.now(timezone.utc).isoformat()
st.session_state.setdefault("credit_apps_in_review", 24)
st.session_state.setdefault("credit_flagged_cases", 5)
st.session_state.setdefault("credit_avg_decision_time", "9 min")
st.session_state.setdefault("credit_ai_performance", 0.92)
st.session_state.setdefault(
    "credit_user",
    {"name": "Operator", "email": "operator@demo.local", "timestamp": datetime.now(timezone.utc).isoformat()},
)
st.session_state.setdefault("credit_logged_in", True)
st.session_state["credit_logged_in"] = True
if not st.session_state.user_info.get("name"):
    st.session_state.user_info["name"] = st.session_state["credit_user"]["name"]
if st.session_state.get("credit_logged_in", False):
    st.session_state.stage = "credit_agent"


def _build_credit_chat_context() -> Dict[str, Any]:
    ss_local = st.session_state
    ctx = {
        "agent_type": "credit",
        "stage": ss_local.get("credit_stage") or ss_local.get("stage"),
        "user": (ss_local.get("credit_user") or {}).get("name"),
        "apps_in_review": ss_local.get("credit_apps_in_review"),
        "flagged_cases": ss_local.get("credit_flagged_cases"),
        "avg_decision_time": ss_local.get("credit_avg_decision_time"),
        "ai_performance": ss_local.get("credit_ai_performance"),
        "last_run_id": ss_local.get("credit_last_run_id"),
        "last_error": ss_local.get("credit_last_error"),
        "selected_model": ss_local.get("selected_model"),
    }
    return {k: v for k, v in ctx.items() if v not in (None, "", [])}


CREDIT_FAQ = [
    "Explain why this borrower was rejected.",
    "Summarize rule breaches for this loan.",
    "Compare PD vs NDI for this applicant.",
    "How can I rerun Stage D ‚Äì Policy?",
]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# HELPERS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _extract_run_fields(res_json):
    """
    Return (run_id, result_dict) from API responses that may be dicts or lists.
    """
    run_id = None
    result_obj = {}

    if isinstance(res_json, dict):
        run_id = (
            res_json.get("run_id")
            or res_json.get("id")
            or (res_json.get("data") or {}).get("run_id")
        )
        result_obj = (
            res_json.get("result")
            or (res_json.get("data") or {}).get("result")
            or {}
        )

    elif isinstance(res_json, list):
        # Find first dict item that contains identifiers/results
        for item in res_json:
            if isinstance(item, dict):
                if not run_id:
                    run_id = item.get("run_id") or item.get("id")
                if not result_obj:
                    result_obj = item.get("result") or {}
                if run_id and result_obj != {}:
                    break
        # If still nothing and list[0] is a dict, use it as best-effort
        if not run_id and res_json and isinstance(res_json[0], dict):
            run_id = res_json[0].get("run_id") or res_json[0].get("id")
            result_obj = res_json[0].get("result") or {}

    # Ensure result is a dict
    if not isinstance(result_obj, dict):
        result_obj = {"value": result_obj}

    return run_id, result_obj


def _clear_qp():
    """Clear query params (modern Streamlit API)."""
    try:
        st.query_params.clear()
    except Exception:
        pass


def load_image(base: str) -> Optional[str]:
    for ext in [".png", ".jpg", ".jpeg", ".webp", ".gif", ".svg"]:
        p = os.path.join(LANDING_IMG_DIR, f"{base}{ext}")
        if os.path.exists(p):
            return p
    return None


def save_uploaded_image(uploaded_file, base: str):
    if not uploaded_file:
        return None
    ext = os.path.splitext(uploaded_file.name)[1].lower() or ".png"
    dest = os.path.join(LANDING_IMG_DIR, f"{base}{ext}")
    with open(dest, "wb") as f:
        f.write(uploaded_file.getvalue())
    return dest


def render_image_tag(agent_id: str, industry: str, emoji_fallback: str) -> str:
    base = agent_id.lower().replace(" ", "_")
    img_path = load_image(base) or load_image(industry.replace(" ", "_"))
    if img_path:
        return (
            f'<img src="file://{img_path}" '
            f'style="width:48px;height:48px;border-radius:10px;object-fit:cover;">'
        )
    return f'<div style="font-size:32px;">{emoji_fallback}</div>'




st.markdown(
    """
    <style>
    [data-testid="stSidebar"], section[data-testid="stSidebar"]{display:none!important}
    [data-testid="stAppViewContainer"]{margin-left:0!important;padding-left:0!important}
    </style>
    """,
    unsafe_allow_html=True,
)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CONSTANTS / PATHS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# You can point this to your FastAPI host
API_URL = os.environ.get("AGENT_API_URL", "http://localhost:8090")

_CHATBOT_REFRESH_STATE: Dict[str, float] = {"last_ts": 0.0}

def _ping_chatbot_refresh(reason: str = "credit", *, min_interval: float = 300.0) -> None:
    """Best-effort, throttled ping so the Gemma chatbot reindexes new CSV artifacts."""
    now = time.time()
    last_ts = _CHATBOT_REFRESH_STATE.get("last_ts", 0.0)
    if (now - last_ts) < min_interval:
        return
    _CHATBOT_REFRESH_STATE["last_ts"] = now

    def _fire():
        try:
            requests.post(f"{API_URL}/chatbot/refresh", json={"reason": reason}, timeout=5)
        except Exception:
            logging.getLogger(__name__).debug("Chatbot refresh skipped", exc_info=True)

    threading.Thread(target=_fire, daemon=True).start()

# Base & temp runs folder
BASE_DIR = os.path.abspath(".")
RUNS_DIR = os.path.join(BASE_DIR, ".tmp_runs")
os.makedirs(RUNS_DIR, exist_ok=True)



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# NAVIGATION ‚Äî Reliable jump to Home / Agents
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _set_query_params_safe(**kwargs):
    """Backwards-compatible setter for Streamlit versions before query_params."""
    try:
        for k, v in kwargs.items():
            st.query_params[k] = v
        return True
    except Exception:
        pass
    try:
        st.experimental_set_query_params(**kwargs)
        return True
    except Exception:
        return False


def _go_stage(target_stage: str):
    """Reliable navigation that returns to Home or Agents even from sub-pages."""
    st.session_state["stage"] = target_stage
    try:
        # Jump back to main app router (must exist in /services/ui/app.py)
        st.switch_page("app.py")
        return
    except Exception:
        pass
    _set_query_params_safe(stage=target_stage)
    st.rerun()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üß≠ NAVBAR + THEME SWITCHER
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def render_nav_bar_app():
    """Top navigation bar with Home, Agents, and Theme switch."""
    ss = st.session_state

    c1, c2, c3 = st.columns([1, 1, 2.5])

    with c1:
        if st.button("üè† Back to Home", key=f"btn_home_{ss.get('stage','landing')}"):
            _go_stage("landing")
    with c2:
        if st.button("ü§ñ Back to Agents", key=f"btn_agents_{ss.get('stage','landing')}"):
            _go_stage("agents")
    with c3:
        render_theme_toggle(
            label="üåó Dark mode",
            key="credit_theme_toggle",
            help="Switch theme",
        )

    st.markdown("---")


# ‚úÖ Render navbar before showing login or main content
render_nav_bar_app()





# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üîê LOGIN GATE
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def login_block():
    st.title("üîê Login to AI Credit Appraisal Platform")
    c1, c2, c3 = st.columns([1, 1, 1])
    with c1:
        user = st.text_input("Username", placeholder="e.g. dzoan")
    with c2:
        email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
    with c3:
        pwd = st.text_input("Password", type="password", placeholder="Enter any password")

    if st.button("Login", key="btn_credit_login", use_container_width=True):
        if (user or "").strip() and (email or "").strip():
            st.session_state["user_info"] = {
                "name": user.strip(),
                "email": email.strip(),
                "flagged": False,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
            st.session_state["credit_logged_in"] = True
            st.session_state["stage"] = "credit_agent"
            st.rerun()
        else:
            st.error("‚ö†Ô∏è Please fill all fields before continuing.")


# ‚úÖ Always render navbar ‚Äî and only then login
if not st.session_state.get("credit_logged_in", False):
    login_block()
    st.stop()

# Operator overview
operator_name = (
    st.session_state.get("user_info", {}).get("name")
    or st.session_state.get("credit_user", {}).get("name")
    or "Operator"
)

credit_ai_minutes = _coerce_minutes(st.session_state.get("credit_avg_decision_time"), 9.0)

render_operator_banner(
    operator_name=operator_name,
    title="Credit Appraisal Command",
    summary="Coordinate intake ‚Üí anonymization ‚Üí AI scoring ‚Üí policy decisions inside one unified cockpit.",
    bullets=[
        "Collect borrower data once and auto-sanitize for analytics.",
        "Score risk with explainable AI plus policy/NDI overlays.",
        "Route exceptions to human reviewers and capture feedback for retraining.",
    ],
    metrics=[
        {
            "label": "Apps in Review",
            "value": st.session_state.get("credit_apps_in_review"),
            "delta": "+2 vs last week",
            "delta_color": "#34d399",
            "color": "#34d399",
            "percent": min(1.0, st.session_state.get("credit_apps_in_review", 0) / 40.0),
            "context": "Human queue avg: 41",
        },
        {
            "label": "Compliance Flags",
            "value": st.session_state.get("credit_flagged_cases"),
            "delta": "-1 cleared",
            "delta_color": "#f87171",
            "color": "#f87171",
            "percent": min(1.0, st.session_state.get("credit_flagged_cases", 0) / 15.0),
            "context": "Manual avg flags: 9",
        },
        {
            "label": "Avg AI Decision Time",
            "value": st.session_state.get("credit_avg_decision_time") or f"{credit_ai_minutes:.0f} min",
            "delta": "-3 min vs last cycle",
            "delta_color": "#60a5fa",
            "color": "#60a5fa",
            "percent": min(1.0, credit_ai_minutes / 45.0),
            "context": "AI adjudication speed",
        },
    ],
    icon="üè¶",
)

# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # LOGIN GATE
# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def login_block():
#     st.title("üîê Login to AI Credit Appraisal Platform")
#     c1, c2, c3 = st.columns([1, 1, 1])
#     with c1:
#         user = st.text_input("Username", placeholder="e.g. dzoan")
#     with c2:
#         email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
#     with c3:
#         pwd = st.text_input("Password", type="password", placeholder="Enter any password")

#     if st.button("Login", key="btn_credit_login", use_container_width=True):
#         if (user or "").strip() and (email or "").strip():
#             st.session_state["user_info"] = {
#                 "name": user.strip(),
#                 "email": email.strip(),
#                 "flagged": False,
#                 "timestamp": datetime.now(timezone.utc).isoformat(),
#             }
#             st.session_state["credit_logged_in"] = True
#             st.session_state["stage"] = "credit_agent"
#             st.rerun()
#         else:
#             st.error("‚ö†Ô∏è Please fill all fields before continuing.")


# if not st.session_state.get("credit_logged_in", False):
#     login_block()
#     st.stop()





# -----------------------------------------------------------
# CREDIT WORKFLOW ACTIVE ONLY IF CREDIT AGENT SELECTED
# -----------------------------------------------------------
ss = st.session_state
stage = ss.get("stage")

if stage == "credit_agent":

    # Header (from your render_credit_header() defined earlier)
    render_credit_header()


       
# ‚úÖ CREDIT APPRAISAL WORKFLOW TABS (1 ‚Üí 8)

    # ============================================================
    # üåà Colorized Tabs (Matches A‚ÜíH badge palette exactly)
    # ============================================================
    st.markdown("""
    <style>
    /* --- Layout adjustments for clean alignment --- */
    .stTabs [data-baseweb="tab-list"] {
    border-bottom: 2px solid #1e293b;
    justify-content: flex-start;
    flex-wrap: wrap;
    gap: .25rem;
    }

    /* --- Base style for all tabs --- */
    .stTabs [data-baseweb="tab"] {
    border-radius: .6rem;
    padding: .45rem .9rem;
    font-weight: 600;
    color: #fff !important;
    border: none;
    opacity: 0.95;
    transition: all 0.2s ease-in-out;
    }

    /* Hover and active effects */
    .stTabs [data-baseweb="tab"]:hover {
    transform: translateY(-2px);
    filter: brightness(1.1);
    opacity: 1;
    }
    .stTabs [data-baseweb="tab"][aria-selected="true"] {
    box-shadow: 0 0 8px rgba(255,255,255,0.15);
    transform: translateY(-1px);
    filter: brightness(1.1);
    }

    /* --- Tab Colors: A‚ÜíH palette --- */
    .stTabs [data-baseweb="tab"]:nth-child(1) { background: #1d4ed8; }  /* A) Blue */
    .stTabs [data-baseweb="tab"]:nth-child(2) { background: #059669; }  /* B) Green */
    .stTabs [data-baseweb="tab"]:nth-child(3) { background: #d97706; }  /* C) Amber */
    .stTabs [data-baseweb="tab"]:nth-child(4) { background: #7c3aed; }  /* D) Violet */
    .stTabs [data-baseweb="tab"]:nth-child(5) { background: #a16207; }  /* E) Gold */
    .stTabs [data-baseweb="tab"]:nth-child(6) { background: #e11d48; }  /* F) Red */
    .stTabs [data-baseweb="tab"]:nth-child(7) { background: #0ea5e9; }  /* G) Cyan */
    .stTabs [data-baseweb="tab"]:nth-child(8) { background: #64748b; }  /* H) Slate */
    </style>
    """, unsafe_allow_html=True)


# TABSLIST =====================================================
    tab_howto, tab_input, tab_clean, tab_run, tab_review, tab_train, tab_deploy, tab_handoff, tab_feedback = st.tabs([
        "üìò How-To",
        "1Ô∏è‚É£ üè¶ Synthetic Data Generator",
        "2Ô∏è‚É£ üßπ Anonymize & Sanitize Data",
        "3Ô∏è‚É£ ü§ñ Credit appraisal by AI assistant",
        "4Ô∏è‚É£ üßë‚Äç‚öñÔ∏è Human Review",
        "5Ô∏è‚É£ üîÅ Training (Feedback ‚Üí Retrain)",
        "6Ô∏è‚É£ üöÄ Deployment of Credit Model",
        "7Ô∏è‚É£ üì¶ Reporting & Handoff",
        "8Ô∏è‚É£ üó£Ô∏è Feedback & Feature Requests"
    ])

    with tab_howto:
        st.title("üìò How to Use This Agent")
        st.markdown("""
### What
An AI-driven credit appraisal agent that evaluates borrower risk using explainable machine-learning models and policy rules.

### Goal
To help financial institutions automate loan decisioning while maintaining transparency, compliance, and human oversight.

### How
1. Upload borrower or loan data, or import datasets from Kaggle or Hugging Face.
2. The agent cleans and anonymizes data, detects target columns, and trains predictive models (LightGBM or HF tabular).
3. It applies business policies such as LTV, DTI, and score thresholds, generating real-time approve/reject/review decisions.
4. Outputs include explainable model charts, confidence scores, and decision summaries.

### So What (Benefits)
- Speeds up credit decisioning from hours to seconds.
- Reduces manual errors and human bias.
- Provides clear, auditable AI logic for regulators.
- Continuously learns from feedback to improve accuracy.

### What Next
1. Try it now‚Äîupload your dataset or choose a public sample.
2. Contact our team to tailor the rules and credit policies to your needs.
3. Once satisfied, import the trained model and decision engine into your production environment to power real-world loan approvals.
        """)

else:
    # Safe placeholders when not on the Credit Agent stage
    tab_input = st.container()
    tab_clean = st.container()
    tab_run = st.container()
    tab_review = st.container()
    tab_train = st.container()
    tab_deploy = st.container()
    tab_handoff = st.container()
    tab_feedback = st.container()




# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GLOBAL UTILS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

BANNED_NAMES = {"race", "gender", "religion", "ethnicity", "ssn", "national_id"}
PII_COLS = {"customer_name", "name", "email", "phone", "address", "ssn", "national_id", "dob"}

EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
PHONE_RE = re.compile(r"\+?\d[\d\-\s]{6,}\d")


def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:
    return df.loc[:, ~df.columns.duplicated(keep="last")]


def scrub_text_pii(s):
    if not isinstance(s, str):
        return s
    s = EMAIL_RE.sub("", s)
    s = PHONE_RE.sub("", s)
    return s.strip()


def drop_pii_columns(df: pd.DataFrame):
    original_cols = list(df.columns)
    keep_cols = [c for c in original_cols if all(k not in c.lower() for k in PII_COLS)]
    dropped = [c for c in original_cols if c not in keep_cols]
    out = df[keep_cols].copy()
    for c in out.select_dtypes(include="object"):
        out[c] = out[c].apply(scrub_text_pii)
    return dedupe_columns(out), dropped


def strip_policy_banned(df: pd.DataFrame) -> pd.DataFrame:
    keep = []
    for c in df.columns:
        cl = c.lower()
        if cl in BANNED_NAMES:
            continue
        keep.append(c)
    return df[keep]


def append_user_info(df: pd.DataFrame) -> pd.DataFrame:
    meta = st.session_state["user_info"]
    out = df.copy()
    out["session_user_name"] = meta["name"]
    out["session_user_email"] = meta["email"]
    out["session_flagged"] = meta["flagged"]
    out["created_at"] = meta["timestamp"]
    return dedupe_columns(out)


def save_to_runs(df: pd.DataFrame, prefix: str) -> str:
    #ts = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M")
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M")
    flag_suffix = "_FLAGGED" if st.session_state["user_info"]["flagged"] else ""
    fname = f"{prefix}_{ts}{flag_suffix}.csv"
    fpath = os.path.join(RUNS_DIR, fname)
    dedupe_columns(df).to_csv(fpath, index=False)
    return fpath


def try_json(x):
    if isinstance(x, (dict, list)):
        return x
    if not isinstance(x, str):
        return None
    try:
        return json.loads(x)
    except Exception:
        return None


def _safe_json(x):
    if isinstance(x, dict):
        return x
    if isinstance(x, str) and x.strip():
        try:
            return json.loads(x)
        except Exception:
            return {}
    return {}


def fmt_currency_label(base: str) -> str:
    sym = st.session_state.get("currency_symbol", "")
    return f"{base} ({sym})" if sym else base


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CURRENCY CATALOG
CURRENCY_OPTIONS = {
    # code: (label, symbol, fx to apply on USD-like base generated numbers)
    "USD": ("USD $", "$", 1.0),
    "EUR": ("EUR ‚Ç¨", "‚Ç¨", 0.93),
    "GBP": ("GBP ¬£", "¬£", 0.80),
    "JPY": ("JPY ¬•", "¬•", 150.0),
    "VND": ("VND ‚Ç´", "‚Ç´", 24000.0),
}


def set_currency_defaults():
    if "currency_code" not in st.session_state:
        st.session_state["currency_code"] = "USD"
    label, symbol, fx = CURRENCY_OPTIONS[st.session_state["currency_code"]]
    st.session_state["currency_label"] = label
    st.session_state["currency_symbol"] = symbol
    st.session_state["currency_fx"] = fx


set_currency_defaults()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DASHBOARD HELPERS (Plotly, dark theme)
def _kpi_card(label: str, value: str, sublabel: str | None = None):
    st.markdown(
        f"""
        <div style="background:#0e1117;border:1px solid #2a2f3e;border-radius:12px;padding:14px 16px;margin-bottom:10px;">
          <div style="font-size:12px;color:#9aa4b2;text-transform:uppercase;letter-spacing:.06em;">{label}</div>
          <div style="font-size:28px;font-weight:700;color:#e6edf3;line-height:1.1;margin-top:2px;">{value}</div>
          {f'<div style="font-size:12px;color:#9aa4b2;margin-top:6px;">{sublabel}</div>' if sublabel else ''}
        </div>
        """,
        unsafe_allow_html=True,
    )


def render_credit_dashboard(df: pd.DataFrame, currency_symbol: str = ""):
    """
    Renders the whole dashboard (TOP-10s ‚Üí Opportunities ‚Üí KPIs & pies/bars ‚Üí Mix table).
    Keeps decision filter in the table only.
    """
    if df is None or df.empty:
        st.info("No data to visualize yet.")
        return

    cols = df.columns

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TOP 10s FIRST ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üîù Top 10 Snapshot")

    # Top 10 loans approved
    if {"decision", "loan_amount", "application_id"} <= set(cols):
        top_approved = df[df["decision"].astype(str).str.lower() == "approved"].copy()
        if not top_approved.empty:
            top_approved = top_approved.sort_values("loan_amount", ascending=False).head(10)
            fig = px.bar(
                top_approved,
                x="loan_amount",
                y="application_id",
                orientation="h",
                title="Top 10 Approved Loans",
                labels={"loan_amount": f"Loan Amount {currency_symbol}", "application_id": "Application"},
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No approved loans available to show top 10.")

    # Top 10 collateral types by average value
    if {"collateral_type", "collateral_value"} <= set(cols):
        cprof = df.groupby("collateral_type", dropna=False).agg(
            avg_value=("collateral_value", "mean"),
            cnt=("collateral_type", "count")
        ).reset_index()
        if not cprof.empty:
            cprof = cprof.sort_values("avg_value", ascending=False).head(10)
            fig = px.bar(
                cprof,
                x="avg_value",
                y="collateral_type",
                orientation="h",
                title="Top 10 Collateral Types (Avg Value)",
                labels={"avg_value": f"Avg Value {currency_symbol}", "collateral_type": "Collateral Type"},
                hover_data=["cnt"]
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    # Top 10 reasons for denial (from rule_reasons False flags)
    if "rule_reasons" in cols and "decision" in cols:
        denied = df[df["decision"].astype(str).str.lower() == "denied"].copy()
        reasons_count = {}
        for _, r in denied.iterrows():
            rr = _safe_json(r.get("rule_reasons"))
            if isinstance(rr, dict):
                for k, v in rr.items():
                    if v is False:
                        reasons_count[k] = reasons_count.get(k, 0) + 1
        if reasons_count:
            items = pd.DataFrame(sorted(reasons_count.items(), key=lambda x: x[1], reverse=True),
                                 columns=["reason", "count"]).head(10)
            fig = px.bar(
                items, x="count", y="reason", orientation="h",
                title="Top 10 Reasons for Denial",
                labels={"count": "Count", "reason": "Rule"},
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No denial reasons detected.")

    # Top 10 loan officer performance (approval rate) if officer column present
    officer_col = None
    for guess in ("loan_officer", "officer", "reviewed_by", "session_user_name"):
        if guess in cols:
            officer_col = guess
            break
    if officer_col and "decision" in cols:
        perf = (
            df.assign(is_approved=(df["decision"].astype(str).str.lower() == "approved").astype(int))
              .groupby(officer_col, dropna=False)["is_approved"]
              .agg(approved_rate="mean", n="count")
              .reset_index()
        )
        if not perf.empty:
            perf["approved_rate_pct"] = (perf["approved_rate"] * 100).round(1)
            perf = perf.sort_values(["approved_rate_pct", "n"], ascending=[False, False]).head(10)
            fig = px.bar(
                perf, x="approved_rate_pct", y=officer_col, orientation="h",
                title="Top 10 Loan Officer Approval Rate (this batch)",
                labels={"approved_rate_pct": "Approval Rate (%)", officer_col: "Officer"},
                hover_data=["n"]
            )
            fig.update_layout(margin=dict(l=10, r=10, t=50, b=10), height=420, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ OPPORTUNITIES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üí° Opportunities")

    # Short-term loan opportunities (simple heuristic)
    opp_rows = []
    if {"income", "loan_amount"}.issubset(cols):
        term_col = "loan_term_months" if "loan_term_months" in cols else ("loan_duration_months" if "loan_duration_months" in cols else None)
        if term_col:
            for _, r in df.iterrows():
                inc = float(r.get("income", 0) or 0)
                amt = float(r.get("loan_amount", 0) or 0)
                term = int(r.get(term_col, 0) or 0)
                dti = float(r.get("DTI", 0) or 0)
                if (term >= 36) and (amt <= inc * 0.8) and (dti <= 0.45):
                    opp_rows.append({
                        "application_id": r.get("application_id"),
                        "suggested_term": 24,
                        "loan_amount": amt,
                        "income": inc,
                        "DTI": dti,
                        "note": "Candidate for short-term plan (<=24m) based on affordability."
                    })
    if opp_rows:
        st.markdown("#### üìé Short-Term Loan Candidates")
        st.dataframe(pd.DataFrame(opp_rows).head(25), use_container_width=True, height=320)
    else:
        st.info("No short-term loan candidates identified in this batch.")

    st.markdown("#### üîÅ Buyback / Consolidation Beneficiaries")
    candidates = []
    need = {"decision", "existing_debt", "loan_amount", "DTI"}
    if need <= set(cols):
        for _, r in df.iterrows():
            dec = str(r.get("decision", "")).lower()
            debt = float(r.get("existing_debt", 0) or 0)
            loan = float(r.get("loan_amount", 0) or 0)
            dti = float(r.get("DTI", 0) or 0)
            proposal = _safe_json(r.get("proposed_consolidation_loan", {}))
            has_bb = bool(proposal)

            if dec == "denied" or dti > 0.45 or debt > loan:
                benefit_score = round((debt / (loan + 1e-6)) * 0.4 + dti * 0.6, 2)
                candidates.append({
                    "application_id": r.get("application_id"),
                    "customer_type": r.get("customer_type"),
                    "existing_debt": debt,
                    "loan_amount": loan,
                    "DTI": dti,
                    "collateral_type": r.get("collateral_type"),
                    "buyback_proposed": has_bb,
                    "buyback_amount": proposal.get("buyback_amount") if has_bb else None,
                    "benefit_score": benefit_score,
                    "note": proposal.get("note") if has_bb else None
                })
    if candidates:
        cand_df = pd.DataFrame(candidates).sort_values("benefit_score", ascending=False)
        st.dataframe(cand_df.head(25), use_container_width=True, height=380)
    else:
        st.info("No additional buyback beneficiaries identified.")

    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PORTFOLIO KPIs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üìà Portfolio Snapshot")
    c1, c2, c3, c4 = st.columns(4)

    # Approval rate
    if "decision" in cols:
        total = len(df)
        approved = int((df["decision"].astype(str).str.lower() == "approved").sum())
        rate = (approved / total * 100) if total else 0.0
        with c1: _kpi_card("Approval Rate", f"{rate:.1f}%", f"{approved} of {total}")

    # Avg approved loan amount
    if {"decision", "loan_amount"} <= set(cols):
        ap = df[df["decision"].astype(str).str.lower() == "approved"]["loan_amount"]
        avg_amt = ap.mean() if len(ap) else 0.0
        with c2: _kpi_card("Avg Approved Amount", f"{currency_symbol}{avg_amt:,.0f}")

    # Decision time (if present)
    if {"created_at", "decision_at"} <= set(cols):
        try:
            t = (pd.to_datetime(df["decision_at"]) - pd.to_datetime(df["created_at"])).dt.total_seconds() / 60.0
            avg_min = float(t.mean())
            with c3: _kpi_card("Avg Decision Time", f"{avg_min:.1f} min")
        except Exception:
            with c3: _kpi_card("Avg Decision Time", "‚Äî")

    # Non-bank share
    if "customer_type" in cols:
        nb = int((df["customer_type"].astype(str).str.lower() == "non-bank").sum())
        total = len(df)
        share = (nb / total * 100) if total else 0.0
        with c4: _kpi_card("Non-bank Share", f"{share:.1f}%", f"{nb} of {total}")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ COMPOSITION & RISK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("## üß≠ Composition & Risk")

    # Approval vs Denial (pie)
    if "decision" in cols:
        pie_df = df["decision"].value_counts().rename_axis("Decision").reset_index(name="Count")
        fig = px.pie(pie_df, names="Decision", values="Count", title="Decision Mix")
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Avg DTI / LTV by decision (grouped bars)
    have_dti = "DTI" in cols
    have_ltv = "LTV" in cols
    if "decision" in cols and (have_dti or have_ltv):
        agg_map = {}
        if have_dti: agg_map["avg_DTI"] = ("DTI", "mean")
        if have_ltv: agg_map["avg_LTV"] = ("LTV", "mean")
        grp = df.groupby("decision").agg(**agg_map).reset_index()
        melted = grp.melt(id_vars=["decision"], var_name="metric", value_name="value")
        fig = px.bar(melted, x="decision", y="value", color="metric",
                     barmode="group", title="Average DTI / LTV by Decision")
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Loan term mix (stacked)
    term_col = "loan_term_months" if "loan_term_months" in cols else ("loan_duration_months" if "loan_duration_months" in cols else None)
    if term_col and "decision" in cols:
        mix = df.groupby([term_col, "decision"]).size().reset_index(name="count")
        fig = px.bar(
            mix, x=term_col, y="count", color="decision", title="Loan Term Mix",
            labels={term_col: "Term (months)", "count": "Count"}, barmode="stack"
        )
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Collateral avg value by type (bar)
    if {"collateral_type", "collateral_value"} <= set(cols):
        cprof = df.groupby("collateral_type").agg(
            avg_col=("collateral_value", "mean"),
            cnt=("collateral_type", "count")
        ).reset_index()
        fig = px.bar(
            cprof.sort_values("avg_col", ascending=False),
            x="collateral_type", y="avg_col",
            title=f"Avg Collateral Value by Type ({currency_symbol})",
            hover_data=["cnt"]
        )
        fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
        st.plotly_chart(fig, use_container_width=True)

    # Top proposed plans (horizontal bar)
    if "proposed_loan_option" in cols:
        plans = df["proposed_loan_option"].dropna().astype(str)
        if len(plans) > 0:
            plan_types = []
            for s in plans:
                p = _safe_json(s)
                plan_types.append(p.get("type") if isinstance(p, dict) and "type" in p else s)
            plan_df = pd.Series(plan_types).value_counts().head(10).rename_axis("plan").reset_index(name="count")
            fig = px.bar(
                plan_df, x="count", y="plan", orientation="h",
                title="Top 10 Proposed Plans"
            )
            fig.update_layout(margin=dict(l=10, r=10, t=60, b=10), height=360, template="plotly_dark")
            st.plotly_chart(fig, use_container_width=True)

    # Customer mix table (bank vs non-bank)
    if "customer_type" in cols:
        mix = df["customer_type"].value_counts().rename_axis("Customer Type").reset_index(name="Count")
        mix["Ratio"] = (mix["Count"] / mix["Count"].sum()).round(3)
        st.markdown("### üë• Customer Mix")
        st.dataframe(mix, use_container_width=True, height=220)




# DATA GENERATORS

def generate_raw_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    names = ["Alice Nguyen","Bao Tran","Chris Do","Duy Le","Emma Tran",
             "Felix Nguyen","Giang Ho","Hanh Vo","Ivan Pham","Julia Ngo"]
    emails = [f"{n.split()[0].lower()}.{n.split()[1].lower()}@gmail.com" for n in names]
    addrs = [
        "23 Elm St, Boston, MA","19 Pine Ave, San Jose, CA","14 High St, London, UK",
        "55 Nguyen Hue, Ho Chi Minh","78 Oak St, Chicago, IL","10 Broadway, New York, NY",
        "8 Rue Lafayette, Paris, FR","21 K√∂nigstr, Berlin, DE","44 Maple Dr, Los Angeles, CA","22 Bay St, Toronto, CA"
    ]
    is_non = rng.random(n) < non_bank_ratio
    cust_type = np.where(is_non, "non-bank", "bank")

    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "customer_name": np.random.choice(names, n),
        "email": np.random.choice(emails, n),
        "phone": [f"+1-202-555-{1000+i:04d}" for i in range(n)],
        "address": np.random.choice(addrs, n),
        "national_id": rng.integers(10_000_000, 99_999_999, n),
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["real_estate","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
        "current_loans": rng.integers(0, 5, n),
        "customer_type": cust_type,
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))

    fx = st.session_state["currency_fx"]
    for c in ("income", "loan_amount", "collateral_value", "assets_owned", "existing_debt"):
        df[c] = (df[c] * fx).round(2)
    df["currency_code"] = st.session_state["currency_code"]
    return dedupe_columns(df)


def generate_anon_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    is_non = rng.random(n) < non_bank_ratio
    cust_type = np.where(is_non, "non-bank", "bank")

    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["real_estate","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
        "current_loans": rng.integers(0, 5, n),
        "customer_type": cust_type,
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))

    fx = st.session_state["currency_fx"]
    for c in ("income", "loan_amount", "collateral_value", "assets_owned", "existing_debt"):
        df[c] = (df[c] * fx).round(2)
    df["currency_code"] = st.session_state["currency_code"]
    return dedupe_columns(df)


def to_agent_schema(df: pd.DataFrame) -> pd.DataFrame:
    """
    Harmonize to the server-side agent‚Äôs expected schema.
    """
    out = df.copy()
    n = len(out)
    if "employment_years" not in out.columns:
        out["employment_years"] = out.get("employment_length", 0)
    if "debt_to_income" not in out.columns:
        if "DTI" in out.columns:
            out["debt_to_income"] = out["DTI"].astype(float)
        elif "existing_debt" in out.columns and "income" in out.columns:
            denom = out["income"].replace(0, np.nan)
            dti = (out["existing_debt"] / denom).fillna(0.0)
            out["debt_to_income"] = dti.clip(0, 10)
        else:
            out["debt_to_income"] = 0.0
    rng = np.random.default_rng(12345)
    if "credit_history_length" not in out.columns:
        out["credit_history_length"] = rng.integers(0, 30, n)
    if "num_delinquencies" not in out.columns:
        out["num_delinquencies"] = np.minimum(rng.poisson(0.2, n), 10)
    if "requested_amount" not in out.columns:
        out["requested_amount"] = out.get("loan_amount", 0)
    if "loan_term_months" not in out.columns:
        out["loan_term_months"] = out.get("loan_duration_months", 0)
    return dedupe_columns(out)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üè¶ TAB 1 ‚Äî Synthetic Data Generator
with tab_input:
    st.subheader("üè¶ Synthetic Credit Data Generator")

    # Currency selector (before generation)
    c1, c2 = st.columns([1, 2])
    with c1:
        code = st.selectbox(
            "Currency",
            list(CURRENCY_OPTIONS.keys()),
            index=list(CURRENCY_OPTIONS.keys()).index(st.session_state["currency_code"]),
            help="All monetary fields will be in this local currency."
        )
        if code != st.session_state["currency_code"]:
            st.session_state["currency_code"] = code
            set_currency_defaults()
    with c2:
        st.markdown(
            f"""
            <div style='background-color:#1e293b; padding:12px 16px; border-radius:8px;'>
                <span style='font-weight:600; color:#f8fafc;'>
                    üí∞ Amounts will be generated in
                    <span style='color:#4ade80;'>{st.session_state['currency_label']}</span>.
                </span>
            </div>
            """,
            unsafe_allow_html=True,
        )

    rows = st.slider("Number of rows to generate", 50, 2000, 200, step=50)
    non_bank_ratio = st.slider("Share of non-bank customers", 0.0, 1.0, 0.30, 0.05)

    colA, colB = st.columns(2)
    with colA:
        if st.button("üî¥ Generate RAW Synthetic Data (with PII)", use_container_width=True):
            raw_df = append_user_info(generate_raw_synthetic(rows, non_bank_ratio))
            st.session_state.synthetic_raw_df = raw_df
            raw_path = save_to_runs(raw_df, "synthetic_raw")
            st.success(f"Generated RAW (PII) dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {raw_path}")
            st.dataframe(raw_df.head(10), use_container_width=True)
            st.download_button(
                "‚¨áÔ∏è Download RAW CSV",
                raw_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(raw_path),
                "text/csv"
            )

    with colB:
        if st.button("üü¢ Generate ANON Synthetic Data (ready for agent)", use_container_width=True):
            anon_df = append_user_info(generate_anon_synthetic(rows, non_bank_ratio))
            st.session_state.synthetic_df = anon_df
            anon_path = save_to_runs(anon_df, "synthetic_anon")
            st.success(f"Generated ANON dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {anon_path}")
            st.dataframe(anon_df.head(10), use_container_width=True)
            st.download_button(
                "‚¨áÔ∏è Download ANON CSV",
                anon_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(anon_path),
                "text/csv"
            )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üßπ TAB 2 ‚Äî Anonymize & Sanitize Data
with tab_clean:
    st.subheader("üßπ Upload & Anonymize Customer Data (PII columns will be DROPPED)")
    st.markdown("Upload your **real CSV**. We drop PII columns and scrub emails/phones in text fields.")

    uploaded = st.file_uploader("Upload CSV file", type=["csv"])
    if uploaded:
        try:
            df = pd.read_csv(uploaded)
        except Exception as e:
            st.error(f"Could not read CSV: {e}")
            st.stop()

        st.write("üìä Original Data Preview:")
        st.dataframe(dedupe_columns(df.head(5)), use_container_width=True)

        sanitized, dropped_cols = drop_pii_columns(df)
        sanitized = append_user_info(sanitized)
        sanitized = dedupe_columns(sanitized)
        st.session_state.anonymized_df = sanitized

        st.success(f"Dropped PII columns: {sorted(dropped_cols) if dropped_cols else 'None'}")
        st.write("‚úÖ Sanitized Data Preview:")
        st.dataframe(sanitized.head(5), use_container_width=True)

        fpath = save_to_runs(sanitized, "anonymized")
        st.success(f"Saved anonymized file: {fpath}")
        st.download_button(
            "‚¨áÔ∏è Download Clean Data",
            sanitized.to_csv(index=False).encode("utf-8"),
            os.path.basename(fpath),
            "text/csv"
        )
    else:
        st.info("Choose a CSV to see the sanitize flow.", icon="‚ÑπÔ∏è")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ü§ñ TAB 3 ‚Äî Credit appraisal by AI assistant
with tab_run:
    st.subheader("ü§ñ Credit appraisal by AI assistant")
    # Anchor for loopback link from Training tab
    st.markdown('<a name="credit-appraisal-stage"></a>', unsafe_allow_html=True)
    st.markdown(
        """
        **How to use this stage**

        1. Pick a trained model (or promote a new one from Stage‚ÄØ5).
        2. Set the Local LLM + host flavor so narratives run on the right hardware.
        3. Choose your data source, then fine-tune rules and run the appraisal workflow.
        4. Export results or loop back into Stage‚ÄØ4/5 for review and retraining.
        """,
        help="Quick reference so operators can move from Intake ‚Üí Review ‚Üí Training without leaving this tab.",
    )

    # Production model banner (optional)
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.status_code == 200:
            meta = resp.json()
            if meta.get("has_production"):
                ver = (meta.get("meta") or {}).get("version", "1.x")
                src = (meta.get("meta") or {}).get("source", "production")
                st.success(f"üü¢ Production model active ‚Äî version: {ver} ‚Ä¢ source: {src}")
            else:
                st.warning("‚ö†Ô∏è No production model promoted yet ‚Äî using baseline.")
        else:
            st.info("‚ÑπÔ∏è Could not fetch production model meta.")
    except Exception:
        st.info("‚ÑπÔ∏è Production meta unavailable.")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß© Model Selection (list all trained models) ‚Äî Hardcoded Stable Version
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    from datetime import datetime
    import os, shutil, streamlit as st

    # Hardcoded absolute paths (your confirmed working setup)
    trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/trained"
    production_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/credit_appraisal/models/production"

    st.caption(f"üì¶ Trained dir = `{trained_dir}`")
    st.caption(f"üì¶ Production dir = `{production_dir}`")

    # ‚îÄ‚îÄ Refresh models list
    if st.button("‚Üª Refresh models", key="credit_refresh_models"):
        st.session_state.pop("selected_trained_model", None)
        st.rerun()

    # ‚îÄ‚îÄ Collect models
    models = []
    if os.path.isdir(trained_dir):
        for f in os.listdir(trained_dir):
            if f.endswith(".joblib"):
                fpath = os.path.join(trained_dir, f)
                ctime = os.path.getctime(fpath)
                created = datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
                models.append((f, fpath, created))
    else:
        st.error(f"‚ùå Trained dir not found: {trained_dir}")

    # ‚îÄ‚îÄ Show list if found
    if models:
        models.sort(key=lambda x: os.path.getctime(x[1]), reverse=True)
        display_names = [f"{m[0]} ‚Äî {m[2]}" for m in models]
        preferred_display = st.session_state.get("credit_selected_display")
        default_index = 0
        if preferred_display in display_names:
            default_index = display_names.index(preferred_display)
        elif st.session_state.get("selected_trained_model"):
            selected_path = st.session_state["selected_trained_model"]
            for idx, (_, path, _) in enumerate(models):
                if path == selected_path:
                    default_index = idx
                    break

        selected_display = st.selectbox(
            "üì¶ Select trained model to use",
            display_names,
            index=default_index,
            key="credit_model_select",
        )
        selected_model = models[display_names.index(selected_display)][1]
        st.success(f"‚úÖ Using model: {os.path.basename(selected_model)}")

        st.session_state["selected_trained_model"] = selected_model
        st.session_state["credit_selected_display"] = selected_display

        # ‚îÄ‚îÄ Promote model
        if st.button("üöÄ Promote this model to Production"):
            try:
                os.makedirs(production_dir, exist_ok=True)
                prod_path = os.path.join(production_dir, "model.joblib")
                shutil.copy2(selected_model, prod_path)
                st.success(f"‚úÖ Model promoted to production: {prod_path}")
            except Exception as e:
                st.error(f"‚ùå Promotion failed: {e}")
    else:
        st.warning("‚ö†Ô∏è No trained models found ‚Äî train one in Step 5 first.")

    OPENSTACK_FLAVORS = {
        "m4.medium": "4 vCPU / 8 GB RAM ‚Äî CPU-only small",
        "m8.large": "8 vCPU / 16 GB RAM ‚Äî CPU-only medium",
        "g1.a10.1": "8 vCPU / 32 GB RAM + 1√óA10 24GB",
        "g1.l40.1": "16 vCPU / 64 GB RAM + 1√óL40 48GB",
        "g2.a100.1": "24 vCPU / 128 GB RAM + 1√óA100 80GB",
    }
    with st.expander("üß† Local LLM & Hardware Profile", expanded=True):
        st.info(
            "Use CPU recommended LLMs first for quick narratives. Switch to GPU picks only when you need deeper reasoning or longer context.",
            icon="‚ö°",
        )
        selected_llm = render_llm_selector(context="credit_appraisal")
        st.session_state["credit_llm_model_label"] = selected_llm["model"]
        st.session_state["credit_llm_model"] = selected_llm["value"]
        llm_value = selected_llm["value"]
        flavor = st.selectbox(
            "OpenStack flavor / host profile",
            list(OPENSTACK_FLAVORS.keys()),
            index=0,
            key="credit_flavor",
        )
        st.caption(OPENSTACK_FLAVORS[flavor])
        st.caption("These are passed to the API as hints; your API can choose Ollama/Flowise backends accordingly.")

    # 2) Data Source
    data_choice = st.selectbox(
        "Select Data Source",
        [
            "Use synthetic (ANON)",
            "Use synthetic (RAW ‚Äì auto-sanitize)",
            "Use anonymized dataset",
            "Upload manually",
        ],
    )
    # Stash the LLM toggle + default selection for downstream payload
    use_llm = st.checkbox("Use LLM narrative", value=False)
    if "credit_llm_model" not in st.session_state:
        st.session_state["credit_llm_model"] = llm_value if "llm_value" in locals() else None
    llm_value = st.session_state.get("credit_llm_model", llm_value if "llm_value" in locals() else None)
    agent_name = "credit_appraisal"

    if data_choice == "Upload manually":
        up = st.file_uploader("Upload your CSV", type=["csv"], key="manual_upload_run_file")
        if up is not None:
            st.session_state["manual_upload_name"] = up.name
            st.session_state["manual_upload_bytes"] = up.getvalue()
            st.success(f"File staged: {up.name} ({len(st.session_state['manual_upload_bytes'])} bytes)")

    # 3) Rules
    st.markdown("### ‚öôÔ∏è Decision Rule Set")
    rule_mode = st.radio(
        "Choose rule mode",
        ["Classic (bank-style metrics)", "NDI (Net Disposable Income) ‚Äî simple"],
        index=0,
        help="NDI = income - all monthly obligations. Approve if NDI and NDI ratio pass thresholds.",
    )

    CLASSIC_DEFAULTS = {
        "max_dti": 0.45,
        "min_emp_years": 2,
        "min_credit_hist": 3,
        "salary_floor": 3000,
        "max_delinquencies": 2,
        "max_current_loans": 3,
        "req_min": 1000,
        "req_max": 200000,
        "loan_terms": [12, 24, 36, 48, 60],
        "threshold": 0.45,
        "target_rate": None,
        "random_band": True,
        "min_income_debt_ratio": 0.35,
        "compounded_debt_factor": 1.0,
        "monthly_debt_relief": 0.50,
    }
    NDI_DEFAULTS = {"ndi_value": 800.0, "ndi_ratio": 0.50, "threshold": 0.45, "target_rate": None, "random_band": True}

    if "classic_rules" not in st.session_state:
        st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    if "ndi_rules" not in st.session_state:
        st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    def reset_classic(): st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    def reset_ndi():     st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    if rule_mode.startswith("Classic"):
        with st.expander("Classic Metrics (with Reset)", expanded=True):
            rc = st.session_state.classic_rules
            r1, r2, r3 = st.columns(3)
            with r1:
                rc["max_dti"] = st.slider("Max Debt-to-Income (DTI)", 0.0, 1.0, rc["max_dti"], 0.01)
                rc["min_emp_years"] = st.number_input("Min Employment Years", 0, 40, rc["min_emp_years"])
                rc["min_credit_hist"] = st.number_input("Min Credit History (years)", 0, 40, rc["min_credit_hist"])
            with r2:
                rc["salary_floor"] = st.number_input(
                    "Minimum Monthly Salary", 0, 1_000_000_000, rc["salary_floor"], step=1000, help=fmt_currency_label("in local currency")
                )
                rc["max_delinquencies"] = st.number_input("Max Delinquencies", 0, 10, rc["max_delinquencies"])
                rc["max_current_loans"] = st.number_input("Max Current Loans", 0, 10, rc["max_current_loans"])
            with r3:
                rc["req_min"] = st.number_input(fmt_currency_label("Requested Amount Min"), 0, 10_000_000_000, rc["req_min"], step=1000)
                rc["req_max"] = st.number_input(fmt_currency_label("Requested Amount Max"), 0, 10_000_000_000, rc["req_max"], step=1000)
                rc["loan_terms"] = st.multiselect("Allowed Loan Terms (months)", [12, 24, 36, 48, 60, 72], default=rc["loan_terms"])

            st.markdown("#### üßÆ Debt Pressure Controls")
            d1, d2, d3 = st.columns(3)
            with d1:
                rc["min_income_debt_ratio"] = st.slider(
                    "Min Income / (Compounded Debt) Ratio", 0.10, 2.00, rc["min_income_debt_ratio"], 0.01
                )
            with d2:
                rc["compounded_debt_factor"] = st.slider(
                    "Compounded Debt Factor (√ó requested)", 0.5, 3.0, rc["compounded_debt_factor"], 0.1
                )
            with d3:
                rc["monthly_debt_relief"] = st.slider("Monthly Debt Relief Factor", 0.10, 1.00, rc["monthly_debt_relief"], 0.05)

            st.markdown("---")
            c1, c2, c3 = st.columns([1, 1, 1])
            with c1:
                use_target = st.toggle("üéØ Use target approval rate", value=(rc["target_rate"] is not None))
            with c2:
                rc["random_band"] = st.toggle(
                    "üé≤ Randomize approval band (20‚Äì60%) when no target", value=rc["random_band"]
                )
            with c3:
                if st.button("‚Ü©Ô∏è Reset to defaults"):
                    reset_classic()
                    st.rerun()

            if use_target:
                rc["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rc["target_rate"] or 0.40, 0.01)
                rc["threshold"] = None
            else:
                rc["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rc["threshold"], 0.01)
                rc["target_rate"] = None
    else:
        with st.expander("NDI Metrics (with Reset)", expanded=True):
            rn = st.session_state.ndi_rules
            n1, n2 = st.columns(2)
            with n1:
                rn["ndi_value"] = st.number_input(
                    fmt_currency_label("Min NDI (Net Disposable Income) per month"),
                    0.0,
                    1e12,
                    float(rn["ndi_value"]),
                    step=50.0,
                )
            with n2:
                rn["ndi_ratio"] = st.slider("Min NDI / Income ratio", 0.0, 1.0, float(rn["ndi_ratio"]), 0.01)
            st.caption("NDI = income - all monthly obligations (rent, food, loans, cards, etc.).")

            st.markdown("---")
            c1, c2, c3 = st.columns([1, 1, 1])
            with c1:
                use_target = st.toggle("üéØ Use target approval rate", value=(rn["target_rate"] is not None))
            with c2:
                rn["random_band"] = st.toggle(
                    "üé≤ Randomize approval band (20‚Äì60%) when no target", value=rn["random_band"]
                )
            with c3:
                if st.button("‚Ü©Ô∏è Reset to defaults (NDI)"):
                    reset_ndi()
                    st.rerun()

            if use_target:
                rn["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rn["target_rate"] or 0.40, 0.01)
                rn["threshold"] = None
            else:
                rn["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rn["threshold"], 0.01)
                rn["target_rate"] = None

    # Helper used below (your function name later referenced as json_to_dataframe in the draft)
    def json_to_df(obj) -> pd.DataFrame:
        if obj is None:
            return pd.DataFrame()
        if isinstance(obj, pd.DataFrame):
            return obj
        if isinstance(obj, bytes):
            try:
                obj = obj.decode("utf-8", errors="ignore")
            except Exception:
                return pd.DataFrame({"value": [repr(obj)]})
        if isinstance(obj, str):
            obj = obj.strip()
            if not obj:
                return pd.DataFrame()
            try:
                j = json.loads(obj)
                return json_to_df(j)
            except Exception:
                lines = [ln for ln in obj.splitlines() if ln.strip()]
                return pd.DataFrame({"value": lines}) if lines else pd.DataFrame()
        if isinstance(obj, list):
            if len(obj) == 0:
                return pd.DataFrame()
            if all(isinstance(x, dict) for x in obj):
                try:
                    return pd.json_normalize(obj)
                except Exception:
                    return pd.DataFrame(obj)
            if all(isinstance(x, list) for x in obj):
                return pd.DataFrame({"row": obj})
            return pd.DataFrame({"value": obj})
        if isinstance(obj, dict):
            for key in ("rows", "data", "result", "results", "items", "records"):
                if key in obj and isinstance(obj[key], (list, dict)):
                    return json_to_df(obj[key])
            try:
                return pd.json_normalize(obj)
            except Exception:
                return pd.DataFrame([obj])
        return pd.DataFrame({"value": [obj]})

    # 4) Run
    if st.button("üöÄ Run Agent", use_container_width=True):
        try:
            files = None
            data: Dict[str, Any] = {
                "use_llm_narrative": str(use_llm).lower(),
                "llm_model": llm_value,
                "hardware_flavor": flavor,
                "currency_code": st.session_state["currency_code"],
                "currency_symbol": st.session_state["currency_symbol"],
            }
            if rule_mode.startswith("Classic"):
                rc = st.session_state.classic_rules
                data.update(
                    {
                        "min_employment_years": str(rc["min_emp_years"]),
                        "max_debt_to_income": str(rc["max_dti"]),
                        "min_credit_history_length": str(rc["min_credit_hist"]),
                        "max_num_delinquencies": str(rc["max_delinquencies"]),
                        "max_current_loans": str(rc["max_current_loans"]),
                        "requested_amount_min": str(rc["req_min"]),
                        "requested_amount_max": str(rc["req_max"]),
                        "loan_term_months_allowed": ",".join(map(str, rc["loan_terms"])) if rc["loan_terms"] else "",
                        "min_income_debt_ratio": str(rc["min_income_debt_ratio"]),
                        "compounded_debt_factor": str(rc["compounded_debt_factor"]),
                        "monthly_debt_relief": str(rc["monthly_debt_relief"]),
                        "salary_floor": str(rc["salary_floor"]),
                        "threshold": "" if rc["threshold"] is None else str(rc["threshold"]),
                        "target_approval_rate": "" if rc["target_rate"] is None else str(rc["target_rate"]),
                        "random_band": str(rc["random_band"]).lower(),
                        "random_approval_band": str(rc["random_band"]).lower(),
                        "rule_mode": "classic",
                    }
                )
            else:
                rn = st.session_state.ndi_rules
                data.update(
                    {
                        "ndi_value": str(rn["ndi_value"]),
                        "ndi_ratio": str(rn["ndi_ratio"]),
                        "threshold": "" if rn["threshold"] is None else str(rn["threshold"]),
                        "target_approval_rate": "" if rn["target_rate"] is None else str(rn["target_rate"]),
                        "random_band": str(rn["random_band"]).lower(),
                        "random_approval_band": str(rn["random_band"]).lower(),
                        "rule_mode": "ndi",
                    }
                )

            def prep_and_pack(df: pd.DataFrame, filename: str):
                safe = dedupe_columns(df)
                safe, _ = drop_pii_columns(safe)
                safe = strip_policy_banned(safe)
                safe = to_agent_schema(safe)
                buf = io.StringIO()
                safe.to_csv(buf, index=False)
                return {"file": (filename, buf.getvalue().encode("utf-8"), "text/csv")}

            if data_choice == "Use synthetic (ANON)":
                if "synthetic_df" not in st.session_state:
                    st.warning("No ANON synthetic dataset found. Generate it in the first tab.")
                    st.stop()
                files = prep_and_pack(st.session_state.synthetic_df, "synthetic_anon.csv")

            elif data_choice == "Use synthetic (RAW ‚Äì auto-sanitize)":
                if "synthetic_raw_df" not in st.session_state:
                    st.warning("No RAW synthetic dataset found. Generate it in the first tab.")
                    st.stop()
                files = prep_and_pack(st.session_state.synthetic_raw_df, "synthetic_raw_sanitized.csv")

            elif data_choice == "Use anonymized dataset":
                if "anonymized_df" not in st.session_state:
                    st.warning("No anonymized dataset found. Create it in the second tab.")
                    st.stop()
                files = prep_and_pack(st.session_state.anonymized_df, "anonymized.csv")

            elif data_choice == "Upload manually":
                up_name = st.session_state.get("manual_upload_name")
                up_bytes = st.session_state.get("manual_upload_bytes")
                if not up_name or not up_bytes:
                    st.warning("Please upload a CSV first.")
                    st.stop()
                try:
                    tmp_df = pd.read_csv(io.BytesIO(up_bytes))
                    files = prep_and_pack(tmp_df, up_name)
                except Exception:
                    files = {"file": (up_name, up_bytes, "text/csv")}
            else:
                st.error("Unknown data source selection.")
                st.stop()

            # ---- RUN REQUEST ----
            try:
                r = requests.post(
                    f"{API_URL}/v1/agents/{agent_name}/run",
                    data=data,
                    files=files,
                    timeout=180,
                )
            except requests.exceptions.RequestException as exc:
                st.error(
                    f"‚ùå Could not reach the agent API at {API_URL}. "
                    "Make sure the backend service is running (port 8090) and try again."
                )
                st.caption(f"Details: {exc}")
                st.stop()

            if r.status_code != 200:
                st.error(f"Run failed ({r.status_code}): {r.text}")
                st.stop()

            res = r.json()
            _ping_chatbot_refresh("credit_run")

            # ---- Robust run_id + data extraction ----
            run_id = None
            payload_rows = None  # fallback rows for rendering

            if isinstance(res, dict):
                run_id = res.get("run_id") or res.get("id")
                payload_rows = res.get("result") or res.get("data") or res.get("results") or res.get("rows")
            elif isinstance(res, list):
                payload_rows = res
            else:
                try:
                    maybe = json.loads(res)
                    if isinstance(maybe, dict):
                        run_id = maybe.get("run_id") or maybe.get("id")
                        payload_rows = maybe.get("result") or maybe.get("data") or maybe.get("results") or maybe.get("rows")
                    elif isinstance(maybe, list):
                        payload_rows = maybe
                except Exception:
                    pass

            # ---- Helper: turn any JSON-like into a DataFrame ----
            def json_to_df(obj) -> pd.DataFrame:
                if obj is None:
                    return pd.DataFrame()
                if isinstance(obj, pd.DataFrame):
                    return obj
                if isinstance(obj, bytes):
                    try:
                        obj = obj.decode("utf-8", errors="ignore")
                    except Exception:
                        return pd.DataFrame({"value": [repr(obj)]})
                if isinstance(obj, str):
                    obj = obj.strip()
                    if not obj:
                        return pd.DataFrame()
                    try:
                        j = json.loads(obj)
                        return json_to_df(j)
                    except Exception:
                        lines = [ln for ln in obj.splitlines() if ln.strip()]
                        return pd.DataFrame({"value": lines}) if lines else pd.DataFrame()
                if isinstance(obj, list):
                    if len(obj) == 0:
                        return pd.DataFrame()
                    if all(isinstance(x, dict) for x in obj):
                        try:
                            return pd.json_normalize(obj)
                        except Exception:
                            return pd.DataFrame(obj)
                    if all(isinstance(x, list) for x in obj):
                        return pd.DataFrame({"row": obj})
                    return pd.DataFrame({"value": obj})
                if isinstance(obj, dict):
                    for key in ("rows", "data", "result", "results", "items", "records"):
                        if key in obj and isinstance(obj[key], (list, dict)):
                            return json_to_df(obj[key])
                    try:
                        return pd.json_normalize(obj)
                    except Exception:
                        return pd.DataFrame([obj])
                return pd.DataFrame({"value": [obj]})

            # ---- Prefer server report via run_id; otherwise fall back to local JSON‚ÜíDF ----
            
            # ============================================================
            # ‚úÖ Prefer server CSV ‚Üí fallback to JSON Parser
            # ============================================================
            if run_id:
                try:
                    rid = run_id
                    merged_url = f"{API_URL}/v1/runs/{rid}/report?format=csv"
                    merged_bytes = requests.get(merged_url, timeout=30).content
                    merged_df = pd.read_csv(io.BytesIO(merged_bytes))
                    st.session_state.last_run_id = rid
                    st.success(f"‚úÖ Run succeeded! Run ID: {rid}")
                except Exception as e:
                    st.warning(f"Could not fetch CSV via run_id ({run_id}): {e}")
                    merged_df = json_to_dataframe(payload_rows)
            else:
                st.warning("‚ö†Ô∏è Backend did not return a run_id. Falling back to JSON.")
                merged_df = json_to_dataframe(payload_rows)

 

            if merged_df is None or merged_df.empty:
                st.error("No data available to render (both report and fallback JSON were empty).")
                st.write("Raw response:", res)
                st.stop()

            # Keep for later tabs
            st.session_state["last_merged_df"] = dedupe_columns(merged_df)
            
            # ‚úÖ Make results available to Stage 7 (Reporting & Handoff)
            try:
                st.session_state["credit_scored_df"] = dedupe_columns(merged_df.copy())
                st.success("‚úÖ Stage C outputs saved for Stage 7 (Reporting & Handoff).")
            except Exception as e:
                st.warning(f"Could not persist scored dataset for Stage 7: {e}")

            # ---- Decisions Table (with filter) ----
            st.markdown("### üìÑ Credit AI Agent Decisions Table (filtered)")
            uniq_dec = sorted([d for d in merged_df.get("decision", pd.Series(dtype=str)).dropna().unique()]) \
                    if "decision" in merged_df.columns else []
            chosen = st.multiselect("Filter decision", options=uniq_dec, default=uniq_dec, key="filter_decisions")
            df_view = merged_df.copy()
            if "decision" in df_view.columns and chosen:
                df_view = df_view[df_view["decision"].isin(chosen)]
            st.dataframe(df_view, use_container_width=True)

            # ---- Dashboard ----
            st.markdown("## üìä Dashboard")
            render_credit_dashboard(merged_df, st.session_state.get("currency_symbol", ""))

            # Add per-row metrics columns if present
            if "rule_reasons" in df_view.columns:
                rr = df_view["rule_reasons"].apply(try_json)
                df_view["metrics_met"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is True])) if isinstance(d, dict) else "")
                df_view["metrics_unmet"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is False])) if isinstance(d, dict) else "")

            cols_show = [c for c in [
                "application_id","customer_type","decision","score","loan_amount","income","metrics_met","metrics_unmet",
                "proposed_loan_option","proposed_consolidation_loan","top_feature","explanation"
            ] if c in df_view.columns]
            if cols_show:
                st.dataframe(df_view[cols_show].head(500), use_container_width=True)

            # ---- Download button (keep your large button style) ----
            ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
            out_name = f"ai-appraisal-outputs-{ts}-{st.session_state['currency_code']}.csv"
            csv_data = merged_df.to_csv(index=False).encode("utf-8")

            st.markdown("""
            <style>
            div[data-testid="stDownloadButton"] button {
                font-size: 90px !important;
                font-weight: 900 !important;
                padding: 28px 48px !important;
                border-radius: 16px !important;
                background: linear-gradient(90deg, #2563eb, #1d4ed8) !important;
                color: white !important;
                border: none !important;
                box-shadow: 0 6px 18px rgba(0,0,0,0.35) !important;
                transition: all 0.3s ease-in-out !important;
            }
            div[data-testid="stDownloadButton"] button:hover {
                background: linear-gradient(90deg, #1e3a8a, #1d4ed8) !important;
                transform: scale(1.03);
            }
            </style>
            """, unsafe_allow_html=True)

            st.download_button(
                "‚¨áÔ∏è Download AI Outputs For Human Review (CSV)",
                csv_data,
                file_name=out_name,
                mime="text/csv",
                use_container_width=True
            )
            
            # ‚úÖ CREATE TRAINING LABEL (Stage C ‚Üí Stage F)
            train_df = merged_df.copy()

            # 1) Default probability ‚Üí binary label
            if "default_probability" in train_df.columns:
                train_df["label"] = (train_df["default_probability"] >= 0.5).astype(int)

            # 2) Fallback: use score column if exists
            elif "score" in train_df.columns:
                train_df["label"] = (train_df["score"] >= 0.5).astype(int)

            # 3) Final fallback to avoid Stage F crash
            else:
                train_df["label"] = 0

            # ‚úÖ SAVE FOR TRAINING PIPELINE
            try:
                st.session_state["credit_train_df"] = train_df.copy()
                st.success("‚úÖ Stage C dataset prepared and saved for Stage F (training).")
            except Exception as e:
                st.error(f"Could not save training dataset for Stage F: {e}")

            
            # ‚úÖ SAVE OUTPUT FOR STAGE F (Training)
            try:
                #st.session_state["credit_train_df"] = scored_df.copy()
                st.session_state["credit_train_df"] = merged_df.copy()

                st.success("‚úÖ Stage C output saved for Stage F (training).")
            except Exception as e:
                st.error(f"Could not save Stage C dataset for training: {e}")

        except Exception as e:
            st.exception(e)

    # Re-download quick section
    if st.session_state.get("last_run_id"):
        st.markdown("---")
        st.subheader("üì• Download Latest Outputs")
        rid = st.session_state.last_run_id
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1: st.markdown(f"[‚¨áÔ∏è PDF]({API_URL}/v1/runs/{rid}/report?format=pdf)")
        with col2: st.markdown(f"[‚¨áÔ∏è Scores CSV]({API_URL}/v1/runs/{rid}/report?format=scores_csv)")
        with col3: st.markdown(f"[‚¨áÔ∏è Explanations CSV]({API_URL}/v1/runs/{rid}/report?format=explanations_csv)")
        with col4: st.markdown(f"[‚¨áÔ∏è Merged CSV]({API_URL}/v1/runs/{rid}/report?format=csv)")
        with col5: st.markdown(f"[‚¨áÔ∏è JSON]({API_URL}/v1/runs/{rid}/report?format=json)")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üßë‚Äç‚öñÔ∏è TAB 4 ‚Äî Human Review
with tab_review:
    st.subheader("üßë‚Äç‚öñÔ∏è Human Review ‚Äî Correct AI Decisions & Score Agreement > Drop your AI appraisal output CSV from previous Stage  below")

    # Allow loading AI output CSV back into review via dropdown upload
    uploaded_review = st.file_uploader("Load AI outputs CSV for review (optional)", type=["csv"], key="review_csv_loader")
    if uploaded_review is not None:
        try:
            st.session_state["last_merged_df"] = pd.read_csv(uploaded_review)
            st.success("Loaded review dataset from uploaded CSV.")
        except Exception as e:
            st.error(f"Could not read uploaded CSV: {e}")

    if "last_merged_df" not in st.session_state:
        st.info("Run the agent (previous tab) or upload an AI outputs CSV to load results for review.")
    else:
        dfm = st.session_state["last_merged_df"].copy()
        st.markdown("#### 1) Select rows to review and correct")

        editable_cols = []
        if "decision" in dfm.columns: editable_cols.append("decision")
        if "rule_reasons" in dfm.columns: editable_cols.append("rule_reasons")
        if "customer_type" in dfm.columns: editable_cols.append("customer_type")

        editable = dfm[["application_id"] + editable_cols].copy()
        editable.rename(columns={"decision": "ai_decision"}, inplace=True)
        editable["human_decision"] = editable.get("ai_decision", "approved")
        editable["human_rule_reasons"] = editable.get("rule_reasons", "")

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # LIGHTER EDITABLE CELL STYLING (improved)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        st.markdown("""
            <style>
            /* Bright background for editable cells */
            [data-testid="stDataFrameCellEditable"] textarea {
                background-color: #fefefe !important;   /* bright white background */
                color: #111 !important;                 /* dark text */
                border: 1px solid #cbd5e1 !important;   /* subtle gray border */
                border-radius: 6px !important;
                padding: 6px 8px !important;
                font-weight: 500 !important;
            }

            /* Hover and focus effect */
            [data-testid="stDataFrameCellEditable"]:focus-within textarea,
            [data-testid="stDataFrameCellEditable"]:hover textarea {
                background-color: #ffffff !important;
                border-color: #22c55e !important;        /* green glow */
                box-shadow: 0 0 0 2px rgba(34,197,94,0.4) !important;
            }

            /* Read-only cells: keep dark */
            [data-testid="stDataFrameCell"] {
                background-color: #1e293b !important;
                color: #e2e8f0 !important;
            }
            </style>
        """, unsafe_allow_html=True)


        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # EDITOR
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        edited = st.data_editor(
            editable,
            num_rows="dynamic",
            use_container_width=True,
            key="review_editor",
            column_config={
                "human_decision": st.column_config.SelectboxColumn(options=["approved", "denied"]),
                "customer_type": st.column_config.SelectboxColumn(options=["bank", "non-bank"], disabled=True)
            }
        )

        st.markdown("#### 2) Compute agreement score")

        if st.button("Compute agreement score"):
            if "ai_decision" in edited.columns and "human_decision" in edited.columns:
                agree = (edited["ai_decision"] == edited["human_decision"]).astype(int)
                score = float(agree.mean()) if len(agree) else 0.0
                st.session_state["last_agreement_score"] = score

                # üå°Ô∏è BEAUTIFUL Gauge
                import plotly.graph_objects as go
                fig = go.Figure(go.Indicator(
                    mode="gauge+number",
                    value=score * 100,
                    number={'suffix': "%", 'font': {'size': 72, 'color': "#f8fafc", 'family': "Arial Black"}},
                    title={'text': "AI ‚Üî Human Agreement", 'font': {'size': 28, 'color': "#93c5fd", 'family': "Arial"}},
                    gauge={
                        'axis': {'range': [0, 100], 'tickwidth': 2, 'tickcolor': "#f8fafc"},
                        'bar': {'color': "#3b82f6", 'thickness': 0.3},
                        'bgcolor': "#1e293b",
                        'borderwidth': 2,
                        'bordercolor': "#334155",
                        'steps': [
                            {'range': [0, 50], 'color': "#ef4444"},
                            {'range': [50, 75], 'color': "#f59e0b"},
                            {'range': [75, 100], 'color': "#22c55e"},
                        ],
                    }
                ))
                fig.update_layout(
                    paper_bgcolor="#0f172a",
                    plot_bgcolor="#0f172a",
                    height=400,
                    margin=dict(t=60, b=20, l=60, r=60)
                )
                st.plotly_chart(fig, use_container_width=True)



            # üí° Detailed disagreement table (AI vs Human + AI metrics explanation)
                mismatched = edited[edited["ai_decision"] != edited["human_decision"]].copy()
                total = len(edited)
                disagree = len(mismatched)

                if disagree > 0:
                    st.markdown(f"### ‚ùå {disagree} loans disagreed out of {total} ({(disagree/total)*100:.1f}% disagreement rate)")

                    import json

                    def parse_ai_reason(r: str):
                        """Parse AI rule_reasons and summarize which metrics passed or failed."""
                        if not isinstance(r, str):
                            return "No metrics available"
                        try:
                            data = json.loads(r.replace("'", "\""))
                            passed = [k for k, v in data.items() if v is True]
                            failed = [k for k, v in data.items() if v is False]
                            result = []
                            if passed:
                                result.append("‚úÖ Pass: " + ", ".join(passed))
                            if failed:
                                result.append("‚ùå Fail: " + ", ".join(failed))
                            return " | ".join(result) if result else "No metrics recorded"
                        except Exception:
                            return "Unreadable metrics"

                    # Extract AI reasoning and Human reason columns
                    mismatched["ai_metrics"] = mismatched["rule_reasons"].apply(parse_ai_reason) if "rule_reasons" in mismatched else "No data"
                    mismatched["human_reason"] = mismatched.get("human_rule_reasons", "Manual review adjustment")

                    # üü©üü• Color styling for AI vs Human
                    def highlight_disagreement(row):
                        ai_color = "background-color: #ef4444; color: white;"      # red for AI decision
                        human_color = "background-color: #22c55e; color: black;"   # green for Human decision
                        return [
                            ai_color if col == "ai_decision" else
                            human_color if col == "human_decision" else
                            ""
                            for col in row.index
                        ]

                    # Columns: ID ‚Üí AI Decision ‚Üí Human Decision ‚Üí AI Metrics ‚Üí Human Reason
                    show_cols = [
                        c for c in ["application_id", "ai_decision", "human_decision", "ai_metrics", "human_reason"]
                        if c in mismatched.columns
                    ]
                    styled_df = mismatched[show_cols].style.apply(highlight_disagreement, axis=1)
                    st.dataframe(styled_df, use_container_width=True, height=420)
                else:
                    st.success("‚úÖ Full agreement ‚Äî no human-AI mismatches found.")



        # Export review CSV (manual loop into training)
        st.markdown("#### 3) Export Human review CSV for Next Step : Training and loopback ")
        model_used = "production"  # if you track specific model names, set it here
        ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
        safe_user = st.session_state["user_info"]["name"].replace(" ", "").lower()
        review_name = f"creditappraisal.{safe_user}.{model_used}.{ts}.csv"
        csv_bytes = edited.to_csv(index=False).encode("utf-8")
        st.download_button("‚¨áÔ∏è Export review CSV", csv_bytes, review_name, "text/csv")
        st.caption(f"Saved file name pattern: **{review_name}**")


# -------------------------------------------------------------
# ‚úÖ STAGE 5 ‚Äî Credit Model Training (Executive Dashboard)
# -------------------------------------------------------------
with tab_train:
    import os, json, glob, shutil, zipfile
    from datetime import datetime, timezone
    from pathlib import Path
    import numpy as np
    import pandas as pd
    import plotly.express as px
    import plotly.graph_objects as go

    from sklearn.model_selection import train_test_split
    from sklearn.metrics import (
        roc_auc_score, accuracy_score, precision_score, 
        recall_score, f1_score, confusion_matrix
    )

    import joblib
    import streamlit as st

    # ---------------------------------------------------------
    # ‚úÖ HEADER
    # ---------------------------------------------------------
    st.markdown("""
    <h2>üß† Stage 5 ‚Äî Credit Model Training</h2>
    <p style='font-size:1.1rem'>
    Train ‚Üí Compare ‚Üí Evaluate ‚Üí Promote<br>
    Build a robust, regulator-friendly credit scoring model.
    </p>
    """, unsafe_allow_html=True)

    # ---------------------------------------------------------
    # ‚úÖ LOAD TRAINING DATA (from Stage C)
    # ---------------------------------------------------------
    train_df = st.session_state.get("credit_train_df")

    if train_df is None or train_df.empty:
        st.error("‚ö†Ô∏è Missing training dataset. Please run Stage C first.")
        st.stop()

    st.success(f"‚úÖ Training dataset detected with {len(train_df):,} rows.")
    st.dataframe(train_df.head(), use_container_width=True)

    st.markdown("---")

    # ---------------------------------------------------------
    # ‚úÖ TRAINING DATA LOADING (Human feedback OR CSV upload)
    # ---------------------------------------------------------
    st.markdown("### üì• Stage 5 Training Data Input")

    train_df = None
    source_label = None

    # ‚úÖ Option A ‚Äî Human Review Stage output is available
    if "credit_human_review_df" in st.session_state:
        df_human = st.session_state.get("credit_human_review_df")
        if isinstance(df_human, pd.DataFrame) and not df_human.empty:
            train_df = df_human.copy()
            source_label = "Human Review Stage (Session State)"

    # ‚úÖ Option B ‚Äî Model Inference Stage C merged_df output (fallback)
    elif "credit_train_df" in st.session_state:
        df_auto = st.session_state.get("credit_train_df")
        if isinstance(df_auto, pd.DataFrame) and not df_auto.empty:
            train_df = df_auto.copy()
            source_label = "Stage C auto-saved dataset"

    
    # ‚úÖ Option C ‚Äî User uploads CSV manually
    uploaded = st.file_uploader("Upload training CSV (optional)", type=["csv"])

    if uploaded is not None:
        try:
            train_df = pd.read_csv(uploaded)
            source_label = f"Uploaded CSV ({len(train_df)} rows)"
        except Exception as e:
            st.error(f"‚ùå Could not read uploaded CSV: {e}")

    # ‚úÖ Hard stop if no dataset is available
    if train_df is None or train_df.empty:
        st.error("""
        ‚ùå No training data found.

        ‚úÖ Provide training dataset by:
        ‚Ä¢ Completing Human Review Stage (Stage D)  
        ‚Ä¢ OR uploading a CSV here  
        ‚Ä¢ OR enabling Stage C to save merged output  
        """)
        st.stop()
    


    # ‚úÖ Show dataset preview + source
    st.success(f"‚úÖ Training dataset loaded from: **{source_label}**")
    st.dataframe(train_df.head(), use_container_width=True)

    st.markdown("---")

    # ---------------------------------------------------------
    # ‚úÖ MODEL SELECTION
    # ---------------------------------------------------------
    dataset_rows = len(train_df)
    feature_count = len(train_df.columns)
    numeric_cols = len(train_df.select_dtypes(include=["number"]).columns)

    def score_model(name: str) -> tuple[int, str]:
        """
        Returns (score, reason) for the recommended-model cards.
        Score is a simple heuristic driven by dataset size + feature mix.
        """
        reason = ""
        score = 0

        if name == "LogisticRegression":
            score = 3 if dataset_rows <= 10_000 else 1
            reason = "Best for <10k rows when regulators want fully explainable coefficients."
        elif name == "RandomForest":
            score = 4 if 10_000 < dataset_rows <= 60_000 else 2
            reason = "Solid all-rounder for midsize datasets with mixed feature types."
        elif name == "LightGBM":
            score = 5 if dataset_rows > 40_000 else 3
            reason = "Handles wide, imbalanced credit files efficiently ‚Äî ideal for production retrains."
        elif name == "XGBoost":
            score = 4 if dataset_rows > 80_000 else 2
            reason = "Max accuracy when you can afford longer training time and need granular splits."

        # Small boost if we detected many numeric columns (helps tree boosters)
        if name in {"LightGBM", "XGBoost", "RandomForest"} and numeric_cols > feature_count * 0.6:
            score += 1
            reason += " Abundant numeric features favour boosted trees."

        return score, reason

    model_profiles = []
    for candidate in ["LightGBM", "RandomForest", "LogisticRegression", "XGBoost"]:
        score, reason = score_model(candidate)
        model_profiles.append(
            {
                "name": candidate,
                "score": score,
                "tagline": {
                    "LightGBM": "Enterprise-ready EQACh default",
                    "RandomForest": "Balanced accuracy + speed",
                    "LogisticRegression": "Audit-friendly baseline",
                    "XGBoost": "Max depth / max lift",
                }[candidate],
                "reason": reason,
            }
        )

    model_profiles.sort(key=lambda x: x["score"], reverse=True)
    top_profiles = model_profiles[:3]

    st.markdown("#### ‚≠ê Recommended models (EQACh signal)")
    st.caption(f"Based on {dataset_rows:,} rows ¬∑ {feature_count} features ¬∑ {numeric_cols} numeric")

    rec_cols = st.columns(len(top_profiles))
    for col, profile in zip(rec_cols, top_profiles):
        with col:
            st.markdown(f"**{profile['name']}**")
            st.caption(profile["tagline"])
            st.write(profile["reason"])
            if st.button(f"Use {profile['name']}", key=f"use_{profile['name']}"):
                st.session_state["credit_model_choice"] = profile["name"]

    st.subheader("ü§ñ Choose training model")

    model_options = ["LogisticRegression", "RandomForest", "LightGBM", "XGBoost"]
    default_choice = st.session_state.get("credit_model_choice")
    if not default_choice:
        default_choice = model_profiles[0]["name"]
    if default_choice not in model_options:
        default_choice = model_options[0]

    model_choice = st.selectbox(
        "Select model:",
        model_options,
        index=model_options.index(default_choice)
    )
    st.session_state["credit_model_choice"] = model_choice

    # ---------------------------------------------------------
    # ‚úÖ Smart Target Auto-Detection (BEFORE training)
    # ---------------------------------------------------------
    def detect_best_target(df):
        """
        Smart target auto-detection for credit scoring.
        Priority:
        1) AI numeric scores
        2) human decisions
        3) any suitable numeric predictive column
        """

        score_candidates = [
            "score", "default_probability", "risk_score",
            "pd", "probability_default"
        ]

        # ‚úÖ 1. Direct AI numeric score column
        for col in score_candidates:
            if col in df.columns:
                return col, "numeric_score"

        # ‚úÖ 2. Human decision labels
        decision_candidates = ["human_decision", "final_decision", "decision"]

        for col in decision_candidates:
            if col in df.columns:
                vals = df[col].dropna().astype(str).str.lower().unique()
                if any(v in ["approved", "rejected"] for v in vals):
                    return col, "decision_label"

        # ‚úÖ 3. Numeric fallback
        numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()

        # exclude leakage columns
        blacklist = ["loan_amount", "requested_amount", "income", "assets_owned"]
        numeric_cols = [c for c in numeric_cols if c not in blacklist]

        if numeric_cols:
            return numeric_cols[0], "numeric_fallback"

        return None, "none"


    # ---------------------------------------------------------
    # ‚úÖ TRAINING LOGIC
    # ---------------------------------------------------------
    if st.button("üöÄ Train Credit Model Now"):
        with st.spinner("Training model‚Ä¶"):

            # ‚úÖ Smart Target Detection
            TARGET_COL, target_mode = detect_best_target(train_df)

            if TARGET_COL is None:
                st.error("‚ùå No suitable target column found in dataset.")
                st.stop()

            st.success(f"‚úÖ Selected target: **{TARGET_COL}** ({target_mode})")

            # ‚úÖ Clean and prepare target
            y_cont = pd.to_numeric(train_df[TARGET_COL], errors="coerce")
            df_clean = train_df.dropna(subset=[TARGET_COL]).copy()
            y_cont = df_clean[TARGET_COL].astype(float)

            # ---------------------------------------------------------
            # ‚úÖ MODE 1: DECISION LABEL (approved / rejected ‚Üí 1/0)
            # ---------------------------------------------------------
            if target_mode == "decision_label":
                y_bin = df_clean[TARGET_COL].astype(str).str.lower().map({
                    "approved": 1,
                    "rejected": 0
                })
                st.info("‚úÖ Using human decisions converted to binary 0/1")

            # ---------------------------------------------------------
            # ‚úÖ MODE 2 & 3: NUMERIC TARGET ‚Üí BINARIZE USING MEDIAN
            # ---------------------------------------------------------
            else:
                threshold = float(y_cont.median())
                y_bin = (y_cont >= threshold).astype(int)
                st.info(f"‚úÖ Numeric target ‚Üí auto-threshold = {threshold:.4f}")

            # ---------------------------------------------------------
            # ‚úÖ FEATURE SELECTION ‚Äî remove target + leakage
            # ---------------------------------------------------------
            LEAKAGE_COLS = [
                TARGET_COL,
                "decision", "confidence",
                "top_feature", "explanation",
                "proposed_loan_option", "proposed_consolidation_loan",
                "rule_reasons"
            ]

            X = df_clean.drop(columns=[c for c in LEAKAGE_COLS if c in df_clean.columns])
            
            
            # ---------------------------------------------------------
            # ‚úÖ Encode non-numeric columns for ML training
            # ---------------------------------------------------------
            X = X.copy()  # safe copy

            # Detect non-numeric columns
            non_numeric_cols = X.select_dtypes(include=["object"]).columns.tolist()

            if non_numeric_cols:
                st.warning(f"Encoding non-numeric columns: {non_numeric_cols}")

                # ‚úÖ Safe label encoding for LightGBM / RF / XGB / LR
                from sklearn.preprocessing import LabelEncoder

                for col in non_numeric_cols:
                    try:
                        le = LabelEncoder()
                        X[col] = le.fit_transform(X[col].astype(str))
                    except Exception as e:
                        st.error(f"‚ùå Failed to encode column '{col}': {e}")
                        st.stop()


            # ---------------------------------------------------------
            # ‚úÖ TRAIN/TEST SPLIT
            # ---------------------------------------------------------
            from sklearn.model_selection import train_test_split
            Xtr, Xte, ytr, yte = train_test_split(
                X, y_bin, test_size=0.2, random_state=42
            )

            # ---------------------------------------------------------
            # ‚úÖ MODEL SELECTION AND TRAINING
            # ---------------------------------------------------------
            if model_choice == "LogisticRegression":
                from sklearn.linear_model import LogisticRegression
                model = LogisticRegression(max_iter=2000)

                model = LogisticRegression(max_iter=2000)
            elif model_choice == "RandomForest":
                from sklearn.ensemble import RandomForestClassifier
                model = RandomForestClassifier(n_estimators=300)

                model = RandomForestClassifier(n_estimators=300)
            elif model_choice == "LightGBM":
                try:
                    from lightgbm import LGBMClassifier
                except ModuleNotFoundError:
                    with st.expander("‚ö†Ô∏è Install LightGBM to use this model", expanded=True):
                        st.error(
                            "LightGBM is not installed in this environment. "
                            "Install it with `pip install lightgbm` (inside the virtualenv) "
                            "or choose RandomForest / LogisticRegression instead."
                        )
                    st.stop()
                model = LGBMClassifier()

                model = LGBMClassifier()
            else:
                from xgboost import XGBClassifier

                model = XGBClassifier()

            model.fit(Xtr, ytr)

            # ---------------------------------------------------------
            # ‚úÖ PREDICTIONS & METRICS
            # ---------------------------------------------------------
            preds_proba = model.predict_proba(Xte)[:, 1]
            preds = (preds_proba >= 0.5).astype(int)

            from sklearn.metrics import (
                roc_auc_score, accuracy_score, precision_score,
                recall_score, f1_score
            )

            metrics = {
                "AUC": roc_auc_score(yte, preds_proba),
                "Accuracy": accuracy_score(yte, preds),
                "Precision": precision_score(yte, preds),
                "Recall": recall_score(yte, preds),
                "F1": f1_score(yte, preds),
            }

            st.success("‚úÖ Model trained successfully!")
            st.json(metrics)

        
    
            # -----------------------------------------------------
            # ‚úÖ LOAD PRODUCTION BASELINE IF EXISTS
            # -----------------------------------------------------
            PROD_DIR = Path("./agents/credit_appraisal/models/production")
            prod_meta_path = PROD_DIR / "production_meta.json"
            if prod_meta_path.exists():
                prod_m = json.load(open(prod_meta_path))["metrics"]
            else:
                prod_m = None

            st.markdown("---")
            st.subheader("üìä A/B Model Comparison")

            # ‚úÖ COMPARISON TABLE
            cmp_df = pd.DataFrame({
                "Metric": list(metrics.keys()),
                "New Model": [f"{v:.4f}" for v in metrics.values()],
                "Production": [
                    f"{prod_m[k]:.4f}" if prod_m else "‚Äî"
                    for k in metrics.keys()
                ]
            })
            st.table(cmp_df)

            # -----------------------------------------------------
            # ‚úÖ EXECUTIVE SUMMARY (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)
            # -----------------------------------------------------
            st.markdown("## üß≠ Executive Summary (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)")

            if prod_m:
                auc_delta = metrics["AUC"] - prod_m["AUC"]
                if auc_delta > 0:
                    st.success(f"‚úÖ Model improves **AUC by {auc_delta:.4f}** ‚Äî better discrimination.")
                else:
                    st.warning(f"‚ö†Ô∏è AUC dropped by {auc_delta:.4f} ‚Äî further tuning required.")
            else:
                st.info("üü¢ First model ‚Äî will become baseline.")

            # -----------------------------------------------------
            # ‚úÖ CONFUSION MATRIX
            # -----------------------------------------------------
            cm = confusion_matrix(yte, preds)
            cm_fig = px.imshow(
                cm, text_auto=True,
                title="Confusion Matrix",
                labels={"x": "Predicted", "y": "Actual"}
            )
            st.plotly_chart(cm_fig, use_container_width=True)

            # -----------------------------------------------------
            # ‚úÖ FEATURE IMPORTANCE
            # -----------------------------------------------------
            st.subheader("üß† Feature Importance")
            if hasattr(model, "feature_importances_"):
                imp = pd.DataFrame({"feature": X.columns, "importance": model.feature_importances_}).sort_values(
                    "importance", ascending=False
                )
                st.bar_chart(imp.set_index("feature"))
            elif hasattr(model, "coef_"):
                coef = pd.DataFrame({"feature": X.columns, "coef": np.ravel(model.coef_)}).sort_values(
                    "coef", key=np.abs, ascending=False
                )
                st.bar_chart(coef.set_index("feature"))
            else:
                st.info("This model does not expose importance metrics.")

            # -----------------------------------------------------
            # ‚úÖ SAVE MODEL
            # -----------------------------------------------------
            TRAINED_DIR = Path("./agents/credit_appraisal/models/trained")
            TRAINED_DIR.mkdir(parents=True, exist_ok=True)

            ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
            model_path = TRAINED_DIR / f"{model_choice}_{ts}.joblib"
            joblib.dump(model, model_path)
            st.success(f"‚úÖ Model saved ‚Üí `{model_path}`")

            # ‚úÖ SAVE REPORT
            RUNS_DIR = Path("./.tmp_runs")
            RUNS_DIR.mkdir(exist_ok=True)

            report = {
                "timestamp": ts,
                "model_choice": model_choice,
                "metrics": metrics,
                "model_path": str(model_path),
                "features": list(X.columns),
                "threshold": threshold,
            }

            rep_path = RUNS_DIR / f"credit_training_report_{ts}.json"
            json.dump(report, open(rep_path, "w"), indent=2)

            # store for next stage
            st.session_state["credit_last_model_path"] = str(model_path)
            st.session_state["credit_last_metrics"] = metrics
            st.session_state["credit_last_report"] = report

            st.caption(f"üìÑ Report saved ‚Üí `{rep_path}`")

            # -----------------------------------------------------
            # ‚úÖ PROMOTION BLOCK
            # -----------------------------------------------------
            st.markdown("## üì§ Promote This Model to Production")
            if st.button("‚úÖ Promote to Production"):
                try:
                    PROD_DIR.mkdir(parents=True, exist_ok=True)
                    shutil.copy(model_path, PROD_DIR / "model.joblib")
                    meta = {
                        "promoted_at": datetime.now(timezone.utc).isoformat(),
                        "metrics": metrics,
                        "model_path": str(model_path),
                        "model_choice": model_choice,
                    }
                    json.dump(meta, open(PROD_DIR / "production_meta.json", "w"), indent=2)
                    st.balloons()
                    st.success("‚úÖ Model promoted successfully!")
                except Exception as e:
                    st.error(f"‚ùå Promotion failed: {e}")

            # -----------------------------------------------------
            # ‚úÖ EXPORT ZIP
            # -----------------------------------------------------
            st.markdown("## üì¶ Export Project ZIP")
            EXPORT_DIR = Path("./exports")
            EXPORT_DIR.mkdir(exist_ok=True)
            zip_name = f"credit_project_bundle_{ts}.zip"
            zip_path = EXPORT_DIR / zip_name

            if st.button("‚¨áÔ∏è Build ZIP Bundle"):
                try:
                    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:

                        # Runs
                        for root, dirs, files in os.walk(RUNS_DIR):
                            for f in files:
                                full = os.path.join(root, f)
                                arc = os.path.relpath(full, RUNS_PATH)
                                zf.write(full, f"runs/{arc}")

                        # Production models
                        if PROD_DIR.exists():
                            for f in PROD_DIR.glob("*"):
                                zf.write(f, f"production/{f.name}")

                        # Trained models
                        for f in TRAINED_DIR.glob("*.joblib"):
                            zf.write(f, f"trained/{f.name}")

                        # Training report
                        zf.write(rep_path, "training_report.json")

                    st.success("‚úÖ ZIP created!")
                    with open(zip_path, "rb") as fp:
                        st.download_button(
                            "‚¨áÔ∏è Download ZIP",
                            data=fp,
                            file_name=zip_name,
                            mime="application/zip",
                            use_container_width=True
                        )
                except Exception as e:
                    st.error(f"‚ùå ZIP creation failed: {e}")


# -------------------------------------------------------------
# ‚úÖ STAGE 6 ‚Äî Deployment of Credit Scoring Model
# -------------------------------------------------------------
with tab_deploy:
    import os, json, shutil, zipfile
    from pathlib import Path
    from datetime import datetime, timezone

    st.title("üöÄ Stage G ‚Äî Deployment & Distribution")
    st.caption("Package ‚Üí Verify ‚Üí Upload ‚Üí Release ‚Üí Distribute to Credit / Legal / Risk units.")


    last_model = st.session_state.get("credit_last_model_path")
    metrics = st.session_state.get("credit_last_metrics")
    report = st.session_state.get("credit_last_report")

    if not last_model:
        st.warning("‚ö†Ô∏è Train a model in Stage F before deploying.")
        st.stop()

    st.success(f"‚úÖ Latest trained model detected:\n`{last_model}`")
    st.json(metrics)
    

    # ---------------------------------------------
    # 1) Load the latest ZIP bundle created in Stage F
    # ---------------------------------------------
    st.markdown("## üì¶ 1) Project Package (Generated in Stage F)")
    
    EXPORT_DIR = Path("./exports")
    EXPORT_DIR.mkdir(exist_ok=True)

    # Find ZIP files
    zip_files = sorted(EXPORT_DIR.glob("asset_project_bundle_*.zip"), reverse=True)
    
    if not zip_files:
        st.warning("‚ö†Ô∏è No project ZIP found. Run Stage F and export a bundle first.")
        st.stop()

    latest_zip = zip_files[0]

    st.success(f"‚úÖ Latest bundle detected: `{latest_zip.name}`")
    st.caption(f"Size: **{latest_zip.stat().st_size/1e6:.2f} MB**")
    

    # ---------------------------------------------
    # 3) Upload Targets (S3 / Swift / GitHub Release)
    # ---------------------------------------------
    st.markdown("## ‚òÅÔ∏è 3) Upload / Publish Package")

    dest = st.radio(
        "Choose destination",
        ["AWS S3", "OpenStack Swift", "GitHub Release"],
        horizontal=True
    )

    if dest == "AWS S3":
        st.info("Upload to S3 (requires AWS credentials)")
        bucket = st.text_input("Bucket Name", "my-ai-models")
        key = st.text_input("Object Key", latest_zip.name)

        if st.button("‚¨ÜÔ∏è Upload to S3"):
            try:
                import boto3
                s3 = boto3.client("s3")
                s3.upload_file(str(latest_zip), bucket, key)
                st.success(f"‚úÖ Uploaded to `s3://{bucket}/{key}`")
            except Exception as e:
                st.error(f"‚ùå Failed: {e}")

    elif dest == "OpenStack Swift":
        st.info("Upload to Swift (requires Swift credentials)")
        container = st.text_input("Container Name", "ai-models")
        if st.button("‚¨ÜÔ∏è Upload to Swift"):
            try:
                from swiftclient.service import SwiftService, SwiftUploadObject
                with SwiftService() as swift:
                    swift.upload(container, [SwiftUploadObject(str(latest_zip))])
                st.success(f"‚úÖ Uploaded to Swift container `{container}`")
            except Exception as e:
                st.error(f"‚ùå Failed: {e}")

    elif dest == "GitHub Release":
        st.info("Publish as a GitHub release asset")
        repo = st.text_input("Repo (owner/repo)", "RackspaceAI/asset-appraisal-agent")
        token = st.text_input("GitHub Personal Access Token", type="password")
        tag = datetime.now().strftime("v%Y%m%d-%H%M%S")

        if st.button("‚¨ÜÔ∏è Publish Release on GitHub"):
            try:
                headers = {
                    "Authorization": f"token {token}",
                    "Accept": "application/vnd.github+json",
                }

                # Create release
                r = requests.post(
                    f"https://api.github.com/repos/{repo}/releases",
                    headers=headers,
                    json={"tag_name": tag, "name": f"Release {tag}", 
                          "body": "Automated export from Stage G"}
                )
                r.raise_for_status()

                upload_url = r.json()["upload_url"].split("{")[0]

                # Upload asset
                with open(latest_zip, "rb") as f:
                    ur = requests.post(
                        f"{upload_url}?name={latest_zip.name}",
                        headers={**headers, "Content-Type": "application/zip"},
                        data=f,
                    )
                ur.raise_for_status()

                st.success(f"‚úÖ GitHub Release `{tag}` published successfully!")
            except Exception as e:
                st.error(f"‚ùå Failed: {e}")




    # ---------------------------------------------
    # 5) Next Steps Checklist
    # ---------------------------------------------
    st.markdown("## ‚úÖ 5) Next Steps for DevOps / IT")

    st.markdown("""
    ### ‚úî For Credit Underwriting
    - Import CSV assets into the Credit Appraisal Agent  
    - Validate LTV, confidence, breaches  
    - Promote selected assets for loan approval  

    ### ‚úî For Legal & Compliance
    - Use verification subset (ownership, encumbrances)  
    - Run through Legal Verification Agent  
    - Flag encumbrances & fraud paths  

    ### ‚úî For Risk Management
    - Use realizable_value, condition_score, legal_penalty  
    - Re-run LTV stress tests  
    - Update risk dashboards monthly  

    ### ‚úî For DevOps / Platform Teams
    - Push ZIP to GitHub / Swift / S3  
    - Deploy production model into RunAI / SageMaker / OpenStack MLOps  
    - Update production_meta.json  
    """)

    st.info("Stage G is complete ‚Äî continue to Stage H for Inter-Department Handoff.")


    # ---------------------------------------------
    # 1) Load the latest ZIP bundle created in Stage F
    # ---------------------------------------------
    st.markdown("## üì¶ 1) Project Package (Generated in Stage F)")
    
    EXPORT_DIR = Path("./exports")
    EXPORT_DIR.mkdir(exist_ok=True)

    # Find ZIP files
    zip_files = sorted(EXPORT_DIR.glob("asset_project_bundle_*.zip"), reverse=True)
    
    if not zip_files:
        st.warning("‚ö†Ô∏è No project ZIP found. Run Stage F and export a bundle first.")
        st.stop()

    latest_zip = zip_files[0]

    st.success(f"‚úÖ Latest bundle detected: `{latest_zip.name}`")
    st.caption(f"Size: **{latest_zip.stat().st_size/1e6:.2f} MB**")
    
    
    
    
    # ---------------------------------------------------------
    # ‚úÖ Promote to production
    # ---------------------------------------------------------
    if st.button("‚úÖ Promote This Model to Production"):
        prod_dir = Path("./agents/credit_appraisal/models/production")
        prod_dir.mkdir(parents=True, exist_ok=True)

        shutil.copy(last_model, prod_dir / "model.joblib")

        prod_meta = {
            "model_path": last_model,
            "promoted_at": datetime.now(timezone.utc).isoformat(),
            "metrics": metrics,
            "report": report,
        }
        json.dump(prod_meta, open(prod_dir / "production_meta.json", "w"), indent=2)

        st.balloons()
        st.success("‚úÖ Model promoted to production successfully!")

    # ---------------------------------------------------------
    # ‚úÖ Export deployment ZIP
    # ---------------------------------------------------------
    EXPORT_DIR = Path("./exports")
    EXPORT_DIR.mkdir(exist_ok=True)
    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    if st.button("‚¨áÔ∏è Export Deployment Bundle"):
        zip_path = EXPORT_DIR / f"credit_deployment_{ts}.zip"
        with zipfile.ZipFile(zip_path, "w") as zf:
            zf.write(last_model, arcname="trained_model.joblib")
            zf.write("./agents/credit_appraisal/models/production/production_meta.json",
                     arcname="production_meta.json")

        with open(zip_path, "rb") as f:
            st.download_button(
                "‚¨áÔ∏è Download Deployment ZIP",
                data=f,
                file_name=zip_path.name,
                mime="application/zip",
            )
        st.success("‚úÖ Deployment bundle ready!")


# -------------------------------------------------------------
# ‚úÖ STAGE 7 ‚Äî Reporting & Handoff
# -------------------------------------------------------------
with tab_handoff:
    import os, json, zipfile
    import numpy as np
    import pandas as pd
    from pathlib import Path
    from datetime import datetime, timezone
    import streamlit as st
    import plotly.express as px

    st.markdown("## üìä Stage 7 ‚Äî Portfolio Reporting & Handoff")
    
    
    # ---------------------------------------------------------
    # ‚úÖ Required outputs from earlier stages ‚Äî MUST COME FIRST
    # ---------------------------------------------------------
    scored_df   = st.session_state.get("credit_scored_df")
    policy_df   = st.session_state.get("credit_policy_df")
    decision_df = st.session_state.get("credit_decision_df")

    # Helper
    import pandas as pd
    def is_nonempty_df(x) -> bool:
        return isinstance(x, pd.DataFrame) and not x.empty

    missing = []
    if not is_nonempty_df(scored_df):
        missing.append("Stage C ‚Äî Credit AI Evaluation (credit_scored_df)")

    if missing:
        st.error("‚ö†Ô∏è Missing required data: " + ", ".join(missing))
        st.info("Please run the missing stages before returning to Stage H.")
        st.stop()

    # ‚úÖ Only now is dfv allowed to be created
    # Prefer final decision table; fall back to Stage C scored output
    if is_nonempty_df(decision_df):
        dfv = decision_df.copy()
    else:
        dfv = scored_df.copy()

    # ---------------------------------------------------------
    # ‚úÖ Load portfolio for Stage 7 visuals/exports
    # 1) Primary: dataset saved by Stage C/E
    # 2) Fallback: Stage C merged output (last_merged_df)
    # 3) Optional: user upload
    # ---------------------------------------------------------
    df = st.session_state.get("credit_scored_df")

    if not is_nonempty_df(df):
        df = st.session_state.get("last_merged_df")

    uploaded_scored = st.file_uploader(
        "‚¨ÜÔ∏è (Optional) Load scored CSV for reporting",
        type=["csv"], key="stage7_upload"
    )
    if uploaded_scored is not None:
        try:
            df = pd.read_csv(uploaded_scored)
            st.success(f"Loaded scored dataset from upload ({len(df)} rows).")
        except Exception as e:
            st.error(f"Could not read uploaded CSV: {e}")

    # Final guard
    if not is_nonempty_df(df):
        st.warning("‚ö†Ô∏è Missing scored dataset. Run Stage 3 (Credit appraisal) or upload a scored CSV above.")
        st.stop()

    st.session_state["credit_scored_df"] = df.copy()

    st.success("‚úÖ Portfolio loaded.")
    st.dataframe(df.head(), use_container_width=True)

    # ‚Ä¶ (keep the rest of Stage 7: metrics, charts, handoff CSV/ZIP) ‚Ä¶




    # ---------------------------------------------------------
    # ‚úÖ Executive dashboard
    # ---------------------------------------------------------
    st.markdown("### üß≠ Executive Summary")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Applications", len(df))
    with col2:
        st.metric("Approved", (df["decision"] == "approve").sum())
    with col3:
        st.metric("Rejected", (df["decision"] == "reject").sum())

    # ---------------------------------------------------------
    # new ‚úÖ MARKET INSIGHTS ‚Äî CITY LEVEL DISTRIBUTION
    # ---------------------------------------------------------
    st.markdown("### üåç Asset Distribution by City")

    if "city" in dfv.columns:
        fig_city = px.histogram(
            dfv, x="city", color="status",
            title="Asset Count per City by Status",
            barmode="group"
        )
        st.plotly_chart(fig_city, use_container_width=True)

    # ---------------------------------------------------------
    # ‚úÖ VALUE INSIGHTS ‚Äî Realizable Value Curve
    # ---------------------------------------------------------
    st.markdown("### üí∞ Value Distribution ‚Äî FMV vs Realizable Value")

    if "realizable_value" in dfv.columns:
        fig_val = go.Figure()
        fig_val.add_trace(go.Violin(y=dfv["fmv"], name="FMV", box_visible=True))
        fig_val.add_trace(go.Violin(y=dfv["realizable_value"], name="Realizable", box_visible=True))
        st.plotly_chart(fig_val, use_container_width=True)

    # ---------------------------------------------------------
    # ‚úÖ FULL PORTFOLIO TABLE
    # ---------------------------------------------------------
    st.markdown("### üìÇ Unified Portfolio (with status)")
    st.dataframe(dfv, use_container_width=True)


    # ---------------------------------------------------------
    # ‚úÖ Approval distribution (robust to 0/1, strings, themes)
    # ---------------------------------------------------------
    st.markdown("### üìà Approval Distribution")

    # 1) Normalize labels
    if "decision" not in df.columns:
        st.info("No 'decision' column found; skipping approval chart.")
    else:
        vals = df["decision"]

        def to_label(v):
            if isinstance(v, str):
                s = v.strip().lower()
                if s in ("approve", "approved", "yes", "y", "1", "true"):
                    return "approve"
                if s in ("reject", "rejected", "no", "n", "0", "false"):
                    return "reject"
                return s or "unknown"
            try:
                return "approve" if float(v) >= 1 else "reject"
            except Exception:
                return "unknown"

        df = df.copy()
        df["decision_label"] = vals.map(to_label).fillna("unknown")

        # 2) Safe color map (must be a LIST ‚Üí map to dict)
        palette = px.colors.qualitative.Set2  # e.g., ['#66c2a5', '#fc8d62', ...]
        color_map = {
            "approve": palette[0] if len(palette) > 0 else "#22c55e",
            "reject":  palette[1] if len(palette) > 1 else "#ef4444",
            "unknown": palette[2] if len(palette) > 2 else "#94a3b8",
        }

        # 3) Fixed category order for readability
        categories = ["approve", "reject", "unknown"]

        fig = px.histogram(
            df,
            x="decision_label",
            color="decision_label",
            category_orders={"decision_label": categories},
            color_discrete_map=color_map,
            title="Approval vs Rejection",
        )
        fig.update_layout(
            legend_title_text="Decision",
            bargap=0.2,
            paper_bgcolor="rgba(0,0,0,0)",
            plot_bgcolor="rgba(0,0,0,0)",
            font_color=("#e2e8f0" if st.session_state.get("theme", "dark") == "dark" else "#0f172a"),
        )
        st.plotly_chart(fig, use_container_width=True)


    # ---------------------------------------------------------
    # ‚úÖ Department Handoff: Credit / Risk / Compliance / CS
    # ---------------------------------------------------------
    st.markdown("## üè¶ Department Handoff Packages")
    
    # ---------- helpers ----------
    def pick(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
        """Return df with only columns that actually exist (no KeyError)."""
        keep = [c for c in cols if c in df.columns]
        return df[keep].copy()

    # ---------- ensure 'reason' exists ----------
    if "reason" not in df.columns:
        if "explanation" in df.columns:
            df["reason"] = df["explanation"].astype(str).str.slice(0, 200)
        elif {"pd", "dti", "ltv"}.issubset(df.columns) or "score" in df.columns:
            def infer_reason(row):
                try:
                    if float(row.get("pd", 0)) >= 0.15:
                        return "High probability of default"
                    if float(row.get("dti", 0)) >= 0.5:
                        return "High debt-to-income"
                    if float(row.get("ltv", 0)) >= 0.8:
                        return "High loan-to-value"
                    if float(row.get("score", 999)) < 600:
                        return "Low credit score"
                except Exception:
                    pass
                return "Policy/Other"
            df["reason"] = df.apply(infer_reason, axis=1)
        else:
            df["reason"] = ""

    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
    HANDOFF_DIR = Path("./credit_handoff")
    ZIP_DIR = HANDOFF_DIR / "zips"
    HANDOFF_DIR.mkdir(exist_ok=True)
    ZIP_DIR.mkdir(exist_ok=True)
    
    credit = pick(df, ["application_id","score","decision","reason","income","loan_amount"])
    risk = pick(df, ["application_id","score","pd","ltv","dti","decision"])
    compliance = pick(df, ["application_id","account_age","delinquencies","fraud_flag","decision"])
    customer = pick(df, ["application_id","score","decision","explanation","reason"])


    # credit = df[["application_id","score","decision","reason","income","loan_amount"]]
    # risk = df[["application_id","score","pd","ltv","dti","decision"]]
    # compliance = df[["application_id","account_age","delinquencies","fraud_flag","decision"]]
    # customer = df[["application_id","score","decision","explanation"]]

    paths = {
        "credit": HANDOFF_DIR / f"credit_{ts}.csv",
        "risk": HANDOFF_DIR / f"risk_{ts}.csv",
        "compliance": HANDOFF_DIR / f"compliance_{ts}.csv",
        "customer": HANDOFF_DIR / f"customer_service_{ts}.csv",
    }

    # Save all
    credit.to_csv(paths["credit"], index=False)
    risk.to_csv(paths["risk"], index=False)
    compliance.to_csv(paths["compliance"], index=False)
    customer.to_csv(paths["customer"], index=False)

    # ---------------------------------------------------------
    # ‚úÖ ZIP bundle
    # ---------------------------------------------------------
    zip_path = ZIP_DIR / f"credit_handoff_{ts}.zip"
    with zipfile.ZipFile(zip_path, "w") as zf:
        for p in paths.values():
            zf.write(p, arcname=os.path.basename(p))

    st.download_button(
        "‚¨áÔ∏è Download Full Handoff ZIP",
        data=open(zip_path, "rb").read(),
        file_name=os.path.basename(zip_path),
        mime="application/zip",
        use_container_width=True,
    )

    # # st.markdown("### üß© Department Package Map")
    # # st.json({k: list(df[list(credit.columns)].columns)})
    # st.markdown("### üß© Department Package Map")
    # st.json({
    #     "credit":   list(credit.columns),
    #     "risk":     list(risk.columns),
    #     "compliance": list(compliance.columns),
    #     "customer_service": list(customer.columns),
    # })

    # Optional: tell user if something was missing
    expected = {
        "credit": ["application_id","score","decision","reason","income","loan_amount"],
        "risk": ["application_id","score","pd","ltv","dti","decision"],
        "compliance": ["application_id","account_age","delinquencies","fraud_flag","decision"],
        "customer_service": ["application_id","score","decision","explanation","reason"],
    }
    missing_report = {
        pkg: [c for c in expected[pkg] if c not in cols]
        for pkg, cols in {
            "credit": credit.columns,
            "risk": risk.columns,
            "compliance": compliance.columns,
            "customer_service": customer.columns,
        }.items()
    }
    if any(missing_report.values()):
        st.info(f"Some expected columns were not present and were skipped: {missing_report}")


with tab_feedback:
    render_feedback_tab("üí≥ Credit Appraisal Agent")



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üó£Ô∏è TAB 8 ‚Äî Feedback & Feature Requests
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tab_feedback:
    st.subheader("üó£Ô∏è Share Your Feedback and Feature Ideas")

    FEEDBACK_FILE = os.path.join(BASE_DIR, "agents_feedback.json")

    def load_feedback() -> dict:
        try:
            with open(FEEDBACK_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    def save_feedback(data: dict):
        try:
            with open(FEEDBACK_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            st.error(f"Could not save feedback: {e}")

    feedback_data = load_feedback()

    # View all current agent feedback
    st.markdown("### üí¨ Current Agent Reviews & Ratings")
    for agent, fb in feedback_data.items():
        with st.expander(f"‚≠ê {agent} ‚Äî {fb.get('rating', 0)}/5  |  üë• {fb.get('users', 0)} users"):
            st.markdown("#### Recent Comments:")
            for cmt in reversed(fb.get("comments", [])):
                st.markdown(f"- {cmt}")
            st.markdown("---")

    st.markdown("### ‚úçÔ∏è Submit Your Own Feedback or Feature Request")

    agent_choice = st.selectbox("Select Agent", list(feedback_data.keys()))
    new_comment = st.text_area("Your Comment or Feature Suggestion", placeholder="e.g. Add multi-language support for reports...")
    new_rating = st.slider("Your Rating", 1, 5, 5)

    # if st.button("üì® Submit Feedback"):
    #     if new_comment.strip():
    #         fb = feedback_data.get(agent_choice, {"rating": 0, "users": 0, "comments": []})
    #         fb["comments"].append(new_comment.strip())
    #         fb["rating"] = round((fb.get("rating", 0) + new_rating) / 2, 2)
    #         fb["users"] = fb.get("users", 0) + 1
    #         feedback_data[agent_choice] = fb
    #         save_feedback(feedback_data)
    #         st.success("‚úÖ Feedback submitted successfully!")
    #         st.rerun()
    #     else:
    #         st.warning("Please enter a comment before submitting.")
    if st.button("üì® Submit Feedback"):
        if new_comment.strip():
            fb = feedback_data.get(agent_choice, {"rating": 0, "users": 0, "comments": []})
            fb["comments"].append(new_comment.strip())
            fb["rating"] = round((fb.get("rating", 0) + new_rating) / 2, 2)
            fb["users"] = fb.get("users", 0) + 1
            feedback_data[agent_choice] = fb
            save_feedback(feedback_data)

            # ‚úÖ Sync latest feedback globally
            st.session_state["feedback_data"] = feedback_data

            # ‚úÖ Force full reload so Landing updates instantly
            st.success("‚úÖ Feedback submitted successfully!")
            st.rerun()
        else:
            st.warning("Please enter a comment before submitting.")




# Legacy credit theme (kept for optional reuse)
LEGACY_CREDIT_THEME_SNIPPET = '''
def legacy_credit_theme(theme: str = "dark"):
    import streamlit as st

    st.markdown("""
    <style>
    /* ===============================================
       üåô MACOS BLUE DARK THEME ‚Äî GLOBAL BASE
    =============================================== */
    html, body, [data-testid="stAppViewContainer"] {
        background: radial-gradient(circle at 20% 20%, #0b0f16, #060a12 85%) !important;
        color: #f8fafc !important;
        font-family: "Inter","SF Pro Display","Segoe UI",system-ui,sans-serif !important;
    }

    h1,h2,h3,h4,h5,h6 {
        color: #f8fafc !important;
        font-weight: 700 !important;
        letter-spacing: -0.02em !important;
    }

    p, li, label, span, div {
        color: #e2e8f0 !important;
    }
    small, .stCaption { color: #94a3b8 !important; }

    a, a:link, a:visited { color: #339dff !important; }
    a:hover { color: #60a5fa !important; text-decoration: underline; }

    hr {
        border: none !important;
        height: 1px !important;
        background: linear-gradient(90deg,transparent,#007aff,transparent) !important;
    }

    /* ===============================================
       üß± CONTAINERS & CARDS
    =============================================== */
    .stMarkdown, .stContainer, .stAlert, [class*="stCard"], [class*="block-container"] {
        background: #0f172a !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 12px !important;
        box-shadow: 0 4px 16px rgba(0,0,0,0.5) !important;
    }

    /* ===============================================
       üîò BUTTONS ‚Äî macOS BLUE
    =============================================== */
    button[kind="primary"], .stButton>button, .stDownloadButton>button, .stDownloadButton button {
        background: linear-gradient(180deg,#007aff,#005ecb) !important;
        color: #ffffff !important;
        border: 1px solid #0051b8 !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
        padding: 0.5rem 1rem !important;
        box-shadow: 0 4px 10px rgba(0,122,255,0.35),
                    inset 0 -1px 0 rgba(255,255,255,0.2) !important;
        transition: all 0.25s ease-in-out !important;
    }
    button[kind="primary"]:hover, .stButton>button:hover, .stDownloadButton>button:hover {
        background: linear-gradient(180deg,#339dff,#006ae6) !important;
        box-shadow: 0 4px 14px rgba(0,122,255,0.45) !important;
        transform: translateY(-1px) !important;
    }
    button[kind="primary"]:active, .stButton>button:active, .stDownloadButton>button:active {
        background: linear-gradient(180deg,#004fc4,#0042a8) !important;
        box-shadow: inset 0 2px 6px rgba(0,122,255,0.3) !important;
        transform: translateY(0) !important;
    }
    .stButton button[disabled], .stDownloadButton button[disabled] {
        background: #1e293b !important;
        color: #64748b !important;
        border: 1px solid #334155 !important;
    }

    /* ===============================================
    üß† INPUTS (Text, Select, Number) & FOCUS STATE
    =============================================== */
    .stTextInput>div>div>input,
    .stSelectbox>div>div>div,
    .stNumberInput input {
        background: #111827 !important;
        color: #f8fafc !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 8px !important;
        padding: 6px 10px !important;
        transition: all 0.25s ease;
    }
    .stTextInput>div>div>input:focus,
    .stSelectbox>div>div>div:focus-within,
    .stNumberInput input:focus {
        outline: none !important;
        border-color: #007aff !important;
        box-shadow: 0 0 0 2px rgba(0,122,255,0.4) !important;
    }
    ::placeholder {
        color: #9ca3af !important;
        opacity: 1 !important;
    }
    /* ===============================================
   üéõ DROPDOWN MENUS
    =============================================== */
    [data-baseweb="popover"], [role="listbox"] {
        background: #0f172a !important;
        color: #f8fafc !important;
        border: 1px solid #1e3a8a !important;
        box-shadow: 0 4px 20px rgba(0,0,0,0.6) !important;
    }
    [data-baseweb="menu-item"] {
        background: #0f172a !important;
        color: #f8fafc !important;
    }
    [data-baseweb="menu-item"]:hover {
        background: #1e3a8a !important;
        color: #ffffff !important;
    }
    /* ===============================================
    üß≠ SIDEBAR THEME
    =============================================== */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg,#0d1320,#060a12) !important;
        border-right: 1px solid #1e3a8a !important;
        color: #f8fafc !important;
    }

    
    /* ===============================================
       ‚òëÔ∏è CHECKBOXES / RADIOS / SLIDERS
    =============================================== */
    input[type="checkbox"], input[type="radio"] {
        accent-color: #007aff !important;
    }
    .stSlider [role="slider"] {
        background-color: #007aff !important;
    }

    /* ===============================================
       üóÇÔ∏è TABS
    =============================================== */
    .stTabs [data-baseweb="tab-list"] button {
        color: #e2e8f0 !important;
        background: #111827 !important;
        border: 1px solid #1e293b !important;
        border-radius: 10px !important;
        font-weight: 500 !important;
        margin-right: 4px !important;
    }
    .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {
        background: #007aff !important;
        color: #ffffff !important;
        box-shadow: 0 0 12px rgba(0,122,255,0.4) !important;
    }

    /* ===============================================
       üß≠ EXPANDERS / ACCORDIONS
    =============================================== */
    .streamlit-expanderHeader {
        background: linear-gradient(90deg,#0d284d,#0a1f3a) !important;
        color: #dbeafe !important;
        border: 1px solid #1e3a5f !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
    }
    .streamlit-expanderContent {
        background: #0f172a !important;
        color: #e2e8f0 !important;
        border: 1px solid #1e3a5f !important;
        border-radius: 0 0 8px 8px !important;
    }

    /* ===============================================
       üìä METRIC CARDS (st.metric)
    =============================================== */
    [data-testid="stMetric"] {
        background: linear-gradient(180deg,#0b1220,#101a2c) !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 10px !important;
        box-shadow: inset 0 0 10px rgba(255,255,255,0.03),
                    0 3px 10px rgba(0,0,0,0.6) !important;
        padding: 10px 14px !important;
        text-align: center !important;
    }
    div[data-testid="stMetricLabel"] {
        color: #94a3b8 !important;
        font-size: 0.85rem !important;
        font-weight: 500 !important;
    }
    div[data-testid="stMetricValue"] {
        color: #ffffff !important;
        font-size: 1.3rem !important;
        font-weight: 600 !important;
    }

    /* ===============================================
       üìä METRIC COMPARISON TABLE ‚Äî FINAL
    =============================================== */
    [data-testid="stDataFrame"] {
        background: radial-gradient(circle at 50% 50%, #0b1220, #060a12 90%) !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 12px !important;
        box-shadow:
            0 0 14px rgba(0,0,0,0.6) inset,
            0 4px 18px rgba(0,0,0,0.7),
            0 0 12px rgba(0,122,255,0.15) !important;
        margin-top: 12px !important;
        padding: 8px !important;
    }
    [data-testid="stDataFrame"] thead tr th {
        background: linear-gradient(90deg,#004fc4,#007aff) !important;
        color: #ffffff !important;
        border-bottom: 2px solid #007aff !important;
        font-weight: 700 !important;
        text-transform: uppercase !important;
        letter-spacing: 0.02em !important;
        font-size: 0.92rem !important;
        padding: 10px 14px !important;
    }
    [data-testid="stDataFrame"] tbody tr {
        background-color: #0b1220 !important;
        color: #ffffff !important;
        transition: background 0.25s ease;
    }
    [data-testid="stDataFrame"] tbody tr:nth-child(even) {
        background-color: #101a2c !important;
    }
    [data-testid="stDataFrame"] tbody tr:hover {
        background-color: #112a52 !important;
        box-shadow: 0 0 8px rgba(0,122,255,0.25) inset !important;
    }
    [data-testid="stDataFrame"] tbody td {
        border-top: 1px solid #1e3a8a !important;
        color: #ffffff !important;
        padding: 9px 14px !important;
        font-size: 0.95rem !important;
        font-weight: 500 !important;
    }
    [data-testid="stDataFrame"] tbody td:last-child {
        color: #60a5fa !important;
        font-weight: 500 !important;
    }

    /* ===============================================
       üìÅ FILE UPLOADER
    =============================================== */
    [data-testid="stFileUploaderDropzone"] {
        background: rgba(255,255,255,0.03) !important;
        border: 1px dashed #1e3a8a !important;
        border-radius: 10px !important;
        color: #cbd5e1 !important;
        transition: all 0.25s ease;
    }
    [data-testid="stFileUploaderDropzone"]:hover {
        border-color: #007aff !important;
        background: rgba(0,122,255,0.1) !important;
    }

    /* ===============================================
       ‚ö†Ô∏è ALERT BOXES
    =============================================== */
    [data-testid^="stAlert"] {
        border-radius: 10px !important;
        border: 1px solid #1e3a8a !important;
        color: #e2e8f0 !important;
        box-shadow: 0 3px 15px rgba(0,0,0,0.4) !important;
    }
    [data-testid="stAlertInfo"]    { background: linear-gradient(145deg,#0d1829,#10243d)!important; }
    [data-testid="stAlertSuccess"] { background: linear-gradient(145deg,#0f2414,#183820)!important; }
    [data-testid="stAlertError"]   { background: linear-gradient(145deg,#2b1617,#1a0c0d)!important; }
    [data-testid="stAlertWarning"] { background: linear-gradient(145deg,#2f2a10,#1c1a0a)!important; }

    </style>
    """, unsafe_allow_html=True)

'''

render_chat_assistant(
    page_id="credit_appraisal",
    context=_build_credit_chat_context(),
    faq_questions=CREDIT_FAQ,
)



==================== ./troubleshooter_agent.py ====================
"""
üß† AI Troubleshooter Agent ‚Äî First Principles + Case Memory
"""

from __future__ import annotations

from datetime import datetime
import random
from textwrap import dedent

import pandas as pd
import streamlit as st

from services.ui.theme_manager import (
    apply_theme as apply_global_theme,
    render_theme_toggle,
)
from services.ui.components.operator_banner import render_operator_banner


STAGE_KEYS = [
    "intake",
    "ticket",
    "appraisal",
    "problem",
    "decision",
    "potential",
    "ai_plan",
    "human_review",
    "training",
    "deployment",
]

STAGE_LABELS = {
    "intake": "Ticket Intake",
    "ticket": "Ticket Generator",
    "appraisal": "Situation Appraisal",
    "problem": "Problem Analysis",
    "decision": "Decision Analysis",
    "potential": "Potential Problems",
    "ai_plan": "AI Troubleshooting Plan",
    "human_review": "Human Review",
    "training": "Training & Escalation",
    "deployment": "Deployment & Feedback",
}


DEFAULT_INCIDENTS = [
    {
        "id": "INC-87342",
        "source": "ServiceNow",
        "service": "Payments API",
        "severity": "P1",
        "status": "Investigating",
        "summary": "Spike in 504 errors from payments cluster A",
    },
    {
        "id": "INC-87318",
        "source": "Jira",
        "service": "Customer Portal",
        "severity": "P2",
        "status": "Pending RCA",
        "summary": "Login redirect loop impacting 12% of sessions",
    },
    {
        "id": "INC-87291",
        "source": "ServiceNow",
        "service": "Core Messaging",
        "severity": "P1",
        "status": "Mitigated",
        "summary": "Kafka consumer lag alerts firing for shard-5",
    },
]

SYNTHETIC_TEMPLATES = [
    {
        "service": "Data Lake",
        "summary": "ETL latency spike after schema change",
        "severity": "P2",
        "status": "Open",
        "source": "Synthetic",
    },
    {
        "service": "Identity Platform",
        "summary": "SSO redirect loop for mobile Safari users",
        "severity": "P1",
        "status": "Investigating",
        "source": "Synthetic",
    },
    {
        "service": "Notifications",
        "summary": "Push queue backlog exceeded SLO for APAC region",
        "severity": "P2",
        "status": "Pending RCA",
        "source": "Synthetic",
    },
]

CASE_MEMORY = pd.DataFrame(
    [
        {
            "case_id": "CASE-101",
            "service": "Payments API",
            "root_cause": "Expired TLS cert on load-balancer",
            "fix_time": "28 min",
            "lessons": "Add cert-expiry alert + self-check",
        },
        {
            "case_id": "CASE-099",
            "service": "Customer Portal",
            "root_cause": "Redis cache replication stall",
            "fix_time": "41 min",
            "lessons": "Auto scale read replicas during deploys",
        },
        {
            "case_id": "CASE-095",
            "service": "Core Messaging",
            "root_cause": "Misconfigured consumer group offset reset",
            "fix_time": "33 min",
            "lessons": "Guard rails on offset reset CLI",
        },
    ]
)


def _set_query_params_safe(**kwargs):
    """Best-effort query param setter for cross-version Streamlit support."""
    try:
        for k, v in kwargs.items():
            st.query_params[k] = v
        return True
    except Exception:
        pass
    try:
        st.experimental_set_query_params(**kwargs)
        return True
    except Exception:
        return False


def _go_stage(target_stage: str) -> None:
    """Hop back into the main landing/agents flow."""
    st.session_state["stage"] = target_stage
    try:
        st.switch_page("app.py")
        return
    except Exception:
        pass
    _set_query_params_safe(stage=target_stage)
    st.rerun()


def render_troubleshooter_nav() -> None:
    stage = st.session_state.get("stage", "landing")
    c1, c2, c3 = st.columns([1, 1, 2.5])

    with c1:
        if st.button("üè† Back to Home", key=f"tr_home_{stage}"):
            _go_stage("landing")
            st.stop()

    with c2:
        if st.button("ü§ñ Back to Agents", key=f"tr_agents_{stage}"):
            _go_stage("agents")
            st.stop()

    with c3:
        render_theme_toggle("üåó Dark mode", key="troubleshooter_theme_toggle_nav")

    st.markdown("---")


def _init_stage_tracker():
    ss = st.session_state
    if "ts_stage_ready" not in ss:
        ss["ts_stage_ready"] = {key: False for key in STAGE_KEYS}
    else:
        for key in STAGE_KEYS:
            ss["ts_stage_ready"].setdefault(key, False)


def _dependencies_met(stage_key: str) -> bool:
    idx = STAGE_KEYS.index(stage_key)
    required = STAGE_KEYS[:idx]
    ready = st.session_state.get("ts_stage_ready", {})
    missing = [STAGE_LABELS[k] for k in required if not ready.get(k)]
    if missing:
        st.info(f"Please complete {', '.join(missing)} before working on this stage.")
        return False
    return True


def _render_stage_completion(stage_key: str):
    ready = st.session_state["ts_stage_ready"]
    if ready.get(stage_key):
        st.success(f"{STAGE_LABELS[stage_key]} marked complete. You can revisit to edit.")
        return
    if st.button(f"Mark {STAGE_LABELS[stage_key]} complete ‚úÖ", key=f"complete_{stage_key}"):
        ready[stage_key] = True
        st.success(f"{STAGE_LABELS[stage_key]} completed ‚Äî proceed to the next tab.")


def _init_state():
    ss = st.session_state
    _init_stage_tracker()
    ss.setdefault("ts_ticket_source", "ServiceNow")
    ss.setdefault("ts_selected_ticket", DEFAULT_INCIDENTS[0])
    ss.setdefault(
        "ts_appraisal",
        {
            "issues": ["Customer Impact", "Regulatory"],
            "concerns": "Customers in SEA unable to complete checkout; regulators pinged for SLA breach.",
            "urgency": 4,
            "impact": 4,
            "notes": "Need status update for leadership within 30 minutes.",
        },
    )
    ss.setdefault(
        "ts_problem",
        {
            "what": "Increased 504 responses on Payments API cluster A",
            "where": "apac-cluster-a, az2",
            "when": "Started 05:42 UTC, recurs every deploy",
            "extent": "8% of traffic, mostly card payments",
            "is_conditions": "Only cluster A, only card transactions, only API v2",
            "is_not_conditions": "Cluster B unaffected, ACH unaffected, API v1 stable",
            "hypotheses": "LB cert expired; node draining mis-config; new WAF rule",
            "tests": "Compare cert expiry dates; replay traffic via staging; disable WAF rule temporarily",
        },
    )
    ss.setdefault(
        "ts_decision",
        {
            "must": "Restore sub-2s latency; keep compliance logging intact.",
            "nice": "Avoid full region failover; no customer comms if possible.",
            "alternatives": [
                {"name": "Rotate LB cert", "benefit": 5, "risk": 2},
                {"name": "Fail traffic to cluster B", "benefit": 4, "risk": 3},
            ],
        },
    )
    ss.setdefault(
        "ts_potential",
        {
            "risks": "Cluster B overload, cert rotation fails, new regression hidden.",
            "preventive": "Warm extra nodes; backup cert ready; post-deploy synthetic checks.",
            "contingency": "If rotation fails, failover to DR; notify comms team.",
            "opportunities": "Automate cert renewal alerts; capture playbook for training.",
        },
    )
    if "ts_ai_candidates" not in ss:
        ss["ts_ai_candidates"] = [
            {
                "case_id": row.case_id,
                "cause": row.root_cause,
                "confidence": round(0.65 - idx * 0.07, 2),
                "lessons": row.lessons,
                "status": "pending",
            }
            for idx, row in enumerate(CASE_MEMORY.itertuples())
        ]
    ss.setdefault("ts_ai_attempts", 0)
    ss.setdefault("ts_ai_solution", None)
    ss.setdefault("ts_escalated", False)
    ss.setdefault("ts_feedback_log", [])
    ss.setdefault("ts_ai_plan", [])
    ss.setdefault("ts_human_reviews", [])
    ss.setdefault("ts_training_records", [])
    ss.setdefault(
        "ts_deployment_meta",
        {"exported": False, "timestamp": None, "notes": "", "feedback": []},
    )
    ss.setdefault(
        "ts_ticket_payload",
        {
            "title": "Payments API 504 surge",
            "impact": "Checkout latency degraded for 8% of SEA customers",
            "resolution": "Rotated LB cert + drained bad node",
            "channel": "ServiceNow",
            "appraisal": None,
            "problem": None,
            "decision": None,
            "potential": None,
            "ai_solution": None,
            "escalation": None,
        },
    )


def render_ticket_intake_stage():
    ss = st.session_state
    if not _dependencies_met("intake"):
        return
    st.subheader("‚ë† Import Ticket & Context")
    col_a, col_b = st.columns([1.2, 1])

    with col_a:
        src = st.radio(
            "Ticket source",
            ("ServiceNow", "Jira", "CSV Upload", "Manual Entry"),
            key="ts_ticket_source_radio",
        )

        if src in {"ServiceNow", "Jira"}:
            env = st.selectbox("Environment", ["Production", "Staging", "DR"])
            ticket_map = {
                f"{item['id']} ¬∑ {item['service']}": item for item in DEFAULT_INCIDENTS
            }
            selection = st.selectbox("Recent incidents", list(ticket_map.keys()))
            if st.button("üì• Load ticket", key="ts_load_ticket"):
                ss["ts_ticket_source"] = src
                ss["ts_selected_ticket"] = ticket_map[selection]
                ss["ts_selected_ticket"]["environment"] = env
                _hydrate_from_ticket(ss["ts_selected_ticket"])
                st.success(f"Loaded {selection} from {src}")

        elif src == "CSV Upload":
            uploaded = st.file_uploader("Upload CSV export", type=["csv"])
            if uploaded:
                try:
                    df = pd.read_csv(uploaded)
                    st.dataframe(df.head(), use_container_width=True)
                    st.info("Pick a row and press 'Load ticket' to continue.")
                except Exception as exc:
                    st.error(f"Could not read CSV: {exc}")
        else:
            manual_id = st.text_input("Ticket ID")
            manual_summary = st.text_area("Summary")
            if st.button("Save manual ticket", key="ts_manual_ticket"):
                ss["ts_selected_ticket"] = {
                    "id": manual_id or "INC-manual",
                    "source": "Manual",
                    "service": "Custom",
                    "severity": "Triage",
                    "status": "Draft",
                    "summary": manual_summary or "Operator entered ticket",
                }
                _hydrate_from_ticket(ss["ts_selected_ticket"])
                st.success("Manual ticket saved.")

    with col_b:
        st.caption("Live intake feed")
        st.dataframe(pd.DataFrame(DEFAULT_INCIDENTS), use_container_width=True, height=240)
        st.info("Need a synthetic practice ticket? Use the dedicated Ticket Generator stage right after this intake step.")

    ticket = ss.get("ts_selected_ticket")
    if isinstance(ticket, dict):
        summary = ticket.get("summary", "N/A")
        service = ticket.get("service", "N/A")
        severity = ticket.get("severity", "N/A")
        status = ticket.get("status", "N/A")
        environment = ticket.get("environment", "Production")
        st.success(
            dedent(
                f"""
                **Active Ticket**: {ticket.get('id', 'INC-draft')} ({environment})
                - Service: {service} | Severity: {severity} | Status: {status}
                - Summary: {summary}
                """
            )
        )
        st.json(ticket, expanded=False)
    _render_stage_completion("intake")


def render_situation_appraisal_stage():
    ss = st.session_state
    if not _dependencies_met("appraisal"):
        return
    st.subheader("‚ë° Situation Appraisal (Kepner-Tregoe)")
    issues = st.multiselect(
        "Concerns / issues in scope",
        ["Customer Impact", "Regulatory", "Security", "Capacity", "Observability", "Vendors"],
        default=ss["ts_appraisal"]["issues"],
    )
    concerns = st.text_area(
        "Describe the situation",
        ss["ts_appraisal"]["concerns"],
        height=120,
    )
    col_u, col_i = st.columns(2)
    with col_u:
        urgency = st.slider("Urgency (time pressure)", 1, 5, ss["ts_appraisal"]["urgency"])
    with col_i:
        impact = st.slider("Impact (blast radius)", 1, 5, ss["ts_appraisal"]["impact"])
    notes = st.text_area(
        "Other notes / stakeholders / next updates",
        ss["ts_appraisal"]["notes"],
        height=100,
    )
    if st.button("Save appraisal"):
        ss["ts_appraisal"] = {
            "issues": issues,
            "concerns": concerns,
            "urgency": urgency,
            "impact": impact,
            "notes": notes,
        }
        st.success("Situation appraisal captured.")
        _cascade_from_appraisal()

    priority = urgency * impact
    if priority >= 16:
        label = "Critical"
    elif priority >= 9:
        label = "High"
    elif priority >= 4:
        label = "Medium"
    else:
        label = "Low"
    st.metric("Priority score", f"{priority}/25", label)
    st.caption(
        "Higher urgency √ó impact indicates which concerns need deeper analysis right now."
    )
    _render_stage_completion("appraisal")


def render_problem_analysis_stage():
    ss = st.session_state
    if not _dependencies_met("problem"):
        return
    st.subheader("‚ë¢ Problem Analysis (Define & Decompose)")
    col_main, col_is = st.columns([1.2, 0.8])

    st.session_state.setdefault("ts_problem_is", ss["ts_problem"]["is_conditions"])
    st.session_state.setdefault("ts_problem_is_not", ss["ts_problem"]["is_not_conditions"])

    with col_main:
        what = st.text_input("What is wrong?", ss["ts_problem"]["what"])
        where = st.text_input("Where does it occur?", ss["ts_problem"]["where"])
        when = st.text_input("When is it observed?", ss["ts_problem"]["when"])
        extent = st.text_input("Extent / magnitude", ss["ts_problem"]["extent"])
        hypotheses = st.text_area(
            "Possible causes (decompose into smallest parts)",
            ss["ts_problem"]["hypotheses"],
            height=120,
        )
        tests = st.text_area(
            "Experiments / diagnostics to run",
            ss["ts_problem"]["tests"],
            height=120,
        )
        if st.button("Save problem analysis"):
            ss["ts_problem"] = {
                "what": what,
                "where": where,
                "when": when,
                "extent": extent,
                "is_conditions": st.session_state.get("ts_problem_is"),
                "is_not_conditions": st.session_state.get("ts_problem_is_not"),
                "hypotheses": hypotheses,
                "tests": tests,
            }
            st.success("Problem analysis updated.")
            _cascade_from_problem()

    with col_is:
        is_conditions = st.text_area(
            "IS (conditions present)",
            ss["ts_problem"]["is_conditions"],
            height=140,
            key="ts_problem_is",
        )
        is_not_conditions = st.text_area(
            "IS NOT (conditions absent)",
            ss["ts_problem"]["is_not_conditions"],
            height=140,
            key="ts_problem_is_not",
        )
        st.caption(
            "Compare IS vs IS NOT to quickly eliminate causes that don't match all observed conditions."
        )
    _render_stage_completion("problem")


def render_decision_analysis_stage():
    ss = st.session_state
    if not _dependencies_met("decision"):
        return
    st.subheader("‚ë£ Decision Analysis (Choose Action)")
    must = st.text_area("Must-have objectives", ss["ts_decision"]["must"], height=120)
    nice = st.text_area("Nice-to-have objectives", ss["ts_decision"]["nice"], height=120)

    st.markdown("##### Alternatives")
    new_alt = st.text_input("Alternative name")
    benefit = st.slider("Benefit score", 1, 5, 3)
    risk = st.slider("Risk score (lower is safer)", 1, 5, 3)
    if st.button("Add alternative"):
        ss["ts_decision"]["alternatives"].append(
            {"name": new_alt or f"Option {len(ss['ts_decision']['alternatives'])+1}", "benefit": benefit, "risk": risk}
        )
        st.success("Alternative added.")

    if ss["ts_decision"]["alternatives"]:
        df = pd.DataFrame(ss["ts_decision"]["alternatives"])
        df["score"] = df["benefit"] - df["risk"]
        st.dataframe(df, hide_index=True, use_container_width=True)
        best = df.sort_values("score", ascending=False).iloc[0]
        st.info(f"Current best option: {best['name']} (score {best['score']})")

    if st.button("Save decision analysis"):
        ss["ts_decision"]["must"] = must
        ss["ts_decision"]["nice"] = nice
        st.success("Decision analysis saved.")
        _cascade_from_decision()
    _render_stage_completion("decision")


def render_potential_problem_stage():
    ss = st.session_state
    if not _dependencies_met("potential"):
        return
    st.subheader("‚ë§ Potential Problem / Opportunity Analysis + AI Suggestions")
    risks = st.text_area("What could go wrong?", ss["ts_potential"]["risks"], height=120)
    preventive = st.text_area(
        "Preventive actions",
        ss["ts_potential"]["preventive"],
        height=120,
    )
    contingency = st.text_area(
        "Contingency plan if risk happens",
        ss["ts_potential"]["contingency"],
        height=120,
    )
    opportunities = st.text_area(
        "Opportunities to amplify",
        ss["ts_potential"]["opportunities"],
        height=120,
    )
    if st.button("Save potential problem analysis"):
        ss["ts_potential"] = {
            "risks": risks,
            "preventive": preventive,
            "contingency": contingency,
            "opportunities": opportunities,
        }
        st.success("Plan saved.")

    st.markdown("##### AI Suggested Root Causes / Leads")
    candidates = ss["ts_ai_candidates"]
    if candidates:
        st.dataframe(
            pd.DataFrame(candidates)[["case_id", "cause", "confidence", "status"]],
            hide_index=True,
            use_container_width=True,
        )
    solution = ss.get("ts_ai_solution")
    escalated = ss.get("ts_escalated")
    if solution:
        st.success(
            f"AI-confirmed cause: {solution['cause']} (case {solution['case_id']}). Include lessons in your plan."
        )
    elif escalated:
        st.warning("AI exhausted local cases; incident escalated to Level 3 (Kira / eService).")
    else:
        options = [c["case_id"] for c in candidates if c["status"] != "rejected"]
        if not options:
            st.error("No more AI candidates. Consider escalating.")
        else:
            selected = st.selectbox("Review AI candidate", options, key="ts_ai_selected")
            current = next(c for c in candidates if c["case_id"] == selected)
            st.markdown(
                f"**Hypothesis:** {current['cause']}  \n**Lessons learned:** {current['lessons']}"
            )
            rating = st.slider(
                "Does this match what you're seeing?",
                1,
                5,
                4,
                key="ts_ai_rating",
            )
            feedback = st.text_input(
                "Feedback to AI (evidence, contradicting clues)",
                key="ts_ai_feedback",
            )
            col_accept, col_reject = st.columns(2)
            if col_accept.button("üëç Accept and apply fix", key="ts_ai_accept"):
                current["status"] = "confirmed"
                ss["ts_ai_solution"] = {
                    "cause": current["cause"],
                    "case_id": current["case_id"],
                    "lessons": current["lessons"],
                }
                st.success("Root cause accepted. Capture mitigation in Decision stage.")
            if col_reject.button("üëé Doesn't match", key="ts_ai_reject"):
                current["status"] = "rejected"
                current["confidence"] = max(0.1, current["confidence"] - 0.15 * (6 - rating))
                ss["ts_ai_attempts"] += 1
                ss["ts_feedback_log"].insert(
                    0,
                    {
                        "case_id": current["case_id"],
                        "rating": rating,
                        "feedback": feedback or "No comment",
                        "timestamp": datetime.utcnow().isoformat(),
                    },
                )
                st.warning("Feedback saved; AI candidate list updated.")
        attempts = ss["ts_ai_attempts"]
        if attempts >= 3 and not ss.get("ts_ai_solution"):
            with st.expander("üîç Research checklist (Quora / vendor forums)"):
                st.markdown(
                    """
                    - Search Quora, vendor communities, or public postmortems for similar incidents.  
                    - Ask LLM copilots for alternate hypotheses given your telemetry.  
                    - Feed new clues back into the candidate selector above.
                    """
                )
        if attempts >= 5 and not ss.get("ts_ai_solution") and not ss.get("ts_escalated"):
            if st.button("Escalate to Level 3 (Kira / eService)", key="ts_escalate_l3"):
                ss["ts_escalated"] = True
                st.success("Escalation recorded; include summary in final ticket.")

    _render_stage_completion("potential")


def render_ai_plan_stage():
    ss = st.session_state
    if not _dependencies_met("ai_plan"):
        return
    st.subheader("‚ë• AI Assisted Troubleshooting Plan")
    st.caption("AI proposes structured steps for each hypothesis; adjust owners & status before execution.")
    plan = ss["ts_ai_plan"] or _generate_ai_plan()
    df = pd.DataFrame(plan) if plan else pd.DataFrame(
        [{"Hypothesis": "No hypothesis captured", "Troubleshooting Step": "", "Owner": "Ops", "Status": "Pending"}]
    )
    edited = st.data_editor(
        df,
        num_rows="dynamic",
        column_config={
            "Hypothesis": st.column_config.TextColumn(width="medium"),
            "Troubleshooting Step": st.column_config.TextColumn(width="large"),
            "Owner": st.column_config.TextColumn(width="small"),
            "Status": st.column_config.SelectboxColumn(options=["Pending", "In Progress", "Done"]),
        },
        use_container_width=True,
    )
    if st.button("Save AI troubleshooting plan"):
        ss["ts_ai_plan"] = edited.to_dict("records")
        st.success("Troubleshooting plan saved.")
    st.caption("Tip: mark steps as you execute them; human review will evaluate these entries.")
    _render_stage_completion("ai_plan")


def render_human_review_stage():
    ss = st.session_state
    if not _dependencies_met("human_review"):
        return
    st.subheader("‚ë¶ Human Review ‚Äî Validate AI Plan")
    plan = ss.get("ts_ai_plan", [])
    if not plan:
        st.warning("No AI plan available. Complete the previous stage first.")
        return
    df = pd.DataFrame(plan)
    if "ReviewStatus" not in df.columns:
        df["ReviewStatus"] = "Pending"
    if "ReviewerNotes" not in df.columns:
        df["ReviewerNotes"] = ""
    edited = st.data_editor(
        df,
        column_config={
            "ReviewStatus": st.column_config.SelectboxColumn(
                options=["Pending", "Working", "Not Working"], label="Review Status"
            ),
            "ReviewerNotes": st.column_config.TextColumn(width="large"),
        },
        use_container_width=True,
        num_rows="dynamic",
    )
    if st.button("Save human review results"):
        records = edited.to_dict("records")
        ss["ts_ai_plan"] = records
        ss["ts_human_reviews"] = records
        st.success("Human review saved.")
        if any(r["ReviewStatus"] == "Working" for r in records):
            ss["ts_ai_solution"] = {
                "cause": records[0].get("Hypothesis", "Hypothesis"),
                "case_id": "AD-HOC",
                "lessons": "Validated by human reviewer",
            }
    if not ss.get("ts_escalated") and ss.get("ts_human_reviews"):
        all_fail = all(r.get("ReviewStatus") == "Not Working" for r in ss["ts_human_reviews"])
        if all_fail:
            if st.button("Escalate to Level 3 (Kira / eService)", key="ts_escalate_human"):
                ss["ts_escalated"] = True
                st.success("Escalation recorded for engineering support.")
    _render_stage_completion("human_review")


def render_training_stage():
    ss = st.session_state
    if not _dependencies_met("training"):
        return
    st.subheader("‚ëß Training & Escalation Package")
    reviews = ss.get("ts_human_reviews", [])
    if reviews:
        st.markdown("##### Latest troubleshooting session")
        st.dataframe(pd.DataFrame(reviews), use_container_width=True, hide_index=True)
    history = _history_payload()
    st.markdown("##### Troubleshooting history snapshot")
    st.json(history, expanded=False)

    col_left, col_right = st.columns(2)
    with col_left:
        label = st.text_input("Training example title", value=history["ticket"].get("title", "Unnamed incident"))
        outcome = st.selectbox("Outcome", ["Resolved", "Mitigated", "Escalated", "Unresolved"])
        if st.button("Add to training dataset"):
            ss["ts_training_records"].append(
                {
                    "title": label,
                    "outcome": outcome,
                    "timestamp": datetime.utcnow().isoformat(),
                    "plan_steps": len(ss.get("ts_ai_plan", [])),
                }
            )
            st.success("Logged to training dataset.")
    with col_right:
        if not ss.get("ts_escalated"):
            escalate_note = st.text_area("Escalation summary (if needed)")
            if st.button("Escalate to engineering with full history"):
                ss["ts_escalated"] = True
                ss["ts_training_records"].append(
                    {
                        "title": f"Escalated - {label}",
                        "outcome": "Escalated",
                        "timestamp": datetime.utcnow().isoformat(),
                        "notes": escalate_note,
                    }
                )
                st.success("Escalation package prepared.")
        else:
            st.info("Escalation already triggered for this incident.")
    st.markdown("##### Training dataset")
    if ss["ts_training_records"]:
        st.dataframe(pd.DataFrame(ss["ts_training_records"]), use_container_width=True, hide_index=True)
    else:
        st.caption("No training records yet.")
    _render_stage_completion("training")


def render_deployment_stage():
    ss = st.session_state
    if not _dependencies_met("deployment"):
        return
    st.subheader("‚ë® Deployment & Feedback")
    records = ss.get("ts_training_records", [])
    if records:
        st.dataframe(pd.DataFrame(records).tail(5), use_container_width=True, hide_index=True)
    export_col, feedback_col = st.columns(2)
    with export_col:
        if ss["ts_deployment_meta"]["exported"]:
            st.success(f"Playbook exported on {ss['ts_deployment_meta']['timestamp']}")
        if st.button("Export troubleshooting model to production"):
            ss["ts_deployment_meta"]["exported"] = True
            ss["ts_deployment_meta"]["timestamp"] = datetime.utcnow().isoformat()
            st.success("Playbook exported and ready for reuse.")
    with feedback_col:
        rating = st.slider("How effective was this session?", 1, 5, 4)
        comment = st.text_area("Final feedback / lessons learned")
        if st.button("Submit feedback"):
            entry = {
                "rating": rating,
                "comment": comment,
                "timestamp": datetime.utcnow().isoformat(),
            }
            ss["ts_deployment_meta"]["feedback"].append(entry)
            ss["ts_feedback_log"].append(comment or "No comment")
            st.success("Feedback captured.")
    if ss["ts_deployment_meta"]["feedback"]:
        st.markdown("##### Recent feedback")
        st.json(ss["ts_deployment_meta"]["feedback"][-3:], expanded=False)
    _render_stage_completion("deployment")
def render_ticket_generator_stage():
    ss = st.session_state
    if not _dependencies_met("ticket"):
        return
    st.subheader("‚ë° Ticket Generator & Import")
    payload = ss["ts_ticket_payload"]

    col_left, col_right = st.columns([1.2, 0.8])
    with col_left:
        title = st.text_input("Ticket title", payload.get("title", "Incident summary"))
        impact = st.text_area("Business impact", payload.get("impact", ""), height=120)
        resolution = st.text_area(
            "Resolution / next steps",
            payload.get("resolution", ""),
            height=160,
        )
        channels = ["ServiceNow", "Jira", "Slack Bridge"]
        default_idx = (
            channels.index(payload.get("channel", "ServiceNow"))
            if payload.get("channel", "ServiceNow") in channels
            else 0
        )
        channel = st.selectbox("Handoff channel", channels, index=default_idx)
        ai_solution = ss.get("ts_ai_solution")
        escalation = ss["ts_escalated"]
        if st.button("Generate ticket payload / import"):
            ticket_record = {
                "title": title,
                "impact": impact,
                "resolution": resolution,
                "channel": channel,
                "appraisal": ss.get("ts_appraisal"),
                "problem": ss.get("ts_problem"),
                "decision": ss.get("ts_decision"),
                "potential": ss.get("ts_potential"),
                "ai_solution": ai_solution,
                "escalation": escalation,
                "id": ss.get("ts_selected_ticket", {}).get("id") or f"INC-DRAFT-{datetime.utcnow().strftime('%H%M%S')}",
                "service": ss.get("ts_selected_ticket", {}).get("service", "Unknown"),
                "severity": ss.get("ts_selected_ticket", {}).get("severity", "P2"),
                "status": "Draft",
                "source": "Generated",
            }
            ss["ts_ticket_payload"] = ticket_record
            ss["ts_selected_ticket"] = ticket_record
            _hydrate_from_ticket(ticket_record)
            st.success(f"Draft ready for {channel} and synced with Situation Appraisal.")

    with col_right:
        if ss.get("ts_ai_solution"):
            st.success(
                f"AI-confirmed fix: {ss['ts_ai_solution']['cause']} (case {ss['ts_ai_solution']['case_id']})"
            )
        elif ss.get("ts_escalated"):
            st.warning("Status: Escalated to Level 3 support (Kira / eService).")
        else:
            st.caption("AI is still investigating ‚Äî capture current notes before sending.")
        st.markdown("##### Rendered payload")
        st.json(ss["ts_ticket_payload"])
        st.caption("Copy/paste into your ITSM of choice or push via API.")
    _render_stage_completion("ticket")


def render_howto_stage():
    st.subheader("üìò How-To Guide")
    st.markdown(
        """
        Welcome to the KT-guided Troubleshooter preview. Follow these steps in order:

        1. **Ticket Intake** ‚Äî import from ServiceNow/Jira, upload CSV, or generate a synthetic ticket for practice.
        2. **Situation Appraisal** ‚Äî capture concerns, urgency, and impact to know what deserves attention.
        3. **Problem Analysis** ‚Äî decompose the deviation into _WHAT/WHERE/WHEN/EXTENT_ plus IS vs IS NOT grids.
        4. **Decision Analysis** ‚Äî compare remediation options using must/nice objectives and benefit‚Äìrisk scoring.
        5. **Potential Problems** ‚Äî anticipate downstream risks and define preventive / contingency actions.
        6. **AI Assist** ‚Äî let the agent suggest root causes from case memory, then refine via your feedback.
        7. **Ticket Generator** ‚Äî export the entire KT+AI context or escalate to Level 3 (Kira / eService).

        Each tab ships with pre-filled examples‚Äîupdate them as you gather real telemetry. Use the completion buttons
        to unlock the next stage. If AI cannot converge, consult the research checklist or escalate so Level 3 receives
        a complete package.
        """
    )
    st.caption("Tip: Need a quick scenario? Use the synthetic ticket generator in Stage 1 to practice the full loop.")


def _generate_synthetic_ticket():
    template = random.choice(SYNTHETIC_TEMPLATES)
    suffix = datetime.utcnow().strftime("%H%M%S")
    return {
        "id": f"INC-SYN-{suffix}",
        "source": template["source"],
        "service": template["service"],
        "severity": template["severity"],
        "status": template["status"],
        "summary": template["summary"],
        "environment": random.choice(["Production", "Staging"]),
    }


def _generate_ai_plan():
    ss = st.session_state
    hypotheses_raw = ss["ts_problem"].get("hypotheses", "")
    hypotheses = [h.strip() for h in hypotheses_raw.replace("\n", ";").split(";") if h.strip()]
    plan = []
    for idx, hypo in enumerate(hypotheses or ["Investigate primary hypothesis"]):
        plan.append(
            {
                "Hypothesis": hypo,
                "Troubleshooting Step": f"Run diagnostic #{idx+1} for {hypo.lower()}",
                "Owner": "Ops",
                "Status": "Pending",
            }
        )
    return plan


def _history_payload():
    ss = st.session_state
    return {
        "ticket": ss.get("ts_selected_ticket", {}),
        "appraisal": ss.get("ts_appraisal"),
        "problem": ss.get("ts_problem"),
        "decision": ss.get("ts_decision"),
        "potential": ss.get("ts_potential"),
        "ai_plan": ss.get("ts_ai_plan"),
        "human_review": ss.get("ts_human_reviews"),
        "escalated": ss.get("ts_escalated"),
    }


def _hydrate_from_ticket(ticket: dict):
    if not ticket:
        return
    ss = st.session_state
    concerns = f"{ticket.get('summary','')} | Service: {ticket.get('service','N/A')} ({ticket.get('environment','Production')})"
    notes = f"Severity {ticket.get('severity','P3')} ‚Ä¢ Source {ticket.get('source','Unknown')} ‚Ä¢ Status {ticket.get('status','Open')}"
    ss["ts_appraisal"].update(
        {
            "concerns": concerns,
            "notes": notes,
            "issues": ["Customer Impact", "Capacity"],
        }
    )
    ss["ts_problem"].update(
        {
            "what": ticket.get("summary", ss["ts_problem"]["what"]),
            "where": f"{ticket.get('service','Unknown')} / {ticket.get('environment','Production')}",
            "extent": f"Severity {ticket.get('severity','P3')}",
            "when": datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC"),
        }
    )
    _cascade_from_appraisal()


def _cascade_from_appraisal():
    ss = st.session_state
    ticket = ss.get("ts_selected_ticket", {})
    ss["ts_problem"].setdefault("what", ticket.get("summary", ""))
    if not ss["ts_problem"].get("hypotheses"):
        ss["ts_problem"]["hypotheses"] = "Potential LB issue; Possible deployment regression; Capacity imbalance"
    if not ss["ts_problem"].get("tests"):
        ss["ts_problem"]["tests"] = "Compare metrics across clusters; replay failing requests; inspect recent deploy diff"


def _cascade_from_problem():
    ss = st.session_state
    hypotheses = ss["ts_problem"].get("hypotheses", "")
    ideas = [h.strip() for h in hypotheses.replace("\n", ";").split(";") if h.strip()]
    if ideas:
        ss["ts_decision"]["alternatives"] = [
            {"name": idea[:60], "benefit": min(5, 3 + idx), "risk": max(1, 3 - idx)}
            for idx, idea in enumerate(ideas[:3])
        ]


def _cascade_from_decision():
    ss = st.session_state
    alts = ss["ts_decision"].get("alternatives", [])
    if alts:
        best = sorted(alts, key=lambda x: x["benefit"] - x["risk"], reverse=True)[0]
        ss["ts_potential"]["risks"] = f"Executing {best['name']} may introduce regression elsewhere."
        ss["ts_potential"]["preventive"] = "Add canary checks + feature flags."
        ss["ts_potential"]["contingency"] = "Roll back change or fail traffic to stable cluster."
        ss["ts_potential"]["opportunities"] = f"Document {best['name']} as reusable runbook."


st.set_page_config(page_title="AI Troubleshooter Agent", layout="wide")
apply_global_theme()
_init_state()

render_troubleshooter_nav()

st.title("üß† AI Troubleshooter Agent (Public Preview)")
st.caption("Import tickets ‚Üí walk the KT method ‚Üí let AI refine causes with your feedback ‚Üí escalate or hand off with confidence.")

render_operator_banner(
    operator_name="Ops Engineer",
    title="Troubleshooter Agent",
    summary="Kepner-Tregoe guided incident responder with live ticket import, AI hypothesis refinement, and structured handoff.",
    bullets=[
        "Progress through Situation, Problem, Decision, and Potential analyses with pre-filled guidance.",
        "AI assistant narrows probable causes using case memory plus your real-time feedback.",
        "Human review, training/export, and escalation packaging ensure fixes reach production or Level 3 support.",
    ],
    metrics=[
        {
            "label": "Status",
            "value": "Being Built",
            "context": "Workflow wiring is live; backend execution coming next.",
        },
        {
            "label": "AI Playbooks",
            "value": f"{len(CASE_MEMORY)} fixtures",
            "context": "Grows whenever you log a new root cause and lessons learned.",
        },
    ],
    icon="üß†",
)

st.info(
    "Follow the KT questionnaires in order, capture AI feedback at each step, and finish with either a validated fix "
    "or an escalation package ready for Kira / eService."
)

stage_tabs = st.tabs(
    [
        "üìò How-To",
        "1Ô∏è‚É£ Ticket Intake",
        "2Ô∏è‚É£ Ticket Generator",
        "3Ô∏è‚É£ Situation Appraisal",
        "4Ô∏è‚É£ Problem Analysis",
        "5Ô∏è‚É£ Decision Analysis",
        "6Ô∏è‚É£ Potential Problems",
        "7Ô∏è‚É£ AI Plan",
        "8Ô∏è‚É£ Human Review",
        "9Ô∏è‚É£ Training & Escalation",
        "üîü Deployment & Feedback",
    ]
)

with stage_tabs[0]:
    render_howto_stage()
with stage_tabs[1]:
    render_ticket_intake_stage()
with stage_tabs[2]:
    render_ticket_generator_stage()
with stage_tabs[3]:
    render_situation_appraisal_stage()
with stage_tabs[4]:
    render_problem_analysis_stage()
with stage_tabs[5]:
    render_decision_analysis_stage()
with stage_tabs[6]:
    render_potential_problem_stage()
with stage_tabs[7]:
    render_ai_plan_stage()
with stage_tabs[8]:
    render_human_review_stage()
with stage_tabs[9]:
    render_training_stage()
with stage_tabs[10]:
    render_deployment_stage()



==================== ./credit_scoring.py ====================
#!/usr/bin/env python3
"""
üí≥ Credit Scoring Agent (Stage-only orchestrator)
-----------------------------------------------
This Streamlit page mirrors the Asset Appraisal structure but focuses on the
credit scoring layer that sits between Anti-Fraud/KYC intake and the existing
credit appraisal agent. The UI keeps the flow lightweight (stages only) while
still wiring outputs back into ``st.session_state["credit_scored_df"]`` so the
main credit appraisal experience can pick up the latest results.

Each stage surfaces context to the embedded Gemma‚Äë2 assistant so operators can
ask stage-aware questions directly from the page.
"""
from __future__ import annotations

import os
from datetime import datetime, timezone
from typing import Any, Dict

import numpy as np
import pandas as pd
import streamlit as st

from services.ui.theme_manager import (
    apply_theme as apply_global_theme,
    get_palette,
    get_theme,
    render_theme_toggle,
)
from services.ui.components.operator_banner import render_operator_banner
from services.ui.components.chat_assistant import render_chat_assistant
from services.common.model_registry import get_hf_models, get_llm_lookup, get_llm_display_info
from services.ui.utils.llm_selector import render_llm_selector
from services.ui.utils.ai_insights import llm_generate_summary
import plotly.graph_objects as go


# ---------------------------------------------------------------------------
# PAGE + SESSION INITIALIZATION
# ---------------------------------------------------------------------------
st.set_page_config(page_title="Credit Scoring Agent", layout="wide")
ss = st.session_state


def _init_defaults() -> None:
    llm_lookup = get_llm_lookup()
    default_label = llm_lookup["labels"][0]
    ss.setdefault("stage", "credit_scoring_agent")
    ss.setdefault("credit_scoring_stage", "stages_only")
    ss.setdefault(
        "credit_scoring_user",
        ss.get("credit_user")
        or ss.get("asset_user")
        or ss.get("afk_user")
        or {"name": "Operator", "email": "operator@demo.local"},
    )
    ss.setdefault("credit_scoring_pending", 24)
    ss.setdefault("credit_scoring_flagged", 6)
    ss.setdefault("credit_scoring_avg_time", "8 min")
    ss.setdefault("credit_scoring_last_run_ts", None)
    ss.setdefault("credit_scoring_llm_label", default_label)
    ss.setdefault("credit_scoring_llm_model", llm_lookup["value_by_label"][default_label])
    ss.setdefault("credit_scoring_llm_score", 85.0)
    ss.setdefault("credit_scoring_status", "idle")
    ss.setdefault("credit_scoring_notes", [])


_init_defaults()
apply_global_theme(get_theme())


def _safe_switch(target: str) -> None:
    """Route back to main app stage safely."""
    ss["stage"] = target
    try:
        st.switch_page("app.py")
        return
    except Exception:
        pass
    try:
        st.query_params["stage"] = target
    except Exception:
        pass
    try:
        st.experimental_rerun()
    except Exception:
        pass


def _render_nav():
    stage = ss.get("stage", "credit_scoring_agent")
    c1, c2, c3 = st.columns([1, 1, 2.5])
    with c1:
        if st.button("üè† Home", key=f"cs_nav_home_{stage}"):
            _safe_switch("landing")
    with c2:
        if st.button("ü§ñ Agents", key=f"cs_nav_agents_{stage}"):
            _safe_switch("agents")
    with c3:
        render_theme_toggle("üåó Theme", key="credit_scoring_theme_toggle")


_render_nav()


# ---------------------------------------------------------------------------
# HELPERS
# ---------------------------------------------------------------------------
def _coerce_minutes(value: Any, fallback: float = 0.0) -> float:
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        cleaned = "".join(ch for ch in value if ch.isdigit() or ch == ".")
        try:
            return float(cleaned)
        except (TypeError, ValueError):
            pass
    return float(fallback)


def _select_source_dataframe() -> pd.DataFrame:
    """
    Pull the freshest dataframe created by Anti-Fraud/KYC. We check multiple
    well-known session keys before falling back to a synthetic dataset so the
    stage demo never blocks.
    """
    candidate_keys = [
        "credit_scoring_source_df",
        "afk_fraud_df",
        "afk_kyc_df",
        "afk_anonymized_df",
        "afk_intake_df",
    ]
    for key in candidate_keys:
        df = ss.get(key)
        if isinstance(df, pd.DataFrame) and not df.empty:
            return df.copy()

    # Synthetic fallback (mirrors the Asset template behaviour of always
    # keeping something on screen for demos).
    rng = np.random.default_rng(20251110)
    base = pd.DataFrame(
        {
            "customer_id": [f"CUST-{1000 + i}" for i in range(12)],
            "full_name": [
                "Amani Mbatha",
                "Lucas Alvarez",
                "Noor Al-Hassan",
                "Maya Chen",
                "Mateo Silva",
                "Yara Ibrahim",
                "Emilia Rossi",
                "Kwame Mensah",
                "Ines Dubois",
                "Sora Tanaka",
                "Hugo Jensen",
                "Priya Kapoor",
            ],
            "country": rng.choice(["NG", "ZA", "BR", "SG", "AE", "VN", "FR", "MX"], 12),
            "monthly_income": rng.integers(500, 9000, 12),
            "existing_loans": rng.integers(0, 4, 12),
            "fraud_probability": rng.uniform(0.01, 0.45, 12).round(3),
            "kyc_risk_score": rng.uniform(0.05, 0.65, 12).round(3),
            "bank_relationship_years": rng.integers(0, 15, 12),
        }
    )
    ss["credit_scoring_source_df"] = base.copy()
    return base


def _score_with_risk_ensemble(source_df: pd.DataFrame) -> pd.DataFrame:
    """Simulate the shared scoring ensemble (same features Asset uses)."""
    rng = np.random.default_rng(321)
    df = source_df.copy()
    if "kyc_risk_score" not in df:
        df["kyc_risk_score"] = rng.uniform(0.05, 0.6, len(df)).round(3)
    if "fraud_probability" not in df:
        df["fraud_probability"] = rng.uniform(0.01, 0.4, len(df)).round(3)

    # Derived credit features
    df["debt_to_income"] = np.clip(
        (df.get("existing_loans", 0) * 0.12) + (df.get("monthly_income", 1) / 10000),
        0.05,
        0.95,
    ).round(3)
    df["utilization_ratio"] = rng.uniform(0.1, 0.92, len(df)).round(3)
    df["credit_score"] = (
        780
        - (df["kyc_risk_score"] * 220)
        - (df["fraud_probability"] * 180)
        - (df["debt_to_income"] * 140)
        + rng.normal(0, 25, len(df))
    )
    df["credit_score"] = df["credit_score"].clip(320, 875).round().astype(int)
    df["recommendation"] = np.where(
        df["credit_score"] >= 680,
        "‚úÖ Pre-Approve",
        np.where(df["credit_score"] >= 620, "üïµÔ∏è Manual Review", "‚ùå Decline"),
    )
    df["ensemble_stage_notes"] = [
        f"Stage {idx%3 + 1}: Shared model flagged DTI={d:.2f}, risk={r:.2f}"
        for idx, (d, r) in enumerate(zip(df["debt_to_income"], df["kyc_risk_score"]))
    ]
    df["stage"] = "Credit Scoring"
    return df


def _build_chat_context() -> Dict[str, Any]:
    ctx = {
        "agent_type": "credit_scoring",
        "stage": ss.get("credit_scoring_stage"),
        "user": (ss.get("credit_scoring_user") or {}).get("name"),
        "pending_cases": ss.get("credit_scoring_pending"),
        "flagged_cases": ss.get("credit_scoring_flagged"),
        "avg_time": ss.get("credit_scoring_avg_time"),
        "ai_performance": ss.get("credit_ai_performance") or 0.92,
        "last_run": ss.get("credit_scoring_last_run_ts"),
        "status": ss.get("credit_scoring_status"),
        "llm_model": ss.get("credit_scoring_llm_model"),
        "llm_label": ss.get("credit_scoring_llm_label"),
        "ollama_url": os.getenv("OLLAMA_URL", f"http://localhost:{os.getenv('GEMMA_PORT', '7001')}"),
    }
    return {k: v for k, v in ctx.items() if v not in (None, "", [])}


# ---------------------------------------------------------------------------
# HEADER + STAGE OVERVIEW
# ---------------------------------------------------------------------------
pal = get_palette()
render_operator_banner(
    operator_name=(ss["credit_scoring_user"] or {}).get("name", "Operator"),
    title="Credit Scoring Agent",
    summary="Bridges Anti-Fraud/KYC evidence into the shared scoring ensemble (same lineup as Asset) so Credit Appraisal inherits consistent signals.",
    bullets=[
        "Stage 1 ‚Üí sync Anti-Fraud/KYC verdicts & trust signals",
        "Stage 2 ‚Üí Shared LLM scoring ensemble (Phi/Mistral/Gemma/etc.)",
        "Stage 3 ‚Üí push normalized credit scores into Credit Appraisal",
    ],
    metrics=[
        {"label": "Pending apps", "value": ss.get("credit_scoring_pending"), "delta": "+4 vs yesterday"},
        {"label": "Flagged queue", "value": ss.get("credit_scoring_flagged"), "delta": "-2 risk hits"},
        {"label": "Avg SLA", "value": ss.get("credit_scoring_avg_time"), "delta": "‚Äë11%"},
    ],
)

source_df = _select_source_dataframe()
fraud_hits = int((source_df.get("fraud_probability", 0) > 0.25).sum())
kyc_feeds = len(source_df)
agreement_pct = float(ss.get("credit_ai_performance", 0.92) or 0.92) * 100
agreement_pct = max(0, min(100, agreement_pct))

refresh_age_val = 12.0
ts = ss.get("credit_scoring_last_run_ts")
if isinstance(ts, str):
    try:
        ts_dt = datetime.fromisoformat(ts.replace("Z", "+00:00"))
        refresh_age_val = max(
            0.0, (datetime.now(timezone.utc) - ts_dt).total_seconds() / 3600.0
        )
    except Exception:
        pass

telemetry = {
    "kyc_feeds": kyc_feeds,
    "fraud_hits": fraud_hits,
    "refresh_age": min(72.0, refresh_age_val),
    "agreement": agreement_pct,
}

llm_score_value = float(ss.get("credit_scoring_llm_score", telemetry["agreement"]) or telemetry["agreement"])
llm_score_value = max(0.0, min(100.0, llm_score_value))
ss["credit_scoring_llm_score"] = llm_score_value

llm_confidence_fig = go.Figure(
    go.Indicator(
        mode="gauge+number",
        value=llm_score_value,
        title={"text": "LLM Confidence / Explanation Strength"},
        gauge={
            "axis": {"range": [0, 100]},
            "bar": {"color": "lime"},
            "steps": [
                {"range": [0, 30], "color": "#f87171"},
                {"range": [30, 70], "color": "#fb923c"},
                {"range": [70, 100], "color": "#4ade80"},
            ],
        },
    )
)
llm_confidence_fig.update_layout(
    height=300,
    margin=dict(l=0, r=0, t=60, b=0),
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(0,0,0,0)",
)
st.plotly_chart(llm_confidence_fig, use_container_width=True)

donut = go.Figure(
    data=[
        go.Pie(
            values=[telemetry["agreement"], 100 - telemetry["agreement"]],
            labels=["Agreement", "Gap"],
            hole=0.65,
            marker_colors=["#3b82f6", "#1e293b"],
        )
    ]
)
donut.update_layout(
    height=320,
    showlegend=False,
    margin=dict(l=0, r=0, t=40, b=0),
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(0,0,0,0)",
)

bullet = go.Figure(
    go.Indicator(
        mode="number+gauge",
        value=telemetry["refresh_age"],
        title={"text": "Data Refresh Age (hours)"},
        gauge={
            "shape": "bullet",
            "axis": {"range": [0, 72]},
            "bar": {"color": "#22c55e"},
            "steps": [
                {"range": [0, 24], "color": "#4ade80"},
                {"range": [24, 48], "color": "#fbbf24"},
                {"range": [48, 72], "color": "#f87171"},
            ],
        },
        domain={"x": [0, 1], "y": [0, 1]},
    )
)
bullet.update_layout(
    height=160,
    margin=dict(l=0, r=0, t=40, b=0),
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(0,0,0,0)",
)

bar = go.Figure(
    go.Bar(
        x=[telemetry["kyc_feeds"], telemetry["fraud_hits"]],
        y=["KYC Feeds", "Fraud Hits"],
        orientation="h",
        marker=dict(color=["#0ea5e9", "#ef4444"]),
    )
)
bar.update_layout(
    height=260,
    margin=dict(l=0, r=0, t=40, b=0),
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(0,0,0,0)",
)

c1, c2 = st.columns(2)
with c1:
    st.markdown("<div class='metric-title'>LLM ‚Üî Human Agreement</div>", unsafe_allow_html=True)
    st.plotly_chart(donut, use_container_width=True)
    st.markdown(
        "<div class='metric-explain'>How closely the LLM's valuation/explanation matches human analysts. "
        "90%+ = high alignment; <70% = review for drift or inconsistencies.</div>",
        unsafe_allow_html=True,
    )
with c2:
    st.markdown("<div class='metric-title'>Data Freshness (Hours Since Last Sync)</div>", unsafe_allow_html=True)
    st.plotly_chart(bullet, use_container_width=True)
    st.markdown(
        "<div class='metric-explain'>Measures how recent Anti-Fraud / KYC signals were last synchronized. "
        "Green <24h = fresh, Yellow <48h = acceptable, Red >48h = stale data risk.</div>",
        unsafe_allow_html=True,
    )
st.markdown("<div class='metric-title'>Operational Signals (KYC / Fraud)</div>", unsafe_allow_html=True)
st.plotly_chart(bar, use_container_width=True)
st.markdown(
    "<div class='metric-explain'>Live operational workload indicators. Fraud hits represent flagged anomalies. "
    "KYC feeds show validated identity signals.</div>",
    unsafe_allow_html=True,
)

ai_summary_text = llm_generate_summary(telemetry)
st.markdown("### üß† AI Recommendation Summary")
st.write(ai_summary_text)

if False:
    st.markdown("### Shared LLM & Hardware Profile")
    hf_models_df = pd.DataFrame(get_hf_models())
    llm_lookup_ui = get_llm_lookup()
    OPENSTACK_FLAVORS = {
        "m4.medium": "4 vCPU / 8 GB RAM (CPU-only small)",
        "m8.large": "8 vCPU / 16 GB RAM (CPU-only medium)",
        "g1.a10.1": "8 vCPU / 32 GB RAM + 1√óA10 24 GB",
        "g1.l40.1": "16 vCPU / 64 GB RAM + 1√óL40 48 GB",
        "g2.a100.1": "24 vCPU / 128 GB RAM + 1√óA100 80 GB",
    }
    with st.expander("üß† Local/HF lineup (shared with Asset)", expanded=False):
        st.dataframe(hf_models_df, use_container_width=True)
        c1, c2 = st.columns([1.2, 1])
        labels = llm_lookup_ui["labels"]
        value_by_label = llm_lookup_ui["value_by_label"]
        hint_by_label = llm_lookup_ui["hint_by_label"]
        saved_label = ss.get("credit_scoring_llm_label", labels[0])
        if saved_label not in labels:
            saved_label = labels[0]
        with c1:
            selected_label = st.selectbox(
                "üî• Local/HF LLM (narratives + explainability)",
                labels,
                index=labels.index(saved_label),
                key="credit_scoring_llm_label",
            )
            st.caption(f"Hint: {hint_by_label[selected_label]}")
        with c2:
            flavor = st.selectbox(
                "OpenStack flavor / host profile",
                list(OPENSTACK_FLAVORS.keys()),
                index=0,
                key="credit_scoring_flavor",
            )
            st.caption(OPENSTACK_FLAVORS[flavor])
        ss["credit_scoring_llm_model"] = value_by_label[selected_label]

OPENSTACK_FLAVORS = {
    "m4.medium": "4 vCPU / 8 GB RAM (CPU-only small)",
    "m8.large": "8 vCPU / 16 GB RAM (CPU-only medium)",
    "g1.a10.1": "8 vCPU / 32 GB RAM + 1√óA10 24 GB",
    "g1.l40.1": "16 vCPU / 64 GB RAM + 1√óL40 48 GB",
    "g2.a100.1": "24 vCPU / 128 GB RAM + 1√óA100 80 GB",
}
selected_llm = render_llm_selector(context="credit_scoring")
ss["credit_scoring_llm_label"] = selected_llm["model"]
ss["credit_scoring_llm_model"] = selected_llm["value"]

flavor = st.selectbox(
    "OpenStack flavor / host profile",
    list(OPENSTACK_FLAVORS.keys()),
    index=0,
    key="credit_scoring_flavor",
)
st.caption(OPENSTACK_FLAVORS[flavor])

st.markdown("### Stage-Only Flow")
st.caption("Every control below corresponds to one stage in the risk ladder. Run them top-to-bottom to keep downstream agents in sync.")


# ---------------------------------------------------------------------------
# STAGE 1 ‚Äî INTAKE
# ---------------------------------------------------------------------------
with st.container():
    st.subheader("Stage 1 ¬∑ Anti-Fraud & KYC intake")
    st.write(
        "Pull the freshest applicant dossier from the Anti-Fraud/KYC agent. "
        "We persist the snapshot locally so operators can re-score without hitting external systems."
    )
    col_left, col_right = st.columns([3, 1], gap="large")
    with col_left:
        st.dataframe(
            source_df.head(15),
            use_container_width=True,
            height=360,
        )
    with col_right:
        st.metric("Rows available", len(source_df))
        st.metric(
            "High fraud suspicion",
            int((source_df.get("fraud_probability", 0) > 0.35).sum()),
        )
        st.metric(
            "Avg KYC risk",
            f"{source_df.get('kyc_risk_score', pd.Series(dtype=float)).mean():.2f}" if "kyc_risk_score" in source_df else "‚Äî",
        )
        if st.button("üîÑ Refresh from Anti-Fraud/KYC", use_container_width=True):
            ss["credit_scoring_source_df"] = source_df.copy()
            ss["credit_scoring_status"] = "refreshed"
            st.success("Source snapshot refreshed from session state / fallback dataset.")


# ---------------------------------------------------------------------------
# STAGE 2 ‚Äî SHARED CREDIT SCORING ENSEMBLE
# ---------------------------------------------------------------------------
with st.container():
    st.subheader("Stage 2 ¬∑ Shared scoring ensemble")
    st.write(
        "Uses the shared Phi/Mistral/Gemma lineup (selected above) to derive a normalized credit score per applicant. "
        "Outputs land both in this page and in ``credit_scored_df`` for the main credit agent."
    )

    scored_df = ss.get("credit_scoring_df")
    run_scoring = st.button("üöÄ Run scoring ensemble", use_container_width=True)
    if run_scoring or (isinstance(scored_df, pd.DataFrame) and not scored_df.empty):
        if run_scoring:
            scored_df = _score_with_risk_ensemble(source_df)
            ts = datetime.now(timezone.utc).isoformat()
            ss["credit_scoring_df"] = scored_df.copy()
            ss["credit_scored_df"] = scored_df.copy()
            ss["credit_scoring_last_run_ts"] = ts
            ss["credit_scoring_status"] = "completed"
            st.success(f"Scoring ensemble completed at {ts} using {ss['credit_scoring_llm_label']}. Results piped to Credit Appraisal.")

        if isinstance(scored_df, pd.DataFrame) and not scored_df.empty:
            st.dataframe(scored_df.head(15), use_container_width=True, height=360)
            st.download_button(
                "‚¨áÔ∏è Export scored applicants",
                data=scored_df.to_csv(index=False).encode("utf-8"),
                file_name=f"credit_scoring_{datetime.now():%Y%m%d-%H%M}.csv",
                mime="text/csv",
                use_container_width=True,
            )
        else:
            st.warning("Run the scoring ensemble to populate this stage.")
    else:
        st.info("Click the button above to trigger the scoring ensemble.")


# ---------------------------------------------------------------------------
# STAGE 3 ‚Äî HANDOFF TO CREDIT APPRAISAL
# ---------------------------------------------------------------------------
with st.container():
    st.subheader("Stage 3 ¬∑ Push to Credit Appraisal Agent")
    st.write(
        "Normalize schema + send the staged dataframe to the existing credit appraisal workflow. "
        "Any downstream Streamlit tab that relies on ``credit_scored_df`` will instantly see the new values."
    )
    scored_df = ss.get("credit_scoring_df")
    if isinstance(scored_df, pd.DataFrame) and not scored_df.empty:
        st.success(
            f"{len(scored_df)} applicants ready. Open the Credit Appraisal page to continue the multi-stage review."
        )
        st.markdown(
            """
            - üìò [Launch Credit Appraisal](/credit_appraisal)  
            - üßæ `credit_scored_df` updated in session state  
            - ‚úÖ Selected LLM: `{label}` (`{model}`) | Endpoint: `{endpoint}`
            """.format(
                label=ss.get("credit_scoring_llm_label"),
                model=ss.get("credit_scoring_llm_model"),
                endpoint=os.getenv("OLLAMA_URL", f"http://localhost:{os.getenv('GEMMA_PORT', '7001')}"),
            )
        )
    else:
        st.warning("Run Stage 2 before attempting the handoff.")


# ---------------------------------------------------------------------------
# GEMMA CHAT (STAGE-AWARE)
# ---------------------------------------------------------------------------
CHAT_FAQ = [
    "Stage 1 ‚Üí how do we trust the KYC feed?",
    "Stage 2 ‚Üí what features influence the scoring ensemble the most?",
    "Stage 3 ‚Üí what schema is required by Credit Appraisal?",
]
render_chat_assistant(
    page_id="credit_scoring",
    context=_build_chat_context(),
    title="üí¨ Shared LLM stage assistant",
    default_open=False,
    faq_questions=CHAT_FAQ,
)


# ---------------------------------------------------------------------------
# FOOTER CONTROLS
# ---------------------------------------------------------------------------
st.divider()
cols = st.columns([1, 1, 1, 2])
with cols[0]:
    render_theme_toggle("üåó Theme", key="credit_scoring_theme_footer")
with cols[1]:
    if st.button("‚Ü©Ô∏è Back to Agents", use_container_width=True, key="credit_scoring_back_agents"):
        ss["stage"] = "agents"
        try:
            st.switch_page("app.py")
        except Exception:
            st.experimental_set_query_params(stage="agents")
            st.experimental_rerun()
with cols[2]:
    if st.button("üè† Back to Home", use_container_width=True, key="credit_scoring_back_home"):
        ss["stage"] = "landing"
        try:
            st.switch_page("app.py")
        except Exception:
            st.experimental_set_query_params(stage="landing")
            st.experimental_rerun()



==================== ./anti_fraud_kyc.py ====================
"""Standalone Streamlit page for the Anti-Fraud & KYC agent."""
from __future__ import annotations

import sys
from pathlib import Path
from typing import Any, Dict

import streamlit as st

st.set_page_config(page_title="Anti-Fraud & KYC Agent", layout="wide")
from services.ui.components.operator_banner import render_operator_banner
from services.ui.components.feedback import render_feedback_tab
from services.ui.components.chat_assistant import render_chat_assistant
from services.ui.theme_manager import (
    apply_theme as apply_global_theme,
    get_palette,
    get_theme,
    render_theme_toggle,
)
from services.ui.utils.llm_selector import render_llm_selector

st.markdown(
    """
    > **Unified Risk Checklist**  
    > ‚úÖ Is the borrower real & safe? (this agent)  
    > ‚úÖ Is the collateral worth enough? (Asset)  
    > ‚úÖ Can they afford the loan? (Credit)  
    > ‚úÖ Should the bank approve overall? (Unified agent)
    """
)

BASE_DIR = Path(__file__).resolve().parents[3]
AFK_ROOT = BASE_DIR / "anti-fraud-kyc-agent"

if not AFK_ROOT.exists():
    st.error("Anti-Fraud agent assets are missing. Run fraudinst.sh first.")
    st.stop()

if str(AFK_ROOT) not in sys.path:
    sys.path.insert(0, str(AFK_ROOT))

try:
    from pages import (
        render_anonymize_tab,
        render_fraud_tab,
        render_intake_tab,
        render_kyc_tab,
        render_policy_tab,
        render_report_tab,
        render_review_tab,
        render_train_tab,
    )
except Exception as exc:  # pragma: no cover - guard for partial installs
    st.error(f"Could not load Anti-Fraud tabs: {exc}")
    st.stop()

RUNS_DIR = AFK_ROOT / ".tmp_runs"
RUNS_DIR.mkdir(exist_ok=True)
ss = st.session_state
ss.setdefault("stage", "agents")
ss.setdefault("afk_logged_in", True)
ss.setdefault("afk_user", {"name": "Operator", "email": "operator@demo.local"})
ss.setdefault("afk_pending", 12)
ss.setdefault("afk_flagged", 4)
ss.setdefault("afk_avg_time", "14 min")
ss.setdefault("afk_ai_performance", 0.91)
ss["afk_logged_in"] = True
if not ss["afk_user"].get("name"):
    ss["afk_user"]["name"] = "Operator"


def _build_chat_context() -> Dict[str, Any]:
    ctx: Dict[str, Any] = {
        "agent_type": "fraud_kyc",
        "stage": ss.get("afk_stage") or ss.get("afk_active_tab"),
        "user": (ss.get("afk_user") or {}).get("name"),
        "pending_cases": ss.get("afk_pending"),
        "flagged_cases": ss.get("afk_flagged"),
        "avg_time": ss.get("afk_avg_time"),
        "ai_performance": ss.get("afk_ai_performance"),
        "run_id": ss.get("afk_last_run_id"),
        "dataset_name": ss.get("afk_dataset_name"),
        "selected_case": ss.get("afk_active_case"),
        "last_error": ss.get("afk_last_error"),
        "next_step": ss.get("afk_next_step"),
    }
    return {k: v for k, v in ctx.items() if v not in (None, "", [])}


def _coerce_minutes(value, fallback: float = 0.0) -> float:
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        cleaned = "".join(ch for ch in value if ch.isdigit() or ch == ".")
        try:
            return float(cleaned)
        except (TypeError, ValueError):
            pass
    return float(fallback)


def _go_stage(target_stage: str) -> None:
    """Navigate back to the shared app router if available."""
    ss["stage"] = target_stage
    try:
        st.switch_page("app.py")
        return
    except Exception:
        pass
    try:
        st.query_params["stage"] = target_stage
    except Exception:
        pass


def _theme_css(theme: str) -> str:
    pal = get_palette(theme)
    bg = pal["bg"]
    text = pal["text"]
    panel = pal["card"]
    accent = pal["accent"]
    accent_alt = pal.get("accent_alt", pal["accent"])
    border = pal["border"]
    input_bg = pal.get("input_bg", panel)
    shadow = pal["shadow"]
    return f"""
    <style>
    .stApp {{
      background: {bg} !important;
      color: {text} !important;
      font-family: "Inter","SF Pro Display","Segoe UI",sans-serif;
    }}
    .afk-hero,
    .afk-status-card,
    .afk-right-panel,
    .stButton>button,
    button[kind="primary"] {{
      background: {panel} !important;
      color: {text} !important;
      border: 1px solid {border} !important;
      border-radius: 14px !important;
      box-shadow: {shadow} !important;
    }}
    .stButton>button {{
      background: linear-gradient(95deg,{accent},{accent_alt}) !important;
      color: #fff !important;
      border: none !important;
    }}
    .stTabs [data-baseweb="tab-list"] button {{
      background: rgba(148,163,184,0.1) !important;
      color: {text} !important;
      border-radius: 14px !important;
      border: 1px solid rgba(148,163,184,0.3) !important;
      padding: 0.9rem 1.2rem !important;
      font-size: 1rem !important;
    }}
    .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {{
      background: linear-gradient(120deg,{accent},#0f172a) !important;
      color: #fff !important;
      box-shadow: 0 10px 25px rgba(15,23,42,0.35) !important;
    }}
    .stTextInput>div>div>input,
    .stNumberInput input,
    .stSelectbox>div>div>div {{
      background: {input_bg} !important;
      color: {text} !important;
      border-radius: 10px !important;
      border: 1px solid rgba(148,163,184,0.4) !important;
    }}
    [data-testid="stDataFrame"] {{
      border-radius: 14px !important;
      border: 1px solid rgba(148,163,184,0.4) !important;
      background: {panel} !important;
      color: {text} !important;
    }}
    </style>
    """


def _apply_theme(theme: str):
    apply_global_theme(theme)
    st.markdown(_theme_css(theme), unsafe_allow_html=True)


_apply_theme(get_theme())

nav_cols = st.columns([1, 1, 4])
with nav_cols[0]:
    if st.button("üè† Back to Home", key="afk_back_home"):
        _go_stage("landing")
with nav_cols[1]:
    if st.button("ü§ñ Back to Agents", key="afk_back_agents"):
        _go_stage("agents")

if not ss["afk_logged_in"]:
    st.title("üîê Anti-Fraud & KYC Agent Login")
    render_theme_toggle("üåó Dark mode", key="afk_theme_toggle_login")
    st.caption("Authenticate to orchestrate intake ‚Üí KYC ‚Üí fraud triage in one cockpit.")

    with st.form("afk_login_form"):
        u = st.text_input("Username", placeholder="e.g. analyst01")
        e = st.text_input("Email", placeholder="name@domain.com")
        pwd = st.text_input("Passphrase", type="password", placeholder="‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢")
        remember = st.checkbox("Remember session", value=True)
        submitted = st.form_submit_button("üöÄ Enter Command Center", use_container_width=True)

        if submitted:
            if u.strip():
                ss["afk_logged_in"] = True
                ss["afk_user"] = {"name": u.strip(), "email": e.strip(), "remember": remember}
                st.success("Welcome back ‚Äî initializing agent workspace‚Ä¶")
                st.rerun()
            else:
                st.error("Enter username to continue")
    st.stop()

st.title("üîí Anti-Fraud & KYC Agent")
st.caption("Unified onboarding ‚Üí privacy ‚Üí verification ‚Üí fraud response ‚Üí reporting workflow.")

_, theme_col = st.columns([5, 1])
with theme_col:
    render_theme_toggle("üåó Dark mode", key="afk_theme_toggle_main")

OPENSTACK_FLAVORS = {
    "m4.medium": "4 vCPU / 8 GB RAM ‚Äî CPU-only small",
    "m8.large": "8 vCPU / 16 GB RAM ‚Äî CPU-only medium",
    "g1.a10.1": "8 vCPU / 32 GB RAM + 1√óA10 24GB",
    "g1.l40.1": "16 vCPU / 64 GB RAM + 1√óL40 48GB",
    "g2.a100.1": "24 vCPU / 128 GB RAM + 1√óA100 80GB",
}
selected_llm = render_llm_selector(context="anti_fraud_kyc")
ss["afk_llm_label"] = selected_llm["model"]
ss["afk_llm_model"] = selected_llm["value"]
afk_flavor = st.selectbox(
    "OpenStack flavor / host profile",
    list(OPENSTACK_FLAVORS.keys()),
    index=0,
    key="afk_flavor",
)
st.caption(OPENSTACK_FLAVORS[afk_flavor])

st.markdown(
    """
    <style>
    .afk-hero {
        background: radial-gradient(circle at top left, #0f172a, #111827);
        border: 1px solid rgba(59,130,246,0.3);
        border-radius: 16px;
        padding: 1.5rem;
        box-shadow: 0 25px 60px rgba(15,23,42,0.45);
        color: #f8fafc;
    }
    .afk-hero h3 { margin-bottom: 0.2rem; }
    .afk-hero p { color: #cbd5f5; }
    .afk-status-card {
        background: rgba(15,23,42,0.8);
        border-radius: 16px;
        padding: 1rem 1.3rem;
        border: 1px solid rgba(148,163,184,0.2);
        box-shadow: inset 0 0 20px rgba(15,23,42,0.5);
    }
    .afk-status-card h4 {
        margin: 0;
        font-size: 0.95rem;
        text-transform: uppercase;
        color: #94a3b8;
    }
    .afk-status-card span.value {
        font-size: 1.8rem;
        font-weight: 700;
        color: #f8fafc;
    }
    .afk-right-panel {
        background: rgba(15,23,42,0.6);
        border-radius: 16px;
        padding: 1rem 1.2rem;
        border: 1px dashed rgba(148,163,184,0.4);
    }
    </style>
    """,
    unsafe_allow_html=True,
)

afk_ai_minutes = _coerce_minutes(ss.get("afk_avg_time"), 14.0)

render_operator_banner(
    operator_name=ss.get("afk_user", {}).get("name", "Guest"),
    title="Anti-Fraud & KYC Command",
    summary="Manage digital onboarding with live KYC, privacy, policy attestation, and fraud scoring.",
    bullets=[
        "Collect minimal viable data ‚Üí anonymize before sharing.",
        "Run IDV + watchlist scans, then route risky cases.",
        "Capture manual overrides to feed training + reports.",
    ],
    metrics=[
        {
            "label": "Pending Applicants",
            "value": ss.get("afk_pending"),
            "delta": "+3 vs yesterday",
            "delta_color": "#34d399",
            "color": "#34d399",
            "percent": 0.72,
            "context": "Human avg queue: 26",
        },
        {
            "label": "Flagged Cases",
            "value": ss.get("afk_flagged"),
            "delta": "-1 cleared",
            "delta_color": "#f87171",
            "color": "#f87171",
            "percent": 0.35,
            "context": "Manual review avg: 7",
        },
        {
            "label": "Avg Verification Time",
            "value": ss.get("afk_avg_time") or f"{afk_ai_minutes:.0f} min",
            "delta": "-2 min vs last week",
            "delta_color": "#60a5fa",
            "color": "#60a5fa",
            "percent": min(1.0, afk_ai_minutes / 40.0),
            "context": "AI verification cycle",
        },
    ],
    icon="üõ°Ô∏è",
)
# Replace sidebar radio with tabbed workflow like asset_appraisal
tab_guide, tab_intake, tab_privacy, tab_kyc, tab_fraud, tab_policy, tab_review, tab_train, tab_report, tab_feedback = st.tabs([
    "üß≠ Guide",
    "A) Intake",
    "B) Privacy",
    "C) KYC Verification",
    "D) Fraud Detection",
    "E) Policy & Controls",
    "F) Human Review",
    "G) Train",
    "H) Reports",
    "üó£Ô∏è Feedback",
])

with tab_guide:
    st.markdown(
        """
        ### What
        An AI and rules-based agent that detects fraudulent behavior and verifies customer identities automatically.

        ### Goal
        To prevent financial losses, strengthen AML/KYC compliance, and ensure trustworthy customer onboarding.

        ### How
        1. Upload transaction or customer datasets, or import data from Kaggle or Hugging Face.
        2. The agent anonymizes PII, validates IDs and emails, and runs AI-driven fraud scoring.
        3. Combines heuristic rules and ML models to flag anomalies, fake identities, or high-risk transactions.
        4. Provides fraud scores, reasoning, and compliance reports.

        ### So What (Benefits)
        - Instantly flags suspicious behavior in real-time.
        - Reduces manual fraud review by over 80%.
        - Learns from feedback to adapt to new fraud patterns.
        - Ensures audit-ready, compliant identity checks.

        ### What Next
        Try it with your transaction or KYC data.
        Contact our team to integrate government ID APIs, risk scoring modules, or AML data sources.
        Once customized, deploy the agent to your production environment to continuously monitor and prevent fraudulent activity.
        """
    )

with tab_intake:
    render_intake_tab(ss, RUNS_DIR)

with tab_privacy:
    render_anonymize_tab(ss, RUNS_DIR)

with tab_kyc:
    render_kyc_tab(ss, RUNS_DIR)

with tab_fraud:
    render_fraud_tab(ss, RUNS_DIR)

with tab_policy:
    render_policy_tab(ss, RUNS_DIR)

with tab_review:
    render_review_tab(ss, RUNS_DIR)

with tab_train:
    render_train_tab(ss, RUNS_DIR)

with tab_report:
    render_report_tab(ss, RUNS_DIR)

with tab_feedback:
    render_feedback_tab("üõ°Ô∏è Anti-Fraud & KYC Agent")

render_chat_assistant(
    page_id="anti_fraud_kyc",
    context=_build_chat_context(),
    faq_questions=[
        "How do I rerun the fraud rules for this borrower?",
        "Show me the latest sanction hits.",
        "Explain what the privacy scrub removed.",
        "Generate a full KYC audit packet.",
    ],
)



==================== ./unified_risk.py ====================
#!/usr/bin/env python3
"""Streamlit dashboard for the Unified Risk Orchestration agent."""
from __future__ import annotations

import json
import os
from datetime import datetime
from typing import Dict, Any
from copy import deepcopy

import pandas as pd
import streamlit as st
import requests
import plotly.express as px
import plotly.graph_objects as go
from services.ui.theme_manager import apply_theme, render_theme_toggle

st.set_page_config(page_title="üß© Unified Risk Orchestration Agent", layout="wide")
apply_theme()

API_URL = os.getenv("AGENT_API_URL", "http://localhost:8090")

SAMPLE_UNIFIED_RUNS = [
    {
        "run_id": "RUN-20251110-001",
        "decision": {
            "borrower_id": "BORR-001",
            "loan_id": "LN-1001",
            "loan_amount": 150000,
            "generated_at": "2025-11-10T06:39:18.105237+00:00",
            "asset": {
                "fmv": 250000,
                "ai_adjusted": 235000,
                "realizable_value": 210000,
                "encumbrance_flags": [],
            },
            "fraud": {
                "risk_score": 0.20,
                "risk_tier": "low",
                "sanction_hits": [],
                "kyc_passed": True,
            },
            "credit": {
                "credit_score": 720,
                "probability_default": 0.08,
                "approval": "approve",
                "ndi": 1.4,
            },
            "metadata": {
                "submitted_from": "sample",
                "asset_city": "San Jose",
                "asset_type": "Residential",
            },
        },
        "risk_summary": {
            "aggregated_score": 0.871,
            "risk_tier": "low",
            "recommendation": "approve",
        },
    },
    {
        "run_id": "RUN-20251109-004",
        "decision": {
            "borrower_id": "BORR-007",
            "loan_id": "LN-2045",
            "loan_amount": 420000,
            "generated_at": "2025-11-09T12:10:05.200000+00:00",
            "asset": {
                "fmv": 480000,
                "ai_adjusted": 450000,
                "realizable_value": 390000,
                "encumbrance_flags": ["Secondary lien"],
            },
            "fraud": {
                "risk_score": 0.42,
                "risk_tier": "medium",
                "sanction_hits": ["Internal_watchlist"],
                "kyc_passed": True,
            },
            "credit": {
                "credit_score": 655,
                "probability_default": 0.19,
                "approval": "review",
                "ndi": 0.95,
            },
            "metadata": {
                "submitted_from": "sample",
                "asset_city": "Los Angeles",
                "asset_type": "Multifamily",
            },
        },
        "risk_summary": {
            "aggregated_score": 0.62,
            "risk_tier": "medium",
            "recommendation": "review",
        },
    },
    {
        "run_id": "RUN-20251108-003",
        "decision": {
            "borrower_id": "BORR-021",
            "loan_id": "LN-3002",
            "loan_amount": 90000,
            "generated_at": "2025-11-08T08:55:40.000000+00:00",
            "asset": {
                "fmv": 100000,
                "ai_adjusted": 92000,
                "realizable_value": 70000,
                "encumbrance_flags": ["Pending tax lien"],
            },
            "fraud": {
                "risk_score": 0.68,
                "risk_tier": "high",
                "sanction_hits": [],
                "kyc_passed": False,
            },
            "credit": {
                "credit_score": 605,
                "probability_default": 0.27,
                "approval": "reject",
                "ndi": 0.65,
            },
            "metadata": {
                "submitted_from": "sample",
                "asset_city": "Houston",
                "asset_type": "Industrial",
            },
        },
        "risk_summary": {
            "aggregated_score": 0.41,
            "risk_tier": "high",
            "recommendation": "reject",
        },
    },
]

ss = st.session_state
ss.setdefault("unified_runs", [])
ss.setdefault("selected_unified_index", 0)
ss.setdefault("prefill_unified_form", None)
if not ss.unified_runs:
    ss.unified_runs = deepcopy(SAMPLE_UNIFIED_RUNS)


def _call_unified_api(payload: Dict[str, Any]) -> Dict[str, Any]:
    url = f"{API_URL.rstrip('/')}/v1/unified/decision"
    resp = requests.post(url, json=payload, timeout=60)
    resp.raise_for_status()
    return resp.json()


def _render_gauge(title: str, value: float, min_val: float, max_val: float, suffix: str = "", key: str | None = None):
    span = max_val - min_val
    if span <= 0:
        max_val = value if value else 1
        min_val = 0
    fig = go.Figure(
        go.Indicator(
            mode="gauge+number",
            value=value,
            number={"suffix": suffix},
            title={"text": title},
            gauge={
                "axis": {"range": [min_val, max_val]},
                "bar": {"color": "#38bdf8"},
                "bgcolor": "#0f172a",
                "borderwidth": 1,
                "bordercolor": "#1e293b",
            },
        )
    )
    fig.update_layout(height=260, margin=dict(l=20, r=20, t=40, b=0))
    st.plotly_chart(fig, use_container_width=True, config={"displayModeBar": False}, key=key or title)


def _flatten_for_table(value: Any, parent_key: str = "") -> list[tuple[str, Any]]:
    rows: list[tuple[str, Any]] = []
    if isinstance(value, dict):
        for key, val in value.items():
            new_key = f"{parent_key}.{key}" if parent_key else key
            rows.extend(_flatten_for_table(val, new_key))
    elif isinstance(value, list):
        if not value:
            rows.append((parent_key or "[]", "[]"))
        else:
            for idx, item in enumerate(value):
                new_key = f"{parent_key}[{idx}]" if parent_key else f"[{idx}]"
                rows.extend(_flatten_for_table(item, new_key))
    else:
        rows.append((parent_key or "value", value))
    return rows


def _dict_to_table(data: Dict[str, Any]) -> pd.DataFrame:
    rows = _flatten_for_table(data)
    return pd.DataFrame(rows, columns=["Field", "Value"])


def _build_runs_dataframe(runs: list[Dict[str, Any]]) -> pd.DataFrame:
    rows = []
    for idx, record in enumerate(runs):
        decision = record["decision"]
        summary = record["risk_summary"]
        fraud = decision.get("fraud", {})
        asset = decision.get("asset", {})
        credit = decision.get("credit", {})
        generated_raw = decision.get("generated_at") or record.get("generated_at")
        generated_dt = None
        if generated_raw:
            try:
                generated_dt = datetime.fromisoformat(generated_raw)
            except Exception:
                generated_dt = None
        loan_amount = decision.get("loan_amount") or decision.get("metadata", {}).get("loan_amount")
        ai_adjusted = asset.get("ai_adjusted") or asset.get("fmv") or 1
        ltv = None
        if loan_amount and ai_adjusted:
            try:
                ltv = loan_amount / ai_adjusted
            except ZeroDivisionError:
                ltv = None
        rows.append(
            {
                "index": idx,
                "run_id": record.get("run_id"),
                "borrower": decision.get("borrower_id"),
                "loan_id": decision.get("loan_id"),
                "generated": generated_dt,
                "aggregated_score": summary.get("aggregated_score"),
                "risk_tier": summary.get("risk_tier", "").lower(),
                "recommendation": summary.get("recommendation", "").lower(),
                "fraud_score": fraud.get("risk_score"),
                "fraud_tier": fraud.get("risk_tier", ""),
                "credit_score": credit.get("credit_score"),
                "credit_pd": credit.get("probability_default"),
                "credit_approval": credit.get("approval", ""),
                "asset_fmv": asset.get("fmv"),
                "asset_ai": ai_adjusted,
                "asset_realizable": asset.get("realizable_value"),
                "ltv": ltv,
                "loan_amount": loan_amount,
                "asset_city": decision.get("metadata", {}).get("asset_city", "Unknown"),
                "asset_type": decision.get("metadata", {}).get("asset_type", "Unknown"),
                "record": record,
            }
        )
    df = pd.DataFrame(rows)
    return df


def _launch_page(target: str):
    mapping = {
        "app": "app.py",
        "agent_template": "pages/agent_template.py",
        "anti_fraud": "pages/anti_fraud_kyc.py",
        "asset": "pages/asset_appraisal.py",
        "credit": "pages/credit_appraisal.py",
        "troubleshooter": "pages/troubleshooter_agent.py",
        "unified": "pages/unified_risk.py",
    }
    page = mapping.get(target)
    if not page:
        return
    try:
        st.switch_page(page)
    except Exception:
        pass


st.title("üß© Unified Risk Orchestration Agent")
st.caption(
    "Master coordinator that compounds Asset, Credit, and Anti-Fraud agents into a single bank-grade decision package."
)

nav_cols = st.columns([1, 1, 1, 1, 1])
with nav_cols[0]:
    if st.button("üè† Home", use_container_width=True):
        _launch_page("app")
with nav_cols[1]:
    if st.button("üè¶ Asset", use_container_width=True):
        _launch_page("asset")
with nav_cols[2]:
    if st.button("üí≥ Credit", use_container_width=True):
        _launch_page("credit")
with nav_cols[3]:
    if st.button("üõ°Ô∏è Anti-Fraud", use_container_width=True):
        _launch_page("anti_fraud")
with nav_cols[4]:
    render_theme_toggle(key="unified_theme_toggle")

st.markdown(
    """
    > **Unified Risk Checklist**  
    > ‚úÖ Is the borrower real & safe? (KYC/Fraud)  
    > ‚úÖ Is the collateral worth enough? (Asset)  
    > ‚úÖ Can they afford the loan? (Credit)  
    > ‚úÖ Should the bank approve overall? (Unified agent)
    """
)

with st.expander("‚ÑπÔ∏è How this agent works", expanded=False):
    st.markdown(
        """
        **Step 1 ‚Äî Portfolio Triage**
        * Start at **Global Portfolio Overview** to review high-risk cases, trendlines, and recommendation mix.
        * Use **Open Case** on any flagged borrower to jump straight into their unified summary.

        **Step 2 ‚Äî Borrower Case Summary**
        * Review the KPI row (recommendation, aggregated score, tier, credit approval, fraud tier, collateral status).
        * Inspect the gauges (FMV, fraud score, credit score) and the mini status panels for asset, fraud/KYC, and credit.

        **Step 3 ‚Äî Drill-Down**
        * Expand Asset / Fraud / Credit panels for detailed metrics, risk flags, and raw tables.
        * Download the full `unified_risk_decision.json` if you need to attach it to credit committee notes.

        **Step 4 ‚Äî Take Action**
        * Click **Re-run this borrower** to prefill the submission form with the current data.
        * Scroll to **Submit A Unified Run**, adjust any values, and regenerate the unified decision.
        """
    )

c1, c2, c3, c4 = st.columns(4)
with c1:
    st.metric("Identity", "KYC/Fraud", "Is borrower real & safe?")
with c2:
    st.metric("Collateral", "Asset", "Is the collateral worth enough?")
with c3:
    st.metric("Credit", "Loan", "Can they afford it?")
with c4:
    st.metric("Unified", "Decision", "Approve / Review / Reject")

st.divider()

st.divider()
st.header("Global Portfolio Overview")
if not ss.unified_runs:
    st.info("No unified runs yet. Submit the form above to generate your first decision.")
else:
    runs_df = _build_runs_dataframe(ss.unified_runs)
    total_borrowers = len(runs_df)
    avg_portfolio_score = runs_df["aggregated_score"].mean()
    rec_counts = runs_df["recommendation"].value_counts(normalize=True)
    approve_pct = rec_counts.get("approve", 0.0)
    review_pct = rec_counts.get("review", 0.0)
    reject_pct = rec_counts.get("reject", 0.0)
    high_risk_count = (runs_df["risk_tier"] == "high").sum()
    avg_pd = runs_df["credit_pd"].mean()
    avg_ltv = runs_df["ltv"].mean(skipna=True)
    avg_realizable = runs_df["asset_realizable"].mean()

    kpi_row1 = st.columns(4)
    kpi_row1[0].metric("Total Active Borrowers", total_borrowers)
    kpi_row1[1].metric("Portfolio Aggregated Score", f"{avg_portfolio_score:.2f}" if pd.notna(avg_portfolio_score) else "‚Äî")
    kpi_row1[2].metric("High-Risk Borrowers", int(high_risk_count))
    kpi_row1[3].metric("Avg PD", f"{avg_pd:.1%}" if pd.notna(avg_pd) else "‚Äî")

    kpi_row2 = st.columns(3)
    kpi_row2[0].metric("% Approve / Review / Reject", f"{approve_pct:.0%} / {review_pct:.0%} / {reject_pct:.0%}")
    kpi_row2[1].metric("Avg Loan-to-Value", f"{avg_ltv:.2f}x" if pd.notna(avg_ltv) else "‚Äî")
    kpi_row2[2].metric("Avg Realizable Value", f"${avg_realizable:,.0f}" if pd.notna(avg_realizable) else "‚Äî")

    chart_cols = st.columns(3)
    with chart_cols[0]:
        st.caption("Aggregated Score Trend")
        trend_df = runs_df.dropna(subset=["generated", "aggregated_score"]).sort_values("generated")
        if not trend_df.empty:
            fig_trend = px.line(trend_df, x="generated", y="aggregated_score", markers=True)
            fig_trend.update_layout(height=260, margin=dict(l=10, r=10, t=30, b=0))
            st.plotly_chart(fig_trend, use_container_width=True, config={"displayModeBar": False})
        else:
            st.info("More runs needed for a trendline.")
    with chart_cols[1]:
        st.caption("Recommendation Mix")
        status_counts = runs_df["recommendation"].value_counts().reset_index()
        status_counts.columns = ["Recommendation", "Count"]
        if not status_counts.empty:
            fig_mix = px.bar(status_counts, x="Recommendation", y="Count", color="Recommendation")
            fig_mix.update_layout(height=260, margin=dict(l=10, r=10, t=30, b=0))
            st.plotly_chart(fig_mix, use_container_width=True, config={"displayModeBar": False})
        else:
            st.info("No recommendations yet.")
    with chart_cols[2]:
        st.caption("Heatmap by City")
        heat_df = runs_df.groupby(["asset_city", "recommendation"]).size().reset_index(name="count")
        if not heat_df.empty:
            fig_heat = px.density_heatmap(heat_df, x="recommendation", y="asset_city", z="count", color_continuous_scale="Blues")
            fig_heat.update_layout(height=260, margin=dict(l=10, r=10, t=30, b=0))
            st.plotly_chart(fig_heat, use_container_width=True, config={"displayModeBar": False})
        else:
            st.info("Need more cases for heatmap.")

    st.subheader("Top 10 High-Risk Cases")
    priority_map = {"high": 3, "medium": 2, "low": 1}
    runs_df["risk_rank"] = runs_df["risk_tier"].map(priority_map).fillna(0)
    high_risk_df = runs_df.sort_values(["risk_rank", "credit_pd", "fraud_score"], ascending=[False, False, False]).head(10)
    if high_risk_df.empty:
        st.info("No high-risk cases yet.")
    else:
        header_cols = st.columns([1.1, 0.9, 0.9, 0.9, 0.9, 0.9, 0.8])
        header_cols[0].markdown("**Borrower ID**")
        header_cols[1].markdown("**Risk Score**")
        header_cols[2].markdown("**Risk Tier**")
        header_cols[3].markdown("**PD**")
        header_cols[4].markdown("**Fraud Score**")
        header_cols[5].markdown("**Recommendation**")
        header_cols[6].markdown("**Action**")
        for _, row in high_risk_df.iterrows():
            cols = st.columns([1.1, 0.9, 0.9, 0.9, 0.9, 0.9, 0.8])
            borrower_label = f"**{row['borrower']}**" if isinstance(row["borrower"], str) else "‚Äî"
            tier_label = row["risk_tier"].title() if isinstance(row["risk_tier"], str) else "Unknown"
            reco_label = row["recommendation"].upper() if isinstance(row["recommendation"], str) else "N/A"
            cols[0].markdown(borrower_label)
            cols[1].write(f"{row['aggregated_score']:.2f}" if pd.notna(row['aggregated_score']) else "‚Äî")
            cols[2].write(tier_label)
            cols[3].write(f"{row['credit_pd']:.1%}" if pd.notna(row['credit_pd']) else "‚Äî")
            cols[4].write(f"{row['fraud_score']:.2f}" if pd.notna(row['fraud_score']) else "‚Äî")
            cols[5].write(reco_label)
            if cols[6].button("Open Case", key=f"open_{row['index']}"):
                ss.selected_unified_index = int(row["index"])
                st.session_state["unified_case_selector"] = ss.selected_unified_index
                st.rerun()

st.divider()
st.header("Borrower Unified Case Summary")
if not ss.unified_runs:
    st.info("Generate a decision to unlock borrower insights.")
else:
    options = list(range(len(ss.unified_runs)))
    current_idx = min(ss.selected_unified_index, len(options) - 1)

    selector_key = "unified_case_selector"
    if selector_key not in st.session_state:
        st.session_state[selector_key] = current_idx
    elif ss.selected_unified_index != st.session_state[selector_key]:
        st.session_state[selector_key] = current_idx

    selected_idx = st.selectbox(
        "Select Borrower",
        options,
        index=st.session_state[selector_key],
        format_func=lambda idx: f"{ss.unified_runs[idx]['decision']['borrower_id']} ‚Äî {ss.unified_runs[idx]['risk_summary']['recommendation'].upper()}",
        key=selector_key,
    )
    ss.selected_unified_index = selected_idx
    selected_record = ss.unified_runs[selected_idx]
    decision = selected_record["decision"]
    summary = selected_record["risk_summary"]
    fraud = decision.get("fraud", {})
    asset = decision.get("asset", {})
    credit = decision.get("credit", {})

    flag_messages = []
    if asset.get("encumbrance_flags"):
        flag_messages.append("Encumbrance alerts")
    if fraud.get("sanction_hits"):
        flag_messages.append("Sanction list hits")
    if not fraud.get("kyc_passed", True):
        flag_messages.append("KYC review required")
    if fraud.get("risk_score", 0) >= 0.5:
        flag_messages.append("Fraud risk elevated")
    if credit.get("probability_default", 0) >= 0.2:
        flag_messages.append("High probability of default")
    if asset.get("realizable_value") and decision.get("loan_amount"):
        coverage = asset["realizable_value"] / decision["loan_amount"] if decision["loan_amount"] else None
        if coverage and coverage < 1:
            flag_messages.append("Collateral below loan amount")
    flags_text = ", ".join(flag_messages) if flag_messages else "All systems normal."

    st.markdown(
        f"**Borrower:** `{decision.get('borrower_id', '‚Äî')}` &nbsp;&nbsp; | &nbsp;&nbsp; "
        f"**Loan ID:** `{decision.get('loan_id', '‚Äî')}` &nbsp;&nbsp; | &nbsp;&nbsp; "
        f"**Decision Generated:** `{decision.get('generated_at', 'n/a')}`"
    )

    badge_cols = st.columns(6)
    badge_cols[0].metric("Recommendation", summary.get("recommendation", "").upper())
    badge_cols[1].metric("Aggregated Score", f"{summary.get('aggregated_score', 0):.2f}")
    badge_cols[2].metric("Unified Risk Tier", summary.get("risk_tier", "").title())
    badge_cols[3].metric("Credit Approval", credit.get("approval", "").upper())
    badge_cols[4].metric("Fraud Tier", fraud.get("risk_tier", "n/a").title())
    collateral_label = (
        "Encumbrance" if asset.get("encumbrance_flags") else "Clear"
    )
    badge_cols[5].metric("Collateral Status", collateral_label)

    st.info(f"**Summary Flags:** {flags_text}")

    gauge_cols = st.columns(3)
    asset_ai = asset.get("ai_adjusted") or asset.get("fmv") or 0
    asset_cap = max((asset.get("fmv") or 0) * 1.2, asset_ai or 1)
    with gauge_cols[0]:
        _render_gauge(
            "AI-Adjusted FMV",
            asset_ai,
            0,
            asset_cap,
            suffix=" USD",
            key=f"gauge_asset_ai_{selected_record['run_id']}",
        )
    with gauge_cols[1]:
        _render_gauge(
            "Fraud Risk Score",
            fraud.get("risk_score", 0.0),
            0,
            1,
            key=f"gauge_fraud_score_{selected_record['run_id']}",
        )
    with gauge_cols[2]:
        _render_gauge(
            "Credit Score",
            credit.get("credit_score", 0),
            300,
            900,
            key=f"gauge_credit_score_{selected_record['run_id']}",
        )

    panels = st.columns(3)
    with panels[0]:
        st.subheader("Asset Status")
        st.write(f"FMV: ${asset.get('fmv', 0):,.0f}")
        st.write(f"AI Adjusted: ${asset_ai:,.0f}")
        st.write(f"Realizable: ${asset.get('realizable_value', 0):,.0f}")
        st.write("Encumbrances: " + (", ".join(asset.get("encumbrance_flags", [])) or "None"))
    with panels[1]:
        st.subheader("Fraud Status")
        st.write(f"Risk Score: {fraud.get('risk_score', 0):.2f} ({fraud.get('risk_tier', 'n/a')})")
        st.write(f"KYC Passed: {'Yes' if fraud.get('kyc_passed', True) else 'No'}")
        st.write("Sanction Hits: " + (", ".join(fraud.get("sanction_hits", [])) or "None"))
    with panels[2]:
        st.subheader("Credit Status")
        st.write(f"Credit Score: {credit.get('credit_score', 0)}")
        st.write(f"PD: {credit.get('probability_default', 0):.1%}")
        st.write(f"NDI: {credit.get('ndi', 'n/a')}")
        st.write(f"Approval: {credit.get('approval', 'n/a').upper()}")

    st.divider()
    st.header("Drill-Down Details")
    with st.expander("Asset Drill-Down", True):
        kpi_cols = st.columns(3)
        kpi_cols[0].metric("FMV", f"${asset.get('fmv', 0):,.0f}")
        kpi_cols[1].metric("AI Adjusted", f"${(asset.get('ai_adjusted') or asset.get('fmv') or 0):,.0f}")
        kpi_cols[2].metric("Realizable Value", f"${asset.get('realizable_value', 0):,.0f}")

        st.subheader("Encumbrance Flags")
        enc_flags = asset.get("encumbrance_flags", []) or []
        if not enc_flags:
            st.success("‚úÖ No encumbrances detected.")
        else:
            for flag in enc_flags:
                st.error(f"‚ùå {flag}")

        st.subheader("Valuation Details")
        metadata = decision.get("metadata", {})
        valuation_details = [
            ("Confidence Score", asset.get("confidence_score", "n/a")),
            ("Asset Type", metadata.get("asset_type", "n/a")),
            ("Condition", asset.get("condition", metadata.get("asset_condition", "n/a"))),
            ("Location", metadata.get("asset_city", "n/a")),
            ("Comps Used", ", ".join(asset.get("comps", [])) or "Not provided"),
            ("Evidence Quality", asset.get("evidence_quality", "Not provided")),
            ("Valuation Comments", asset.get("valuation_comment", "No adjustments recorded.")),
        ]
        st.table(pd.DataFrame(valuation_details, columns=["Item", "Description"]))

        asset_lat = asset.get("lat") or metadata.get("asset_lat")
        asset_lon = asset.get("lon") or metadata.get("asset_lon")
        if asset_lat and asset_lon:
            st.map(pd.DataFrame({"lat": [asset_lat], "lon": [asset_lon]}))

        with st.expander("üîç Raw Asset Data", False):
            st.table(_dict_to_table(asset))
    with st.expander("Fraud / KYC Drill-Down", True):
        st.subheader("üïµÔ∏è Fraud & KYC Risks")
        fraud_cols = st.columns([1.2, 1, 1, 1])
        with fraud_cols[0]:
            _render_gauge(
                "Fraud Risk Score",
                fraud.get("risk_score", 0.0),
                0,
                1,
                key=f"gauge_fraud_drill_{selected_record['run_id']}",
            )
        with fraud_cols[1]:
            tier = (fraud.get("risk_tier") or "n/a").title()
            tier_color = {"Low": "üü© Low", "Medium": "üüß Medium", "High": "üü• High"}.get(tier, tier)
            st.metric("Fraud Tier", tier_color)
        with fraud_cols[2]:
            st.metric("Sanction Hits", len(fraud.get("sanction_hits", [])))
        with fraud_cols[3]:
            kyc_passed = fraud.get("kyc_passed", True)
            st.metric("KYC Passed", "‚úÖ Yes" if kyc_passed else "‚ùå No")

        hits = fraud.get("sanction_hits", [])
        if hits:
            chips = " ".join(f"`{hit}`" for hit in hits)
            st.warning(f"Sanctions: {chips}")
        else:
            st.success("No sanction hits on record.")

        with st.expander("üîç Raw Fraud Data", False):
            st.table(_dict_to_table(fraud))

    with st.expander("Credit Drill-Down", True):
        st.subheader("üí≥ Credit Risk Details")
        credit_cols = st.columns([1.2, 1, 1, 1])
        with credit_cols[0]:
            _render_gauge(
                "Credit Score",
                credit.get("credit_score", 0),
                300,
                900,
                key=f"gauge_credit_drill_{selected_record['run_id']}",
            )
        with credit_cols[1]:
            st.metric("Probability of Default", f"{credit.get('probability_default', 0):.1%}")
        with credit_cols[2]:
            ndi_val = credit.get("ndi")
            ndi_display = f"{ndi_val:.2f}" if isinstance(ndi_val, (int, float)) else ndi_val or "n/a"
            st.metric("NDI", ndi_display)
        with credit_cols[3]:
            approval = (credit.get("approval") or "n/a").upper()
            badge = {
                "APPROVE": "üü© APPROVE",
                "REVIEW": "üüß REVIEW",
                "REJECT": "üü• REJECT",
            }.get(approval, approval)
            st.metric("Approval Status", badge)

        with st.expander("üîç Raw Credit Data", False):
            st.table(_dict_to_table(credit))
    with st.expander("Full Unified Decision Data", True):
        st.table(_dict_to_table(decision))
        st.download_button(
            "Download unified_risk_decision.json",
            data=json.dumps(decision, indent=2).encode("utf-8"),
            file_name=f"{selected_record['run_id']}.json",
            mime="application/json",
        )

    st.divider()
    st.subheader("Submit A Unified Run")

    with st.form("unified_form"):
        prefill = ss.get("prefill_unified_form") or {}
        prefill_asset = prefill.get("asset", {})
        prefill_fraud = prefill.get("fraud", {})
        prefill_credit = prefill.get("credit", {})

        existing_borrowers = sorted({run["decision"].get("borrower_id") for run in ss.unified_runs if run.get("decision")})
        default_borrower = prefill.get("borrower_id") or (existing_borrowers[0] if existing_borrowers else "BORR-001")
        if default_borrower and default_borrower not in existing_borrowers:
            existing_borrowers = [default_borrower] + existing_borrowers
        borrower_select_options = existing_borrowers + ["‚ûï Custom entry"] if existing_borrowers else ["‚ûï Custom entry"]
        default_index = borrower_select_options.index(default_borrower) if default_borrower in borrower_select_options else len(borrower_select_options) - 1
        borrower_choice = st.selectbox("Borrower ID", borrower_select_options, index=default_index, key="borrower_select")
        if borrower_choice == "‚ûï Custom entry":
            borrower_id = st.text_input("Enter Borrower ID", value=default_borrower, key="borrower_text_input")
        else:
            borrower_id = borrower_choice

        loan_id = st.text_input("Loan ID", value=prefill.get("loan_id", "LN-1001"))
        loan_amount = st.number_input("Loan Amount", value=float(prefill.get("loan_amount", 150000.0)), step=5000.0)

        col_asset_a, col_asset_b, col_asset_c = st.columns(3)
        with col_asset_a:
            fmv = st.number_input("Asset FMV", value=float(prefill_asset.get("fmv", 250000.0)), step=1000.0)
        with col_asset_b:
            ai_adjusted = st.number_input("AI Adjusted", value=float(prefill_asset.get("ai_adjusted", 235000.0)), step=1000.0)
        with col_asset_c:
            realizable = st.number_input("Realizable Value", value=float(prefill_asset.get("realizable_value", 210000.0)), step=1000.0)

        col_fraud_a, col_fraud_b = st.columns(2)
        with col_fraud_a:
            fraud_score = st.slider("Fraud Risk Score", 0.0, 1.0, float(prefill_fraud.get("risk_score", 0.2)), 0.01)
        with col_fraud_b:
            default_tier = prefill_fraud.get("risk_tier", "low")
            fraud_tier = st.selectbox("Fraud Tier", ["low", "medium", "high"], index=["low","medium","high"].index(default_tier) if default_tier in ["low","medium","high"] else 0)
        sanctions_prefill = ", ".join(prefill_fraud.get("sanction_hits", [])) if prefill_fraud.get("sanction_hits") else ""
        sanctions_text = st.text_input("Sanction hits (comma separated)", value=sanctions_prefill)
        sanctions = [s.strip() for s in sanctions_text.split(",") if s.strip()]

        col_credit_a, col_credit_b, col_credit_c = st.columns(3)
        with col_credit_a:
            credit_score = st.number_input("Credit Score", min_value=300, max_value=900, value=int(prefill_credit.get("credit_score", 720)))
        with col_credit_b:
            probability_default = st.slider("Probability of Default", 0.0, 1.0, float(prefill_credit.get("probability_default", 0.08)), 0.01)
        with col_credit_c:
            default_credit = prefill_credit.get("approval", "approve")
            credit_decision = st.selectbox("Credit Decision", ["approve", "review", "reject"], index=["approve","review","reject"].index(default_credit) if default_credit in ["approve","review","reject"] else 0)

        submitted = st.form_submit_button("Generate Unified Decision", use_container_width=True)

        if submitted:
            payload = {
                "borrower_id": borrower_id,
                "loan_id": loan_id,
                "loan_amount": loan_amount,
                "asset": {
                    "fmv": fmv,
                    "ai_adjusted": ai_adjusted,
                    "realizable_value": realizable,
                    "encumbrance_flags": [],
                },
                "fraud": {
                    "risk_score": fraud_score,
                    "risk_tier": fraud_tier,
                    "sanction_hits": sanctions,
                    "kyc_passed": fraud_score < 0.6,
                },
                "credit": {
                    "credit_score": credit_score,
                    "probability_default": probability_default,
                    "approval": credit_decision,
                    "ndi": None,
                },
                "metadata": {"submitted_from": "streamlit"},
            }
            try:
                response = _call_unified_api(payload)
                ss.unified_runs.insert(0, response)
                ss.selected_unified_index = 0
                ss.prefill_unified_form = None
                st.success(f"Unified decision created for {borrower_id} ({response['risk_summary']['recommendation'].upper()}).")
            except requests.HTTPError as exc:  # pragma: no cover - network
                st.error(f"API error: {exc.response.text}")
            except Exception as exc:  # pragma: no cover - network
                st.error(f"Unexpected error: {exc}")

st.subheader("Re-run Unified Decision")
st.caption("Need to refresh the orchestration with updated borrower data? Re-run directly from this case.")
if st.button("‚Ü∫ Re-run this borrower", key="btn_rerun_selected"):
    ss.prefill_unified_form = deepcopy(
        {
            "borrower_id": decision.get("borrower_id"),
            "loan_id": decision.get("loan_id"),
            "loan_amount": decision.get("loan_amount"),
            "asset": asset,
            "fraud": fraud,
            "credit": credit,
        }
    )
    st.info("Prefilled the submission form with this borrower's latest data. Complete the form below to finalize the rerun.")
    st.rerun()



==================== ./agent_template.py ====================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üè¶ Asset Appraisal Agent ‚Äî Full E2E Flow (Inputs ‚Üí Anonymize ‚Üí AI ‚Üí Human Review ‚Üí Training)
Author:  Nguyen Dzoan
Version: 2025-11-01

Includes:
- Stage 1: CSV + evidence (images/PDFs) + manual row; synthetic fallback + "why" table
- Stage 2: Explicit anonymization pipeline (RAW & ANON kept)
- Stage 3: AI appraisal with runtime flavor selector, agent discovery+probe, rule_reasons when backend omits
  + Production banner + asset-trained model selector + promote inside Stage 3
- Stage 4: Human Review with AI‚ÜîHuman agreement gauge; export feedback CSV
- Stage 5: Training (upload feedback) ‚Üí Train candidate ‚Üí Promote to PRODUCTION
"""

import os
import io
import re
import json
from datetime import datetime, timezone  # ‚úÖ clean, safe, supports datetime.now()
from pathlib import Path
from textwrap import dedent
from typing import Any, Dict

# ‚îÄ‚îÄ Third-party
import requests
import numpy as np
import pandas as pd
import streamlit as st
import streamlit.components.v1 as components
import plotly.express as px
import plotly.graph_objects as go
import csv
import zipfile  # ‚úÖ ADD THIS

from services.ui.theme_manager import (
    apply_theme as apply_global_theme,
    get_palette,
    get_theme,
    render_theme_toggle,
)
from services.ui.components.operator_banner import render_operator_banner
from services.ui.components.telemetry_dashboard import render_telemetry_dashboard
from services.ui.components.feedback import render_feedback_tab







# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# PAGE CONFIG + THEME
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import streamlit as st

st.set_page_config(page_title="Asset Appraisal Agent", layout="wide")
ss = st.session_state

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SESSION DEFAULTS (idempotent)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _init_defaults():
    ss.setdefault("asset_logged_in", True)
    ss.setdefault("asset_stage", "asset_flow")   # login ‚Üí asset_flow
    ss.setdefault("asset_user", {"name": "Operator", "email": "operator@demo.local"})
    ss.setdefault("asset_pending_cases", 18)
    ss.setdefault("asset_flagged_cases", 3)
    ss.setdefault("asset_avg_time", "22 min")
    # Working tables/artifacts per our matrix (placeholders)
    ss.setdefault("asset_intake_df", None)
    ss.setdefault("asset_evidence_index", None)
    ss.setdefault("asset_anon_df", None)
    ss.setdefault("asset_features_df", None)
    ss.setdefault("asset_comps_used", None)
    ss.setdefault("asset_valued_df", None)
    ss.setdefault("asset_verified_df", None)
    ss.setdefault("asset_policy_df", None)
    ss.setdefault("asset_decision_df", None)
    ss.setdefault("asset_human_review_df", None)
    ss.setdefault("asset_feedback_csv", None)
    ss.setdefault("asset_trained_model_meta", None)
    ss.setdefault("asset_gpu_profile", None)  # will be set only in C.4
    ss.setdefault("asset_ai_performance", 0.88)
    os.makedirs("./.tmp_runs", exist_ok=True)

_init_defaults()

# Always bypass login step during operator demos
ss["asset_logged_in"] = True
if ss.get("asset_stage") == "login":
    ss["asset_stage"] = "asset_flow"


def _coerce_minutes(value, fallback: float = 0.0) -> float:
    """Convert values like '22 min' to floats for gauge percentages."""
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        cleaned = "".join(ch for ch in value if ch.isdigit() or ch == ".")
        try:
            return float(cleaned)
        except (TypeError, ValueError):
            pass
    return float(fallback)

def render_nav_bar_app():
    st.markdown(
        "<div style='display:flex;gap:12px;align-items:center'>"
        "<a href='?stage=agents' class='macbtn'>ü§ñ Agents</a>"
        "<span style='opacity:.6'>/</span>"
        "<span>üèõÔ∏è Asset Appraisal Agent</span>"
        "</div>",
        unsafe_allow_html=True,
    )


# ---- Global runs dir (used everywhere) ----
RUNS_DIR = os.path.abspath("./.tmp_runs")
os.makedirs(RUNS_DIR, exist_ok=True)






# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# API CONFIG
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

API_URL = os.getenv("API_URL", "http://localhost:8090")

# Default fallbacks (will be superseded by discovery)

ASSET_AGENT_IDS = [a.strip() for a in os.getenv("ASSET_AGENT_IDS", "asset_appraisal,asset").split(",") if a.strip()]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# NAV (reliable jump to Home / Agents from a page)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _set_query_params_safe(**kwargs):
    # New API (Streamlit ‚â•1.40)
    try:
        for k, v in kwargs.items():
            st.query_params[k] = v
        return True
    except Exception:
        pass
    # Older versions
    try:
        st.experimental_set_query_params(**kwargs)
        return True
    except Exception:
        return False

def _go_stage(target_stage: str):
    # 1) let app.py‚Äôs router know what to show
    st.session_state["stage"] = target_stage

    # 2) preferred: jump to main app file
    try:
        # path is relative to the run root when you launch:
        #   streamlit run services/ui/app.py
        st.switch_page("app.py")
        return
    except Exception:
        pass

    # 3) fallback: set query param and rerun so app.py picks it up
    _set_query_params_safe(stage=target_stage)
    st.rerun()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# UTILITIES ‚Äî DataFrame selection helpers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ---- DataFrame selection helpers (avoid boolean ambiguity) ----
def first_nonempty_df(*candidates):
    """Return the first candidate that is a non-empty pandas DataFrame, else None."""
    for df in candidates:
        if isinstance(df, pd.DataFrame) and not df.empty:
            return df
    return None

def is_nonempty_df(x) -> bool:
    return isinstance(x, pd.DataFrame) and not x.empty

def render_nav_bar_app():
    stage = st.session_state.get("stage", "landing")

    # three columns: home, agents, theme toggle
    c1, c2, c3 = st.columns([1, 1, 2.5])

    with c1:
        if st.button("üè† Back to Home", key=f"btn_home_{stage}"):
            _go_stage("landing")
            st.stop()

    with c2:
        if st.button("ü§ñ Back to Agents", key=f"btn_agents_{stage}"):
            _go_stage("agents")
            st.stop()

    with c3:
        render_theme_toggle(
            label="üåó Dark mode",
            key="asset_theme_toggle",
            help="Switch theme",
        )

    st.markdown("---")




# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GEO UTILITIES: EXIF GPS, Geocode, Geohash   ‚Üê PASTE START
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from typing import Optional, Tuple

def _exif_to_degrees(value):
    try:
        d = float(value[0][0]) / float(value[0][1])
        m = float(value[1][0]) / float(value[1][1])
        s = float(value[2][0]) / float(value[2][1])
        return d + (m / 60.0) + (s / 3600.0)
    except Exception:
        return None

def extract_gps_from_image(path: str) -> Optional[Tuple[float, float]]:
    try:
        from PIL import Image
        from PIL.ExifTags import TAGS, GPSTAGS
        img = Image.open(path)
        exif = img._getexif() or {}
        tagged = {TAGS.get(k, k): v for k, v in exif.items()}
        gps_info = tagged.get("GPSInfo")
        if not gps_info:
            return None
        gps_data = {GPSTAGS.get(k, k): v for k, v in gps_info.items()}
        lat = _exif_to_degrees(gps_data.get("GPSLatitude"))
        lon = _exif_to_degrees(gps_data.get("GPSLongitude"))
        if lat is None or lon is None:
            return None
        lat_ref = gps_data.get("GPSLatitudeRef", "N")
        lon_ref = gps_data.get("GPSLongitudeRef", "E")
        if lat_ref == "S": lat = -lat
        if lon_ref == "W": lon = -lon
        return (lat, lon)
    except Exception:
        return None

_GEOCODE_CACHE_PATH = "./.tmp_runs/geocode_cache.json"

def _load_geocode_cache():
    try:
        with open(_GEOCODE_CACHE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _save_geocode_cache(cache: dict):
    os.makedirs("./.tmp_runs", exist_ok=True)
    with open(_GEOCODE_CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def geocode_freeform(addr: str) -> Optional[Tuple[float, float]]:
    """Nominatim via geopy; cached locally. Returns None if offline."""
    try:
        cache = _load_geocode_cache()
        key = addr.strip().lower()
        if key in cache:
            v = cache[key]
            return (v["lat"], v["lon"])
        from geopy.geocoders import Nominatim
        geolocator = Nominatim(user_agent="asset-appraisal-agent")
        loc = geolocator.geocode(addr, timeout=10)
        if not loc:
            return None
        cache[key] = {"lat": float(loc.latitude), "lon": float(loc.longitude)}
        _save_geocode_cache(cache)
        return (float(loc.latitude), float(loc.longitude))
    except Exception:
        return None

def geohash_decode(s: str) -> Optional[Tuple[float, float]]:
    try:
        import geohash  # pip install python-geohash
        lat, lon = geohash.decode(s)
        return (float(lat), float(lon))
    except Exception:
        return None



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SESSION
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ss = st.session_state
ss.setdefault("asset_stage", "login")
ss.setdefault("asset_logged_in", False)
ss.setdefault("asset_user", None)

# Stage caches
ss.setdefault("asset_raw_df", None)     # Stage 1 raw (after CSV/manual merge)
ss.setdefault("asset_evidence", [])     # evidence filenames (images/pdfs)
ss.setdefault("asset_anon_df", None)    # Stage 2 anonymized
ss.setdefault("asset_stage2_df", None)  # Stage 3 input (resolved source)
ss.setdefault("asset_ai_df", None)      # Stage 3 AI output
ss.setdefault("asset_selected_model", None)  # trained model path

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# HELPERS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def anonymize_text_cols(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for col in out.columns:
        if out[col].dtype == "object":
            out[col] = (
                out[col].astype(str)
                .apply(lambda x: re.sub(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+", "[EMAIL]", x))
            )
    return out

def quick_synth(rows: int = 150) -> pd.DataFrame:
    """Generate asset rows + finance metrics for demo/backup."""
    rng = np.random.default_rng(42)
    cities = [
        ("Hanoi", 21.0285, 105.8542),
        ("HCMC", 10.7769, 106.7009),
        ("Da Nang", 16.0544, 108.2022),
        ("Hue", 16.4637, 107.5909),
        ("Can Tho", 10.0452, 105.7469),
    ]
    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, rows + 1)],
        "asset_id": [f"A{i:04d}" for i in range(1, rows + 1)],
        "asset_type": rng.choice(["House","Apartment","Car","Land","Factory"], rows),
        "age_years": rng.integers(1, 40, rows),
        "market_value": rng.integers(50_000, 2_000_000, rows),
        "condition_score": rng.uniform(0.6, 1.0, rows),
        "legal_penalty": rng.uniform(0.95, 1.0, rows),          # legal/title risk adj
        "employment_years": rng.integers(0, 30, rows),
        "credit_history_years": rng.integers(0, 25, rows),
        "delinquencies": rng.integers(0, 6, rows),
        "current_loans": rng.integers(0, 8, rows),
        "loan_amount": rng.integers(10_000, 200_000, rows),
        "customer_type": rng.choice(["bank","non-bank"], rows, p=[0.7,0.3]),
    })
    cdf = pd.DataFrame(cities, columns=["city","lat","lon"])
    df["city"] = rng.choice(cdf["city"], rows)
    df = df.merge(cdf, on="city", how="left")
    df["depreciation_rate"] = (1 - df["condition_score"]) * 100
    df["market_segment"] = np.where(df["market_value"] > 500_000, "High", "Mass")
    df["DTI"] = rng.uniform(0.05, 0.9, rows)
    df["LTV"] = np.clip(df["loan_amount"] / np.maximum(df["market_value"], 1), 0.05, 1.5)
    df["evidence_files"] = [[] for _ in range(rows)]
    return df

def synth_why_table() -> pd.DataFrame:
    return pd.DataFrame([
        {"Metric": "DTI", "Why": "Debt service relative to income ‚Äî proxy for payability."},
        {"Metric": "LTV", "Why": "Loan vs asset value ‚Äî proxy for collateral adequacy."},
        {"Metric": "condition_score", "Why": "Asset physical state impacts fair value/depreciation."},
        {"Metric": "legal_penalty", "Why": "Legal/title flags reduce realizable value."},
        {"Metric": "employment_years / credit_history_years", "Why": "Stability/track record."},
        {"Metric": "delinquencies / current_loans", "Why": "Current risk pressure."},
        {"Metric": "market_segment / city / lat,lon", "Why": "Market & location effects on pricing."},
    ])

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DATAFRAME SELECTION (avoid boolean ambiguity)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def first_nonempty_df(*candidates):
    """Return the first candidate that is a non-empty pandas DataFrame, else None."""
    for df in candidates:
        if isinstance(df, pd.DataFrame) and not df.empty:
            return df
    return None



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# UNIVERSAL INGEST + NORMALIZATION HELPERS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _slug(name: str) -> str:
    return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

def _ts() -> str:
    return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

def _read_any_table(uploaded_file) -> pd.DataFrame:
    """
    Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback.
    Accepts Streamlit UploadedFile or a file-like object.
    """
    name = getattr(uploaded_file, "name", "").lower()

    # Excel first
    if name.endswith((".xlsx", ".xls")):
        return pd.read_excel(uploaded_file)

    # Text (CSV/TSV/TXT): try utf-8, then latin-1; sniff delimiter.
    raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
    for enc in ("utf-8", "latin-1"):
        try:
            text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
            lines = text.splitlines()
            sample = "\n".join(lines[:5]) if lines else ""
            try:
                dialect = csv.Sniffer().sniff(sample) if sample else csv.excel
                sep = getattr(dialect, "delimiter", ",")
            except Exception:
                sep = ","
            return pd.read_csv(io.StringIO(text), sep=sep)
        except Exception:
            continue
    # last resort
    return pd.read_csv(io.BytesIO(raw), engine="python")

def _normalize_for_agents(df: pd.DataFrame) -> pd.DataFrame:
    """
    Light normalization for credit/asset agents.
    Creates a consistent thin schema if columns exist; leaves extras intact.
    """
    out = df.copy()

    # alias map (extend freely)
    aliases = {
        "application_id": ["application_id", "app_id", "loan_id", "id", "request_id"],
        "asset_id":       ["asset_id", "property_id", "house_id", "assetid"],
        "asset_type":     ["asset_type", "type", "category"],
        "address":        ["address", "addr", "street", "location"],
        "city":           ["city", "town"],
        "state":          ["state", "province", "region"],
        "country":        ["country"],
        "price":          ["price", "value", "market_value", "listing_price", "sale_price"],
        "bedrooms":       ["bedrooms", "beds"],
        "bathrooms":      ["bathrooms", "baths"],
        "parking_space":  ["parking_space", "parking", "garage"],
        "title":          ["title", "name"],
    }

    # rename by first matching alias
    rename_map = {}
    cols = set(out.columns)
    for target, cands in aliases.items():
        for c in cands:
            if c in cols:
                rename_map[c] = target
                break
    out = out.rename(columns=rename_map)

    # ensure presence of common columns
    required = ["application_id", "asset_id", "asset_type", "address", "city", "state", "price"]
    for col in required:
        if col not in out.columns:
            out[col] = None

    # numeric coercions
    for col in ("price", "bedrooms", "bathrooms", "parking_space"):
        if col in out.columns:
            out[col] = pd.to_numeric(out[col], errors="coerce")

    # provenance
    out["source_dataset"] = st.session_state.get("asset_intake_source_name", "uploaded")
    return out


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# THEME SYSTEM (Light/Dark CSS + map style)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from textwrap import dedent

THEME_EXTRAS = {
    "light": {
        "success": "#16A34A",
        "warn": "#D97706",
        "danger": "#DC2626",
        "stripe": "#F1F5F9",
        "shadow": "0 6px 24px rgba(15,23,42,0.08)",
    },
    "dark": {
        "success": "#22C55E",
        "warn": "#FBBF24",
        "danger": "#F87171",
        "stripe": "#111827",
        "shadow": "0 8px 30px rgba(0,0,0,0.35)",
    },
}


def _theme_tokens(theme: str) -> dict[str, str]:
    pal = get_palette(theme)
    extra = THEME_EXTRAS.get(theme, THEME_EXTRAS["dark"])
    return {
        "bg": pal["bg"],
        "panel": pal["card"],
        "text": pal["text"],
        "muted": pal["subtext"],
        "primary": pal["accent"],
        "accent": pal["accent_alt"],
        "success": extra["success"],
        "warn": extra["warn"],
        "danger": extra["danger"],
        "stripe": extra["stripe"],
        "shadow": extra["shadow"],
    }


def _theme_css(theme: str) -> str:
    t = _theme_tokens(theme)
    return dedent(f"""
    <style>
      /* Fonts: Inter + JetBrains Mono */
      @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=JetBrains+Mono:wght@400;600&display=swap');

      :root {{
        --bg: {t['bg']};
        --panel: {t['panel']};
        --text: {t['text']};
        --muted: {t['muted']};
        --primary: {t['primary']};
        --success: {t['success']};
        --warn: {t['warn']};
        --danger: {t['danger']};
        --accent: {t['accent']};
        --stripe: {t['stripe']};
        --shadow: {t['shadow']};
        --radius: 14px;
      }}

      html, body, .stApp {{
        background: var(--bg) !important;
        color: var(--text) !important;
        font-family: 'Inter', system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif !important;
      }}

      /* Panel-like boxes (use .left-box/.right-box or your custom containers) */
      .left-box, .right-box, .stExpander, .stTabs [data-baseweb="tab-highlight"] {{
        background: var(--panel) !important;
        border-radius: var(--radius);
        box-shadow: var(--shadow);
      }}

      /* Headings */
      h1, h2, h3, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 {{
        color: var(--text) !important;
        font-weight: 700;
        letter-spacing: -0.01em;
      }}

      /* Buttons */
      .stButton>button, button[kind="primary"] {{
        background: var(--primary) !important;
        color: #fff !important;
        border-radius: 12px !important;
        border: none !important;
        box-shadow: 0 3px 12px rgba(37,99,235,0.35) !important;
      }}
      .stDownloadButton button {{
        background: var(--success) !important;
        color: #fff !important;
        border-radius: 12px !important;
        border: none !important;
        box-shadow: 0 3px 12px rgba(34,197,94,0.35) !important;
      }}

      /* Tables (dataframe) */
      .stDataFrame thead tr th {{
        background: var(--panel) !important;
        color: var(--muted) !important;
        font-weight: 600 !important;
      }}
      .stDataFrame tbody tr:nth-child(odd) {{
        background: var(--stripe) !important;
      }}

      /* Chips / small badges */
      .chip {{
        display:inline-block; padding:4px 10px; border-radius:999px;
        background: var(--panel); color: var(--muted); border:1px solid rgba(148,163,184,0.35);
      }}

      /* Code font */
      code, pre, .stCodeBlock, .st-emotion-cache-ffhzg2 {{
        font-family: 'JetBrains Mono', ui-monospace, SFMono-Regular, Menlo, Consolas, monospace !important;
      }}
      
      /* NEW  Optional: hide sidebar without touching theme */
      [data-testid="stSidebar"], section[data-testid="stSidebar"], nav[data-testid="stSidebarNav"] {{
        display: none !important;
      }}
      [data-testid="stAppViewContainer"] {{
        margin-left: 0 !important;
        padding-left: 0 !important;
      }}
      
      
    </style>
    """)

def apply_theme(theme: str | None = None):
    """Inject shared Streamlit theme plus asset-specific tokens."""
    theme = theme or get_theme()
    apply_global_theme(theme)
    st.markdown(_theme_css(theme), unsafe_allow_html=True)


apply_theme()



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# MAP THEME HELPERS (Mapbox style + token + adapters)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os
import streamlit as st

def get_mapbox_token() -> str | None:
    """Find a Mapbox token from secrets or env. Return None if not set."""
    try:
        tok = st.secrets.get("MAPBOX_TOKEN")
    except Exception:
        tok = None
    if not tok:
        tok = os.environ.get("MAPBOX_TOKEN") or os.environ.get("MAPBOX_ACCESS_TOKEN")
    return tok or None


def plotly_map_style() -> str:
    """
    Return a Plotly-compatible map style.
    - Uses bright style in light mode even without a Mapbox token.
    - Falls back to CARTO 'positron' if Mapbox token not set.
    """
    theme = get_theme()
    token = get_mapbox_token()
    if token:
        return "light" if theme == "light" else "dark"
    else:
        # fallback to open CARTO tiles (bright)
        return "carto-positron" if theme == "light" else "carto-darkmatter"


def get_map_style() -> str:
    """
    Return a Mapbox style URL (for pydeck only).
    Light ‚Üí bright, Dark ‚Üí dark. Defaults to light for safety.
    """
    theme = get_theme()
    return "mapbox://styles/mapbox/light-v11" if theme == "light" else "mapbox://styles/mapbox/dark-v11"


def apply_plotly_mapbox_defaults():
    """Set Plotly's Mapbox token globally (if available)."""
    import plotly.express as px
    token = get_mapbox_token()
    if token:
        px.set_mapbox_access_token(token)
    else:
        st.info("‚ÑπÔ∏è Mapbox token not set ‚Äî using free bright map style (carto-positron).")


def make_pydeck_view_state(lat=10.7769, lon=106.7009, zoom=10, pitch=0, bearing=0):
    import pydeck as pdk
    return pdk.ViewState(latitude=lat, longitude=lon, zoom=zoom, pitch=pitch, bearing=bearing)


def pydeck_map_style() -> str:
    """pydeck uses the same Mapbox style URLs when a token is available."""
    return get_map_style()



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# AGENT DISCOVERY & PROBE
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _safe_get_json(url: str, timeout: int = 8):
    try:
        r = requests.get(url, timeout=timeout)
        if r.ok:
            try:
                return True, r.json()
            except Exception as e:
                return False, f"parse error: {e}\nBody:\n{r.text[:2000]}"
        return False, f"{r.status_code} {r.reason}\nBody:\n{r.text[:2000]}"
    except Exception as e:
        return False, f"request error: {e}"

def discover_asset_agents() -> list[str]:
    """Try common discovery endpoints and extract agent ids. Cache in session."""
    cached = st.session_state.get("asset_agent_ids")
    if isinstance(cached, list) and cached:
        return cached

    candidates = []

    # 1) /v1/agents (prefer)
    ok, data = _safe_get_json(f"{API_URL}/v1/agents")
    if ok:
        try:
            if isinstance(data, dict) and "agents" in data:
                items = data["agents"]
                if isinstance(items, list):
                    for it in items:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name") or it.get("agent") or it.get("slug")
                            if aid: candidates.append(aid)
            elif isinstance(data, list):
                for it in data:
                    if isinstance(it, str):
                        candidates.append(it)
                    elif isinstance(it, dict):
                        aid = it.get("id") or it.get("name")
                        if aid: candidates.append(aid)
        except Exception:
            pass

    # 2) /v1/agents/list (alt)
    if not candidates:
        ok2, data2 = _safe_get_json(f"{API_URL}/v1/agents/list")
        if ok2:
            try:
                if isinstance(data2, dict):
                    for k in ("agents", "data", "items"):
                        if k in data2 and isinstance(data2[k], list):
                            for it in data2[k]:
                                if isinstance(it, str):
                                    candidates.append(it)
                                elif isinstance(it, dict):
                                    aid = it.get("id") or it.get("name")
                                    if aid: candidates.append(aid)
                elif isinstance(data2, list):
                    for it in data2:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name")
                            if aid: candidates.append(aid)
            except Exception:
                pass

    # 3) /v1/health (sometimes lists agents)
    if not candidates:
        ok3, data3 = _safe_get_json(f"{API_URL}/v1/health")
        if ok3 and isinstance(data3, dict):
            for k in ("agents", "services", "available_agents"):
                val = data3.get(k)
                if isinstance(val, list):
                    for it in val:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name")
                            if aid: candidates.append(aid)

    discovered = [c for c in dict.fromkeys(candidates) if c]  # de-dupe
    if not discovered:
        discovered = ASSET_AGENT_IDS[:]  # fallback to env/defaults

    st.session_state["asset_agent_ids"] = discovered
    return discovered

def probe_api() -> dict:
    """Collect quick diagnostics for UI."""
    diag = {}
    for path in ("/v1/health", "/v1/agents", "/v1/agents/list"):
        ok, data = _safe_get_json(f"{API_URL}{path}")
        diag[path] = data if ok else {"error": data}
    diag["API_URL"] = API_URL
    diag["discovered_agents"] = discover_asset_agents()
    return diag

# NEW: run_id extractor for various API payload shapes
def _extract_run_id(obj) -> str | None:
    """Find a run_id in a nested dict/list API response."""
    if isinstance(obj, dict):
        rid = obj.get("run_id")
        if isinstance(rid, str) and rid:
            return rid
        for k in ("data", "meta", "result", "payload"):
            v = obj.get(k)
            if isinstance(v, dict):
                rid = v.get("run_id")
                if isinstance(rid, str) and rid:
                    return rid
    elif isinstance(obj, list):
        for it in obj:
            rid = _extract_run_id(it)
            if rid:
                return rid
    return None

def try_run_asset_agent(csv_bytes: bytes, form_fields: dict, timeout_sec: int = 180):
    """
    Discover agent ids, then try each. Rebuild multipart for each attempt.
    Preferred: use run_id to GET merged CSV and DataFrame it.
    Fallback: normalize 'result' only (not whole JSON).

    Returns (ok: bool, DataFrame | error_string)
    """
    agent_ids = discover_asset_agents()
    errors = []
    for agent_id in agent_ids:
        files = {"file": ("asset_verified.csv", io.BytesIO(csv_bytes), "text/csv")}
        url = f"{API_URL}/v1/agents/{agent_id}/run"
        try:
            resp = requests.post(url, files=files, data=form_fields, timeout=timeout_sec)
        except Exception as e:
            errors.append(f"[{agent_id}] request error: {e}")
            continue

        if resp.ok:
            body_text = resp.text[:4000]
            try:
                payload = resp.json()
            except Exception as e:
                errors.append(f"[{agent_id}] parse error: {e}\nBody:\n{body_text}")
                continue

            rid = _extract_run_id(payload)
            if rid:
                # Preferred: fetch merged CSV
                try:
                    r_csv = requests.get(f"{API_URL}/v1/runs/{rid}/report?format=csv", timeout=60)
                    if r_csv.ok:
                        df = pd.read_csv(io.BytesIO(r_csv.content))
                        st.session_state["asset_last_run_id"] = rid
                        st.session_state["asset_last_runner"] = ((payload.get("meta") or {}).get("runner_used"))
                        return True, df
                    else:
                        errors.append(
                            f"[{agent_id}] report GET {r_csv.status_code} {r_csv.reason} for run_id={rid}\n"
                            f"Body:\n{r_csv.text[:2000]}"
                        )
                except Exception as e:
                    errors.append(f"[{agent_id}] report GET error for run_id={rid}: {e}")

            # Fallback: try to render just 'result'
            result_part = payload.get("result")
            if isinstance(result_part, list):
                try:
                    df = pd.json_normalize(result_part)
                    return True, df
                except Exception as e:
                    errors.append(f"[{agent_id}] fallback normalize error: {e}\nBody:\n{body_text}")
            elif isinstance(result_part, dict):
                try:
                    df = pd.json_normalize(result_part)
                    return True, df
                except Exception as e:
                    errors.append(f"[{agent_id}] fallback normalize error: {e}\nBody:\n{body_text}")
            else:
                errors.append(f"[{agent_id}] no run_id and empty/unknown 'result'.\nBody:\n{body_text}")
        else:
            errors.append(f"[{agent_id}] {resp.status_code} {resp.reason}\nBody:\n{resp.text[:2000]}")

    return False, "All agent attempts failed (discovered=" + ", ".join(agent_ids) + "):\n" + "\n\n".join(errors)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# LOGIN
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if ss["asset_stage"] == "login" and not ss["asset_logged_in"]:
    render_nav_bar_app()
    st.title("üîê Login to AI Asset Appraisal Platform")
    c1, c2, c3 = st.columns([1,1,1])
    with c1:
        user = st.text_input("Username", placeholder="e.g. dzoan")
    with c2:
        email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
    with c3:
        pwd = st.text_input("Password", type="password", placeholder="Enter any password")
    if st.button("Login", key="btn_asset_login", use_container_width=True):
        if (user or "").strip() and (email or "").strip():
            ss["asset_user"] = {
                "name": user.strip(),
                "email": email.strip(),
                "timestamp": datetime.now(timezone.utc).isoformat(),  # ‚úÖ fixed
            }
            ss["asset_logged_in"] = True
            ss["asset_stage"] = "asset_flow"
            st.rerun()
        else:
            st.error("‚ö†Ô∏è Please fill all fields before continuing.")
    st.stop()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# WORKFLOW (A‚ÜíG)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if ss.get("asset_logged_in") and ss.get("asset_stage") in ("asset_flow", "asset_agent"):
    render_nav_bar_app()
    st.title("üèõÔ∏è Asset Appraisal Agent")
    st.caption(
        "A‚ÜíG pipeline ‚Äî Intake ‚Üí Privacy ‚Üí Valuation ‚Üí Policy ‚Üí Human Review ‚Üí Model Training ‚Üí Reporting "
        f"| üëã {ss['asset_user']['name']}"
    )

    asset_ai_minutes = _coerce_minutes(ss.get("asset_avg_time"), 22.0)

    render_operator_banner(
        operator_name=ss.get("asset_user", {}).get("name", "Operator"),
        title="Asset Appraisal Command",
        summary="Automate collateral intake, anonymization, valuation, and policy checks with AI copilots.",
        bullets=[
            "Unify intake sources (CSV, HF, Kaggle) and auto-enrich with geospatial context.",
            "Apply FMV modeling + haircut & LTV policy thresholds before human review.",
            "Capture reviewer feedback to retrain valuation models and improve reports.",
        ],
        metrics=[
            {
                "label": "Avg AI Turnaround",
                "value": ss.get("asset_avg_time") or f"{asset_ai_minutes:.0f} min",
                "delta": "-5 min vs last week",
                "delta_color": "#60a5fa",
                "color": "#60a5fa",
                "percent": min(1.0, asset_ai_minutes / 50.0),
                "context": "AI valuation cycle time",
            },
            {
                "label": "Assets Pending Valuation",
                "value": ss.get("asset_pending_cases"),
                "delta": "+4 vs prior cycle",
                "delta_color": "#34d399",
                "color": "#34d399",
                "percent": min(1.0, ss.get("asset_pending_cases", 0) / 40.0),
                "context": "Manual backlog avg: 35",
            },
            {
                "label": "Flagged Risk Cases",
                "value": ss.get("asset_flagged_cases"),
                "delta": "+1 escalation",
                "delta_color": "#f87171",
                "color": "#f87171",
                "percent": min(1.0, ss.get("asset_flagged_cases", 0) / 12.0),
                "context": "Human avg: 6 risk flags",
            },
        ],
        icon="üèõÔ∏è",
    )
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # TABS (How-To + A..H) ‚Äî Live tabs
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    tabHow, tabA, tabB, tabC, tabD, tabE, tabF, tabG, tabH, tabFeedback = st.tabs([
        "üìò How-To",
        "üü¶ A) Intake & Evidence",
        "üü© B) Privacy & Features",
        "üü® C) Valuation & Verification",
        "üüß D) Policy & Decision",
        "üü™ E) Human Review & Feedback",
        "üü´ F) Model Training & Promotion",
        "üü´ G) Deployment & Export üöÄ",     # ‚úÖ corrected label (was double F)
        "‚¨ú H) Reporting & Handoff üßæ",
        "üó£Ô∏è Feedback"
    ])

    with tabHow:
        st.title("üìò How to Use This Agent")
        st.markdown("""
### What
An AI-powered agent that performs collateral and asset valuation automatically using market data, machine learning, and verification logic.

### Goal
To provide instant, data-driven, and consistent asset valuations for credit, insurance, or regulatory use.

### How
1. Import property or asset data from CSV, Kaggle, or Hugging Face sources.
2. The agent anonymizes and enriches data, extracts geospatial and condition features, and predicts fair market value (FMV).
3. Applies haircut and LTV policy thresholds to derive realizable value.
4. Verifies ownership, encumbrances, and generates professional reports.

### So What (Benefits)
- Cuts appraisal turnaround from days to minutes.
- Standardizes valuations across asset classes.
- Increases confidence with explainable AI valuations.
- Ensures data consistency and legal traceability.

### What Next
1. Run a test with your asset dataset or public examples.
2. Contact our team to customize valuation rules, haircut logic, and report templates.
3. Once validated, integrate the agent into your production credit or appraisal systems to automate end-to-end asset evaluation.
        """)


    # Runtime tip
    st.caption(
        "üìò Tip: Move sequentially from A‚ÜíG or revisit individual stages. "
        "If a stage reports missing data, rerun the previous one or load demo data."
    )

    with tabFeedback:
        render_feedback_tab("üè¶ Asset Appraisal Agent")

else:
    st.warning("Please log in first to access the Asset Appraisal workflow.")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üü¶ STAGE A ‚Äî INTAKE & EVIDENCE
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tabA:
    import io, os, json, hashlib, pandas as pd
    from datetime import datetime, timezone
    
    
    ss = st.session_state  # ‚úÖ make 'ss' available in this scope
    st.subheader("A. Intake & Evidence")
    st.caption("Steps: (1) Upload / Import, (2) Normalize, (3) Generate unified intake CSV")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üìò Quick User Guide (updated)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    with st.expander("üìò Quick User Guide", expanded=False):
        st.markdown("""
        **Goal:** Collect, normalize, and unify all asset-related data before appraisal.

        **1Ô∏è‚É£ Upload Your Data**
        - Upload **field agent reports**, **loan lists with collateral**, and **legal property documents**.
        - Supported: `.csv`, `.xlsx`, `.zip` (evidence images/docs).

        **2Ô∏è‚É£ Import Open Data**
        - Search **Kaggle** or **Hugging Face** for relevant valuation datasets.
        - You can mix public + internal uploads ‚Äî AI will normalize columns.

        **3Ô∏è‚É£ Normalize**
        - After upload/import, click **"Normalize Data"** to merge and standardize features.
        - Output: `intake_table.csv` ready for Stage B (Anonymization).

        **4Ô∏è‚É£ Generate Synthetic Data**
        - If no input data is available, the AI can synthesize a demo dataset representing:
          `asset_id, asset_type, city, market_value, loan_amount, legal_source, condition_score`.

        **5Ô∏è‚É£ Output**
        - A unified CSV file is produced ‚Üí download or proceed directly to **Stage B**.
        """)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # (A.1) UPLOAD ZONE ‚Äî Human Inputs
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üì§ Upload Data Files (Field Agents / Loans / Legal Docs)")
    uploaded_files = st.file_uploader(
        "Upload multiple files",
        type=["csv", "xlsx", "zip"],
        accept_multiple_files=True,
        key="asset_upload_files"
    )

    uploaded_dfs = []
    if uploaded_files:
        for f in uploaded_files:
            try:
                if f.name.endswith(".csv"):
                    df = pd.read_csv(f)
                elif f.name.endswith(".xlsx"):
                    df = pd.read_excel(f)
                else:
                    st.info(f"üì¶ Skipping non-tabular file: {f.name}")
                    continue
                st.success(f"‚úÖ Loaded `{f.name}` ({len(df)} rows, {len(df.columns)} cols)")
                uploaded_dfs.append(df)
            except Exception as e:
                st.error(f"‚ùå Failed to read {f.name}: {e}")


    # ‚îÄ‚îÄNew  global runs dir (shared across stages) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    RUNS_DIR = os.path.abspath("./.tmp_runs")
    os.makedirs(RUNS_DIR, exist_ok=True)
    
   

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # (A.2) PUBLIC DATASETS ‚Äî Kaggle / HF / OpenML
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üåç Import Public Datasets (Kaggle / Hugging Face / OpenML / Portals)")

    # keep a place to persist search results across reruns
    ss.setdefault("kaggle_search_df", pd.DataFrame())

    src = st.selectbox(
        "Select source",
        ["Kaggle (API)", "Hugging Face", "OpenML", "Public Domain Portals"],
        key="asset_pubsrc"
    )
    query = st.text_input("Search keywords", "house prices real estate valuation", key="asset_pubquery")

    # helpers
    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    
    def _safe_read_csv(fp: str) -> pd.DataFrame:
        """
        Robust CSV reader:
        - Tries multiple encodings (utf-8, utf-8-sig, cp1252, latin-1)
        - Tries common separators
        - Skips bad rows rather than crashing
        """
        encodings = ["utf-8", "utf-8-sig", "cp1252", "latin-1"]
        seps = [",", ";", "\t", "|"]

        last_err = None
        for enc in encodings:
            for sep in seps:
                try:
                    df_try = pd.read_csv(
                        fp,
                        encoding=enc,
                        sep=sep,
                        engine="python",
                        on_bad_lines="skip",   # pandas >=1.3
                    )
                    # Require at least 2 columns to consider it valid
                    if df_try.shape[1] >= 2:
                        return df_try
                except Exception as e:
                    last_err = e
                    continue

        # Final fallback: read bytes, decode with latin-1 replacement, then parse in-memory
        try:
            with open(fp, "rb") as f:
                raw = f.read()
            text = raw.decode("latin-1", errors="replace")
            for sep in seps:
                try:
                    return pd.read_csv(
                        io.StringIO(text),
                        sep=sep,
                        engine="python",
                        on_bad_lines="skip",
                    )
                except Exception:
                    pass
        except Exception as e:
            last_err = e

        raise RuntimeError(f"Could not parse CSV with common encodings/separators. Last error: {last_err}")

    


    # Kaggle search
    if st.button("üîé Search dataset", key="btn_asset_pubsearch"):
        with st.spinner("Searching datasets..."):
            try:
                if src == "Kaggle (API)":
                    import subprocess, io
                    cmd = ["kaggle", "datasets", "list", "-s", query, "-v"]  # -v => CSV output
                    out = subprocess.run(cmd, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                    if out.returncode != 0:
                        st.error(f"Kaggle CLI failed: {out.stderr.strip() or out.stdout.strip()}")
                        st.info("üí° Ensure ~/.kaggle/kaggle.json exists and has chmod 600.")
                        ss["kaggle_search_df"] = pd.DataFrame()
                    else:
                        df_pub = pd.read_csv(io.StringIO(out.stdout))
                        keep = [c for c in ["ref","title","size","lastUpdated","downloadCount","voteCount","usabilityRating"] if c in df_pub.columns]
                        ss["kaggle_search_df"] = df_pub[keep]
                        st.success("‚úÖ Kaggle API results shown.")
                elif src == "Hugging Face":
                    from huggingface_hub import list_datasets
                    results = list_datasets(search=query)
                    df_pub = pd.DataFrame([{"Dataset": r.id, "Tags": ", ".join(r.tags)} for r in results[:50]])
                    st.dataframe(df_pub, use_container_width=True, hide_index=True)
                    st.success("‚úÖ Hugging Face datasets retrieved.")
                elif src == "OpenML":
                    st.markdown(f"[üìä OpenML Search ‚ÜóÔ∏è](https://www.openml.org/search?type=data&q={query})")
                elif src == "Public Domain Portals":
                    st.markdown("""
                    - [üåé data.gov](https://www.data.gov/)
                    - [üá™üá∫ data.europa.eu](https://data.europa.eu/)
                    - [üá∏üá¨ data.gov.sg](https://data.gov.sg/)
                    - [üáªüá≥ data.gov.vn](https://data.gov.vn/)
                    """)
            except Exception as e:
                st.error(f"Search failed: {e}")

    # If we have Kaggle results, show table + import controls
    if src == "Kaggle (API)" and not ss["kaggle_search_df"].empty:
        st.dataframe(ss["kaggle_search_df"], use_container_width=True, hide_index=True)

        with st.expander("‚¨áÔ∏è Import Selected Kaggle Dataset", expanded=True):
            refs = ss["kaggle_search_df"]["ref"].astype(str).tolist()
            selected_ref = st.selectbox("Choose a dataset (ref)", refs, key="asset_kaggle_ref")

            kag_dir = os.path.join(RUNS_DIR, "kaggle")
            os.makedirs(kag_dir, exist_ok=True)
            safe_ref = re.sub(r"[^a-zA-Z0-9._/-]+", "_", selected_ref)
            safe_ref_for_file = safe_ref.replace("/", "__")
            dest = os.path.join(kag_dir, safe_ref_for_file)
            os.makedirs(dest, exist_ok=True)

            # Optional: let user set a server-side save folder (relative to project root)
            st.markdown("**Optional server-side save folder (relative to project root)**")
            default_svdir = os.path.join(RUNS_DIR, "kaggle_exports")
            svdir = st.text_input(
                "Save to folder (server-side)",
                value=default_svdir,
                key="asset_kaggle_svdir",
                help="This saves on the server/WSL side (not your desktop). Use Download button below for local Save As."
            )

            # Main import button
            if st.button("üì• Download & Import Selected", key="btn_asset_kaggle_dl", use_container_width=True):
                try:
                    import subprocess
                    cmd = ["kaggle", "datasets", "download", "-d", selected_ref, "-p", dest, "--unzip"]
                    r = subprocess.run(cmd, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                    if r.returncode != 0:
                        raise RuntimeError(r.stderr.strip() or r.stdout.strip())

                    csvs = [f for f in os.listdir(dest) if f.lower().endswith(".csv")]
                    if not csvs:
                        raise FileNotFoundError("No CSV found in the downloaded archive.")
                    fp = os.path.join(dest, csvs[0])

                    # Load & stash for downstream stages + download button
                    df_imp = _safe_read_csv(fp)
                    ss["asset_intake_df"] = df_imp

                    # Save a unified copy with timestamp in RUNS_DIR
                    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
                    uni_fp = os.path.join(RUNS_DIR, f"intake_table.{ts}.csv")
                    df_imp.to_csv(uni_fp, index=False)

                    st.success(f"‚úÖ Imported {len(df_imp):,} rows from `{selected_ref}`")
                    st.caption(f"Saved unified intake copy: `{uni_fp}`")
                    st.dataframe(df_imp.head(100), use_container_width=True)

                    # ‚îÄ‚îÄ NEW: Local Download (browser) with dataset-name.csv ‚îÄ‚îÄ
                    st.markdown("#### üíæ Download")
                    default_fname = f"{safe_ref_for_file}.csv"
                    st.download_button(
                        label="‚¨áÔ∏è Download CSV",
                        file_name=default_fname,
                        data=df_imp.to_csv(index=False).encode("utf-8"),
                        mime="text/csv",
                        key="asset_kaggle_download"
                    )

                    # ‚îÄ‚îÄ NEW: Optional server-side save with user folder ‚îÄ‚îÄ
                    st.markdown("#### üóÇÔ∏è Save on Server (optional)")
                    # sanitize: keep within project root
                    project_root = os.path.abspath(os.path.join(RUNS_DIR, "..", ".."))
                    svdir_abs = os.path.abspath(svdir)
                    if not svdir_abs.startswith(project_root):
                        st.warning("‚ö†Ô∏è Path is outside project root; resetting to default exports folder.")
                        svdir_abs = os.path.abspath(default_svdir)

                    os.makedirs(svdir_abs, exist_ok=True)
                    save_name = f"{safe_ref_for_file}.csv"
                    server_save_path = os.path.join(svdir_abs, save_name)

                    if st.button("üíΩ Save CSV on Server", key="btn_asset_kaggle_save_server"):
                        try:
                            df_imp.to_csv(server_save_path, index=False)
                            rel_path = os.path.relpath(server_save_path, start=project_root)
                            st.success(f"‚úÖ Saved on server: `{server_save_path}`")
                            st.caption(f"(Relative to project root: ./{rel_path})")
                        except Exception as e:
                            st.error(f"Server save failed: {e}")

                except Exception as e:
                    st.error(f"Import failed: {e}")
                    st.info("Tip: check Kaggle auth and try another dataset.")

    
   

    # Quick HF import (optional direct load)
    if src == "Hugging Face":
        st.markdown("#### Or load directly by repo id")
        hf_repo = st.text_input("ü§ó Dataset repo (e.g. uciml/real-estate-valuation)", value="uciml/real-estate-valuation", key="asset_hf_repo")
        if st.button("üì• Load from HF", key="btn_asset_hf_load", use_container_width=True):
            try:
                from datasets import load_dataset
                ds = load_dataset(hf_repo)
                split = next(iter(ds.keys()))
                df_imp = ds[split].to_pandas()
                ss["asset_intake_df"] = df_imp
                uni_fp = os.path.join(RUNS_DIR, f"intake_table.{_ts()}.csv")
                df_imp.to_csv(uni_fp, index=False)
                st.success(f"‚úÖ Loaded {len(df_imp):,} rows from {hf_repo} (split: {split})")
                st.caption(f"Saved unified intake copy: `{uni_fp}`")
                st.dataframe(df_imp.head(100), use_container_width=True)
            except Exception as e:
                st.error(f"HF load failed: {e}")

    st.divider()
    
    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # (A.3) NORMALIZE & GENERATE UNIFIED CSV
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üßπ Normalize & Combine All Inputs")

    # ---------- helpers ----------
    import io, csv, re
    from pathlib import Path

    def _slug(name: str) -> str:
        return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

    def _read_any_table(uploaded_file) -> pd.DataFrame:
        """Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback."""
        name = uploaded_file.name.lower()

        # Excel
        if name.endswith((".xlsx", ".xls")):
            return pd.read_excel(uploaded_file)

        # Text (CSV/TSV/TXT): try utf-8 then latin-1; sniff delimiter
        raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
        for enc in ("utf-8", "latin-1"):
            try:
                text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
                head = "\n".join(text.splitlines()[:10]) or text
                try:
                    dialect = csv.Sniffer().sniff(head)
                    sep = dialect.delimiter
                except Exception:
                    sep = ","
                return pd.read_csv(io.StringIO(text), sep=sep)
            except Exception:
                continue
        # last resort (python engine)
        return pd.read_csv(io.BytesIO(raw), engine="python")

    # ---------- optional upload right here ----------
    st.markdown("#### ‚¨ÜÔ∏è Optional: Upload a CSV/TSV/TXT/XLSX to normalize")
    uploaded = st.file_uploader(
        "Upload a dataset file (or skip if you already imported via Kaggle/HF).",
        type=["csv", "tsv", "txt", "xlsx"],
        key="norm_upload_once",
        accept_multiple_files=False
    )

    if uploaded is not None:
        try:
            df_up = _read_any_table(uploaded)
            ss["asset_intake_df"] = df_up
            ss["last_dataset_name"] = Path(uploaded.name).stem  # remember original name
            st.success(f"‚úÖ Loaded {len(df_up):,} rows from **{uploaded.name}**")
            st.dataframe(df_up.head(100), use_container_width=True, hide_index=True)
        except Exception as e:
            st.error(f"Could not read file: {e}")

    # ---------- normalization source ----------
    df_src = ss.get("asset_intake_df")
    # Best-effort name priority: last_dataset_name (Kaggle/HF/upload) ‚Üí fallback
    base_name = _slug(ss.get("last_dataset_name") or ss.get("asset_intake_source_name") or "dataset")

    if df_src is None or len(df_src) == 0:
        st.info("Upload/import a dataset first (Kaggle/HF/Upload), then come back to normalize.")
    else:
        with st.expander("‚öôÔ∏è Normalization options", expanded=False):
            drop_dupes = st.checkbox("Drop duplicate rows", value=True)
            trim_whitespace = st.checkbox("Trim whitespace in string columns", value=True)
            lower_columns = st.checkbox("Lowercase column names", value=True)

        def _normalize(df: pd.DataFrame) -> pd.DataFrame:
            out = df.copy()
            # 1) basic cleanup
            if lower_columns:
                out.columns = [c.strip().lower() for c in out.columns]
            if trim_whitespace:
                for c in out.select_dtypes(include=["object"]).columns:
                    out[c] = out[c].astype(str).str.strip()
            if drop_dupes:
                out = out.drop_duplicates().reset_index(drop=True)
            # (Optional) add any schema harmonization here later
            return out

        if st.button("üß™ Normalize & Generate Unified CSV", key="btn_normalize", use_container_width=True):
            norm_df = _normalize(df_src)

            # Ensure output dir
            norm_dir = os.path.join(RUNS_DIR, "normalized")
            os.makedirs(norm_dir, exist_ok=True)

            # Build file name: <original>-Normalized.csv   (slug-safe base_name)
            norm_name = f"{base_name}-Normalized.csv"
            norm_path = os.path.join(norm_dir, norm_name)

            # Save to disk with utf-8-sig (friendlier for Excel)
            norm_df.to_csv(norm_path, index=False, encoding="utf-8-sig")

            # Prepare one bytes blob for both download buttons
            _norm_bytes = norm_df.to_csv(index=False).encode("utf-8-sig")
            _rows, _cols = len(norm_df), len(norm_df.columns)
            _size_kb = max(1, int(len(_norm_bytes) / 1024))
            _norm_file_only = Path(norm_path).name

            # Sticky banner: filename ‚Ä¢ rows√ócols ‚Ä¢ size
            st.markdown(
                f"""
                <div style="
                    position: sticky;
                    top: 64px;
                    z-index: 50;
                    background: rgba(16,185,129,0.10);
                    border: 1px solid #10b981;
                    padding: 12px 16px;
                    border-radius: 12px;
                    margin: 8px 0 14px 0;
                ">
                <b>‚úÖ Normalized CSV:</b> <code>{_norm_file_only}</code>
                &nbsp;‚Ä¢&nbsp; {_rows:,} rows √ó {_cols} cols &nbsp;‚Ä¢&nbsp; {_size_kb} KB
                </div>
                """,
                unsafe_allow_html=True
            )

            # BIG centered primary download button (TOP)
            cL, cM, cR = st.columns([1, 2.5, 1])
            with cM:
                st.download_button(
                    "‚¨áÔ∏è  Download Normalized CSV",
                    data=_norm_bytes,
                    file_name=_norm_file_only,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_norm_top"
                )

            # Copyable saved path
            st.text_input("Saved to (server path)", norm_path, disabled=True, label_visibility="collapsed")

            # Preview table
            st.dataframe(norm_df.head(100), use_container_width=True, hide_index=True)

            # BIG centered primary download button (BOTTOM)
            cL2, cM2, cR2 = st.columns([1, 2.5, 1])
            with cM2:
                st.download_button(
                    "‚¨áÔ∏è  Download Normalized CSV",
                    data=_norm_bytes,
                    file_name=_norm_file_only,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_norm_bottom"
                )

    # # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # # (A.1b) QUICK START ‚Äî Generate Synthetic Data (always visible here)
    # # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # st.markdown("### üé≤ Quick Start: Generate Synthetic Data")
    # c_syn1, c_syn2 = st.columns([2, 1])
    # with c_syn1:
    #     nrows = st.slider("Number of synthetic rows", 20, 1000, 150, step=10, key="slider_synth_rows_A_quick")
    # with c_syn2:
    #     if st.button("üöÄ Generate Synthetic Dataset Now", key="btn_generate_synth_A_quick", use_container_width=True):
    #         try:
    #             df_synth = quick_synth(nrows)
    #             ss["asset_intake_df"] = df_synth
    #             os.makedirs("./.tmp_runs", exist_ok=True)
    #             synth_path = f"./.tmp_runs/intake_table_synth_{_ts()}.csv"
    #             df_synth.to_csv(synth_path, index=False, encoding="utf-8-sig")
    #             st.success(f"‚úÖ Synthetic dataset created ({len(df_synth)} rows). Saved: `{synth_path}`")
    #             st.dataframe(df_synth.head(20), use_container_width=True, hide_index=True)
    #             st.download_button(
    #                 "‚¨áÔ∏è Download Synthetic CSV",
    #                 df_synth.to_csv(index=False).encode("utf-8-sig"),
    #                 file_name="synthetic_intake.csv",
    #                 mime="text/csv",
    #                 key="dl_synth_A_quick"
    #             )
    #         except Exception as e:
    #             st.error(f"Synthetic generation failed: {e}")

       
        
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # (A.4) SYNTHETIC DATA GENERATION
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### ü§ñ Generate Synthetic Data (Fallback)")
    nrows = st.slider("Number of synthetic rows", 10, 500, 150, step=10, key="slider_synth_rows")
    if st.button("üé≤ Generate Synthetic Dataset", key="btn_generate_synth"):
        try:
            df_synth = quick_synth(nrows)
            ss["asset_intake_df"] = df_synth
            os.makedirs("./.tmp_runs", exist_ok=True)
            synth_path = f"./.tmp_runs/intake_table_synth_{datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')}.csv"
            df_synth.to_csv(synth_path, index=False)
            st.success(f"‚úÖ Synthetic dataset created ({len(df_synth)} rows).")
            st.dataframe(df_synth.head(20), use_container_width=True)
            st.download_button("üíæ Download Synthetic CSV", df_synth.to_csv(index=False), "synthetic_intake.csv", "text/csv")
        except Exception as e:
            st.error(f"Synthetic generation failed: {e}")





# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# B ‚Äî PRIVACY & FEATURES (2..3)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tabB:
    st.subheader("B. Privacy & Features")
    st.caption("Steps: **2) Anonymize**, **3) Feature Engineering + Comps**")

    import re, math, json, os, time
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    
    # ----------------------------
    # B.2 ‚Äî Anonymize / Sanitize PII
    # ----------------------------
    st.markdown("### **2) Anonymize / Sanitize PII**")

    import io, csv, re, os
    from pathlib import Path

    ss = st.session_state  # make sure this exists globally

    def _slug(name: str) -> str:
        return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

    def _read_any_table(uploaded_file) -> pd.DataFrame:
        """Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback."""
        name = uploaded_file.name.lower()
        if name.endswith((".xlsx", ".xls")):
            return pd.read_excel(uploaded_file)

        raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
        for enc in ("utf-8", "latin-1"):
            try:
                text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
                head = "\n".join(text.splitlines()[:10]) or text
                try:
                    sep = csv.Sniffer().sniff(head).delimiter
                except Exception:
                    sep = ","
                return pd.read_csv(io.StringIO(text), sep=sep)
            except Exception:
                continue
        return pd.read_csv(io.BytesIO(raw), engine="python")

    def _anonymize(df: pd.DataFrame) -> pd.DataFrame:
        """Mask likely-PII text while preserving common join keys."""
        if df is None or df.empty:
            return df
        out = df.copy()
        join_keys = {"loan_id", "asset_id", "application_id"}
        pii_like = re.compile(r"(name|email|phone|addr|address|national|passport|id|nid)", re.I)

        for col in out.columns:
            if col in join_keys:
                continue
            if out[col].dtype == "object" and pii_like.search(col):
                s = out[col].astype(str)
                s = s.str.replace(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+", "[EMAIL]", regex=True)
                s = s.str.replace(r"\b(\+?\d{1,3}[-.\s]?)?\d{7,}\b", "[PHONE]", regex=True)
                s = s.str.replace(r"\b\d{9,16}\b", "[ID]", regex=True)
                out[col] = s
        return out.drop_duplicates().reset_index(drop=True)

    # ---------- Source picker (Stage A or Upload here) ----------
    st.markdown("#### üîÑ Choose data source")
    src_choice = st.radio(
        "Pick how you want to provide data:",
        ["Use data from Stage A", "Upload a new file here"],
        horizontal=True,
        key="b_src_choice"
    )

    df_src, base_name = None, "dataset"

    if src_choice == "Use data from Stage A":
        df_src = ss.get("asset_intake_df")
        if isinstance(df_src, pd.DataFrame) and not df_src.empty:
            base_name = _slug(ss.get("last_dataset_name") or "dataset")
            st.success(f"Using Stage-A dataset: **{base_name}** ‚Ä¢ {len(df_src):,} rows")
            st.dataframe(df_src.head(10), use_container_width=True, hide_index=True)
        else:
            st.warning("No data found from Stage A. Upload below instead.")
    else:
        st.markdown(
            """
            <div style="border:2px dashed #22c55e;padding:14px;border-radius:12px;
                        background:rgba(34,197,94,0.06);margin:6px 0 12px 0;">
            <b>‚¨ÜÔ∏è Upload CSV/TSV/TXT/XLSX for anonymization</b>
            </div>
            """,
            unsafe_allow_html=True,
        )
        up = st.file_uploader(
            "Upload a dataset file",
            type=["csv","tsv","txt","xlsx"],
            key="b_upload",
            accept_multiple_files=False,
        )
        if up is not None:
            try:
                df_src = _read_any_table(up)
                base_name = _slug(Path(up.name).stem)
                ss["asset_intake_df"] = df_src
                ss["last_dataset_name"] = base_name
                st.success(f"‚úÖ Loaded {len(df_src):,} rows from **{up.name}**")
                st.dataframe(df_src.head(20), use_container_width=True, hide_index=True)
            except Exception as e:
                st.error(f"Could not read file: {e}")

    if not (isinstance(df_src, pd.DataFrame) and not df_src.empty):
        st.info("Provide data (via Stage A or upload here) to enable anonymization.")
    else:
        if st.button("üõ°Ô∏è Run Anonymization & Export CSV", type="primary", use_container_width=True, key="btn_b_anon"):
            anon_df = _anonymize(df_src)
            out_dir = os.path.join(RUNS_DIR, "normalized")
            os.makedirs(out_dir, exist_ok=True)
            out_name = f"{base_name}-Anonymized.csv"
            out_path = os.path.join(out_dir, out_name)
            anon_df.to_csv(out_path, index=False, encoding="utf-8-sig")
            ss["asset_anon_df"] = anon_df

            _bytes = anon_df.to_csv(index=False).encode("utf-8-sig")

            st.markdown(
                f"""
                <div style="position:sticky;top:64px;z-index:50;background:rgba(59,130,246,0.10);
                            border:1px solid #3b82f6;padding:12px 16px;border-radius:12px;margin:8px 0 14px 0;">
                <b>‚úÖ Anonymized CSV:</b> <code>{out_name}</code> ‚Ä¢ {len(anon_df):,} rows √ó {len(anon_df.columns)} cols
                </div>
                """,
                unsafe_allow_html=True,
            )

            cL, cM, cR = st.columns([1, 2.6, 1])
            with cM:
                st.download_button(
                    "‚¨áÔ∏è  Download Anonymized CSV",
                    data=_bytes,
                    file_name=out_name,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_b_anon_top",
                )

            st.text_input("Saved to (server path)", out_path, disabled=True, label_visibility="collapsed")
            st.dataframe(anon_df.head(100), use_container_width=True, hide_index=True)

            cL2, cM2, cR2 = st.columns([1, 2.6, 1])
            with cM2:
                st.download_button(
                    "‚¨áÔ∏è  Download Anonymized CSV",
                    data=_bytes,
                    file_name=out_name,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_b_anon_bottom",
                )

    st.markdown("---")

    
    

    
    # ----------------------------
    # B.3 ‚Äî Feature Engineering & Comps
    # ----------------------------
    st.markdown("### **3) Feature Engineering & Comps**")

    def feature_engineer(df: pd.DataFrame, evidence_index=None) -> pd.DataFrame:
        """
        Light feature engineering (safe):
        - Ensure city/lat/lon/age_years/delinquencies/current_loans
        - Coerce lat/lon including "10,762" ‚Üí 10.762
        - Create stable 'geohash' (lat,lon preferred; fallback short city hash)
        - Derive condition_score (0..1) heuristically if inputs exist
        - Ensure legal_penalty numeric
        - Keep join keys up front if present
        """
        if not isinstance(df, pd.DataFrame) or df.empty:
            return pd.DataFrame()

        out = df.copy()

        # Ensure expected columns exist
        for c in ("city", "lat", "lon", "age_years", "delinquencies", "current_loans"):
            if c not in out.columns:
                out[c] = pd.NA

        # Coerce lat/lon
        for c in ("lat", "lon"):
            if out[c].dtype == "object":
                out[c] = out[c].astype(str).str.replace(",", ".", regex=False)
            out[c] = pd.to_numeric(out[c], errors="coerce")

        # Row-wise geokey: prefer lat/lon else short md5(city)
        import hashlib
        def _row_geokey(row) -> str:
            lat = row.get("lat")
            lon = row.get("lon")
            if pd.notna(lat) and pd.notna(lon):
                return f"{float(lat):.3f},{float(lon):.3f}"
            city_val = row.get("city")
            city_txt = "" if pd.isna(city_val) else str(city_val)
            return hashlib.md5(city_txt.encode("utf-8")).hexdigest()[:7]

        out["geohash"] = out.apply(_row_geokey, axis=1).astype(str)

        # Heuristic condition_score (0..1)
        age = pd.to_numeric(out.get("age_years"), errors="coerce").fillna(0.0)
        delinq = pd.to_numeric(out.get("delinquencies"), errors="coerce").fillna(0.0)
        curr_loans = pd.to_numeric(out.get("current_loans"), errors="coerce").fillna(0.0)
        cond = 1.0 - (0.02 * age) - (0.05 * delinq) - (0.03 * curr_loans)
        out["condition_score"] = pd.Series(cond, index=out.index).clip(0.10, 0.98)

        # legal_penalty safe numeric
        if "legal_penalty" not in out.columns:
            out["legal_penalty"] = 0.0
        else:
            out["legal_penalty"] = pd.to_numeric(out["legal_penalty"], errors="coerce").fillna(0.0)

        # Keep join keys in front
        front_cols = [c for c in ["loan_id", "application_id", "asset_id"] if c in out.columns]
        other_cols = [c for c in out.columns if c not in front_cols]
        out = out[front_cols + other_cols]

        return out


    def _fmt_mean(df, col, fmt="{:.2f}"):
        if isinstance(df, pd.DataFrame) and col in df.columns:
            v = pd.to_numeric(df[col], errors="coerce").mean()
            if pd.notna(v):
                return fmt.format(v)
        return "‚Äî"


    def fetch_and_clean_comps(df_feats: pd.DataFrame) -> dict:
        """Deterministic stub; replace with real comps feed."""
        mv = pd.to_numeric(df_feats.get("market_value", pd.Series(dtype=float)), errors="coerce")
        base = float(mv.median()) if mv.notna().any() else 100000.0
        comps = [{"comp_id": f"C-{i+1:03d}", "price": round(base * (0.95 + 0.02 * i), 2)} for i in range(5)]
        return {"used": comps, "count": len(comps), "median_baseline": base}


    if not isinstance(ss.get("asset_anon_df"), pd.DataFrame) or ss["asset_anon_df"].empty:
        st.info("Run **Anonymization (B.2)** first to prepare inputs for features.")
    else:
        c3a, c3b = st.columns([1.2, 0.8])

        with c3a:
            if st.button("Build Features & Fetch Comps", key="btn_build_features", use_container_width=True):
                # Build features in this rerun and persist
                feats = feature_engineer(ss["asset_anon_df"], ss.get("asset_evidence_index"))
                ss["asset_features_df"] = feats

                # Metrics (from feats)
                m1, m2, m3 = st.columns(3)
                with m1:
                    st.metric("Avg condition_score", _fmt_mean(feats, "condition_score"))
                with m2:
                    st.metric("Avg market_value", _fmt_mean(feats, "market_value", "{:,.0f}"))
                with m3:
                    st.metric("Avg loan_amount", _fmt_mean(feats, "loan_amount", "{:,.0f}"))

                # Persist features.parquet (BytesIO for download)
                features_path = os.path.join(RUNS_DIR, f"features.{_ts()}.parquet")
                feats.to_parquet(features_path, index=False)
                st.success(f"Saved features ‚Üí `{features_path}`")

                import io as _io
                _buf = _io.BytesIO()
                feats.to_parquet(_buf, index=False)
                st.download_button(
                    "‚¨áÔ∏è Download features.parquet",
                    data=_buf.getvalue(),
                    file_name="features.parquet",
                    mime="application/octet-stream",
                    key="dl_features_parquet",
                    use_container_width=True
                )

                # Comps (and persist)
                comps = fetch_and_clean_comps(feats)
                ss["asset_comps_used"] = comps
                comps_path = os.path.join(RUNS_DIR, f"comps_used.{_ts()}.json")
                with open(comps_path, "w", encoding="utf-8") as fp:
                    json.dump(comps, fp, ensure_ascii=False, indent=2)
                st.success(f"Saved comps ‚Üí `{comps_path}`")

        with c3b:
            # Show last built features (if any) and offer a second download
            df_feats = ss.get("asset_features_df")
            if isinstance(df_feats, pd.DataFrame) and not df_feats.empty:
                import io as _io2
                _buf2 = _io2.BytesIO()
                df_feats.to_parquet(_buf2, index=False)
                st.download_button(
                    "‚¨áÔ∏è Download last features.parquet",
                    data=_buf2.getvalue(),
                    file_name="features.parquet",
                    mime="application/octet-stream",
                    key="dl_features_parquet_last",
                    use_container_width=True
                )

    # Outside the columns: show current features + comps if present
    df_feats = ss.get("asset_features_df")
    if isinstance(df_feats, pd.DataFrame) and not df_feats.empty:
        k1, k2, k3 = st.columns(3)
        with k1:
            st.metric("Rows (features)", f"{len(df_feats):,}")
        with k2:
            st.metric("Avg condition_score", _fmt_mean(df_feats, "condition_score"))
        with k3:
            # evidence_count column is optional; show 0 if absent
            ev = pd.to_numeric(df_feats.get("evidence_count", pd.Series([0]*len(df_feats))), errors="coerce").fillna(0)
            st.metric("Evidence count (stub)", int(ev.mean()))

        st.dataframe(df_feats.head(30), use_container_width=True)

    if ss.get("asset_comps_used") is not None:
        st.caption("Comps used (stub)")
        st.json(ss["asset_comps_used"])

    


# ========== 3) AI APPRAISAL & VALUATION ==========
with tabC:
    st.subheader("ü§ñ Stage 3 ‚Äî AI Appraisal & Valuation")

    import os, json, numpy as np, requests, pandas as pd, plotly.express as px
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß≠ HOW TO USE THIS STAGE
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("""
    ### üß≠ How to Use This Stage
    1. **Select a model** ‚Äî choose between production, trained, or open-source (Hugging Face) models.  
    2. **Check hardware** ‚Äî confirm your GPU/CPU profile supports the chosen model.  
    3. **Select dataset** ‚Äî use Stage 2 outputs (Features / Anonymized) or fallback synthetic data.  
    4. **Run appraisal** ‚Äî compute AI-based valuation (`fmv`, `ai_adjusted`, `confidence`, `why`).  
    5. **Review outputs** ‚Äî compare customer vs AI results, run projections, dashboards, and reports.  
    6. **Verify ownership** ‚Äî perform Legal / Encumbrance checks (C.5).  
    """)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß† MODEL FAMILY TABLE
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üß† Model Families & Recommended Use-Cases")

    model_ref = pd.DataFrame([
        {"Category": "Local / Trained", "Model": "LightGBM / XGBoost / CatBoost",
         "Use Case": "Numeric ‚Üí FMV prediction", "GPU": "CPU OK",
         "Notes": "Fast, explainable baseline model"},
        {"Category": "Production (‚≠ê)", "Model": "asset_lgbm-v1 / credit_lr",
         "Use Case": "Enterprise-grade deployed valuation", "GPU": "CPU OK",
         "Notes": "Stable, low-latency predictions"},
        {"Category": "LLM (HF)", "Model": "Mistral 7B / Gemma 2 9B",
         "Use Case": "Text reasoning + narratives", "GPU": "‚â• 8 GB",
         "Notes": "Fast reasoning for appraisal explanations"},
        {"Category": "LLM (HF)", "Model": "LLaMA 3 8B / Qwen 2 7B",
         "Use Case": "Multilingual valuation reports", "GPU": "‚â• 12 GB",
         "Notes": "Strong contextual generation"},
        {"Category": "LLM (HF)", "Model": "Mixtral 8√ó7B",
         "Use Case": "High-end MoE valuation", "GPU": "‚â• 24 GB",
         "Notes": "Premium precision for portfolios"},
    ])
    st.dataframe(model_ref, use_container_width=True)
    st.markdown("---")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üü¢ PRODUCTION MODEL BANNER
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.status_code == 200:
            meta = resp.json()
            if meta.get("has_production"):
                ver = (meta.get("meta") or {}).get("version", "1.x")
                src = (meta.get("meta") or {}).get("source", "production")
                st.success(f"üü¢ Production model active ‚Äî version {ver} ‚Ä¢ source {src}")
            else:
                st.warning("‚ö†Ô∏è No production model promoted yet ‚Äî using baseline.")
        else:
            st.info("‚ÑπÔ∏è Could not fetch production model meta.")
    except Exception:
        st.info("‚ÑπÔ∏è Production meta unavailable.")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß© Model Selection (list all trained models) ‚Äî HARD-CODED TEST
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    from datetime import datetime
    import os, shutil, streamlit as st

    # Hardcoded absolute paths for your environment
    trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/trained"
    production_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/production"

    # Debug info
    st.caption(f"üìÇ Trained dir: `{trained_dir}`")
    st.caption(f"üì¶ Production dir: `{production_dir}`")

    # Refresh button ‚Äî use unique key for asset
    if st.button("‚Üª Refresh models", key="asset_refresh_models"):
        st.session_state.pop("asset_selected_model", None)
        st.rerun()

    models = []
    if os.path.isdir(trained_dir):
        for f in os.listdir(trained_dir):
            if f.endswith(".joblib"):
                fpath = os.path.join(trained_dir, f)
                ctime = os.path.getctime(fpath)
                created = datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
                models.append((f, fpath, created))
    else:
        st.error(f"‚ùå Trained dir not found: {trained_dir}")

    if models:
        # Sort by creation time (latest first)
        models.sort(key=lambda x: os.path.getctime(x[1]), reverse=True)
        display_names = [f"{m[0]} ‚Äî {m[2]}" for m in models]

        selected_display = st.selectbox("üì¶ Select trained model to use", display_names, key="asset_model_select")
        selected_model = models[display_names.index(selected_display)][1]
        st.success(f"‚úÖ Using model: {os.path.basename(selected_model)}")

        st.session_state["asset_selected_model"] = selected_model

        # Promote to production
        if st.button("üöÄ Promote this model to Production", key="asset_promote_button"):
            try:
                os.makedirs(production_dir, exist_ok=True)
                prod_path = os.path.join(production_dir, "model.joblib")
                shutil.copy2(selected_model, prod_path)
                st.balloons()
                st.success(f"‚úÖ Model promoted to production: {prod_path}")
            except Exception as e:
                st.error(f"‚ùå Promotion failed: {e}")
    else:
        st.warning("‚ö†Ô∏è No trained models found ‚Äî train one in Stage F first.")


    

    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üß† LLM + HARDWARE PROFILE (LOCAL + HUGGING FACE)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üß† LLM & Hardware Profile (Local + Hugging Face Models)")

    HF_MODELS = [
        {"Model": "mistralai/Mistral-7B-Instruct-v0.3",
         "Type": "Reasoning / valuation narrative",
         "GPU": "‚â• 8 GB", "Notes": "Fast multilingual contextual LLM"},
        {"Model": "google/gemma-2-9b-it",
         "Type": "Instruction-tuned financial reports",
         "GPU": "‚â• 12 GB", "Notes": "Great for valuation explanations"},
        {"Model": "meta-llama/Meta-Llama-3-8B-Instruct",
         "Type": "Valuation summarization",
         "GPU": "‚â• 12 GB", "Notes": "High accuracy + low hallucination"},
        {"Model": "Qwen/Qwen2-7B-Instruct",
         "Type": "Multilingual reasoning (VN + EN)",
         "GPU": "‚â• 12 GB", "Notes": "Excellent for VN asset appraisal"},
        {"Model": "microsoft/Phi-3-mini-4k-instruct",
         "Type": "Compact instruction LLM",
         "GPU": "‚â§ 8 GB", "Notes": "Fast lightweight valuation logic"},
        {"Model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
         "Type": "MoE premium reasoning",
         "GPU": "‚â• 24 GB", "Notes": "Top-tier valuation model"},
        {"Model": "LightAutoML/LightGBM",
         "Type": "Tabular regression baseline",
         "GPU": "CPU OK", "Notes": "Numeric FMV baseline"},
    ]
    st.dataframe(pd.DataFrame(HF_MODELS), use_container_width=True)

    LLM_MODELS = [
        {"label": "CPU Recommended ‚Äî Phi-3 Mini (3.8B)", "value": "phi3:3.8b", "hint": "CPU 8 GB RAM (fast)", "tier": "cpu"},
        {"label": "CPU Recommended ‚Äî Mistral 7B Instruct", "value": "mistral:7b-instruct", "hint": "GPU ‚â• 8 GB (fast)", "tier": "balanced"},
        {"label": "GPU Recommended ‚Äî Gemma-2 9B", "value": "gemma2:9b", "hint": "GPU ‚â• 12 GB (high accuracy)", "tier": "gpu"},
        {"label": "GPU Recommended ‚Äî LLaMA-3 8B", "value": "llama3:8b-instruct", "hint": "GPU ‚â• 12 GB (context heavy)", "tier": "gpu"},
        {"label": "GPU Recommended ‚Äî Qwen-2 7B", "value": "qwen2:7b-instruct", "hint": "GPU ‚â• 12 GB (multilingual)", "tier": "gpu"},
        {"label": "GPU Heavy ‚Äî Mixtral 8√ó7B", "value": "mixtral:8x7b-instruct", "hint": "GPU 24-48 GB (batch)", "tier": "gpu_large"},
    ]

    cpu_first = [m for m in LLM_MODELS if m["tier"] in {"cpu", "balanced"}]
    gpu_first = [m for m in LLM_MODELS if m["tier"] not in {"cpu", "balanced"}]
    ordered_models = cpu_first + gpu_first

    LLM_LABELS = [m["label"] for m in ordered_models]
    LLM_VALUE_BY_LABEL = {m["label"]: m["value"] for m in ordered_models}
    LLM_HINT_BY_LABEL  = {m["label"]: m["hint"] for m in ordered_models}

    OPENSTACK_FLAVORS = {
        "m4.medium": "4 vCPU / 8 GB RAM (CPU-only small)",
        "m8.large": "8 vCPU / 16 GB RAM (CPU-only medium)",
        "g1.a10.1": "8 vCPU / 32 GB RAM + 1√óA10 24 GB",
        "g1.l40.1": "16 vCPU / 64 GB RAM + 1√óL40 48 GB",
        "g2.a100.1": "24 vCPU / 128 GB RAM + 1√óA100 80 GB",
    }

    with st.expander("üß† Choose Model & Hardware Profile", expanded=True):
        st.info("CPU picks land first so you can generate valuation narratives without waiting on GPUs. Jump to the GPU section only if you need deeper reasoning or longer context windows.", icon="‚öôÔ∏è")
        c1, c2 = st.columns([1.2, 1])
        with c1:
            model_label = st.selectbox(
                "üî• Local/HF LLM for narratives & explanations",
                LLM_LABELS, index=0, key="asset_llm_label")
            llm_value = LLM_VALUE_BY_LABEL[model_label]
            use_llm = st.checkbox("Use LLM narrative (explanations)",
                                  value=False, key="asset_use_llm")
            st.caption(f"Hint: {LLM_HINT_BY_LABEL[model_label]}")
        with c2:
            flavor = st.selectbox("OpenStack flavor / host profile",
                                  list(OPENSTACK_FLAVORS.keys()), index=0,
                                  key="asset_flavor")
            st.caption(OPENSTACK_FLAVORS[flavor])
        st.caption("These parameters are passed to backend (Ollama / Flowise / RunAI).")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # GPU PROFILE AND DATASET SOURCE (keep existing logic)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### **C.4 ‚Äî Valuation (AI)**")
    gpu_profile = st.selectbox(
        "GPU Profile (for valuation compute)",
        ["CPU (slow)", "GPU: 1√óA100", "GPU: 1√óH100", "GPU: 2√óL40S"],
        index=1, key="asset_gpu_profile_c4")
    ss["asset_gpu_profile"] = gpu_profile

    # Gather candidates from prior stages (they may be None/empty)
    cand_features = ss.get("asset_features_df")
    cand_anon     = ss.get("asset_anon_df")
    cand_intake   = ss.get("asset_intake_df")

    src = st.selectbox(
        "Data source for AI run",
        [
            "Use FEATURES (Stage 2/3)",
            "Use ANON (Stage 2)",
            "Use RAW ‚Üí auto-sanitize",
            "Use synthetic (fallback)",
        ],
        key="asset_c4_source"
    )

    # Decide df2 explicitly based on choice
    df2 = None
    if src == "Use FEATURES (Stage 2/3)":
        # First non-empty among features ‚Üí anon ‚Üí intake
        df2 = first_nonempty_df(cand_features, cand_anon, cand_intake)

    elif src == "Use ANON (Stage 2)":
        df2 = cand_anon

    elif src == "Use RAW ‚Üí auto-sanitize":
        # If intake exists, sanitize; else leave None
        df2 = anonymize_text_cols(cand_intake) if isinstance(cand_intake, pd.DataFrame) and not cand_intake.empty else None

    else:  # "Use synthetic (fallback)"
        df2 = quick_synth(150)

    # Final safety check
    if not isinstance(df2, pd.DataFrame) or df2.empty:
        st.warning("No usable dataset found. Please complete Stage A (Intake) and Stage B (Privacy/Features), or choose the synthetic fallback.")
        st.stop()

    # Preview selected data
    st.dataframe(df2.head(10), use_container_width=True)



    # Probe API (health & agents)
    with st.expander("üîé Probe API (health & agents)", expanded=False):
        if st.button("Run probe now", key="btn_probe_api"):
            diag = probe_api()
            st.json(diag)

    # Run model button (runtime flavor + gpu_profile included)
    if st.button("üöÄ Run AI Appraisal now", key="btn_run_ai"):
        csv_bytes = df2.to_csv(index=False).encode("utf-8")

        form_fields = {
            "use_llm": str(use_llm).lower(),
            "llm": llm_value,
            "flavor": flavor,
            "gpu_profile": gpu_profile,  # NEW: pass GPU profile to backend
            "selected_model": ss.get("asset_selected_model", ""),
            "agent_name": "asset_appraisal",
        }

        with st.spinner("Calling asset agent‚Ä¶"):
            ok, result = try_run_asset_agent(csv_bytes, form_fields=form_fields, timeout_sec=180)

        if not ok:
            st.error("‚ùå Model API error.")
            st.info("Tip: open 'üîé Probe API' above to see health and discovered agent ids.")
            st.code(str(result)[:8000])
            st.stop()

        df_app = result.copy()

        # Ensure core valuation columns per blueprint
        if "ai_adjusted" not in df_app.columns and "market_value" in df_app.columns:
            df_app["ai_adjusted"] = df_app["market_value"]
        if "fmv" not in df_app.columns:
            # heuristics: if model returns fmv, keep; else set fmv ~ ai_adjusted
            df_app["fmv"] = pd.to_numeric(df_app.get("ai_adjusted", np.nan), errors="coerce")
        if "confidence" not in df_app.columns:
            df_app["confidence"] = 80.0
        if "why" not in df_app.columns:
            df_app["why"] = ["Condition, comps, and features (placeholder)"] * len(df_app)

        # Persist valuation artifact
        val_path = os.path.join(RUNS_DIR, f"valuation_ai.{_ts()}.csv")
        df_app.to_csv(val_path, index=False)
        
        st.success(f"Saved valuation artifact ‚Üí `{val_path}`")

        # # ‚úÖ PATCH: Save Stage C valuation table for Stage H (use df_app, not ai_df)
        # try:
        #     st.session_state["asset_ai_df"] = df_app.copy()
        #     ss["asset_ai_df"] = df_app.copy()
        #     st.info("‚úÖ Stage C valuation stored for Stage D / E / H.")
        # except Exception as e:
        #     st.warning(f"Could not store Stage C output: {e}")

        # st.success(f"Saved valuation artifact ‚Üí `{val_path}`")
        # # ‚úÖ PATCH: Save Stage C valuation table for Stage H
        # try:
        #     st.session_state["asset_ai_df"] = ai_df.copy()
        #     st.info("‚úÖ Stage C results stored for Stage H portfolio view.")
        # except Exception as e:
        #     st.warning(f"Could not store Stage C output: {e}")


        # Keep table for downstream steps
        ss["asset_ai_df"] = df_app

        # Display minimal KPIs
        k1, k2, k3 = st.columns(3)
        try:
            k1.metric("Avg FMV", f"{pd.to_numeric(df_app['fmv'], errors='coerce').mean():,.0f}")
        except Exception:
            k1.metric("Avg FMV", "‚Äî")
        try:
            k2.metric("Avg Confidence", f"{pd.to_numeric(df_app['confidence'], errors='coerce').mean():.2f}")
        except Exception:
            k2.metric("Avg Confidence", "‚Äî")
        k3.metric("Rows", len(df_app))

        st.markdown("### üßæ Valuation Output (preview)")
        cols_first = [c for c in [
            "application_id","asset_id","asset_type","city",
            "fmv","ai_adjusted","confidence","why"
        ] if c in df_app.columns]
        st.dataframe(df_app[cols_first].head(50), use_container_width=True)

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # Customer vs AI ‚Äî Details & 5-Year Deltas
        # (Place this right after the valuation preview table)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        st.markdown("### üìã Customer & Loan Details (Declared) + AI Alignment")

        import numpy as np
        from datetime import datetime, timezone
        import os

        RUNS_DIR = "./.tmp_runs"
        os.makedirs(RUNS_DIR, exist_ok=True)
        _ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

        
        # ‚úÖ Save Stage C output for Stage H (single source of truth)
        st.session_state["asset_ai_df"] = df_app.copy()
        ss["asset_ai_df"] = df_app.copy()
        st.info("‚úÖ Stage C valuation stored for Stage D / E / H.")

        # ‚úÖ Use the saved valuation table
        ai_df = st.session_state["asset_ai_df"]

        # ‚úÖ Merge intake (customer-declared) if available
        intake_df = ss.get("asset_intake_df")
        if intake_df is not None and not intake_df.empty:
            join_keys = [
                k for k in ["application_id", "asset_id"]
                if k in ai_df.columns and k in intake_df.columns
            ]
            if join_keys:
                merged = intake_df.merge(
                    ai_df, on=join_keys,
                    suffixes=("_cust", "_ai"),
                    how="left"
                )
            else:
                merged = ai_df.copy()
        else:
            merged = ai_df.copy()

        # Optional: show preview table
        st.markdown("### üîç Stage C Output Preview")
        st.dataframe(merged, use_container_width=True)

        
        # # ‚úÖ Save Stage C output for Stage H (using df_app, not undefined ai_df)
        # st.session_state["asset_ai_df"] = df_app.copy()
        # #ai_df = df_app.copy()
        # ai_df = st.session_state["asset_ai_df"]
        

        

        # ‚úÖ Merge intake (customer-declared) if available
        intake_df = ss.get("asset_intake_df")
        if intake_df is not None and not intake_df.empty:
            # Choose join keys available in both frames
            join_keys = [
                k for k in ["application_id", "asset_id"]
                if k in ai_df.columns and k in intake_df.columns
            ]
            if join_keys:
                merged = intake_df.merge(
                    ai_df, on=join_keys,
                    suffixes=("_cust", "_ai"),
                    how="left"
                )
            else:
                merged = ai_df.copy()
        else:
            merged = ai_df.copy()

        
        # # ‚úÖ Save Stage C output for Stage H
        # #st.session_state["asset_ai_df"] = ai_df.copy()
        # ai_df = ss.get("asset_ai_df")
        # if ai_df is None or len(ai_df) == 0:
        #     st.info("Run the AI appraisal first to populate these tables.")
        # else:
        #     # Merge intake (customer-declared) if available
        #     intake_df = ss.get("asset_intake_df")
        #     if intake_df is not None and not intake_df.empty:
        #         # Choose join keys available in both frames
        #         join_keys = [k for k in ["application_id", "asset_id"] if k in ai_df.columns and k in intake_df.columns]
        #         if join_keys:
        #             merged = intake_df.merge(ai_df, on=join_keys, suffixes=("_cust", "_ai"), how="left")
        #         else:
        #             merged = ai_df.copy()
        #     else:
        #         merged = ai_df.copy()

            # Canonical column mapping
            # customer declared value (prefer *_cust if merge happened)
            customer_val_col = "market_value_cust" if "market_value_cust" in merged.columns else (
                "market_value" if "market_value" in merged.columns else None
            )
            # AI value (prefer fmv, fallback ai_adjusted)
            ai_val_col = "fmv" if "fmv" in merged.columns else (
                "ai_adjusted" if "ai_adjusted" in merged.columns else None
            )

            # Build Customer & Loan Details table
            details_cols = [c for c in [
                "application_id","asset_id","asset_type","city",
                customer_val_col,
                "loan_amount",
                ai_val_col, "confidence","why"
            ] if c and c in merged.columns]

            details_tbl = merged[details_cols].copy() if details_cols else merged.copy()

            # Rename for clarity in the UI
            rename_map = {}
            if customer_val_col:
                rename_map[customer_val_col] = "customer_declared_value"
            if ai_val_col:
                rename_map[ai_val_col] = "ai_estimate_value"
            details_tbl = details_tbl.rename(columns=rename_map)

            # Explanation / Source
            selected_model = os.path.basename(str(ss.get("asset_selected_model","") or ""))
            comps_count = int((ss.get("asset_comps_used") or {}).get("count", 0))
            details_tbl["explanation_source"] = details_tbl.apply(
                lambda r: f"Customer input CSV vs AI model {selected_model or 'production'} (comps={comps_count})",
                axis=1
            )

            st.dataframe(details_tbl.head(50), use_container_width=True)

            # Persist details table
            details_path = os.path.join(RUNS_DIR, f"customer_loan_details.{_ts}.csv")
            details_tbl.to_csv(details_path, index=False)
            st.download_button(
                "‚¨áÔ∏è Download Customer & Loan Details (CSV)",
                data=details_tbl.to_csv(index=False).encode("utf-8"),
                file_name="customer_loan_details.csv",
                mime="text/csv"
            )

            st.markdown("---")
            st.markdown("### üìà 5-Year Deltas: Customer vs AI (per-year Œî and %Œî)")

            # Controls for forward projections
            cgr_a, cgr_b = st.columns(2)
            with cgr_a:
                cust_cagr = st.slider("Customer Expected CAGR (%)", min_value=-20, max_value=40, value=5, step=1) / 100.0
            with cgr_b:
                ai_cagr = st.slider("AI Expected CAGR (%)", min_value=-20, max_value=40, value=4, step=1) / 100.0

            if not customer_val_col or not ai_val_col:
                st.warning("Missing base columns to compute deltas. Ensure both customer and AI values exist.")
            else:
                base_cust = merged[customer_val_col].astype(float)
                base_ai   = merged[ai_val_col].astype(float)

                # Build long-format 5-year projection table
                rows = []
                years = [1, 2, 3, 4, 5]
                for idx in range(len(merged)):
                    cust0 = base_cust.iloc[idx]
                    ai0   = base_ai.iloc[idx]
                    app_id = merged.iloc[idx].get("application_id", None)
                    asset_id = merged.iloc[idx].get("asset_id", None)
                    asset_type = merged.iloc[idx].get("asset_type", None)
                    city = merged.iloc[idx].get("city", None)

                    for y in years:
                        cust_y = cust0 * ((1.0 + cust_cagr) ** y) if np.isfinite(cust0) else np.nan
                        ai_y   = ai0   * ((1.0 + ai_cagr) ** y)   if np.isfinite(ai0)   else np.nan
                        delta  = ai_y - cust_y if (np.isfinite(ai_y) and np.isfinite(cust_y)) else np.nan
                        pct    = (delta / cust_y * 100.0) if (np.isfinite(delta) and cust_y not in [0, np.nan]) else np.nan

                        rows.append({
                            "application_id": app_id,
                            "asset_id": asset_id,
                            "asset_type": asset_type,
                            "city": city,
                            "year_ahead": y,
                            "customer_value": cust_y,
                            "ai_value": ai_y,
                            "delta": delta,
                            "delta_pct": pct,
                            "explanation_source": f"Customer CAGR={cust_cagr*100:.1f}% vs AI CAGR={ai_cagr*100:.1f}%; AI model {selected_model or 'production'} (comps={comps_count})"
                        })

                deltas_tbl = pd.DataFrame(rows)

            # Display & export
            # Round for readability
            for c in ["customer_value","ai_value","delta","delta_pct"]:
                if c in deltas_tbl.columns:
                    deltas_tbl[c] = pd.to_numeric(deltas_tbl[c], errors="coerce")

            st.dataframe(deltas_tbl.head(100), use_container_width=True)

            deltas_path = os.path.join(RUNS_DIR, f"valuation_deltas_5y.{_ts}.csv")
            deltas_tbl.to_csv(deltas_path, index=False)
            st.download_button(
                "‚¨áÔ∏è Download 5-Year Deltas (CSV)",
                data=deltas_tbl.to_csv(index=False).encode("utf-8"),
                file_name="valuation_deltas_5y.csv",
                mime="text/csv"
            )


        st.markdown("---")
        # üîí C.5 ‚Äî Legal/Ownership Verification (encumbrances, liens, fraud)
        st.markdown("### **C.5 ‚Äî Legal/Ownership Verification**")

        def _verify_stub(df_in: pd.DataFrame) -> pd.DataFrame:
            df = df_in.copy()
            if "verification_status" not in df.columns:
                df["verification_status"] = "verified"
            if "encumbrance_flag" not in df.columns:
                df["encumbrance_flag"] = False
            if "verified_owner" not in df.columns:
                df["verified_owner"] = np.where(df.get("asset_type","").astype(str).str.lower().str.contains("car"), "DMV Registry", "Land Registry")
            if "notes" not in df.columns:
                df["notes"] = "Registry check passed (stub)"
            return df

        if st.button("üîç Run Legal/Ownership Checks", key="btn_run_verification"):
            base_df = ss.get("asset_ai_df")
            if base_df is None:
                st.warning("Run valuation first.")
            else:
                verified_df = _verify_stub(base_df)
                ss["asset_verified_df"] = verified_df
                ver_path = os.path.join(RUNS_DIR, f"verification_status.{_ts()}.csv")
                verified_df.to_csv(ver_path, index=False)
                st.success(f"Saved verification artifact ‚Üí `{ver_path}`")

                v1, v2 = st.columns(2)
                with v1:
                    try:
                        pct = (verified_df["verification_status"] == "verified").mean()
                        st.metric("Verified %", f"{pct:.0%}")
                    except Exception:
                        st.metric("Verified %", "‚Äî")
                with v2:
                    try:
                        st.metric("Encumbrance Flags", int(pd.to_numeric(verified_df["encumbrance_flag"]).sum()))
                    except Exception:
                        st.metric("Encumbrance Flags", "‚Äî")

                cols_ver = [c for c in [
                    "application_id","asset_id","verified_owner","verification_status","encumbrance_flag","notes"
                ] if c in verified_df.columns]
                st.dataframe(verified_df[cols_ver].head(50), use_container_width=True)

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # üìä Executive Portfolio Dashboard (Spectacular)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        st.divider()
        st.subheader("üìä Executive Portfolio Dashboard")

        df_src = ss.get("asset_ai_df")
        ft = ss.get("asset_first_table")  # loan-centric projection you already built
        if df_src is None or (hasattr(df_src, "empty") and df_src.empty):
            st.info("Run appraisal to populate the dashboard.")
        else:
            df = df_src.copy()

            # ---- Safe numerics
            def _num(series, default=None):
                s = pd.to_numeric(series, errors="coerce")
                if default is not None:
                    s = s.fillna(default)
                return s

            for c in ["ai_adjusted","realizable_value","loan_amount",
                    "valuation_gap_pct","ltv_ai","ltv_cap","confidence",
                    "condition_score","legal_penalty"]:
                if c in df.columns:
                    df[c] = _num(df[c])

            # ---- KPIs row
            k1, k2, k3, k4, k5 = st.columns(5)
            total_ai        = float(df.get("ai_adjusted", pd.Series(dtype=float)).sum()) if "ai_adjusted" in df.columns else 0.0
            total_realiz    = float(df.get("realizable_value", pd.Series(dtype=float)).sum()) if "realizable_value" in df.columns else 0.0
            avg_conf        = float(df.get("confidence", pd.Series(dtype=float)).mean()) if "confidence" in df.columns else 0.0
            ltv_breach_rate = 0.0
            if {"ltv_ai","ltv_cap"}.issubset(df.columns):
                ltv_breach_rate = float((df["ltv_ai"] > df["ltv_cap"]).mean() * 100)
            approved_cnt = int(df.get("decision","").astype(str).str.lower().eq("approved").sum()) if "decision" in df.columns else 0

            k1.metric("AI Gross Value",       f"${total_ai:,.0f}")
            k2.metric("Realizable Value",     f"${total_realiz:,.0f}")
            k3.metric("Avg Confidence",       f"{avg_conf:.1f}%")
            k4.metric("LTV Breach Rate",      f"{ltv_breach_rate:.1f}%")
            k5.metric("Approved Count",       f"{approved_cnt:,}")

            # ---- Row 1: Top-10 Assets & Decision Mix
            r1c1, r1c2 = st.columns([1.2, 1])
            with r1c1:
                value_col = "realizable_value" if "realizable_value" in df.columns else ("ai_adjusted" if "ai_adjusted" in df.columns else None)
                if value_col:
                    df_top = (df.assign(_val=df[value_col])
                                .sort_values("_val", ascending=False)
                                .head(10))
                    fig_top = px.bar(
                        df_top,
                        x="_val", y=df_top.get("asset_id", df_top.index).astype(str),
                        color="asset_type" if "asset_type" in df_top.columns else None,
                        orientation="h",
                        title=f"Top 10 Assets by {value_col.replace('_',' ').title()}",
                        hover_data=[c for c in ["application_id","asset_id","asset_type","city","_val"] if c in df_top.columns]
                    )
                    fig_top.update_layout(template="plotly_dark", height=380, yaxis_title=None, xaxis_title=value_col)
                    st.plotly_chart(fig_top, use_container_width=True)
            with r1c2:
                names_series = (df["decision"].astype(str).str.title()
                                if "decision" in df.columns
                                else np.where(df.get("policy_breaches","").astype(str).str.len().gt(0),
                                            "Has Breach","No Breach"))
                fig_mix = px.pie(df, names=names_series, title="Decision / Breach Mix")
                fig_mix.update_layout(template="plotly_dark", height=380)
                st.plotly_chart(fig_mix, use_container_width=True)

            # ---- Row 2: By Asset Type & City Concentration
            r2c1, r2c2 = st.columns(2)
            with r2c1:
                if "asset_type" in df.columns:
                    df_type = (df
                            .assign(value=df[value_col] if value_col else 0)
                            .groupby("asset_type", dropna=False)["value"]
                            .sum().sort_values(ascending=False).reset_index())
                    fig_type = px.bar(df_type, x="asset_type", y="value",
                                    title="Value by Asset Type",
                                    text_auto=True)
                    fig_type.update_layout(template="plotly_dark", height=360, xaxis_title=None, yaxis_title="Value")
                    st.plotly_chart(fig_type, use_container_width=True)
            with r2c2:
                if "city" in df.columns and value_col:
                    df_city = (df.groupby("city", dropna=False)[value_col]
                                .sum().sort_values(ascending=False)
                                .head(10).reset_index())
                    fig_city = px.pie(df_city, values=value_col, names="city",
                                    title="Top-10 City Concentration")
                    fig_city.update_layout(template="plotly_dark", height=360)
                    st.plotly_chart(fig_city, use_container_width=True)

            # ---- Row 3: LTV vs Cap & Condition√óLegal Heat
            r3c1, r3c2 = st.columns(2)
            with r3c1:
                if {"ltv_ai","ltv_cap"}.issubset(df.columns):
                    fig_sc = px.scatter(
                        df, x="ltv_cap", y="ltv_ai",
                        color="asset_type" if "asset_type" in df.columns else None,
                        hover_data=[c for c in ["application_id","asset_id","asset_type","city","loan_amount"] if c in df.columns],
                        title="LTV (AI) vs LTV Cap"
                    )
                    try:
                        max_cap = float((df["ltv_cap"].max() or 1.2))
                        fig_sc.add_shape(type="line", x0=0, y0=0, x1=max_cap, y1=max_cap, line=dict(dash="dash"))
                    except Exception:
                        pass
                    fig_sc.update_layout(template="plotly_dark", height=360,
                                        xaxis_title="LTV Cap", yaxis_title="LTV (AI)")
                    st.plotly_chart(fig_sc, use_container_width=True)
            with r3c2:
                if {"condition_score","legal_penalty"}.issubset(df.columns):
                    try:
                        cond_bins  = pd.cut(df["condition_score"], bins=[0,0.70,0.85,1.00], labels=["<0.70","0.70‚Äì0.85",">0.85"])
                        legal_bins = pd.cut(df["legal_penalty"],  bins=[0,0.97,0.99,1.00], labels=["<0.97","0.97‚Äì0.99",">=0.99"])
                        heat = (df.assign(cond=cond_bins, legal=legal_bins)
                                .groupby(["cond","legal"]).size().reset_index(name="count"))
                        fig_hm = px.density_heatmap(heat, x="legal", y="cond", z="count",
                                                    title="Condition vs Legal ‚Äî Density")
                        fig_hm.update_layout(template="plotly_dark", height=360)
                        st.plotly_chart(fig_hm, use_container_width=True)
                    except Exception:
                        pass

            # ---- Row 4: City Leaderboard + Per-City Asset List
            st.markdown("### üèôÔ∏è City Leaderboard & Assets")
            if "city" in df.columns:
                value_col = value_col or "ai_adjusted"
                city_sum = (df.groupby("city", dropna=False)[value_col]
                            .sum().sort_values(ascending=False).reset_index()
                            .rename(columns={value_col: "total_value"}))
                left, right = st.columns([1, 2])
                with left:
                    st.dataframe(city_sum, use_container_width=True)
                with right:
                    # show top assets per top city
                    top_cities = city_sum["city"].astype(str).head(5).tolist()
                    for city in top_cities:
                        with st.expander(f"üìç {city} ‚Äî top assets", expanded=False):
                            sub = (df[df["city"].astype(str)==city]
                                .assign(value=df[value_col])
                                .sort_values("value", ascending=False)
                                [[c for c in ["application_id","asset_id","asset_type","value","loan_amount","confidence"] if c in df.columns]]
                                .head(15))
                            st.dataframe(sub, use_container_width=True)


            # ---- Optional Map (if lat/lon present)
            st.markdown("### üó∫Ô∏è Map (optional)")
            st.caption("Visualize asset locations ‚Äî map color and style follow the current UI theme.")

            map_cols = [("lat","lon"), ("latitude","longitude"), ("gps_lat","gps_lon")]
            have_map = False

            for la, lo in map_cols:
                if la in df.columns and lo in df.columns:
                    have_map = True
                    map_df = df[[la, lo] + [
                        c for c in ["asset_id","asset_type","city","ai_adjusted","realizable_value","confidence"]
                        if c in df.columns
                    ]].copy()
                    map_df = map_df.rename(columns={la: "lat", lo: "lon"})
                    map_df = map_df.dropna(subset=["lat", "lon"])

                    if not map_df.empty:
                        try:
                            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                            # Prefer Plotly (bright light / dark dark)
                            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                            import plotly.express as px
                            apply_plotly_mapbox_defaults()
                            style_name = plotly_map_style()

                            fig = px.scatter_mapbox(
                                map_df,
                                lat="lat",
                                lon="lon",
                                hover_name="asset_id" if "asset_id" in map_df.columns else "city",
                                hover_data={c: True for c in ["asset_type","city","ai_adjusted","realizable_value","confidence"] if c in map_df.columns},
                                color_discrete_sequence=["#38bdf8"],
                                zoom=8,
                                height=420,
                            )

                            fig.update_layout(
                                mapbox_style=style_name,
                                margin=dict(l=0, r=0, t=0, b=0),
                                paper_bgcolor="rgba(0,0,0,0)",
                                plot_bgcolor="rgba(0,0,0,0)",
                            )
                            st.plotly_chart(fig, use_container_width=True)

                        except Exception as e:
                            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                            # Fallback to pydeck if Plotly unavailable
                            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                            import pydeck as pdk
                            view_state = make_pydeck_view_state(
                                lat=float(map_df["lat"].mean()),
                                lon=float(map_df["lon"].mean()),
                                zoom=8
                            )
                            layer = pdk.Layer(
                                "ScatterplotLayer",
                                data=map_df,
                                get_position='[lon, lat]',
                                get_color='[0, 128, 255, 200]',
                                get_radius=120,
                                pickable=True,
                            )
                            deck = pdk.Deck(
                                map_style=pydeck_map_style(),
                                initial_view_state=view_state,
                                layers=[layer],
                                tooltip={"text": "{asset_id} ¬∑ {asset_type}\n{city}\nAI: {ai_adjusted}\nRealiz: {realizable_value}\nConf: {confidence}"}
                            )
                            st.pydeck_chart(deck)
                    else:
                        st.info("‚ÑπÔ∏è No valid coordinates found to display on the map.")
                    break

            if not have_map:
                st.caption("No lat/lon columns found (lat/lon or latitude/longitude or gps_lat/gps_lon). Map hidden.")


            # ---- Exports of aggregates
            st.markdown("#### üì§ Export dashboard aggregates")
            exports = {}
            if "asset_type" in df.columns:
                exports["by_asset_type.csv"] = df_type.to_csv(index=False) if 'df_type' in locals() else ""
            if "city" in df.columns and value_col:
                exports["by_city_top10.csv"] = df_city.to_csv(index=False) if 'df_city' in locals() else ""
            if 'df_top' in locals():
                exports["top_assets.csv"] = df_top.drop(columns=["_val"], errors="ignore").to_csv(index=False)

            ex1, ex2, ex3 = st.columns(3)
            for i, (fname, data) in enumerate(exports.items()):
                if not data:
                    continue
                col = [ex1, ex2, ex3][i % 3]
                with col:
                    st.download_button(f"‚¨áÔ∏è {fname}", data=data.encode("utf-8"), file_name=fname, mime="text/csv")
                    
            
            # ‚úÖ NEW: Export full AI decision file for Stage E (Human Review)
            st.markdown("### üßæ Export AI Decision for Human Review (Stage E)")

            if 'ai_df' in locals() and isinstance(ai_df, pd.DataFrame) and not ai_df.empty:
                ai_export_name = f"ai_decision_stageC_{datetime.now().strftime('%Y%m%d-%H%M%S')}.csv"
                ai_csv_data = ai_df.to_csv(index=False, encoding="utf-8-sig")

                st.download_button(
                    "‚¨áÔ∏è Export AI Decisions (send to Stage E)",
                    data=ai_csv_data,
                    file_name=ai_export_name,
                    mime="text/csv",
                    key="dl_ai_stagec_export"
                )
            else:
                st.info("AI table (ai_df) not available ‚Äî run valuation first.")

            # ========================
            # Stage C ‚Äî AI Appraisal & Valuation
            # ========================
            # Now send AI results to Stage E for review
            if st.button("üí¨ Review in Stage E"):
                # Store AI appraisal results in session_state for Stage E
                st.session_state["ai_review_df"] = ai_df  # ai_df should be the AI results dataframe from the current stage
                st.session_state["current_stage"] = "human_review"
                st.success("AI results sent to Stage E for human review!")
                st.rerun()

            
            # # ========================
            # # Stage C ‚Äî AI Appraisal & Valuation
            # # ========================
            # # Now send AI results to Stage E for review
            # if st.button("üí¨ Review in Stage E"):
            #     # Store AI appraisal results in session_state for Stage E
            #     st.session_state["ai_review_df"] = ai_df  # ai_df should be the AI results dataframe from the current stage
            #     st.session_state["current_stage"] = "human_review"
            #     st.success("AI results sent to Stage E for human review!")
            #     st.rerun()  # Trigger a page refresh to go to the next stage




# ========== 4) POLICY & DECISION (Stage D: steps 6‚Äì7) ==========
with tabD:
    st.subheader("üßÆ Stage 4 ‚Äî Policy & Decision (D.6 / D.7)")

    import os, json
    import numpy as np
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)
    def _ts(): return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    # ---- Input table: prefer verified ‚Üí else AI valuation (safe selector) ----
    base_df = first_nonempty_df(ss.get("asset_verified_df"), ss.get("asset_ai_df"))
    if not is_nonempty_df(base_df):
        st.warning("Run Stage C first (valuation, and optionally verification).")
        st.stop()

    st.caption("Input: valuation + (optional) verification outputs.")

    # ‚îÄ‚îÄ D.6 and D.7 continue here (your existing haircuts / caps / breaches / decision code) ‚îÄ‚îÄ

    # -------- D.6 ‚Äî Policy & Haircuts ‚Üí realizable_value --------
    st.markdown("### **D.6 ‚Äî Policy & Haircuts**")
    p1, p2, p3 = st.columns(3)
    with p1:
        base_haircut_pct = st.slider("Base haircut (%)", 0, 60, 10, 1, key="policy_base_haircut")
    with p2:
        condition_weight = st.slider("Condition multiplier min", 0.50, 1.00, 0.80, 0.01, key="policy_cond_min")
    with p3:
        legal_weight = st.slider("Legal multiplier min", 0.50, 1.00, 0.95, 0.01, key="policy_legal_min")

    if st.button("Apply Haircuts", key="btn_apply_haircuts"):
        df = base_df.copy()

        # Ensure necessary inputs exist
        for col, default in [("ai_adjusted", np.nan), ("condition_score", 0.9), ("legal_penalty", 1.0)]:
            if col not in df.columns:
                df[col] = default

        ai_adj = pd.to_numeric(df["ai_adjusted"], errors="coerce")
        cond   = pd.to_numeric(df["condition_score"], errors="coerce").clip(condition_weight, 1.0)
        legal  = pd.to_numeric(df["legal_penalty"],  errors="coerce").clip(legal_weight, 1.0)
        base_cut = (1.0 - float(base_haircut_pct) / 100.0)

        df["realizable_value"] = ai_adj * cond * legal * base_cut

        # Persist artifact
        policy_path = os.path.join(RUNS_DIR, f"policy_haircuts.{_ts()}.csv")
        df.to_csv(policy_path, index=False)

        # ‚úÖ Save Stage D policy results for Stage H
        try:
            # Save into BOTH namespaces safely
            st.session_state["asset_policy_df"] = df.copy()
            ss["asset_policy_df"] = df.copy()

            st.info("‚úÖ Stage D policy results stored for Stage H.")
        except Exception as e:
            st.warning(f"Could not store Stage D output: {e}")

        st.success(f"Saved: `{policy_path}`")

        # KPIs
        k1, k2, k3 = st.columns(3)
        with k1:
            st.metric(
                "Avg Realizable Value",
                f"{pd.to_numeric(df['realizable_value'], errors='coerce').mean():,.0f}"
            )
        with k2:
            st.metric("Rows", len(df))
        with k3:
            st.metric("Base Haircut", f"{base_haircut_pct}%")

        st.dataframe(df.head(30), use_container_width=True)

        
        # # ‚úÖ Save Stage D policy results for Stage H
        # try:
        #     st.session_state["asset_policy_df"] = df.copy()
        #     ss["asset_policy_df"] = df.copy()
        #     st.info("‚úÖ Stage D policy results stored for Stage H.")
        # except Exception as e:
        #     st.warning(f"Could not store Stage D output: {e}")

        # st.success(f"Saved: `{policy_path}`")

        # # KPIs
        # k1, k2, k3 = st.columns(3)
        # with k1:
        #     st.metric("Avg Realizable Value", f"{pd.to_numeric(df['realizable_value'], errors='coerce').mean():,.0f}")
        # with k2:
        #     st.metric("Rows", len(df))
        # with k3:
        #     st.metric("Base Haircut", f"{base_haircut_pct}%")

        # st.dataframe(df.head(30), use_container_width=True)

    # # -------- D.6 ‚Äî Policy & Haircuts ‚Üí realizable_value --------
    # st.markdown("### **D.6 ‚Äî Policy & Haircuts**")
    # p1, p2, p3 = st.columns(3)
    # with p1:
    #     base_haircut_pct = st.slider("Base haircut (%)", 0, 60, 10, 1, key="policy_base_haircut")
    # with p2:
    #     condition_weight = st.slider("Condition multiplier min", 0.50, 1.00, 0.80, 0.01, key="policy_cond_min")
    # with p3:
    #     legal_weight = st.slider("Legal multiplier min", 0.50, 1.00, 0.95, 0.01, key="policy_legal_min")

    # if st.button("Apply Haircuts", key="btn_apply_haircuts"):
    #     df = base_df.copy()

    #     # Ensure necessary inputs exist
    #     for col, default in [("ai_adjusted", np.nan), ("condition_score", 0.9), ("legal_penalty", 1.0)]:
    #         if col not in df.columns:
    #             df[col] = default

    #     ai_adj = pd.to_numeric(df["ai_adjusted"], errors="coerce")
    #     cond   = pd.to_numeric(df["condition_score"], errors="coerce").clip(condition_weight, 1.0)
    #     legal  = pd.to_numeric(df["legal_penalty"],  errors="coerce").clip(legal_weight, 1.0)
    #     base_cut = (1.0 - float(base_haircut_pct) / 100.0)

    #     df["realizable_value"] = ai_adj * cond * legal * base_cut

    #     # Persist policy_haircuts artifact
    #     policy_path = os.path.join(RUNS_DIR, f"policy_haircuts.{_ts()}.csv")
    #     df.to_csv(policy_path, index=False)
        
    #     # ‚úÖ Save Stage D policy for Stage H
    #     st.session_state["asset_policy_df"] = policy_df.copy()

    #     ss["asset_policy_df"] = df
    #     st.success(f"Saved: `{policy_path}`")

    #     # KPIs
    #     k1, k2, k3 = st.columns(3)
    #     with k1:
    #         st.metric("Avg Realizable Value", f"{pd.to_numeric(df['realizable_value'], errors='coerce').mean():,.0f}")
    #     with k2:
    #         st.metric("Rows", len(df))
    #     with k3:
    #         st.metric("Base Haircut", f"{base_haircut_pct}%")

    #     st.dataframe(df.head(30), use_container_width=True)

    # st.markdown("---")

    # -------- D.7 ‚Äî Risk / Decision --------
    st.markdown("### **D.7 ‚Äî Risk / Decision**")

    if ss.get("asset_policy_df") is None:
        st.info("Run D.6 first to compute `realizable_value`.")
    else:
        df = ss["asset_policy_df"].copy()

        # Inputs
        r1, r2, r3 = st.columns(3)
        with r1:
            loan_amount_default = float(pd.to_numeric(df.get("loan_amount", pd.Series([60000])).median()))
            loan_amount = st.number_input("Loan amount (default=median)", value=loan_amount_default, min_value=0.0, step=1000.0, key="risk_loan_amt")
        with r2:
            ltv_mode = st.selectbox("LTV cap mode", ["Fixed cap", "Per asset_type"], index=0, key="risk_ltv_mode")
        with r3:
            fixed_ltv_cap = st.slider("Fixed LTV cap (√ó)", 0.10, 2.00, 0.80, 0.05, key="risk_ltv_cap_fixed")

        # Per-type caps if requested
        type_caps = {}
        if ltv_mode == "Per asset_type":
            types = sorted(list(map(str, (df.get("asset_type") or pd.Series(["Asset"])).dropna().unique())))[0:10]
            st.caption("Tune LTV caps per asset_type")
            grid = st.columns(4 if len(types) > 3 else max(1, len(types)))
            for i, t in enumerate(types):
                with grid[i % len(grid)]:
                    type_caps[t] = st.number_input(f"{t} cap √ó", 0.10, 2.00, 0.80, 0.05, key=f"cap_{t}")

        # Thresholds for decisioning
        t1, t2, t3 = st.columns(3)
        with t1:
            min_conf = st.slider("Min confidence (%)", 0, 100, 70, 1, key="risk_min_conf")
        with t2:
            min_cond = st.slider("Min condition_score", 0.60, 1.00, 0.75, 0.01, key="risk_min_cond")
        with t3:
            min_legal = st.slider("Min legal_penalty", 0.80, 1.00, 0.97, 0.01, key="risk_min_legal")

        if st.button("Compute Decision", key="btn_compute_decision"):
            # Compute ltv_ai
            df["ltv_ai"] = pd.to_numeric(loan_amount, errors="coerce") / pd.to_numeric(df.get("ai_adjusted", np.nan), errors="coerce")

            # ltv_cap
            if ltv_mode == "Fixed cap":
                df["ltv_cap"] = float(fixed_ltv_cap)
            else:
                atypes = df.get("asset_type").astype(str) if "asset_type" in df.columns else pd.Series(["Asset"] * len(df))
                df["ltv_cap"] = atypes.map(lambda t: float(type_caps.get(t, fixed_ltv_cap)))

            # Breaches
            conf = pd.to_numeric(df.get("confidence", 100.0), errors="coerce")
            cond = pd.to_numeric(df.get("condition_score", 1.0), errors="coerce")
            legal= pd.to_numeric(df.get("legal_penalty", 1.0),  errors="coerce")
            ltv  = pd.to_numeric(df["ltv_ai"], errors="coerce")
            lcap = pd.to_numeric(df["ltv_cap"], errors="coerce")

            breaches = []
            for i in range(len(df)):
                b = []
                if pd.notna(conf.iat[i]) and conf.iat[i] < min_conf:
                    b.append(f"confidence<{min_conf}%")
                if pd.notna(cond.iat[i]) and cond.iat[i] < min_cond:
                    b.append(f"condition<{min_cond:.2f}")
                if pd.notna(legal.iat[i]) and legal.iat[i] < min_legal:
                    b.append(f"legal<{min_legal:.2f}")
                if pd.notna(ltv.iat[i]) and pd.notna(lcap.iat[i]) and ltv.iat[i] > lcap.iat[i]:
                    b.append("ltv>cap")
                breaches.append(", ".join(b))
            df["policy_breaches"] = breaches

            # Decision rule
            # - reject if LTV>cap OR confidence << min_conf (<= min_conf-10)
            # - review if any breach but not hard reject
            # - approve otherwise
            hard_reject = (
                (ltv > lcap) |
                (pd.to_numeric(conf, errors="coerce") <= (min_conf - 10))
            )
            any_breach = df["policy_breaches"].str.len().gt(0)

            df["decision"] = np.select(
                [
                    hard_reject,
                    any_breach
                ],
                ["reject", "review"],
                default="approve"
            )

            # Persist risk_decision artifact
            risk_path = os.path.join(RUNS_DIR, f"risk_decision.{_ts()}.csv")
            df.to_csv(risk_path, index=False)
            ss["asset_decision_df"] = df
            st.success(f"Saved: `{risk_path}`")

            # KPIs + Table
            k1, k2, k3 = st.columns(3)
            with k1:
                st.metric("Avg LTV (AI)", f"{pd.to_numeric(df['ltv_ai'], errors='coerce').mean():.2f}")
            with k2:
                try:
                    st.metric("Breach Rate", f"{(df['policy_breaches'].str.len().gt(0)).mean():.0%}")
                except Exception:
                    st.metric("Breach Rate", "‚Äî")
            with k3:
                mix = df["decision"].value_counts(dropna=False)
                st.metric("Approve/Review/Reject", f"{int(mix.get('approve',0))}/{int(mix.get('review',0))}/{int(mix.get('reject',0))}")

            cols_view = [c for c in [
                "application_id","asset_id","asset_type","city",
                "ai_adjusted","realizable_value",
                "loan_amount","ltv_ai","ltv_cap",
                "confidence","condition_score","legal_penalty",
                "policy_breaches","decision"
            ] if c in df.columns]
            st.dataframe(df[cols_view].head(50), use_container_width=True)

            st.download_button(
                "‚¨áÔ∏è Download Policy+Decision (CSV)",
                data=df.to_csv(index=False).encode("utf-8"),
                file_name="risk_decision.csv",
                mime="text/csv"
            )



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# E ‚Äî HUMAN REVIEW & FEEDBACK DASHBOARD
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from datetime import datetime, timezone  # ensure available inside this block
import os, glob, json
import numpy as np
import pandas as pd
import plotly.graph_objects as go

with tabE:
    st.subheader("üßë‚Äç‚öñÔ∏è Stage E ‚Äî Human Review & Feedback")
    st.caption("Compare AI-estimated collateral values against business metrics, adjust valuations, and record justification for retraining.")

    
    # Workspace
    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Stage C loader controls (Auto-load + picker)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _find_stage_c_candidates():
        pats = ["valuation_ai*.csv", "valuation_ai*.parquet"]
        files = []
        for pat in pats:
            files.extend(glob.glob(os.path.join(RUNS_DIR, pat)))
        return sorted(files, key=os.path.getmtime, reverse=True)

    if "stage_c_selected_path" not in st.session_state:
        st.session_state["stage_c_selected_path"] = None

    ctrl1, ctrl2, ctrl3 = st.columns([1.2, 1, 2.8])
    with ctrl1:
        btn_autoload = st.button("üîÑ Auto-load latest Stage C", use_container_width=True)
    with ctrl2:
        btn_refresh = st.button("üîÅ Refresh list", use_container_width=True)
    with ctrl3:
        st.caption("Looks for `valuation_ai*.csv|.parquet` under `./.tmp_runs`")

    if btn_refresh:
        pass  # triggers rerun ‚Üí list will refresh

    candidates = _find_stage_c_candidates()
    if not candidates:
        st.warning("‚ö†Ô∏è No AI appraisal results found. Please complete Stage C first.")
        st.stop()

    # Pick newest on first load or when autoload pressed
    if btn_autoload:
        st.session_state["stage_c_selected_path"] = candidates[0]
    elif not st.session_state["stage_c_selected_path"]:
        st.session_state["stage_c_selected_path"] = candidates[0]
    # Ensure the selected one still exists
    if st.session_state["stage_c_selected_path"] not in candidates:
        st.session_state["stage_c_selected_path"] = candidates[0]

    # Human-friendly label
    def _fmt(p):
        ts = datetime.fromtimestamp(os.path.getmtime(p)).strftime("%Y-%m-%d %H:%M:%S")
        return f"{os.path.basename(p)}  ‚Ä¢  {ts}"

    current_idx = candidates.index(st.session_state["stage_c_selected_path"])
    picked = st.selectbox(
        "Stage C output to review",
        options=candidates,
        index=current_idx,
        format_func=_fmt,
    )
    st.session_state["stage_c_selected_path"] = picked

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‚úÖ NEW: Direct Upload of Stage C Export (CSV)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### üì§ Or Upload Stage C Export")
    uploaded_c = st.file_uploader(
        "Upload a Stage C file (valuation_ai*.csv)",
        type=["csv"],
        key="stage_c_upload"
    )

    if uploaded_c is not None:
        try:
            df_ai = pd.read_csv(uploaded_c)
            #st.success(f"‚úÖ Imported uploaded file ({len[df_ai)} rows).")
            st.success(f"‚úÖ Imported uploaded file ({len(df_ai)} rows).")


            temp_path = os.path.join(RUNS_DIR, f"uploaded_stage_c_{datetime.now().timestamp()}.csv")
            df_ai.to_csv(temp_path, index=False, encoding="utf-8-sig")

            st.session_state["stage_c_selected_path"] = temp_path
            st.session_state["df_ai_current"] = df_ai.copy()

            st.rerun()

        except Exception as e:
            st.error(f"Upload failed: {e}")
    

    # Load the selected Stage C table ‚Üí df_ai
    ai_path = st.session_state["stage_c_selected_path"]
    try:
        if ai_path.lower().endswith(".parquet"):
            df_ai = pd.read_parquet(ai_path)
        else:
            df_ai = pd.read_csv(ai_path)
        st.success(f"‚úÖ Loaded Stage C: {os.path.basename(ai_path)}  ({len(df_ai)} rows √ó {df_ai.shape[1]} cols)")
    except Exception as e:
        st.error(f"Failed to read `{ai_path}`: {e}")
        st.stop()

    # Ensure join keys exist to avoid editor KeyErrors later
    for col in ["application_id", "asset_id", "asset_type", "city"]:
        if col not in df_ai.columns:
            df_ai[col] = None

    # Ensure human_value / justification columns for adjustments
    if "human_value" not in df_ai.columns:
        df_ai["human_value"] = pd.to_numeric(df_ai["fmv"], errors="coerce") if "fmv" in df_ai.columns else np.nan
    if "justification" not in df_ai.columns:
        df_ai["justification"] = ""

    

    # ‚îÄ‚îÄ Market Projections (safe)
    st.markdown("### üìà Market Projections")
    horizon = st.select_slider("Projection Horizon (years)", options=[3, 5, 10], value=5)
    growth = st.slider("Expected Market Growth (%)", -10, 25, 4) / 100

    df_proj = df_ai.copy()
    if "fmv" in df_proj.columns:
        fmv_num = pd.to_numeric(df_proj["fmv"], errors="coerce")
        df_proj[f"fmv_proj_{horizon}y"] = (fmv_num * ((1 + growth) ** horizon)).round(0)
        st.line_chart(df_proj[["fmv", f"fmv_proj_{horizon}y"]])
    else:
        st.info("FMV column not found; projection chart will appear after you run Stage C.")

    # ‚úÖ Helper: return the first column present in dataframe
    def _first_present(df, candidates):
        for c in candidates:
            if c in df.columns:
                return c
        return None


    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‚úÖ Human Adjustment Table (LIVE + REFRESH SAFE)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    st.markdown("### ‚úèÔ∏è Human Adjustments & Justification")

    


    # Ensure editable columns exist
    if "human_value" not in df_ai.columns:
        df_ai["human_value"] = pd.to_numeric(df_ai["fmv"], errors="coerce") if "fmv" in df_ai.columns else np.nan
    if "justification" not in df_ai.columns:
        df_ai["justification"] = ""

     # Editable columns for the reviewer
    base_editable = ["application_id", "asset_id", "asset_type", "city", "fmv", "ai_adjusted", "confidence", "loan_amount", "human_value", "justification"]
    editable_cols = [c for c in base_editable if c in df_ai.columns]  # filter to present
    if not editable_cols:
        editable_cols = df_ai.columns.tolist()  # last resort: allow full frame
    
    # Display the editable table for human review

    edited = st.data_editor(df_ai[editable_cols], num_rows="dynamic", use_container_width=True)

    # ‚îÄ‚îÄ Agreement / Deviation Gauge + Mismatch list
    st.markdown("### üéØ Human vs AI Agreement / Deviation")

    # Resolve decision columns if present
    def _first_present(df, candidates):
        return next((c for c in candidates if c in df.columns), None)

    ai_dec_col = _first_present(edited, ["ai_decision", "ai_label", "ai_outcome", "decision_ai"])
    human_dec_col = _first_present(edited, ["human_decision", "human_label", "final_decision", "decision_human"])

    if ai_dec_col and human_dec_col:
        # Agreement gauge (%)
        a = edited[ai_dec_col].astype(str).str.strip().str.lower()
        h = edited[human_dec_col].astype(str).str.strip().str.lower()
        matches = (a == h)
        agree_pct = float(matches.mean() * 100.0) if len(matches) else 0.0

        fig = go.Figure(go.Indicator(
            mode="gauge+number",
            value=round(agree_pct, 2),
            number={'suffix': '%'},
            gauge={
                'axis': {'range': [0, 100]},
                'bar': {'thickness': 0.35},
                'steps': [
                    {'range': [0, 50], 'color': '#fee2e2'},
                    {'range': [50, 80], 'color': '#fef9c3'},
                    {'range': [80, 100], 'color': '#dcfce7'},
                ],
                'threshold': {'line': {'color': '#2563eb', 'width': 4}, 'thickness': 0.9, 'value': round(agree_pct, 2)}
            },
            title={'text': "AI ‚Üî Human Agreement"}
        ))
        st.plotly_chart(fig, use_container_width=True)

        # Mismatch table (if any)
        mis_df = edited.loc[~matches].copy()
        key_cols = [c for c in ["application_id", "asset_id", "asset_type", "city"] if c in edited.columns]
        value_ai = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
        value_hu = _first_present(edited, ["human_value", "reviewed_value", "final_value"])

        if not mis_df.empty:
            show_cols = key_cols + [c for c in [ai_dec_col, human_dec_col, value_ai, value_hu, "justification"] if c]
            show_cols = [c for c in show_cols if c in mis_df.columns]
            st.markdown("#### üîé Mismatches ‚Äî what did humans change?")
            st.dataframe(mis_df[show_cols].head(300), use_container_width=True, hide_index=True)
        else:
            st.success("üéâ Perfect agreement ‚Äî no mismatches.")
    else:
        # Fall back to deviation score if decisions are not present
        if all(c in edited.columns for c in ("human_value", "fmv")):
            hv = pd.to_numeric(edited["human_value"], errors="coerce")
            fmv = pd.to_numeric(edited["fmv"], errors="coerce").replace(0, np.nan)
            deviation = (hv - fmv).abs() / fmv
            score = max(0.0, 100.0 - float(np.nanmean(deviation) * 200.0)) if len(deviation) else 0.0

            fig = go.Figure(go.Indicator(
                mode="gauge+number",
                value=round(score, 1),
                number={'suffix': ' / 100'},
                gauge={
                    'axis': {'range': [0, 100]},
                    'bar': {'thickness': 0.35},
                    'steps': [
                        {'range': [0, 50], 'color': '#fee2e2'},
                        {'range': [50, 80], 'color': '#fef9c3'},
                        {'range': [80, 100], 'color': '#dcfce7'},
                    ],
                    'threshold': {'line': {'color': '#2563eb', 'width': 4}, 'thickness': 0.9, 'value': round(score, 1)}
                },
                title={'text': "Alignment Score (by value deviation)"}
            ))
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Provide decision columns (ai_decision / human_decision) for agreement gauge, or both FMV and human_value for deviation.")

    
        # ‚îÄ‚îÄ Human Changes Only (colored)
        st.markdown("### üñçÔ∏è Human Changes Only (colored)")

        # Reuse helper and edited df from above
        value_ai = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
        value_hu = _first_present(edited, ["human_value", "reviewed_value", "final_value"])
        ai_dec_col = _first_present(edited, ["ai_decision", "ai_label", "ai_outcome", "decision_ai"])
        human_dec_col = _first_present(edited, ["human_decision", "human_label", "final_decision", "decision_human"])

        if value_ai and value_hu:
            ai_vals = pd.to_numeric(edited[value_ai], errors="coerce")
            hu_vals = pd.to_numeric(edited[value_hu], errors="coerce")

            # decisions -> Series aligned to edited.index
            if ai_dec_col and human_dec_col:
                a = edited[ai_dec_col].astype(str).str.strip().str.lower()
                h = edited[human_dec_col].astype(str).str.strip().str.lower()
                dec_changed = (a != h)  # Series
            else:
                dec_changed = pd.Series(False, index=edited.index)

            # justification -> Series aligned
            just_present = edited.get("justification", pd.Series("", index=edited.index)) \
                                .astype(str).str.strip().ne("")

            # treat tiny diffs as equal
            rel_tol = 1e-9
            val_changed = (ai_vals.fillna(np.nan) - hu_vals.fillna(np.nan)).abs() > (
                (ai_vals.abs() + hu_vals.abs()).fillna(0) * rel_tol
            )

            changed_mask = val_changed | dec_changed | just_present
            diff_df = edited.loc[changed_mask].copy()

            if diff_df.empty:
                st.success("üéâ No human changes detected.")
            else:
                # compute deltas on the FILTERED subset ONLY
                ai_sub = ai_vals.reindex(diff_df.index)
                hu_sub = hu_vals.reindex(diff_df.index)

                diff_df["Œî_value"] = (hu_sub - ai_sub)
                base = ai_sub.replace(0, np.nan)
                diff_df["Œî_%"] = ((diff_df["Œî_value"] / base) * 100.0).round(2)

                key_cols = [c for c in ["application_id", "asset_id", "asset_type", "city"] if c in diff_df.columns]
                show_cols = key_cols + [c for c in [value_ai, value_hu, "Œî_value", "Œî_%", ai_dec_col, human_dec_col, "justification"] if c in diff_df.columns]
                show_df = diff_df[show_cols].copy()

                def _color_row(row):
                    styles = [""] * len(row.index)

                    def _idx(colname):
                        try:
                            return show_df.columns.get_loc(colname)
                        except Exception:
                            return None

                    idx_ai = _idx(value_ai)
                    idx_hu = _idx(value_hu)
                    idx_dv = _idx("Œî_value")
                    idx_dp = _idx("Œî_%")

                    # Value changes
                    try:
                        ai_v = float(row.get(value_ai, np.nan))
                        hu_v = float(row.get(value_hu, np.nan))
                    except Exception:
                        ai_v, hu_v = np.nan, np.nan

                    if pd.notna(ai_v) and pd.notna(hu_v):
                        if hu_v > ai_v:  # green for up
                            for i in [idx_hu, idx_dv, idx_dp]:
                                if i is not None:
                                    styles[i] = "background-color:#dcfce7; color:#065f46; font-weight:600;"
                            if idx_ai is not None:
                                styles[idx_ai] = "background-color:#ecfdf5; color:#064e3b;"
                        elif hu_v < ai_v:  # red for down
                            for i in [idx_hu, idx_dv, idx_dp]:
                                if i is not None:
                                    styles[i] = "background-color:#fee2e2; color:#7f1d1d; font-weight:600;"
                            if idx_ai is not None:
                                styles[idx_ai] = "background-color:#fef2f2; color:#7f1d1d;"

                    # Decision changes ‚Üí amber
                    if ai_dec_col in show_df.columns and human_dec_col in show_df.columns:
                        ai_d = str(row.get(ai_dec_col, "")).strip().lower()
                        hu_d = str(row.get(human_dec_col, "")).strip().lower()
                        if ai_d != "" and hu_d != "" and ai_d != hu_d:
                            for colname in [ai_dec_col, human_dec_col]:
                                j = _idx(colname)
                                if j is not None:
                                    styles[j] = "background-color:#fef9c3; color:#7c2d12; font-weight:600;"

                    # Justification present ‚Üí blue
                    if "justification" in show_df.columns:
                        just = str(row.get("justification", "")).strip()
                        if just:
                            j = _idx("justification")
                            if j is not None:
                                styles[j] = "background-color:#e0f2fe; color:#0c4a6e;"

                    return styles

                styled = show_df.style.apply(_color_row, axis=1) \
                                    .format({value_ai: "{:,.0f}", value_hu: "{:,.0f}", "Œî_value": "{:,.0f}", "Œî_%": "{:.2f}%"})
                st.dataframe(styled, use_container_width=True, hide_index=True)
        else:
            st.info("To show the colorful Human-Changes table, ensure value columns exist (e.g., ai_adjusted/fmv and human_value).")

        

    
    # ‚îÄ‚îÄ Export for Retraining
    st.markdown("### üíæ Save & Export for Training")
    # Lightweight export view for training: keep keys + AI/Human value/decisions if present
    train_cols_base = ["application_id", "asset_id", "asset_type", "city"]
    ai_val_col = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
    hu_val_col = _first_present(edited, ["human_value", "reviewed_value", "final_value"])
    keep_cols = [c for c in train_cols_base if c in edited.columns] + \
                [c for c in [ai_dec_col, human_dec_col, ai_val_col, hu_val_col, "confidence", "loan_amount", "justification"] if c in edited.columns]
    export_df = edited[keep_cols].copy() if keep_cols else edited.copy()

    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
    out_path = os.path.join(RUNS_DIR, f"reviewed_appraisal.{ts}.csv")

    cE1, cE2 = st.columns([1.2, 1])
    with cE1:
        st.text_input("Will save to (server path)", out_path, label_visibility="collapsed")
    with cE2:
        st.download_button(
            "‚¨áÔ∏è Download Human vs AI CSV",
            export_df.to_csv(index=False).encode("utf-8-sig"),
            file_name=os.path.basename(out_path),
            mime="text/csv",
            key="dl_reviewed_appraisal_stageE"
        )

    if st.button("üíæ Save Human Feedback (server)", key="btn_save_feedback"):
        try:
            export_df.to_csv(out_path, index=False, encoding="utf-8-sig")
            st.success(f"‚úÖ Saved human-reviewed data ‚Üí `{out_path}`")
        except Exception as e:
            st.error(f"Save failed: {e}")


# # ============================================================
# # ‚úÖ STAGE F FOOTER FUNCTION ‚Äî must be defined BEFORE Stage F
# # ============================================================

def render_stage_f_footer(
    new_m, prod_m, RUNS_DIR, model_path, report,
    df_train=None, yte=None, y_pred_new=None
):
    import streamlit as st
    import pandas as pd
    import numpy as np
    import os, json, glob, shutil, zipfile
    from datetime import datetime, timezone
    import plotly.express as px

    st.markdown("## üß≠ Executive Model Evaluation Dashboard (Stage F)")

    # -----------------------------------------
    # ‚úÖ Compute deltas
    # -----------------------------------------
    if prod_m:
        delta_mae = (prod_m["MAE"] - new_m["MAE"]) / prod_m["MAE"] * 100
        delta_rmse = (prod_m["RMSE"] - new_m["RMSE"]) / prod_m["RMSE"] * 100
        delta_mape = (prod_m["MAPE%"] - new_m["MAPE%"]) / prod_m["MAPE%"] * 100
        delta_r2  = (new_m["R2"] - prod_m["R2"]) * 100

        improved = {
            "MAE": delta_mae > 0,
            "RMSE": delta_rmse > 0,
            "MAPE%": delta_mape > 0,
            "R2": delta_r2 > 0
        }

        # Main headline message
        headline = f"‚úÖ The new model outperforms the production model by **{delta_mae:+.1f}% MAE** and **{delta_r2:+.2f} R¬≤ points**."
        headline_color = "#D1FAE5"  # greenish
        reward_phrase = "‚úî This is a strong improvement and beneficial for production use."
    else:
        headline = "üü¢ First model trained ‚Äî this will become the initial production baseline."
        headline_color = "#DBEAFE"  # blueish
        reward_phrase = "‚úî You can safely promote this model."

    # -----------------------------------------
    # ‚úÖ WHAT ‚Äî Big one-sentence discovery
    # -----------------------------------------
    st.markdown(
        f"""
        <div style="
            padding: 18px;
            border-radius: 12px;
            background-color: {headline_color};
            font-size: 1.3rem;
            font-weight: 600;
        ">
        {headline}
        </div>
        """,
        unsafe_allow_html=True
    )

    # -----------------------------------------
    # ‚úÖ SO WHAT ‚Äî Why does this matter?
    # -----------------------------------------
    st.markdown("### üßê SO WHAT ‚Äî Why does this matter?")
    if prod_m:
        st.write(
            f"""
            The new model shows measurable improvements across key financial and ML metrics:

            - **MAE** (Average absolute error) improved by **{delta_mae:+.1f}%**  
            - **RMSE** (Hard penalties on large mismatches) improved by **{delta_rmse:+.1f}%**  
            - **MAPE** (Percentage error relative to asset value) improved by **{delta_mape:+.1f}%**  
            - **R¬≤** (How well the model explains variance) improved by **{delta_r2:+.2f} points**  

            These metrics together mean:
            - ‚úÖ More accurate valuation predictions  
            - ‚úÖ Smaller high-error outliers  
            - ‚úÖ Better stability with fewer ‚Äúshocks‚Äù  
            - ‚úÖ Higher confidence for underwriting, credit, and collateral decisions  
            """
        )
    else:
        st.info(
            """
            Since there is **no existing production model**, this trained model becomes 
            the best available baseline for your valuation pipeline.
            """
        )

    # -----------------------------------------
    # ‚úÖ KEY COMPARISON TABLE
    # -----------------------------------------
    st.markdown("### üìä Metric Comparison (New vs Production)")

    if prod_m:
        df_cmp = pd.DataFrame([
            ["MAE",   f"{new_m['MAE']:,.0f}",   f"{prod_m['MAE']:,.0f}",   f"{delta_mae:+.1f}%",  "Lower is better"],
            ["RMSE",  f"{new_m['RMSE']:,.0f}",  f"{prod_m['RMSE']:,.0f}",  f"{delta_rmse:+.1f}%", "Penalizes large errors"],
            ["MAPE%", f"{new_m['MAPE%']:.2f}%", f"{prod_m['MAPE%']:.2f}%", f"{delta_mape:+.1f}%", "Percent accuracy"],
            ["R¬≤",    f"{new_m['R2']:.3f}",     f"{prod_m['R2']:.3f}",     f"{delta_r2:+.2f}",    "Explained variance"],
        ], columns=["Metric", "New Model", "Production", "Œî (Change)", "Meaning"])
    else:
        df_cmp = pd.DataFrame([
            ["MAE",   f"{new_m['MAE']:,.0f}",   "‚Äî",  "‚Äî", "Lower is better"],
            ["RMSE",  f"{new_m['RMSE']:,.0f}",  "‚Äî",  "‚Äî", "Penalizes large errors"],
            ["MAPE%", f"{new_m['MAPE%']:.2f}%", "‚Äî",  "‚Äî", "Percent accuracy"],
            ["R¬≤",    f"{new_m['R2']:.3f}",     "‚Äî",  "‚Äî", "Explained variance"],
        ], columns=["Metric", "New Model", "Production", "Œî (Change)", "Meaning"])

    st.table(df_cmp)

    # -----------------------------------------
    # ‚úÖ NOW WHAT ‚Äî Recommended Action
    # -----------------------------------------
    st.markdown("### üöÄ NOW WHAT ‚Äî Recommended Next Action")

    if not prod_m or (delta_mae > 0 and delta_r2 > 0):
        st.success(
            f"""
            ### ‚úÖ Recommendation: **Promote the new model to production.**

            {reward_phrase}

            #### Why?
            - It reduces valuation errors.
            - It improves consistency and confidence scores.
            - It captures market variance better (higher R¬≤).
            - It reduces underwriting risk.
            - It generates more stable predictions for credit, risk & collateral workflows.
            """
        )
        promote_ready = True
    else:
        st.warning(
            f"""
            ### ‚ö†Ô∏è Recommendation: **Do NOT promote yet.**

            Some metrics degrade when compared to production.

            #### Before promoting:
            - Tune hyperparameters  
            - Add more diverse training samples  
            - Validate anomalies / outliers  
            - Re-check human_value labels from Stage E  
            """
        )
        promote_ready = False

    # -----------------------------------------
    # ‚úÖ Next Steps Checklist
    # -----------------------------------------
    st.markdown("### ‚úÖ Next Steps Checklist")

    if promote_ready:
        st.markdown(
            """
            ‚úÖ Promote to production  
            ‚úÖ Export ZIP bundle  
            ‚úÖ Notify Credit / Risk agents  
            ‚úÖ Schedule monitoring in Stage I  
            ‚úÖ Optional: widen training dataset  
            """
        )
    else:
        st.markdown(
            """
            üîÑ Retrain with more data  
            üßπ Clean labeling inconsistencies  
            üîç Inspect outliers via residual plots  
            üîß Try Gradient Boosting or Random Forest  
            """
        )

    # -----------------------------------------
    # ‚úÖ Show Drift Trend (mini chart)
    # -----------------------------------------
    st.markdown("### üìà Performance Trend (MAE & R¬≤ over time)")
    reports = sorted(glob.glob(os.path.join(RUNS_DIR, "training_report_*.json")), reverse=True)[:10]
    trend = []
    for f in reports:
        try:
            with open(f) as jf:
                rep = json.load(jf)
            trend.append({
                "timestamp": rep["timestamp"],
                "MAE": rep["metrics_new"]["MAE"],
                "R2": rep["metrics_new"]["R2"],
            })
        except:
            pass

    if trend:
        df_tr = pd.DataFrame(trend).sort_values("timestamp")
        st.line_chart(df_tr.set_index("timestamp")[["MAE", "R2"]])

    # -----------------------------------------
    # ‚úÖ Promotion Button
    # -----------------------------------------
    st.markdown("### üì§ Promote to Production")

    if st.button("‚úÖ Promote This Model Now"):
        try:
            #prod_dir = "./agents/asset_appraisal/models/production"
            prod_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/production"

            
            os.makedirs(prod_dir, exist_ok=True)

            shutil.copy(model_path, os.path.join(prod_dir, "model.joblib"))
            json.dump(
                {"model_path": model_path, "promoted_at": datetime.now(timezone.utc).isoformat(), "report": report},
                open(os.path.join(prod_dir, "production_meta.json"), "w"),
                indent=2
            )
            st.balloons()
            st.success("‚úÖ Model promoted successfully!")
        except Exception as e:
            st.error(f"‚ùå Promotion failed: {e}")
    



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# F ‚Äî MODEL TRAINING & PROMOTION (A/B with Prod)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tabF:
    import os, json, glob
    from datetime import datetime, timezone
    import numpy as np
    import pandas as pd
    import plotly.graph_objects as go
    import plotly.express as px
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
    from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
    from sklearn.linear_model import LinearRegression
    import joblib
    import shutil
    from pathlib import Path


    st.subheader("üß™ Stage F ‚Äî Model Training & Promotion")
    st.caption("Train or retrain with human feedback, compare against production (A/B), and promote if better.")
    
    
    # -----------------------------------------
    # ‚úÖ HOW TO USE THIS TRAINING STAGE (Dark / Collapsible)
    # -----------------------------------------
    with st.expander("üß≠ How this stage works", expanded=False):
        st.markdown("""
        <div style="
            padding:18px;
            border-radius:12px;
            background:linear-gradient(145deg,#0d1829,#10243d);
            border:1px solid #1e3a5f;
            color:#e2e8f0;
            font-size:1.05rem;
            line-height:1.55;
        ">
        <b>üìò How this stage works:</b><br><br>
        You are now in <b>Stage F</b>, where your appraisal model is trained, compared, and prepared for production.
        <br>This stage takes the <b>human-reviewed values</b> produced in Stage E and builds a model that predicts
        future valuations with better accuracy.
        <br><br><b>‚úÖ Follow these steps:</b><br>
        <b>1Ô∏è‚É£ Load Training Data</b><br>‚Ä¢ Auto-detect latest <code>reviewed_appraisal*.csv</code> from Stage E.<br>
        ‚Ä¢ Or upload CSV with <code>human_value</code> labels.
        <br><br><b>2Ô∏è‚É£ Select Features</b><br>‚Ä¢ Numeric columns auto-selected; leakage columns excluded.
        <br><br><b>3Ô∏è‚É£ Choose a Model</b><br>Select GradientBoosting, RandomForest, or LinearRegression (fast cycle).
        <br><br><b>4Ô∏è‚É£ Train & Compare</b><br>‚Ä¢ Train on data ‚Üí evaluate holdout ‚Üí A/B compare if baseline exists.
        <br><br><b>5Ô∏è‚É£ Review Metrics & Insights</b><br>‚Ä¢ Actual vs predicted charts, residuals, importance, summary.
        <br><br><b>6Ô∏è‚É£ Save / Promote / Export</b><br>‚Ä¢ Promote best model ‚Üí Stage G ZIP bundle.
        <br><br><b>üéØ Goal:</b> Produce a model that‚Äôs <b>more accurate, more stable, and more explainable</b>.
        </div>
        """, unsafe_allow_html=True)

    # # -----------------------------------------
    # # ‚úÖ HOW TO USE THIS TRAINING STAGE
    # # -----------------------------------------
    # st.markdown("""
    # <div style="
    #     padding: 18px;
    #     border-radius: 12px;
    #     background-color: #EFF6FF;
    #     border-left: 6px solid #2563EB;
    #     font-size: 1.05rem;
    # ">
    # <b>üìò How this stage works:</b><br><br>

    # You are now in <b>Stage F</b>, where your appraisal model is trained, compared, and prepared for production.

    # This stage takes the <b>human-reviewed values</b> produced in Stage E and builds a model that predicts future valuations with better accuracy.

    # <br><br>

    # <b>‚úÖ Follow these steps:</b><br>

    # <b>1Ô∏è‚É£ Load Training Data</b><br>
    # ‚Ä¢ The system auto-detects your latest <code>reviewed_appraisal*.csv</code> from Stage E.  
    # ‚Ä¢ If you prefer, upload a new CSV containing <code>human_value</code> labels.  

    # <br>

    # <b>2Ô∏è‚É£ Select Features</b><br>
    # ‚Ä¢ Numeric and relevant features are automatically selected.  
    # ‚Ä¢ ID columns and leakage columns (asset_id, ai_adjusted, etc.) are excluded.

    # <br>

    # <b>3Ô∏è‚É£ Choose a Model</b><br>
    # Select an algorithm (GradientBoosting, RandomForest, LinearRegression).  
    # The system will auto-tune nothing‚Äîthis is a fast-iteration training cycle.

    # <br>

    # <b>4Ô∏è‚É£ Train & Compare</b><br>
    # ‚Ä¢ The model trains on your data.  
    # ‚Ä¢ A holdout test set evaluates performance.  
    # ‚Ä¢ If a production model exists, an A/B comparison is displayed.  

    # <br>

    # <b>5Ô∏è‚É£ Review Metrics & Insights</b><br>
    # ‚Ä¢ Actual vs predicted charts  
    # ‚Ä¢ Residual distributions  
    # ‚Ä¢ Feature importance analysis  
    # ‚Ä¢ Executive summary (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)  
    # ‚Ä¢ AI recommendation (promote / retrain)

    # <br>

    # <b>6Ô∏è‚É£ Save, Promote or Export</b><br>
    # ‚Ä¢ Save trained models  
    # ‚Ä¢ Promote to production (Stage G uses it for ZIP packaging)  
    # ‚Ä¢ Export full ZIP bundles for deployment (AWS S3, Swift, GitHub)

    # <br><br>

    # <b>üéØ Goal of Stage F:</b><br>
    # Produce a model that is <b>more accurate, more stable, and more explainable</b> than your current production baseline ‚Äî and ready for deployment in Stage G.
    # </div>
    # """, unsafe_allow_html=True)

    

    # ---------- helpers ----------
    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    def _rmse(y_true, y_pred):
        return float(np.sqrt(mean_squared_error(y_true, y_pred)))

    def _mape(y_true, y_pred):
        y_true = np.asarray(y_true, dtype=float)
        y_pred = np.asarray(y_pred, dtype=float)
        mask = (y_true != 0) & np.isfinite(y_true) & np.isfinite(y_pred)
        if not mask.any():
            return float("nan")
        return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0)

    def _safe_len_df(x):
        return (0 if not isinstance(x, pd.DataFrame) else len(x))

    # ---------- diagnostics (always visible) ----------
    st.markdown("#### üîé Data availability (snapshots)")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("decision_df",  _safe_len_df(ss.get("asset_decision_df")))
    c2.metric("policy_df",    _safe_len_df(ss.get("asset_policy_df")))
    c3.metric("verified_df",  _safe_len_df(ss.get("asset_verified_df")))
    c4.metric("ai_df",        _safe_len_df(ss.get("asset_ai_df")))

    with st.expander("Load demo portfolio (if earlier stages not run)"):
        if st.button("Load demo portfolio (10 rows)", key="btn_demo_portfolio"):
            rng = np.random.default_rng(42)
            demo = pd.DataFrame({
                "application_id": [f"APP_{i:04d}" for i in range(10)],
                "asset_id":      [f"A{i:04d}" for i in range(10)],
                "asset_type":    rng.choice(["House","Apartment","Car","Land"], 10),
                "city":          rng.choice(["HCMC","Hanoi","Da Nang","Hue"], 10),
                "market_value":  rng.integers(80_000, 800_000, 10),
                "ai_adjusted":   rng.integers(75_000, 820_000, 10),
                "loan_amount":   rng.integers(30_000, 500_000, 10),
                "confidence":    rng.integers(60, 98, 10),
                "condition_score": rng.uniform(0.6, 1.0, 10).round(3),
                "legal_penalty":   rng.uniform(0.95, 1.0, 10).round(3),
                "human_value":   rng.integers(75_000, 820_000, 10),
            })
            ss["asset_decision_df"] = demo
            st.success("Demo portfolio loaded into ss['asset_decision_df'].")

    st.divider()

    # ---------- training data source ----------
    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    # Auto-pick latest reviewed CSV from Stage E
    reviewed = sorted([f for f in os.listdir(RUNS_DIR)
                       if f.startswith("reviewed_appraisal") and f.endswith(".csv")], reverse=True)
    df_train = None
    auto_path = None
    if reviewed:
        auto_path = os.path.join(RUNS_DIR, reviewed[0])
        try:
            df_train = pd.read_csv(auto_path)
        except Exception as e:
            st.warning(f"Could not read `{auto_path}`: {e}")

    st.markdown("#### üì• Training dataset")
    colU1, colU2 = st.columns([1.4, 1])
    with colU1:
        st.text_input("Auto-detected Stage E file", value=(auto_path or "‚Äî"), disabled=True)
    with colU2:
        up = st.file_uploader("Or upload CSV with human_value", type=["csv"], key="train_csv_upload")

    if up is not None:
        try:
            df_train = pd.read_csv(up)
            st.success(f"Loaded uploaded CSV ({len(df_train)} rows).")
        except Exception as e:
            st.error(f"Upload read failed: {e}")

    if df_train is None or df_train.empty:
        st.warning("‚ö†Ô∏è No training data available. Use Stage E to export `reviewed_appraisal*.csv` or upload a CSV above.")
        st.stop()

    st.markdown(f"**Using training rows:** {len(df_train):,}")
    st.dataframe(df_train.head(20), use_container_width=True)

    # ---------- feature building ----------
    st.markdown("#### üß± Feature selection")
    target_col = "human_value"
    if target_col not in df_train.columns:
        st.error("CSV must include a 'human_value' column (target).")
        st.stop()

    # Exclude obvious leak/IDs/targets from X
    drop_cols = {
        target_col, "fmv", "ai_adjusted",  # avoid leakage; AI numbers used only for comparison
        "ai_decision", "human_decision", "decision", "final_decision",
        "justification", "reviewed_value", "final_value",
        "application_id", "asset_id", "asset_type", "city"
    }
    num_cols = [c for c in df_train.columns
                if c not in drop_cols and pd.api.types.is_numeric_dtype(df_train[c])]

    if not num_cols:
        st.error("No numeric features left after filtering. Please include numeric columns for training.")
        st.stop()

    dataset_rows = len(df_train)
    numeric_feature_count = len(num_cols)

    X = df_train[num_cols].copy()
    y = pd.to_numeric(df_train[target_col], errors="coerce")

    # Drop rows with missing target
    mask = pd.notna(y)
    X, y = X.loc[mask], y.loc[mask]

    st.markdown("#### ‚≠ê Recommended models (EQACh signal)")
    st.caption(f"{dataset_rows:,} labeled assets ¬∑ {numeric_feature_count} numeric features")

    def score_asset_model(name: str) -> tuple[int, str]:
        """Coarse scoring so operators see why a regressor fits their data."""
        reason = ""
        score = 0

        if name == "GradientBoostingRegressor":
            score = 5 if dataset_rows > 5_000 else 3
            reason = "Captures nonlinear patterns and handles wide appraisal signals."
        elif name == "RandomForestRegressor":
            score = 4 if 1_000 < dataset_rows <= 10_000 else 2
            reason = "Stable when you have mixed-quality human feedback and want robustness."
        elif name == "LinearRegression":
            score = 3 if dataset_rows <= 2_000 else 1
            reason = "Fast, fully explainable baseline for regulators or smoke tests."

        if numeric_feature_count >= 8 and name != "LinearRegression":
            score += 1
            reason += " Extra numeric features boost tree ensembles."

        return score, reason

    model_profiles = []
    for candidate in ["GradientBoostingRegressor", "RandomForestRegressor", "LinearRegression"]:
        score, reason = score_asset_model(candidate)
        model_profiles.append(
            {
                "name": candidate,
                "score": score,
                "tagline": {
                    "GradientBoostingRegressor": "Enterprise-ready EQACh default",
                    "RandomForestRegressor": "Resilient midsize option",
                    "LinearRegression": "Audit-friendly baseline",
                }[candidate],
                "reason": reason,
            }
        )

    model_profiles.sort(key=lambda x: x["score"], reverse=True)
    rec_cols = st.columns(len(model_profiles))
    for col, profile in zip(rec_cols, model_profiles):
        with col:
            st.markdown(f"**{profile['name']}**")
            st.caption(profile["tagline"])
            st.write(profile["reason"])
            if st.button(f"Use {profile['name']}", key=f"use_asset_{profile['name']}"):
                ss["asset_model_choice"] = profile["name"]

    # Train/Test split
    test_size = st.slider("Holdout size", 10, 40, 20, step=5) / 100.0
    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, random_state=42)

    # ---------- model choice ----------
    st.markdown("#### ü§ñ Choose model")
    model_options = ["GradientBoostingRegressor", "RandomForestRegressor", "LinearRegression"]
    default_choice = ss.get("asset_model_choice", model_profiles[0]["name"])
    if default_choice not in model_options:
        default_choice = model_options[0]

    model_choice = st.selectbox(
        "Select model algorithm",
        model_options,
        index=model_options.index(default_choice)
    )
    ss["asset_model_choice"] = model_choice
    ModelCls = {
        "GradientBoostingRegressor": GradientBoostingRegressor,
        "RandomForestRegressor": RandomForestRegressor,
        "LinearRegression": LinearRegression,
    }[model_choice]

    # ---------- train & compare ----------
    if st.button("üöÄ Train & Compare (A/B vs Production)", key="btn_train_model"):
        # Train new
        new_model = ModelCls().fit(Xtr, ytr)
        y_pred_new = new_model.predict(Xte)

        # Load production baseline if exists
        prod_model_path = "./agents/asset_appraisal/models/production/model.joblib"
        prod_exists = os.path.exists(prod_model_path)
        y_pred_prod = None
        if prod_exists:
            try:
                prod_model = joblib.load(prod_model_path)
                # guard: try only if shapes align
                y_pred_prod = prod_model.predict(Xte)
            except Exception as e:
                st.warning(f"Production model failed to score holdout: {e}")

        # Metrics
        def _metrics(y_true, y_pred):
            return {
                "MAE": float(mean_absolute_error(y_true, y_pred)),
                "RMSE": _rmse(y_true, y_pred),
                "MAPE%": _mape(y_true, y_pred),
                "R2": float(r2_score(y_true, y_pred)),
            }

        new_m = _metrics(yte, y_pred_new)
        prod_m = _metrics(yte, y_pred_prod) if y_pred_prod is not None else None

        # ===== Dashboard: KPIs & deltas =====
        st.markdown("### üìä A/B Metrics (Holdout)")
        k1, k2, k3, k4, k5 = st.columns(5)
        with k1:
            st.metric("New MAE", f"{new_m['MAE']:,.0f}",
                      delta=(f"{(new_m['MAE'] - prod_m['MAE']):+.0f}" if prod_m else None))
        with k2:
            st.metric("New RMSE", f"{new_m['RMSE']:,.0f}",
                      delta=(f"{(new_m['RMSE'] - prod_m['RMSE']):+.0f}" if prod_m else None))
        with k3:
            st.metric("New MAPE", f"{new_m['MAPE%']:.2f}%",
                      delta=(f"{(new_m['MAPE%'] - prod_m['MAPE%']):+.2f}%" if prod_m else None))
        with k4:
            st.metric("New R¬≤", f"{new_m['R2']:.3f}",
                      delta=(f"{(new_m['R2'] - prod_m['R2']):+.3f}" if prod_m else None))
        with k5:
            st.metric("Test rows", f"{len(yte):,}")

        # ===== Plots: Actual vs Pred, Residuals =====
        plot_df = pd.DataFrame({
            "y_true": yte.values,
            "y_pred_new": y_pred_new,
            "y_pred_prod": (y_pred_prod if y_pred_prod is not None else np.full_like(y_pred_new, np.nan))
        })

        # Actual vs Pred overlay
        fig_scatter = go.Figure()
        fig_scatter.add_trace(go.Scatter(
            x=plot_df["y_true"], y=plot_df["y_pred_new"],
            mode="markers", name="New", opacity=0.7
        ))
        if y_pred_prod is not None:
            fig_scatter.add_trace(go.Scatter(
                x=plot_df["y_true"], y=plot_df["y_pred_prod"],
                mode="markers", name="Production", opacity=0.6
            ))
        # diagonal reference
        minv, maxv = np.nanmin(plot_df[["y_true","y_pred_new","y_pred_prod"]].values), np.nanmax(plot_df[["y_true","y_pred_new","y_pred_prod"]].values)
        fig_scatter.add_trace(go.Scatter(x=[minv, maxv], y=[minv, maxv], mode="lines", name="Ideal", line=dict(dash="dash")))
        fig_scatter.update_layout(title="Actual vs Predicted (Holdout)", xaxis_title="Actual", yaxis_title="Predicted")
        st.plotly_chart(fig_scatter, use_container_width=True)

        # Residuals hist
        plot_df["res_new"]  = plot_df["y_true"] - plot_df["y_pred_new"]
        if y_pred_prod is not None:
            plot_df["res_prod"] = plot_df["y_true"] - plot_df["y_pred_prod"]

        fig_res = go.Figure()
        fig_res.add_trace(go.Histogram(x=plot_df["res_new"], name="New", opacity=0.7))
        if y_pred_prod is not None:
            fig_res.add_trace(go.Histogram(x=plot_df["res_prod"], name="Production", opacity=0.6))
        fig_res.update_layout(barmode="overlay", title="Residuals Distribution (Actual - Predicted)")
        fig_res.update_traces(nbinsx=40)
        st.plotly_chart(fig_res, use_container_width=True)

        # ===== Feature importance / coefficients =====
        st.markdown("### üß† Feature Importance / Coefficients")
        if hasattr(new_model, "feature_importances_"):
            imp = pd.DataFrame({
                "feature": num_cols,
                "importance": new_model.feature_importances_
            }).sort_values("importance", ascending=False)
            st.bar_chart(imp.set_index("feature"))
        elif hasattr(new_model, "coef_"):
            coef = pd.DataFrame({
                "feature": num_cols,
                "coef": np.ravel(new_model.coef_)
            }).sort_values("coef", key=np.abs, ascending=False)
            st.bar_chart(coef.set_index("feature"))
        else:
            st.info("This model does not expose importances/coefficients.")

        # ===== Persist artifacts =====
        #trained_dir = "./agents/asset_appraisal/models/trained"
        trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/trained"
        
        os.makedirs(trained_dir, exist_ok=True)
        ts = _ts()
        model_path = os.path.join(trained_dir, f"{model_choice}_asset_{ts}.joblib")
        joblib.dump(new_model, model_path)

        preds_csv = os.path.join(RUNS_DIR, f"training_preds_{ts}.csv")
        plot_df.to_csv(preds_csv, index=False)

        report = {
            "timestamp": ts,
            "model_choice": model_choice,
            "trained_model_path": model_path,
            "features": num_cols,
            "metrics_new": new_m,
            "metrics_prod": prod_m,
            "holdout_rows": int(len(yte)),
            "source_file": (auto_path or "uploaded"),
            "preds_csv": preds_csv,
        }
        report_path = os.path.join(RUNS_DIR, f"training_report_{ts}.json")
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2)

        st.success(f"‚úÖ Trained model saved ‚Üí `{model_path}`")
        st.caption(f"Report ‚Üí `{report_path}` | Predictions ‚Üí `{preds_csv}`")

        # Download helpers
        cdl1, cdl2 = st.columns(2)
        with cdl1:
            st.download_button("‚¨áÔ∏è Download training report (JSON)",
                               data=json.dumps(report, indent=2).encode("utf-8"),
                               file_name=os.path.basename(report_path),
                               mime="application/json")
        with cdl2:
            st.download_button("‚¨áÔ∏è Download holdout predictions (CSV)",
                               data=plot_df.to_csv(index=False).encode("utf-8-sig"),
                               file_name=os.path.basename(preds_csv),
                               mime="text/csv")
        # ‚úÖ ‚úÖ ‚úÖ CALL DASHBOARD ‚Äî FIX FOR YOUR ISSUE
        render_stage_f_footer(
            new_m=new_m,
            prod_m=prod_m,
            RUNS_DIR=RUNS_DIR,
            model_path=model_path,
            report=report,
            df_train=df_train,
            yte=yte,
            y_pred_new=y_pred_new
        )
        
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚úÖ Helper functions required by Stage G
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def is_nonempty_df(x):
    import pandas as pd
    return isinstance(x, pd.DataFrame) and not x.empty

def first_nonempty_df(*candidates):
    for c in candidates:
        if is_nonempty_df(c):
            return c
    return None


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# G ‚Äî DEPLOYMENT & DISTRIBUTION STAGE
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tabG:
    import os, json, hashlib, zipfile, requests
    from datetime import datetime, timezone
    from pathlib import Path
    import streamlit as st

    st.title("üöÄ Stage G ‚Äî Deployment & Distribution")
    st.caption("Package ‚Üí Verify ‚Üí Upload ‚Üí Release ‚Üí Distribute to Credit / Legal / Risk units.")
    EXPORT_DIR = Path("./exports")
    EXPORT_DIR.mkdir(exist_ok=True)

    st.markdown("## üì¶ Build Project Bundle (Model + Reports + Artifacts)")
    build_zip_name = f"asset_project_bundle_{_ts()}.zip"
    build_zip_path = EXPORT_DIR / build_zip_name

    if st.button("‚¨áÔ∏è Build & Download Project ZIP", key="btn_build_stage_g_zip"):
        try:
            with zipfile.ZipFile(build_zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
                for root, dirs, files in os.walk(RUNS_DIR):
                    for f in files:
                        full = os.path.join(root, f)
                        arc = os.path.relpath(full, RUNS_DIR)
                        zf.write(full, f"runs/{arc}")

                if os.path.exists("./agents/asset_appraisal/models/production"):
                    for root, dirs, files in os.walk("./agents/asset_appraisal/models/production"):
                        for f in files:
                            full = os.path.join(root, f)
                            zf.write(full, f"production_models/{f}")

                if os.path.exists("./agents/asset_appraisal/models/trained"):
                    for root, dirs, files in os.walk("./agents/asset_appraisal/models/trained"):
                        for f in files:
                            full = os.path.join(root, f)
                            zf.write(full, f"trained_models/{f}")

                latest_report = sorted(Path(RUNS_DIR).glob("training_report_*.json"), reverse=True)
                if latest_report:
                    zf.write(latest_report[0], "training_report.json")

            st.success(f"‚úÖ Exported: {build_zip_name}")
            with open(build_zip_path, "rb") as fp:
                st.download_button(
                    "‚¨áÔ∏è Download ZIP Now",
                    data=fp,
                    file_name=build_zip_name,
                    mime="application/zip",
                    use_container_width=True,
                    key="btn_download_stage_g_zip",
                )
        except Exception as e:
            st.error(f"‚ùå ZIP creation failed: {e}")

    # ---------------------------------------------
    # 1) Load the latest ZIP bundle created in Stage F
    # ---------------------------------------------
    st.markdown("## üì¶ 1) Project Package (Generated in Stage F)")

    # Find ZIP files
    zip_files = sorted(EXPORT_DIR.glob("asset_project_bundle_*.zip"), reverse=True)
    
    if not zip_files:
        st.warning("‚ö†Ô∏è No project ZIP found. Run Stage F and export a bundle first.")
        st.stop()

    latest_zip = zip_files[0]

    st.success(f"‚úÖ Latest bundle detected: `{latest_zip.name}`")
    st.caption(f"Size: **{latest_zip.stat().st_size/1e6:.2f} MB**")
    
    # # Show preview
    # with zipfile.ZipFile(latest_zip, "r") as z:
    #     preview = z.namelist()[:20]
    #     st.code("\n".join(preview), language="text")
    

    # ---------------------------------------------
    # 2) Integrity Check (SHA256)
    # ---------------------------------------------
    st.markdown("## ‚úÖ 2) File Integrity Check (SHA256)")

    sha256 = hashlib.sha256(latest_zip.read_bytes()).hexdigest()
    st.code(sha256)

    checksum_path = latest_zip.with_suffix(".sha256")
    checksum_path.write_text(sha256)
    st.caption(f"Checksum written ‚Üí `{checksum_path.name}`")

    # Simple signature
    sig_path = latest_zip.with_suffix(".sig")
    sig_path.write_text(f"AI-Agent-Hub signed @ {datetime.now(timezone.utc).isoformat()}")
    st.caption(f"Signature stub ‚Üí `{sig_path.name}`")


    # ---------------------------------------------
    # 3) Upload Targets (S3 / Swift / GitHub Release)
    # ---------------------------------------------
    st.markdown("## ‚òÅÔ∏è 3) Upload / Publish Package")

    dest = st.radio(
        "Choose destination",
        ["AWS S3", "OpenStack Swift", "GitHub Release"],
        horizontal=True
    )

    if dest == "AWS S3":
        st.info("Upload to S3 (requires AWS credentials)")
        bucket = st.text_input("Bucket Name", "my-ai-models")
        key = st.text_input("Object Key", latest_zip.name)

        if st.button("‚¨ÜÔ∏è Upload to S3"):
            try:
                import boto3
                s3 = boto3.client("s3")
                s3.upload_file(str(latest_zip), bucket, key)
                st.success(f"‚úÖ Uploaded to `s3://{bucket}/{key}`")
            except Exception as e:
                st.error(f"‚ùå Failed: {e}")

    elif dest == "OpenStack Swift":
        st.info("Upload to Swift (requires Swift credentials)")
        container = st.text_input("Container Name", "ai-models")
        if st.button("‚¨ÜÔ∏è Upload to Swift"):
            try:
                from swiftclient.service import SwiftService, SwiftUploadObject
                with SwiftService() as swift:
                    swift.upload(container, [SwiftUploadObject(str(latest_zip))])
                st.success(f"‚úÖ Uploaded to Swift container `{container}`")
            except Exception as e:
                st.error(f"‚ùå Failed: {e}")

    elif dest == "GitHub Release":
        st.info("Publish as a GitHub release asset")
        repo = st.text_input("Repo (owner/repo)", "RackspaceAI/asset-appraisal-agent")
        token = st.text_input("GitHub Personal Access Token", type="password")
        tag = datetime.now().strftime("v%Y%m%d-%H%M%S")

        if st.button("‚¨ÜÔ∏è Publish Release on GitHub"):
            try:
                headers = {
                    "Authorization": f"token {token}",
                    "Accept": "application/vnd.github+json",
                }

                # Create release
                r = requests.post(
                    f"https://api.github.com/repos/{repo}/releases",
                    headers=headers,
                    json={"tag_name": tag, "name": f"Release {tag}", 
                          "body": "Automated export from Stage G"}
                )
                r.raise_for_status()

                upload_url = r.json()["upload_url"].split("{")[0]

                # Upload asset
                with open(latest_zip, "rb") as f:
                    ur = requests.post(
                        f"{upload_url}?name={latest_zip.name}",
                        headers={**headers, "Content-Type": "application/zip"},
                        data=f,
                    )
                ur.raise_for_status()

                st.success(f"‚úÖ GitHub Release `{tag}` published successfully!")
            except Exception as e:
                st.error(f"‚ùå Failed: {e}")


    # ---------------------------------------------
    # 4) Deployment Audit Log
    # ---------------------------------------------
    st.markdown("## üßæ 4) Deployment Audit Log")

    audit = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "export_file": latest_zip.name,
        "checksum": sha256,
        "target": dest,
    }

    audit_path = EXPORT_DIR / "deployment_audit.jsonl"
    with open(audit_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(audit) + "\n")

    st.success(f"Audit record added ‚Üí `{audit_path.name}`")


    # ---------------------------------------------
    # 5) Next Steps Checklist
    # ---------------------------------------------
    st.markdown("## ‚úÖ 5) Next Steps for DevOps / IT")

    st.markdown("""
    ### ‚úî For Credit Underwriting
    - Import CSV assets into the Credit Appraisal Agent  
    - Validate LTV, confidence, breaches  
    - Promote selected assets for loan approval  

    ### ‚úî For Legal & Compliance
    - Use verification subset (ownership, encumbrances)  
    - Run through Legal Verification Agent  
    - Flag encumbrances & fraud paths  

    ### ‚úî For Risk Management
    - Use realizable_value, condition_score, legal_penalty  
    - Re-run LTV stress tests  
    - Update risk dashboards monthly  

    ### ‚úî For DevOps / Platform Teams
    - Push ZIP to GitHub / Swift / S3  
    - Deploy production model into RunAI / SageMaker / OpenStack MLOps  
    - Update production_meta.json  
    """)

    st.info("Stage G is complete ‚Äî continue to Stage H for Inter-Department Handoff.")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚úÖ STAGE H ‚Äî Executive Dashboard + Handoff Export
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

with tabH:
    import os, json, zipfile
    from pathlib import Path
    from datetime import datetime, timezone
    import pandas as pd
    import numpy as np
    import streamlit as st
    import plotly.express as px
    import plotly.graph_objects as go

    st.markdown("## üß≠ Stage H ‚Äî Unified Portfolio, Insights & Handoff Export")
    st.caption("Executive summary ‚Ä¢ Asset insights ‚Ä¢ Fraud/risk signals ‚Ä¢ Department deliverables")


    
    # ---------------------------------------------------------
    # ‚úÖ Required outputs from earlier stages ‚Äî MUST COME FIRST
    # ---------------------------------------------------------
    ai_df        = st.session_state.get("asset_ai_df")
    policy_df    = st.session_state.get("asset_policy_df")
    decision_df  = st.session_state.get("asset_decision_df")

    missing = []

    if ai_df is None or ai_df.empty:
        missing.append("Stage C (valuation)")
    if decision_df is None or decision_df.empty:
        missing.append("Stage D (risk & decision)")

    if missing:
        st.error("‚ö†Ô∏è Missing required data: " + ", ".join(missing))
        st.info("Please run the missing stages before returning to Stage H.")
        st.stop()

    # ‚úÖ Only now is dfv allowed to be created
    dfv = decision_df.copy()


    # ---------------------------------------------------------
    # ‚úÖ STATUS LABEL (Validated / Risky / Fraud)
    # ---------------------------------------------------------
    def label_row(r):
        if r.get("fraud_flag") in [True, "True", 1]:
            return "FRAUD"
        if r.get("encumbrance_flag") in [True, "True", 1]:
            return "ENCUMBERED"
        if str(r.get("decision", "")).lower() == "reject":
            return "RISKY"
        if str(r.get("policy_breaches", "")).strip():
            return "RISKY"
        return "VALIDATED"

    dfv["status"] = dfv.apply(label_row, axis=1)

    # ---------------------------------------------------------
    # ‚úÖ EXECUTIVE SUMMARY METRICS
    # ---------------------------------------------------------
    st.markdown("### üìä Executive Summary")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Assets", len(dfv))
    with col2:
        st.metric("Validated", (dfv["status"] == "VALIDATED").sum())
    with col3:
        st.metric("Risky", (dfv["status"] == "RISKY").sum())
    with col4:
        st.metric("Fraud / Encumbered", (dfv["status"].isin(["FRAUD","ENCUMBERED"])).sum())

    # ---------------------------------------------------------
    # ‚úÖ HEATMAP ‚Äî Asset Risk & Fraud Signals
    # ---------------------------------------------------------
    st.markdown("### üî• Fraud / Risk Heatmap")
    
    try:
        hm = dfv[["confidence", "ltv_ai"]].copy()
        hm = hm.dropna()

        fig_hm = px.density_heatmap(
            hm, x="confidence", y="ltv_ai",
            nbinsx=30, nbinsy=30,
            color_continuous_scale="YlOrRd",
            title="Fraud/Anomaly Density ‚Äî (Low confidence + High LTV = Hot Zones)"
        )
        st.plotly_chart(fig_hm, use_container_width=True)
    except Exception:
        st.info("Heatmap unavailable until confidence / LTV data is complete.")

    # ---------------------------------------------------------
    # ‚úÖ MARKET INSIGHTS ‚Äî CITY LEVEL DISTRIBUTION
    # ---------------------------------------------------------
    st.markdown("### üåç Asset Distribution by City")

    if "city" in dfv.columns:
        fig_city = px.histogram(
            dfv, x="city", color="status",
            title="Asset Count per City by Status",
            barmode="group"
        )
        st.plotly_chart(fig_city, use_container_width=True)

    # ---------------------------------------------------------
    # ‚úÖ VALUE INSIGHTS ‚Äî Realizable Value Curve
    # ---------------------------------------------------------
    st.markdown("### üí∞ Value Distribution ‚Äî FMV vs Realizable Value")

    if "realizable_value" in dfv.columns:
        fig_val = go.Figure()
        fig_val.add_trace(go.Violin(y=dfv["fmv"], name="FMV", box_visible=True))
        fig_val.add_trace(go.Violin(y=dfv["realizable_value"], name="Realizable", box_visible=True))
        st.plotly_chart(fig_val, use_container_width=True)

    # ---------------------------------------------------------
    # ‚úÖ FULL PORTFOLIO TABLE
    # ---------------------------------------------------------
    st.markdown("### üìÇ Unified Portfolio (with status)")
    st.dataframe(dfv, use_container_width=True)

    # ---------------------------------------------------------
    # ‚úÖ DEPARTMENT HANDOFF EXPORTS (bulletproof)
    # ---------------------------------------------------------
    st.markdown("## üè¶ Department Handoff Packages")
    st.caption("Each team receives only what they need. Clear, simple, compliant.")

    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    HANDOFF_DIR = Path("./handoff")
    ZIP_DIR      = HANDOFF_DIR / "zips"
    HANDOFF_DIR.mkdir(exist_ok=True)
    ZIP_DIR.mkdir(exist_ok=True)

    # ---------------------------
    # ‚úÖ CREDIT APPRAISAL EXPORT
    # ---------------------------
    credit_cols = [ 
        "application_id","asset_id","asset_type","city",
        "ai_adjusted","fmv","realizable_value",
        "loan_amount","ltv_ai","ltv_cap",
        "decision","policy_breaches"
    ]
    credit = dfv[[c for c in credit_cols if c in dfv.columns]].copy()
    credit_path = HANDOFF_DIR / f"credit_appraisal_{ts}.csv"
    credit.to_csv(credit_path, index=False)

    # Download button
    with open(credit_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Credit Appraisal CSV", f, file_name=credit_path.name, mime="text/csv")

    # ---------------------------
    # ‚úÖ LEGAL / TITLE EXPORT
    # ---------------------------
    legal_cols = [
        "application_id","asset_id","verified_owner",
        "encumbrance_flag","legal_penalty","condition_score","notes"
    ]
    legal = dfv[[c for c in legal_cols if c in dfv.columns]].copy()
    legal_path = HANDOFF_DIR / f"legal_pack_{ts}.csv"
    legal.to_csv(legal_path, index=False)

    with open(legal_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Legal & Title CSV", f, file_name=legal_path.name, mime="text/csv")

    # ---------------------------
    # ‚úÖ RISK MANAGEMENT EXPORT
    # ---------------------------
    risk_cols = [
        "application_id","asset_id","confidence",
        "ltv_ai","ltv_cap","policy_breaches","decision","status"
    ]
    risk = dfv[[c for c in risk_cols if c in dfv.columns]].copy()
    risk_path = HANDOFF_DIR / f"risk_management_{ts}.csv"
    risk.to_csv(risk_path, index=False)

    with open(risk_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Risk Management CSV", f, file_name=risk_path.name, mime="text/csv")

    # ---------------------------
    # ‚úÖ CUSTOMER SERVICE EXPORT
    # ---------------------------
    cust_cols = [
        "application_id","asset_id","asset_type","city",
        "fmv","ai_adjusted","decision","status","why"
    ]
    cust = dfv[[c for c in cust_cols if c in dfv.columns]].copy()
    cust_path = HANDOFF_DIR / f"customer_service_{ts}.csv"
    cust.to_csv(cust_path, index=False)

    with open(cust_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Customer Service CSV", f, file_name=cust_path.name, mime="text/csv")

    # ---------------------------
    # ‚úÖ PORTFOLIO SUMMARY
    # ---------------------------
    portfolio_path = HANDOFF_DIR / f"portfolio_{ts}.csv"
    dfv.to_csv(portfolio_path, index=False)

    with open(portfolio_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Portfolio Summary CSV", f, file_name=portfolio_path.name, mime="text/csv")

    # ---------------------------
    # ‚úÖ AUDIT RECORD
    # ---------------------------
    audit = {
        "timestamp": ts,
        "rows": len(dfv),
        "status": dfv["status"].value_counts().to_dict(),
        "avg_confidence": float(dfv["confidence"].mean() if "confidence" in dfv else 0.0),
    }
    audit_path = HANDOFF_DIR / f"audit_{ts}.json"
    with open(audit_path, "w") as f:
        json.dump(audit, f, indent=2)

    with open(audit_path, "rb") as f:
        st.download_button("‚¨áÔ∏è Audit Record (JSON)", f, file_name=audit_path.name, mime="application/json")

    # # ---------------------------
    # # ‚úÖ FULL ZIP BUNDLE
    # # ---------------------------
    # zip_path = ZIP_DIR / f"handoff_bundle_{ts}.zip"
    # with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
    #     for fp in [credit_path, legal_path, risk_path, cust_path, portfolio_path, audit_path]:
    #         zf.write(fp, arcname=os.path.basename(fp))

    # with open(zip_path, "rb") as f:
    #     st.download_button("‚¨áÔ∏è Download FULL Handoff ZIP", f,
    #                     file_name=zip_path.name, mime="application/zip",
    #                     use_container_width=True)

    
    
   
    # ---------------------------------------------------------
    # ‚úÖ ZIP bundle
    # ---------------------------------------------------------
    zip_path = ZIP_DIR / f"handoff_bundle_{ts}.zip"
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
        for fp in [credit_path, legal_path, risk_path, cust_path, portfolio_path, audit_path]:
            zf.write(fp, arcname=os.path.basename(fp))

    st.markdown("### üì¶ Download Unified Handoff Bundle")
    with open(zip_path, "rb") as f:
        st.download_button(
            "‚¨áÔ∏è Download Full Handoff ZIP",
            data=f,
            file_name=os.path.basename(zip_path),
            mime="application/zip",
            use_container_width=True
        )


# Legacy macOS blue theme (kept for optional reuse)
LEGACY_ASSET_THEME_SNIPPET = '''
def legacy_asset_theme(theme: str = "dark"):
    import streamlit as st

    st.markdown("""
    <style>
    /* ===============================================
       üåô MACOS BLUE DARK THEME ‚Äî GLOBAL BASE
    =============================================== */
    html, body, [data-testid="stAppViewContainer"] {
        background: radial-gradient(circle at 20% 20%, #0b0f16, #060a12 85%) !important;
        color: #f8fafc !important;
        font-family: "Inter","SF Pro Display","Segoe UI",system-ui,sans-serif !important;
    }

    h1,h2,h3,h4,h5,h6 {
        color: #f8fafc !important;
        font-weight: 700 !important;
        letter-spacing: -0.02em !important;
    }

    p, li, label, span, div {
        color: #e2e8f0 !important;
    }
    small, .stCaption { color: #94a3b8 !important; }

    a, a:link, a:visited { color: #339dff !important; }
    a:hover { color: #60a5fa !important; text-decoration: underline; }

    hr {
        border: none !important;
        height: 1px !important;
        background: linear-gradient(90deg,transparent,#007aff,transparent) !important;
    }

    /* ===============================================
       üß± CONTAINERS & CARDS
    =============================================== */
    .stMarkdown, .stContainer, .stAlert, [class*="stCard"], [class*="block-container"] {
        background: #0f172a !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 12px !important;
        box-shadow: 0 4px 16px rgba(0,0,0,0.5) !important;
    }

    /* ===============================================
       üîò BUTTONS ‚Äî macOS BLUE
    =============================================== */
    button[kind="primary"], .stButton>button, .stDownloadButton>button, .stDownloadButton button {
        background: linear-gradient(180deg,#007aff,#005ecb) !important;
        color: #ffffff !important;
        border: 1px solid #0051b8 !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
        padding: 0.5rem 1rem !important;
        box-shadow: 0 4px 10px rgba(0,122,255,0.35),
                    inset 0 -1px 0 rgba(255,255,255,0.2) !important;
        transition: all 0.25s ease-in-out !important;
    }
    button[kind="primary"]:hover, .stButton>button:hover, .stDownloadButton>button:hover {
        background: linear-gradient(180deg,#339dff,#006ae6) !important;
        box-shadow: 0 4px 14px rgba(0,122,255,0.45) !important;
        transform: translateY(-1px) !important;
    }
    button[kind="primary"]:active, .stButton>button:active, .stDownloadButton>button:active {
        background: linear-gradient(180deg,#004fc4,#0042a8) !important;
        box-shadow: inset 0 2px 6px rgba(0,122,255,0.3) !important;
        transform: translateY(0) !important;
    }
    .stButton button[disabled], .stDownloadButton button[disabled] {
        background: #1e293b !important;
        color: #64748b !important;
        border: 1px solid #334155 !important;
    }

    /* ===============================================
    üß† INPUTS (Text, Select, Number) & FOCUS STATE
    =============================================== */
    .stTextInput>div>div>input,
    .stSelectbox>div>div>div,
    .stNumberInput input {
        background: #111827 !important;
        color: #f8fafc !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 8px !important;
        padding: 6px 10px !important;
        transition: all 0.25s ease;
    }
    .stTextInput>div>div>input:focus,
    .stSelectbox>div>div>div:focus-within,
    .stNumberInput input:focus {
        outline: none !important;
        border-color: #007aff !important;
        box-shadow: 0 0 0 2px rgba(0,122,255,0.4) !important;
    }
    ::placeholder {
        color: #9ca3af !important;
        opacity: 1 !important;
    }
    /* ===============================================
   üéõ DROPDOWN MENUS
    =============================================== */
    [data-baseweb="popover"], [role="listbox"] {
        background: #0f172a !important;
        color: #f8fafc !important;
        border: 1px solid #1e3a8a !important;
        box-shadow: 0 4px 20px rgba(0,0,0,0.6) !important;
    }
    [data-baseweb="menu-item"] {
        background: #0f172a !important;
        color: #f8fafc !important;
    }
    [data-baseweb="menu-item"]:hover {
        background: #1e3a8a !important;
        color: #ffffff !important;
    }
    /* ===============================================
    üß≠ SIDEBAR THEME
    =============================================== */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg,#0d1320,#060a12) !important;
        border-right: 1px solid #1e3a8a !important;
        color: #f8fafc !important;
    }

    
    /* ===============================================
       ‚òëÔ∏è CHECKBOXES / RADIOS / SLIDERS
    =============================================== */
    input[type="checkbox"], input[type="radio"] {
        accent-color: #007aff !important;
    }
    .stSlider [role="slider"] {
        background-color: #007aff !important;
    }

    /* ===============================================
       üóÇÔ∏è TABS
    =============================================== */
    .stTabs [data-baseweb="tab-list"] button {
        color: #e2e8f0 !important;
        background: #111827 !important;
        border: 1px solid #1e293b !important;
        border-radius: 10px !important;
        font-weight: 500 !important;
        margin-right: 4px !important;
    }
    .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {
        background: #007aff !important;
        color: #ffffff !important;
        box-shadow: 0 0 12px rgba(0,122,255,0.4) !important;
    }

    /* ===============================================
       üß≠ EXPANDERS / ACCORDIONS
    =============================================== */
    .streamlit-expanderHeader {
        background: linear-gradient(90deg,#0d284d,#0a1f3a) !important;
        color: #dbeafe !important;
        border: 1px solid #1e3a5f !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
    }
    .streamlit-expanderContent {
        background: #0f172a !important;
        color: #e2e8f0 !important;
        border: 1px solid #1e3a5f !important;
        border-radius: 0 0 8px 8px !important;
    }

    /* ===============================================
       üìä METRIC CARDS (st.metric)
    =============================================== */
    [data-testid="stMetric"] {
        background: linear-gradient(180deg,#0b1220,#101a2c) !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 10px !important;
        box-shadow: inset 0 0 10px rgba(255,255,255,0.03),
                    0 3px 10px rgba(0,0,0,0.6) !important;
        padding: 10px 14px !important;
        text-align: center !important;
    }
    div[data-testid="stMetricLabel"] {
        color: #94a3b8 !important;
        font-size: 0.85rem !important;
        font-weight: 500 !important;
    }
    div[data-testid="stMetricValue"] {
        color: #ffffff !important;
        font-size: 1.3rem !important;
        font-weight: 600 !important;
    }

    /* ===============================================
       üìä METRIC COMPARISON TABLE ‚Äî FINAL
    =============================================== */
    [data-testid="stDataFrame"] {
        background: radial-gradient(circle at 50% 50%, #0b1220, #060a12 90%) !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 12px !important;
        box-shadow:
            0 0 14px rgba(0,0,0,0.6) inset,
            0 4px 18px rgba(0,0,0,0.7),
            0 0 12px rgba(0,122,255,0.15) !important;
        margin-top: 12px !important;
        padding: 8px !important;
    }
    [data-testid="stDataFrame"] thead tr th {
        background: linear-gradient(90deg,#004fc4,#007aff) !important;
        color: #ffffff !important;
        border-bottom: 2px solid #007aff !important;
        font-weight: 700 !important;
        text-transform: uppercase !important;
        letter-spacing: 0.02em !important;
        font-size: 0.92rem !important;
        padding: 10px 14px !important;
    }
    [data-testid="stDataFrame"] tbody tr {
        background-color: #0b1220 !important;
        color: #ffffff !important;
        transition: background 0.25s ease;
    }
    [data-testid="stDataFrame"] tbody tr:nth-child(even) {
        background-color: #101a2c !important;
    }
    [data-testid="stDataFrame"] tbody tr:hover {
        background-color: #112a52 !important;
        box-shadow: 0 0 8px rgba(0,122,255,0.25) inset !important;
    }
    [data-testid="stDataFrame"] tbody td {
        border-top: 1px solid #1e3a8a !important;
        color: #ffffff !important;
        padding: 9px 14px !important;
        font-size: 0.95rem !important;
        font-weight: 500 !important;
    }
    [data-testid="stDataFrame"] tbody td:last-child {
        color: #60a5fa !important;
        font-weight: 500 !important;
    }

    /* ===============================================
       üìÅ FILE UPLOADER
    =============================================== */
    [data-testid="stFileUploaderDropzone"] {
        background: rgba(255,255,255,0.03) !important;
        border: 1px dashed #1e3a8a !important;
        border-radius: 10px !important;
        color: #cbd5e1 !important;
        transition: all 0.25s ease;
    }
    [data-testid="stFileUploaderDropzone"]:hover {
        border-color: #007aff !important;
        background: rgba(0,122,255,0.1) !important;
    }

    /* ===============================================
       ‚ö†Ô∏è ALERT BOXES
    =============================================== */
    [data-testid^="stAlert"] {
        border-radius: 10px !important;
        border: 1px solid #1e3a8a !important;
        color: #e2e8f0 !important;
        box-shadow: 0 3px 15px rgba(0,0,0,0.4) !important;
    }
    [data-testid="stAlertInfo"]    { background: linear-gradient(145deg,#0d1829,#10243d)!important; }
    [data-testid="stAlertSuccess"] { background: linear-gradient(145deg,#0f2414,#183820)!important; }
    [data-testid="stAlertError"]   { background: linear-gradient(145deg,#2b1617,#1a0c0d)!important; }
    [data-testid="stAlertWarning"] { background: linear-gradient(145deg,#2f2a10,#1c1a0a)!important; }

    </style>
    """, unsafe_allow_html=True)

'''



==================== ./persona_chatroom.py ====================
#!/usr/bin/env python3
"""Virtual meeting room to invite multiple agent personas into the same discussion."""
from __future__ import annotations

import os
from datetime import datetime, timezone
from typing import Dict, List

import requests
import streamlit as st

from services.common.personas import get_persona, list_personas
from services.ui.theme_manager import apply_theme as apply_global_theme, get_theme, render_theme_toggle
from services.ui.components.operator_banner import render_operator_banner

API_URL = os.getenv("API_URL", "http://localhost:8090")

st.set_page_config(page_title="Persona Strategy Room", layout="wide")
apply_global_theme(get_theme())

ss = st.session_state
ss.setdefault("stage", "persona_chatroom")
ss.setdefault("persona_chat_history", [])
ss.setdefault("persona_case_summary", "")

all_personas = list_personas()
persona_lookup: Dict[str, Dict[str, str]] = {p["id"]: p for p in all_personas}

render_operator_banner(
    operator_name=ss.get("user_info", {}).get("name", "Operator"),
    title="Persona Strategy Room",
    summary="Spin up an ad-hoc meeting between the domain personas (asset, credit, fraud/KYC, scoring, compliance).",
    bullets=[
        "Invite any combination of personas for a live discussion.",
        "Share a case summary so everyone has the same context.",
        "Log the transcript and export it for audits.",
    ],
    metrics=[
        {"label": "Personas available", "value": len(all_personas)},
        {"label": "Active meeting lines", "value": len(ss["persona_chat_history"])},
    ],
    icon="üßë‚ÄçüöÄ",
)

st.markdown("### Invite personas")

default_invite = ["asset_appraisal", "credit_appraisal", "anti_fraud_kyc"]
selected_ids = st.multiselect(
    "Choose which personas to invite",
    options=[p["id"] for p in all_personas],
    format_func=lambda pid: f"{persona_lookup[pid]['emoji']} {persona_lookup[pid]['name']} ‚Äî {persona_lookup[pid]['title']}",
    default=[pid for pid in default_invite if pid in persona_lookup],
)
selected_personas: List[Dict[str, str]] = [persona_lookup[pid] for pid in selected_ids]

col_case, col_settings = st.columns([2.5, 1])
with col_case:
    ss["persona_case_summary"] = st.text_area(
        "Case brief",
        value=ss.get("persona_case_summary", ""),
        placeholder="Example: SME borrower #4481 requesting 1.8M USD. Asset FMV 2.1M. Fraud cleared. Waiting on compliance.",
    )
with col_settings:
    render_theme_toggle("üåó Theme", key="persona_room_theme")
    if st.button("üßπ Clear transcript", use_container_width=True):
        ss["persona_chat_history"] = []
        st.success("Cleared meeting transcript.")

st.markdown("### Drive the conversation")
meeting_prompt = st.text_area(
    "Ask a question or set the agenda",
    placeholder="e.g. 'Should we approve SME-4481 today? Highlight blockers from each agent.'",
    key="persona_room_prompt",
)

def _record_message(role: str, speaker: str, content: str) -> None:
    history = ss.setdefault("persona_chat_history", [])
    history.append(
        {
            "role": role,
            "speaker": speaker,
            "content": content,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
    )


def _serialize_history() -> List[Dict[str, str]]:
    return [
        {
            "role": item.get("role", "user"),
            "content": f"{item.get('speaker', 'Operator')}: {item.get('content')}",
            "timestamp": item.get("timestamp"),
        }
        for item in ss.get("persona_chat_history", [])
    ][-40:]


send_disabled = not meeting_prompt.strip() or not selected_personas

if st.button("üöÄ Ask the room", use_container_width=True, disabled=send_disabled):
    if not selected_personas:
        st.warning("Invite at least one persona.")
    elif not meeting_prompt.strip():
        st.warning("Add a prompt for the meeting.")
    else:
        _record_message("user", "Operator", meeting_prompt.strip())
        payload = {
            "message": meeting_prompt.strip(),
            "page_id": "persona_chatroom",
            "context": {
                "stage": "persona_roundtable",
                "case_summary": ss.get("persona_case_summary"),
                "invited": [p["name"] for p in selected_personas],
            },
            "history": _serialize_history(),
            "persona_id": selected_personas[0]["id"] if selected_personas else None,
            "invited_personas": [p["id"] for p in selected_personas],
            "global_room": True,
        }
        try:
            resp = requests.post(f"{API_URL}/v1/chat", json=payload, timeout=60)
            resp.raise_for_status()
            data = resp.json()
            reply = data.get("reply") or "No reply."
            speaker = get_persona("control_tower") or {"name": "Control Tower"}
            _record_message("assistant", speaker.get("name", "Control Tower"), reply)
            st.success("Control Tower shared the group intelligence.")
        except Exception as exc:
            st.error(f"Meeting call failed: {exc}")

st.markdown("### Transcript")
chat_history = ss.get("persona_chat_history", [])
if not chat_history:
    st.info("No messages yet. Invite personas and start the discussion.")
else:
    for item in chat_history:
        role = item.get("role", "user")
        speaker = item.get("speaker") or ("Operator" if role == "user" else "Control Tower")
        with st.chat_message("assistant" if role != "user" else "user"):
            st.markdown(f"**{speaker}:** {item.get('content')}")



