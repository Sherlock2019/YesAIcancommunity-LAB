#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¦ Asset Appraisal Agent â€” Full E2E Flow (Inputs â†’ Anonymize â†’ AI â†’ Human Review â†’ Training)
Author:  Nguyen Dzoan
Version: 2025-11-01

Includes:
- Stage 1: CSV + evidence (images/PDFs) + manual row; synthetic fallback + "why" table
- Stage 2: Explicit anonymization pipeline (RAW & ANON kept)
- Stage 3: AI appraisal with runtime flavor selector, agent discovery+probe, rule_reasons when backend omits
  + Production banner + asset-trained model selector + promote inside Stage 3
- Stage 4: Human Review with AIâ†”Human agreement gauge; export feedback CSV
- Stage 5: Training (upload feedback) â†’ Train candidate â†’ Promote to PRODUCTION
"""

import os
import io
import re
import json
from datetime import datetime, timezone  # âœ… clean, safe, supports datetime.now()
from pathlib import Path
from typing import Any, Dict

# â”€â”€ Third-party
import requests
import numpy as np
import pandas as pd
import streamlit as st
import streamlit.components.v1 as components
import plotly.express as px
import plotly.graph_objects as go
import csv

# Theme bootstrapping
if "ui_theme" not in st.session_state:
    st.session_state["ui_theme"] = "light"   # default bright


#THEME SWITCHER

def apply_theme(theme: str = "light"):
    # Keep palette compact so it's easy to tune
    if theme == "light":
        bg      = "#ffffff"
        text    = "#0f172a"
        subtext = "#334155"
        card    = "#f8fafc"
        border  = "#e2e8f0"
        accent  = "#2563eb"
        accent2 = "#22c55e"
        tab_bg  = "#eef2ff"
        table_bg= "#ffffff"
        table_head_bg = "#e2e8f0"
        table_head_tx = "#0f172a"
    else:  # dark
        bg      = "#0E1117"
        text    = "#f1f5f9"
        subtext = "#93a4b8"
        card    = "#0f172a"
        border  = "#334155"
        accent  = "#3b82f6"
        accent2 = "#22c55e"
        tab_bg  = "#111418"
        table_bg= "#0f172a"
        table_head_bg = "#1e293b"
        table_head_tx = "#93c5fd"

    st.markdown(f"""
    <style>
      /* App bg + text */
      .stApp {{
        background: {bg} !important;
        color: {text} !important;
      }}
      .stCaption, .stMarkdown p, .stMarkdown li, .st-emotion-cache-16idsys {{
        color: {subtext} !important;
      }}

      /* Buttons */
      .stButton>button {{
        background-color: {accent} !important;
        color: white !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
        border: 1px solid {border} !important;
      }}
      .stButton>button:hover {{
        filter: brightness(0.95);
      }}

      /* Tabs */
      .stTabs [data-baseweb="tab-list"] button {{
        color: {text} !important;
        background: {tab_bg} !important;
        border-radius: 10px !important;
        margin-right: 4px !important;
        border: 1px solid {border} !important;
      }}
      .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {{
        background-color: {accent} !important;
        color: #ffffff !important;
      }}

      /* Dataframe/Data Editor container */
      [data-testid="stDataFrame"] {{
        background-color: {table_bg} !important;
        color: {text} !important;
        border-radius: 10px !important;
        border: 1px solid {border} !important;
        box-shadow: 0 4px 18px rgba(0,0,0,0.2) !important;
      }}
      [data-testid="stDataFrame"] thead tr th {{
        background: {table_head_bg} !important;
        color: {table_head_tx} !important;
        font-weight: 700 !important;
        border-bottom: 2px solid {accent} !important;
      }}
      [data-testid="stDataFrameCell"]:not([data-testid="stDataFrameCellEditable"]) {{
        background-color: {table_bg} !important;
        color: {text} !important;
        border-color: {border} !important;
      }}
      [data-testid="stDataFrameCellEditable"] textarea {{
        background-color: {card} !important;
        color: {text} !important;
        border: 1px solid {border} !important;
        border-radius: 6px !important;
      }}
      [data-testid="stDataFrameCellEditable"]:focus-within textarea,
      [data-testid="stDataFrameCellEditable"]:hover textarea {{
        border-color: {accent2} !important;
        box-shadow: 0 0 0 2px rgba(34,197,94,0.35) !important;
      }}

      /* Horizontal rules */
      hr, .stMarkdown hr {{
        border-color: {border} !important;
      }}
    </style>
    """, unsafe_allow_html=True)



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PAGE CONFIG â€” must be the first Streamlit call
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(page_title="Asset Appraisal Agent", layout="wide")
ss = st.session_state

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SESSION DEFAULTS (idempotent)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _init_defaults():
    ss.setdefault("asset_logged_in", False)
    ss.setdefault("asset_stage", "login")   # login â†’ asset_flow
    ss.setdefault("asset_user", {"name": "Guest", "email": None})
    # Working tables/artifacts per our matrix (placeholders)
    ss.setdefault("asset_intake_df", None)
    ss.setdefault("asset_evidence_index", None)
    ss.setdefault("asset_anon_df", None)
    ss.setdefault("asset_features_df", None)
    ss.setdefault("asset_comps_used", None)
    ss.setdefault("asset_valued_df", None)
    ss.setdefault("asset_verified_df", None)
    ss.setdefault("asset_policy_df", None)
    ss.setdefault("asset_decision_df", None)
    ss.setdefault("asset_human_review_df", None)
    ss.setdefault("asset_feedback_csv", None)
    ss.setdefault("asset_trained_model_meta", None)
    ss.setdefault("asset_gpu_profile", None)  # will be set only in C.4
    os.makedirs("./.tmp_runs", exist_ok=True)

_init_defaults()

def render_nav_bar_app():
    st.markdown(
        "<div style='display:flex;gap:12px;align-items:center'>"
        "<a href='?stage=agents' class='macbtn'>ğŸ¤– Agents</a>"
        "<span style='opacity:.6'>/</span>"
        "<span>ğŸ›ï¸ Asset Appraisal Agent</span>"
        "</div>",
        unsafe_allow_html=True,
    )


# ---- Global runs dir (used everywhere) ----
RUNS_DIR = os.path.abspath("./.tmp_runs")
os.makedirs(RUNS_DIR, exist_ok=True)




# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # THEME INJECTION (Dark + Sidebar hide, with MutationObserver)
# # Run immediately on every script execution to avoid flicker on rerun
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# import streamlit.components.v1 as components # <--- IMPORT IS HERE
# # 1) CSS (global, persistent)
# # Note: Removed the inject_dark_theme_once() function and session state check
# st.markdown("""
#     <style>
#       :root { color-scheme: dark; } /* Hint built-ins */
#       html, body, .stApp {
#         background-color:#0f172a !important;
#         color:#e5e7eb !important;
#         font-family: 'Inter', system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
#       }
#       /* Sidebar off + container padding */
#       [data-testid="stSidebar"], section[data-testid="stSidebar"], nav[data-testid="stSidebarNav"] { display:none !important; }
#       [data-testid="stAppViewContainer"] { margin-left:0 !important; padding-left:0 !important; }
#       /* Headings */
#       h1, h2, h3, h4, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 { color:#e5e7eb !important; }
#       /* Tabs */
#       .stTabs [data-baseweb="tab-list"] { gap:6px; border-bottom:1px solid #1f2937; }
#       .stTabs [data-baseweb="tab"] {
#         background:#0b1222; border:1px solid #1f2937; border-bottom:none;
#         padding:10px 14px; border-top-left-radius:10px; border-top-right-radius:10px; color:#cbd5e1;
#       }
#       .stTabs [aria-selected="true"] { background:#111827 !important; color:#e5e7eb !important; }
#       /* Inputs */
#       .stTextInput input, .stNumberInput input, .stSelectbox [data-baseweb="select"] > div {
#         background:#0b1222 !important; color:#e5e7eb !important; border:1px solid #1f2937 !important;
#       }
#       /* Buttons */
#       .stButton button {
#         background:linear-gradient(180deg,#1f3b57 0%,#0e1f33 100%) !important;
#         color:#e6f3ff !important; border:1px solid #1d2b3a !important; border-radius:10px !important;
#         box-shadow:0 0 10px rgba(56,189,248,.15);
#       }
#       .stButton button:hover { filter:brightness(1.05); box-shadow:0 0 16px rgba(56,189,248,.25); }
#       /* Metrics */
#       div[data-testid="stMetric"] { background:#0b1222; border:1px solid #1f2937; border-radius:12px; padding:10px 12px; }
#       div[data-testid="stMetricValue"] { color:#38bdf8 !important; }
#       /* Tables */
#       .stDataFrame, .stTable { background:#0b1222 !important; border:1px solid #1f2937 !important; border-radius:10px !important; }
#       /* Expanders */
#       details { background:#0b1222 !important; border:1px solid #1f2937 !important; border-radius:10px !important; padding:6px 10px !important; }
#       /* Plotly */
#       .js-plotly-plot .plotly .main-svg { background-color:transparent !important; }
#     </style>
#     """, unsafe_allow_html=True)

# # 2) JS observer to re-assert dark after Streamlit mutates
# components.html("""
#     <script>
#       (function() {
#         const apply = () => {
#           try {
#             const root = parent.document.documentElement;
#             const app = parent.document.querySelector('.stApp');
#             if (root) root.style.setProperty('color-scheme','dark');
#             if (app) app.classList.add('dark-hold');
#           } catch(e) {}
#         };
#         apply();
#         const mo = new MutationObserver(apply);
#         mo.observe(parent.document.documentElement, {childList:true, subtree:true});
#       })();
#     </script>
#     """, height=0)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API CONFIG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

API_URL = os.getenv("API_URL", "http://localhost:8090")

# Default fallbacks (will be superseded by discovery)

ASSET_AGENT_IDS = [a.strip() for a in os.getenv("ASSET_AGENT_IDS", "asset_appraisal,asset").split(",") if a.strip()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NAV (reliable jump to Home / Agents from a page)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _set_query_params_safe(**kwargs):
    # New API (Streamlit â‰¥1.40)
    try:
        for k, v in kwargs.items():
            st.query_params[k] = v
        return True
    except Exception:
        pass
    # Older versions
    try:
        st.experimental_set_query_params(**kwargs)
        return True
    except Exception:
        return False

def _go_stage(target_stage: str):
    # 1) let app.pyâ€™s router know what to show
    st.session_state["stage"] = target_stage

    # 2) preferred: jump to main app file
    try:
        # path is relative to the run root when you launch:
        #   streamlit run services/ui/app.py
        st.switch_page("app.py")
        return
    except Exception:
        pass

    # 3) fallback: set query param and rerun so app.py picks it up
    _set_query_params_safe(stage=target_stage)
    st.rerun()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UTILITIES â€” DataFrame selection helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---- DataFrame selection helpers (avoid boolean ambiguity) ----
def first_nonempty_df(*candidates):
    """Return the first candidate that is a non-empty pandas DataFrame, else None."""
    for df in candidates:
        if isinstance(df, pd.DataFrame) and not df.empty:
            return df
    return None

def is_nonempty_df(x) -> bool:
    return isinstance(x, pd.DataFrame) and not x.empty

def render_nav_bar_app():
    stage = st.session_state.get("stage", "landing")

    # three columns: home, agents, theme toggle
    c1, c2, c3 = st.columns([1, 1, 2.5])

    with c1:
        if st.button("ğŸ  Back to Home", key=f"btn_home_{stage}"):
            _go_stage("landing")
            st.stop()

    with c2:
        if st.button("ğŸ¤– Back to Agents", key=f"btn_agents_{stage}"):
            _go_stage("agents")
            st.stop()

    with c3:
        is_dark = (ss.get("ui_theme", "dark") == "dark")
        new_is_dark = st.toggle("ğŸŒ™ Dark mode", value=is_dark, key="ui_theme_toggle", help="Switch theme")
        new_theme = "dark" if new_is_dark else "light"
        if new_theme != ss["ui_theme"]:
            ss["ui_theme"] = new_theme
            apply_theme(ss["ui_theme"])

    st.markdown("---")


# def render_nav_bar_app():
#     # read the global stage (default to â€˜landingâ€™)
#     stage = st.session_state.get("stage", "landing")

#     # show both buttons on this page
#     c1, c2, _ = st.columns([1, 1, 6])
#     with c1:
#         if st.button("ğŸ  Back to Home", key=f"btn_home_{stage}"):
#             _go_stage("landing")
#             st.stop()
#     with c2:
#         if st.button("ğŸ¤– Back to Agents", key=f"btn_agents_{stage}"):
#             _go_stage("agents")
#             st.stop()
    
#     with c3:
#         is_dark = (ss.get("ui_theme", "dark") == "dark")
#         new_is_dark = st.toggle("ğŸŒ™ Dark mode", value=is_dark, key="ui_theme_toggle", help="Switch theme")
#         new_theme = "dark" if new_is_dark else "light"
#         if new_theme != ss["ui_theme"]:
#             ss["ui_theme"] = new_theme
#             apply_theme(ss["ui_theme"])
 


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GEO UTILITIES: EXIF GPS, Geocode, Geohash   â† PASTE START
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from typing import Optional, Tuple

def _exif_to_degrees(value):
    try:
        d = float(value[0][0]) / float(value[0][1])
        m = float(value[1][0]) / float(value[1][1])
        s = float(value[2][0]) / float(value[2][1])
        return d + (m / 60.0) + (s / 3600.0)
    except Exception:
        return None

def extract_gps_from_image(path: str) -> Optional[Tuple[float, float]]:
    try:
        from PIL import Image
        from PIL.ExifTags import TAGS, GPSTAGS
        img = Image.open(path)
        exif = img._getexif() or {}
        tagged = {TAGS.get(k, k): v for k, v in exif.items()}
        gps_info = tagged.get("GPSInfo")
        if not gps_info:
            return None
        gps_data = {GPSTAGS.get(k, k): v for k, v in gps_info.items()}
        lat = _exif_to_degrees(gps_data.get("GPSLatitude"))
        lon = _exif_to_degrees(gps_data.get("GPSLongitude"))
        if lat is None or lon is None:
            return None
        lat_ref = gps_data.get("GPSLatitudeRef", "N")
        lon_ref = gps_data.get("GPSLongitudeRef", "E")
        if lat_ref == "S": lat = -lat
        if lon_ref == "W": lon = -lon
        return (lat, lon)
    except Exception:
        return None

_GEOCODE_CACHE_PATH = "./.tmp_runs/geocode_cache.json"

def _load_geocode_cache():
    try:
        with open(_GEOCODE_CACHE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _save_geocode_cache(cache: dict):
    os.makedirs("./.tmp_runs", exist_ok=True)
    with open(_GEOCODE_CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def geocode_freeform(addr: str) -> Optional[Tuple[float, float]]:
    """Nominatim via geopy; cached locally. Returns None if offline."""
    try:
        cache = _load_geocode_cache()
        key = addr.strip().lower()
        if key in cache:
            v = cache[key]
            return (v["lat"], v["lon"])
        from geopy.geocoders import Nominatim
        geolocator = Nominatim(user_agent="asset-appraisal-agent")
        loc = geolocator.geocode(addr, timeout=10)
        if not loc:
            return None
        cache[key] = {"lat": float(loc.latitude), "lon": float(loc.longitude)}
        _save_geocode_cache(cache)
        return (float(loc.latitude), float(loc.longitude))
    except Exception:
        return None

def geohash_decode(s: str) -> Optional[Tuple[float, float]]:
    try:
        import geohash  # pip install python-geohash
        lat, lon = geohash.decode(s)
        return (float(lat), float(lon))
    except Exception:
        return None
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† PASTE END


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SESSION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ss = st.session_state
# Theme default
ss.setdefault("ui_theme", "dark")   # "dark" or "light"
ss.setdefault("asset_stage", "login")
ss.setdefault("asset_logged_in", False)
ss.setdefault("asset_user", None)

# Stage caches
ss.setdefault("asset_raw_df", None)     # Stage 1 raw (after CSV/manual merge)
ss.setdefault("asset_evidence", [])     # evidence filenames (images/pdfs)
ss.setdefault("asset_anon_df", None)    # Stage 2 anonymized
ss.setdefault("asset_stage2_df", None)  # Stage 3 input (resolved source)
ss.setdefault("asset_ai_df", None)      # Stage 3 AI output
ss.setdefault("asset_selected_model", None)  # trained model path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HELPERS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def anonymize_text_cols(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for col in out.columns:
        if out[col].dtype == "object":
            out[col] = (
                out[col].astype(str)
                .apply(lambda x: re.sub(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+", "[EMAIL]", x))
            )
    return out

def quick_synth(rows: int = 150) -> pd.DataFrame:
    """Generate asset rows + finance metrics for demo/backup."""
    rng = np.random.default_rng(42)
    cities = [
        ("Hanoi", 21.0285, 105.8542),
        ("HCMC", 10.7769, 106.7009),
        ("Da Nang", 16.0544, 108.2022),
        ("Hue", 16.4637, 107.5909),
        ("Can Tho", 10.0452, 105.7469),
    ]
    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, rows + 1)],
        "asset_id": [f"A{i:04d}" for i in range(1, rows + 1)],
        "asset_type": rng.choice(["House","Apartment","Car","Land","Factory"], rows),
        "age_years": rng.integers(1, 40, rows),
        "market_value": rng.integers(50_000, 2_000_000, rows),
        "condition_score": rng.uniform(0.6, 1.0, rows),
        "legal_penalty": rng.uniform(0.95, 1.0, rows),          # legal/title risk adj
        "employment_years": rng.integers(0, 30, rows),
        "credit_history_years": rng.integers(0, 25, rows),
        "delinquencies": rng.integers(0, 6, rows),
        "current_loans": rng.integers(0, 8, rows),
        "loan_amount": rng.integers(10_000, 200_000, rows),
        "customer_type": rng.choice(["bank","non-bank"], rows, p=[0.7,0.3]),
    })
    cdf = pd.DataFrame(cities, columns=["city","lat","lon"])
    df["city"] = rng.choice(cdf["city"], rows)
    df = df.merge(cdf, on="city", how="left")
    df["depreciation_rate"] = (1 - df["condition_score"]) * 100
    df["market_segment"] = np.where(df["market_value"] > 500_000, "High", "Mass")
    df["DTI"] = rng.uniform(0.05, 0.9, rows)
    df["LTV"] = np.clip(df["loan_amount"] / np.maximum(df["market_value"], 1), 0.05, 1.5)
    df["evidence_files"] = [[] for _ in range(rows)]
    return df

def synth_why_table() -> pd.DataFrame:
    return pd.DataFrame([
        {"Metric": "DTI", "Why": "Debt service relative to income â€” proxy for payability."},
        {"Metric": "LTV", "Why": "Loan vs asset value â€” proxy for collateral adequacy."},
        {"Metric": "condition_score", "Why": "Asset physical state impacts fair value/depreciation."},
        {"Metric": "legal_penalty", "Why": "Legal/title flags reduce realizable value."},
        {"Metric": "employment_years / credit_history_years", "Why": "Stability/track record."},
        {"Metric": "delinquencies / current_loans", "Why": "Current risk pressure."},
        {"Metric": "market_segment / city / lat,lon", "Why": "Market & location effects on pricing."},
    ])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DATAFRAME SELECTION (avoid boolean ambiguity)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def first_nonempty_df(*candidates):
    """Return the first candidate that is a non-empty pandas DataFrame, else None."""
    for df in candidates:
        if isinstance(df, pd.DataFrame) and not df.empty:
            return df
    return None



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UNIVERSAL INGEST + NORMALIZATION HELPERS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _slug(name: str) -> str:
    return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

def _ts() -> str:
    return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

def _read_any_table(uploaded_file) -> pd.DataFrame:
    """
    Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback.
    Accepts Streamlit UploadedFile or a file-like object.
    """
    name = getattr(uploaded_file, "name", "").lower()

    # Excel first
    if name.endswith((".xlsx", ".xls")):
        return pd.read_excel(uploaded_file)

    # Text (CSV/TSV/TXT): try utf-8, then latin-1; sniff delimiter.
    raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
    for enc in ("utf-8", "latin-1"):
        try:
            text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
            lines = text.splitlines()
            sample = "\n".join(lines[:5]) if lines else ""
            try:
                dialect = csv.Sniffer().sniff(sample) if sample else csv.excel
                sep = getattr(dialect, "delimiter", ",")
            except Exception:
                sep = ","
            return pd.read_csv(io.StringIO(text), sep=sep)
        except Exception:
            continue
    # last resort
    return pd.read_csv(io.BytesIO(raw), engine="python")

def _normalize_for_agents(df: pd.DataFrame) -> pd.DataFrame:
    """
    Light normalization for credit/asset agents.
    Creates a consistent thin schema if columns exist; leaves extras intact.
    """
    out = df.copy()

    # alias map (extend freely)
    aliases = {
        "application_id": ["application_id", "app_id", "loan_id", "id", "request_id"],
        "asset_id":       ["asset_id", "property_id", "house_id", "assetid"],
        "asset_type":     ["asset_type", "type", "category"],
        "address":        ["address", "addr", "street", "location"],
        "city":           ["city", "town"],
        "state":          ["state", "province", "region"],
        "country":        ["country"],
        "price":          ["price", "value", "market_value", "listing_price", "sale_price"],
        "bedrooms":       ["bedrooms", "beds"],
        "bathrooms":      ["bathrooms", "baths"],
        "parking_space":  ["parking_space", "parking", "garage"],
        "title":          ["title", "name"],
    }

    # rename by first matching alias
    rename_map = {}
    cols = set(out.columns)
    for target, cands in aliases.items():
        for c in cands:
            if c in cols:
                rename_map[c] = target
                break
    out = out.rename(columns=rename_map)

    # ensure presence of common columns
    required = ["application_id", "asset_id", "asset_type", "address", "city", "state", "price"]
    for col in required:
        if col not in out.columns:
            out[col] = None

    # numeric coercions
    for col in ("price", "bedrooms", "bathrooms", "parking_space"):
        if col in out.columns:
            out[col] = pd.to_numeric(out[col], errors="coerce")

    # provenance
    out["source_dataset"] = st.session_state.get("asset_intake_source_name", "uploaded")
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# THEME SYSTEM (Light/Dark CSS + map style)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from textwrap import dedent

THEME_VARS = {
    "light": {
        "bg": "#FFFFFF",
        "panel": "#F8FAFC",
        "text": "#0F172A",
        "muted": "#475569",
        "primary": "#2563EB",
        "success": "#16A34A",
        "warn": "#D97706",
        "danger": "#DC2626",
        "accent": "#0EA5E9",
        "stripe": "#F1F5F9",
        "shadow": "0 6px 24px rgba(15,23,42,0.08)",
    },
    "dark": {
        "bg": "#0B1020",
        "panel": "#101727",
        "text": "#E5E7EB",
        "muted": "#94A3B8",
        "primary": "#60A5FA",
        "success": "#22C55E",
        "warn": "#FBBF24",
        "danger": "#F87171",
        "accent": "#38BDF8",
        "stripe": "#111827",
        "shadow": "0 8px 30px rgba(0,0,0,0.35)",
    },
}

def _theme_css(theme: str) -> str:
    t = THEME_VARS[theme]
    return dedent(f"""
    <style>
      /* Fonts: Inter + JetBrains Mono */
      @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=JetBrains+Mono:wght@400;600&display=swap');

      :root {{
        --bg: {t['bg']};
        --panel: {t['panel']};
        --text: {t['text']};
        --muted: {t['muted']};
        --primary: {t['primary']};
        --success: {t['success']};
        --warn: {t['warn']};
        --danger: {t['danger']};
        --accent: {t['accent']};
        --stripe: {t['stripe']};
        --shadow: {t['shadow']};
        --radius: 14px;
      }}

      html, body, .stApp {{
        background: var(--bg) !important;
        color: var(--text) !important;
        font-family: 'Inter', system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif !important;
      }}

      /* Panel-like boxes (use .left-box/.right-box or your custom containers) */
      .left-box, .right-box, .stExpander, .stTabs [data-baseweb="tab-highlight"] {{
        background: var(--panel) !important;
        border-radius: var(--radius);
        box-shadow: var(--shadow);
      }}

      /* Headings */
      h1, h2, h3, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 {{
        color: var(--text) !important;
        font-weight: 700;
        letter-spacing: -0.01em;
      }}

      /* Buttons */
      .stButton>button, button[kind="primary"] {{
        background: var(--primary) !important;
        color: #fff !important;
        border-radius: 12px !important;
        border: none !important;
        box-shadow: 0 3px 12px rgba(37,99,235,0.35) !important;
      }}
      .stDownloadButton button {{
        background: var(--success) !important;
        color: #fff !important;
        border-radius: 12px !important;
        border: none !important;
        box-shadow: 0 3px 12px rgba(34,197,94,0.35) !important;
      }}

      /* Tables (dataframe) */
      .stDataFrame thead tr th {{
        background: var(--panel) !important;
        color: var(--muted) !important;
        font-weight: 600 !important;
      }}
      .stDataFrame tbody tr:nth-child(odd) {{
        background: var(--stripe) !important;
      }}

      /* Chips / small badges */
      .chip {{
        display:inline-block; padding:4px 10px; border-radius:999px;
        background: var(--panel); color: var(--muted); border:1px solid rgba(148,163,184,0.35);
      }}

      /* Code font */
      code, pre, .stCodeBlock, .st-emotion-cache-ffhzg2 {{
        font-family: 'JetBrains Mono', ui-monospace, SFMono-Regular, Menlo, Consolas, monospace !important;
      }}
      
      /* NEW  Optional: hide sidebar without touching theme */
      [data-testid="stSidebar"], section[data-testid="stSidebar"], nav[data-testid="stSidebarNav"] {{
        display: none !important;
      }}
      [data-testid="stAppViewContainer"] {{
        margin-left: 0 !important;
        padding-left: 0 !important;
      }}
      
      
    </style>
    """)

def apply_theme(theme: str = None):
    """Inject CSS theme; defaults to session theme."""
    theme = theme or st.session_state.get("ui_theme", "light")
    if theme not in THEME_VARS:
        theme = "light"
    st.markdown(_theme_css(theme), unsafe_allow_html=True)

# def get_map_style() -> str:
#     """Return a mapbox/pydeck style matching the theme."""
#     theme = st.session_state.get("ui_theme", "light")
#     return "mapbox://styles/mapbox/light-v11" if theme == "light" else "mapbox://styles/mapbox/dark-v11"



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAP THEME HELPERS (Mapbox style + token + adapters)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import streamlit as st

def get_mapbox_token() -> str | None:
    """Find a Mapbox token from secrets or env. Return None if not set."""
    try:
        tok = st.secrets.get("MAPBOX_TOKEN")
    except Exception:
        tok = None
    if not tok:
        tok = os.environ.get("MAPBOX_TOKEN") or os.environ.get("MAPBOX_ACCESS_TOKEN")
    return tok or None


def plotly_map_style() -> str:
    """
    Return a Plotly-compatible map style.
    - Uses bright style in light mode even without a Mapbox token.
    - Falls back to CARTO 'positron' if Mapbox token not set.
    """
    theme = st.session_state.get("ui_theme", "light")
    token = get_mapbox_token()
    if token:
        return "light" if theme == "light" else "dark"
    else:
        # fallback to open CARTO tiles (bright)
        return "carto-positron" if theme == "light" else "carto-darkmatter"


def get_map_style() -> str:
    """
    Return a Mapbox style URL (for pydeck only).
    Light â†’ bright, Dark â†’ dark. Defaults to light for safety.
    """
    theme = st.session_state.get("ui_theme", "light")
    return "mapbox://styles/mapbox/light-v11" if theme == "light" else "mapbox://styles/mapbox/dark-v11"


def apply_plotly_mapbox_defaults():
    """Set Plotly's Mapbox token globally (if available)."""
    import plotly.express as px
    token = get_mapbox_token()
    if token:
        px.set_mapbox_access_token(token)
    else:
        st.info("â„¹ï¸ Mapbox token not set â€” using free bright map style (carto-positron).")


def make_pydeck_view_state(lat=10.7769, lon=106.7009, zoom=10, pitch=0, bearing=0):
    import pydeck as pdk
    return pdk.ViewState(latitude=lat, longitude=lon, zoom=zoom, pitch=pitch, bearing=bearing)


def pydeck_map_style() -> str:
    """pydeck uses the same Mapbox style URLs when a token is available."""
    return get_map_style()


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # MAP THEME HELPERS (Mapbox style + token + adapters)
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# import os

# def get_mapbox_token() -> str | None:
#     """Find a Mapbox token from secrets or env. Return None if not set."""
#     # Prefer Streamlit secrets if present
#     tok = None
#     try:
#         tok = st.secrets.get("MAPBOX_TOKEN")  # type: ignore[attr-defined]
#     except Exception:
#         pass
#     if not tok:
#         tok = os.environ.get("MAPBOX_TOKEN") or os.environ.get("MAPBOX_ACCESS_TOKEN")
#     return tok or None

# def get_map_style() -> str:
#     """
#     Return a Mapbox style URL matched to current theme.
#     Light â†’ bright, Dark â†’ dark. Defaults to light for safety.
#     """
#     theme = st.session_state.get("ui_theme", "light")
#     return "mapbox://styles/mapbox/light-v11" if theme == "light" else "mapbox://styles/mapbox/dark-v11"

# # Optional: convenience adapters for Plotly + pydeck
# def apply_plotly_mapbox_defaults():
#     """
#     Set Plotly's Mapbox token globally. Call once before creating px/scatter_mapbox etc.
#     """
#     import plotly.express as px  # local import to avoid hard dependency at import time
#     token = get_mapbox_token()
#     if token:
#         px.set_mapbox_access_token(token)
#     else:
#         st.info("â„¹ï¸ Mapbox token not set. Set MAPBOX_TOKEN in env or st.secrets to enable styled basemaps.")

# def make_pydeck_view_state(lat=10.7769, lon=106.7009, zoom=10, pitch=0, bearing=0):
#     import pydeck as pdk
#     return pdk.ViewState(latitude=lat, longitude=lon, zoom=zoom, pitch=pitch, bearing=bearing)

# def pydeck_map_style() -> str:
#     """
#     pydeck uses the same Mapbox style URLs when a token is available.
#     """
#     return get_map_style()



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AGENT DISCOVERY & PROBE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _safe_get_json(url: str, timeout: int = 8):
    try:
        r = requests.get(url, timeout=timeout)
        if r.ok:
            try:
                return True, r.json()
            except Exception as e:
                return False, f"parse error: {e}\nBody:\n{r.text[:2000]}"
        return False, f"{r.status_code} {r.reason}\nBody:\n{r.text[:2000]}"
    except Exception as e:
        return False, f"request error: {e}"

def discover_asset_agents() -> list[str]:
    """Try common discovery endpoints and extract agent ids. Cache in session."""
    cached = st.session_state.get("asset_agent_ids")
    if isinstance(cached, list) and cached:
        return cached

    candidates = []

    # 1) /v1/agents (prefer)
    ok, data = _safe_get_json(f"{API_URL}/v1/agents")
    if ok:
        try:
            if isinstance(data, dict) and "agents" in data:
                items = data["agents"]
                if isinstance(items, list):
                    for it in items:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name") or it.get("agent") or it.get("slug")
                            if aid: candidates.append(aid)
            elif isinstance(data, list):
                for it in data:
                    if isinstance(it, str):
                        candidates.append(it)
                    elif isinstance(it, dict):
                        aid = it.get("id") or it.get("name")
                        if aid: candidates.append(aid)
        except Exception:
            pass

    # 2) /v1/agents/list (alt)
    if not candidates:
        ok2, data2 = _safe_get_json(f"{API_URL}/v1/agents/list")
        if ok2:
            try:
                if isinstance(data2, dict):
                    for k in ("agents", "data", "items"):
                        if k in data2 and isinstance(data2[k], list):
                            for it in data2[k]:
                                if isinstance(it, str):
                                    candidates.append(it)
                                elif isinstance(it, dict):
                                    aid = it.get("id") or it.get("name")
                                    if aid: candidates.append(aid)
                elif isinstance(data2, list):
                    for it in data2:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name")
                            if aid: candidates.append(aid)
            except Exception:
                pass

    # 3) /v1/health (sometimes lists agents)
    if not candidates:
        ok3, data3 = _safe_get_json(f"{API_URL}/v1/health")
        if ok3 and isinstance(data3, dict):
            for k in ("agents", "services", "available_agents"):
                val = data3.get(k)
                if isinstance(val, list):
                    for it in val:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name")
                            if aid: candidates.append(aid)

    discovered = [c for c in dict.fromkeys(candidates) if c]  # de-dupe
    if not discovered:
        discovered = ASSET_AGENT_IDS[:]  # fallback to env/defaults

    st.session_state["asset_agent_ids"] = discovered
    return discovered

def probe_api() -> dict:
    """Collect quick diagnostics for UI."""
    diag = {}
    for path in ("/v1/health", "/v1/agents", "/v1/agents/list"):
        ok, data = _safe_get_json(f"{API_URL}{path}")
        diag[path] = data if ok else {"error": data}
    diag["API_URL"] = API_URL
    diag["discovered_agents"] = discover_asset_agents()
    return diag

# NEW: run_id extractor for various API payload shapes
def _extract_run_id(obj) -> str | None:
    """Find a run_id in a nested dict/list API response."""
    if isinstance(obj, dict):
        rid = obj.get("run_id")
        if isinstance(rid, str) and rid:
            return rid
        for k in ("data", "meta", "result", "payload"):
            v = obj.get(k)
            if isinstance(v, dict):
                rid = v.get("run_id")
                if isinstance(rid, str) and rid:
                    return rid
    elif isinstance(obj, list):
        for it in obj:
            rid = _extract_run_id(it)
            if rid:
                return rid
    return None

def try_run_asset_agent(csv_bytes: bytes, form_fields: dict, timeout_sec: int = 180):
    """
    Discover agent ids, then try each. Rebuild multipart for each attempt.
    Preferred: use run_id to GET merged CSV and DataFrame it.
    Fallback: normalize 'result' only (not whole JSON).

    Returns (ok: bool, DataFrame | error_string)
    """
    agent_ids = discover_asset_agents()
    errors = []
    for agent_id in agent_ids:
        files = {"file": ("asset_verified.csv", io.BytesIO(csv_bytes), "text/csv")}
        url = f"{API_URL}/v1/agents/{agent_id}/run"
        try:
            resp = requests.post(url, files=files, data=form_fields, timeout=timeout_sec)
        except Exception as e:
            errors.append(f"[{agent_id}] request error: {e}")
            continue

        if resp.ok:
            body_text = resp.text[:4000]
            try:
                payload = resp.json()
            except Exception as e:
                errors.append(f"[{agent_id}] parse error: {e}\nBody:\n{body_text}")
                continue

            rid = _extract_run_id(payload)
            if rid:
                # Preferred: fetch merged CSV
                try:
                    r_csv = requests.get(f"{API_URL}/v1/runs/{rid}/report?format=csv", timeout=60)
                    if r_csv.ok:
                        df = pd.read_csv(io.BytesIO(r_csv.content))
                        st.session_state["asset_last_run_id"] = rid
                        st.session_state["asset_last_runner"] = ((payload.get("meta") or {}).get("runner_used"))
                        return True, df
                    else:
                        errors.append(
                            f"[{agent_id}] report GET {r_csv.status_code} {r_csv.reason} for run_id={rid}\n"
                            f"Body:\n{r_csv.text[:2000]}"
                        )
                except Exception as e:
                    errors.append(f"[{agent_id}] report GET error for run_id={rid}: {e}")

            # Fallback: try to render just 'result'
            result_part = payload.get("result")
            if isinstance(result_part, list):
                try:
                    df = pd.json_normalize(result_part)
                    return True, df
                except Exception as e:
                    errors.append(f"[{agent_id}] fallback normalize error: {e}\nBody:\n{body_text}")
            elif isinstance(result_part, dict):
                try:
                    df = pd.json_normalize(result_part)
                    return True, df
                except Exception as e:
                    errors.append(f"[{agent_id}] fallback normalize error: {e}\nBody:\n{body_text}")
            else:
                errors.append(f"[{agent_id}] no run_id and empty/unknown 'result'.\nBody:\n{body_text}")
        else:
            errors.append(f"[{agent_id}] {resp.status_code} {resp.reason}\nBody:\n{resp.text[:2000]}")

    return False, "All agent attempts failed (discovered=" + ", ".join(agent_ids) + "):\n" + "\n\n".join(errors)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LOGIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ss["asset_stage"] == "login" and not ss["asset_logged_in"]:
    render_nav_bar_app()
    st.title("ğŸ” Login to AI Asset Appraisal Platform")
    c1, c2, c3 = st.columns([1,1,1])
    with c1:
        user = st.text_input("Username", placeholder="e.g. dzoan")
    with c2:
        email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
    with c3:
        pwd = st.text_input("Password", type="password", placeholder="Enter any password")
    if st.button("Login", key="btn_asset_login", use_container_width=True):
        if (user or "").strip() and (email or "").strip():
            ss["asset_user"] = {
                "name": user.strip(),
                "email": email.strip(),
                "timestamp": datetime.now(timezone.utc).isoformat(),  # âœ… fixed
            }
            ss["asset_logged_in"] = True
            ss["asset_stage"] = "asset_flow"
            st.rerun()
        else:
            st.error("âš ï¸ Please fill all fields before continuing.")
    st.stop()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WORKFLOW (Aâ†’G)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ss.get("asset_logged_in") and ss.get("asset_stage") in ("asset_flow", "asset_agent"):
    render_nav_bar_app()
    st.title("ğŸ›ï¸ Asset Appraisal Agent")
    st.caption(
        "Aâ†’G pipeline â€” Intake â†’ Privacy â†’ Valuation â†’ Policy â†’ Human Review â†’ Model Training â†’ Reporting "
        f"| ğŸ‘‹ {ss['asset_user']['name']}"
    )


    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TABS (A..G) â€” Live tabs
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tabA, tabB, tabC, tabD, tabE, tabF, tabG = st.tabs([
        "ğŸŸ¦ A) Intake & Evidence",
        "ğŸŸ© B) Privacy & Features",
        "ğŸŸ¨ C) Valuation & Verification",
        "ğŸŸ§ D) Policy & Decision",
        "ğŸŸª E) Human Review & Feedback",
        "ğŸŸ« F) Model Training & Promotion",
        "â¬œ G) Reporting & Handoff"
    ])

    # Runtime tip
    st.caption(
        "ğŸ“˜ Tip: Move sequentially from Aâ†’G or revisit individual stages. "
        "If a stage reports missing data, rerun the previous one or load demo data."
    )

else:
    st.warning("Please log in first to access the Asset Appraisal workflow.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸŸ¦ STAGE A â€” INTAKE & EVIDENCE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tabA:
    import io, os, json, hashlib, pandas as pd
    from datetime import datetime, timezone
    
    
    ss = st.session_state  # âœ… make 'ss' available in this scope
    st.subheader("A. Intake & Evidence")
    st.caption("Steps: (1) Upload / Import, (2) Normalize, (3) Generate unified intake CSV")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ“˜ Quick User Guide (updated)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ“˜ Quick User Guide", expanded=False):
        st.markdown("""
        **Goal:** Collect, normalize, and unify all asset-related data before appraisal.

        **1ï¸âƒ£ Upload Your Data**
        - Upload **field agent reports**, **loan lists with collateral**, and **legal property documents**.
        - Supported: `.csv`, `.xlsx`, `.zip` (evidence images/docs).

        **2ï¸âƒ£ Import Open Data**
        - Search **Kaggle** or **Hugging Face** for relevant valuation datasets.
        - You can mix public + internal uploads â€” AI will normalize columns.

        **3ï¸âƒ£ Normalize**
        - After upload/import, click **"Normalize Data"** to merge and standardize features.
        - Output: `intake_table.csv` ready for Stage B (Anonymization).

        **4ï¸âƒ£ Generate Synthetic Data**
        - If no input data is available, the AI can synthesize a demo dataset representing:
          `asset_id, asset_type, city, market_value, loan_amount, legal_source, condition_score`.

        **5ï¸âƒ£ Output**
        - A unified CSV file is produced â†’ download or proceed directly to **Stage B**.
        """)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # (A.1) UPLOAD ZONE â€” Human Inputs
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ“¤ Upload Data Files (Field Agents / Loans / Legal Docs)")
    uploaded_files = st.file_uploader(
        "Upload multiple files",
        type=["csv", "xlsx", "zip"],
        accept_multiple_files=True,
        key="asset_upload_files"
    )

    uploaded_dfs = []
    if uploaded_files:
        for f in uploaded_files:
            try:
                if f.name.endswith(".csv"):
                    df = pd.read_csv(f)
                elif f.name.endswith(".xlsx"):
                    df = pd.read_excel(f)
                else:
                    st.info(f"ğŸ“¦ Skipping non-tabular file: {f.name}")
                    continue
                st.success(f"âœ… Loaded `{f.name}` ({len(df)} rows, {len(df.columns)} cols)")
                uploaded_dfs.append(df)
            except Exception as e:
                st.error(f"âŒ Failed to read {f.name}: {e}")


    # â”€â”€New  global runs dir (shared across stages) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    RUNS_DIR = os.path.abspath("./.tmp_runs")
    os.makedirs(RUNS_DIR, exist_ok=True)
    
   

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # (A.2) PUBLIC DATASETS â€” Kaggle / HF / OpenML
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸŒ Import Public Datasets (Kaggle / Hugging Face / OpenML / Portals)")

    # keep a place to persist search results across reruns
    ss.setdefault("kaggle_search_df", pd.DataFrame())

    src = st.selectbox(
        "Select source",
        ["Kaggle (API)", "Hugging Face", "OpenML", "Public Domain Portals"],
        key="asset_pubsrc"
    )
    query = st.text_input("Search keywords", "house prices real estate valuation", key="asset_pubquery")

    # helpers
    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    
    def _safe_read_csv(fp: str) -> pd.DataFrame:
        """
        Robust CSV reader:
        - Tries multiple encodings (utf-8, utf-8-sig, cp1252, latin-1)
        - Tries common separators
        - Skips bad rows rather than crashing
        """
        encodings = ["utf-8", "utf-8-sig", "cp1252", "latin-1"]
        seps = [",", ";", "\t", "|"]

        last_err = None
        for enc in encodings:
            for sep in seps:
                try:
                    df_try = pd.read_csv(
                        fp,
                        encoding=enc,
                        sep=sep,
                        engine="python",
                        on_bad_lines="skip",   # pandas >=1.3
                    )
                    # Require at least 2 columns to consider it valid
                    if df_try.shape[1] >= 2:
                        return df_try
                except Exception as e:
                    last_err = e
                    continue

        # Final fallback: read bytes, decode with latin-1 replacement, then parse in-memory
        try:
            with open(fp, "rb") as f:
                raw = f.read()
            text = raw.decode("latin-1", errors="replace")
            for sep in seps:
                try:
                    return pd.read_csv(
                        io.StringIO(text),
                        sep=sep,
                        engine="python",
                        on_bad_lines="skip",
                    )
                except Exception:
                    pass
        except Exception as e:
            last_err = e

        raise RuntimeError(f"Could not parse CSV with common encodings/separators. Last error: {last_err}")

    


    # Kaggle search
    if st.button("ğŸ” Search dataset", key="btn_asset_pubsearch"):
        with st.spinner("Searching datasets..."):
            try:
                if src == "Kaggle (API)":
                    import subprocess, io
                    cmd = ["kaggle", "datasets", "list", "-s", query, "-v"]  # -v => CSV output
                    out = subprocess.run(cmd, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                    if out.returncode != 0:
                        st.error(f"Kaggle CLI failed: {out.stderr.strip() or out.stdout.strip()}")
                        st.info("ğŸ’¡ Ensure ~/.kaggle/kaggle.json exists and has chmod 600.")
                        ss["kaggle_search_df"] = pd.DataFrame()
                    else:
                        df_pub = pd.read_csv(io.StringIO(out.stdout))
                        keep = [c for c in ["ref","title","size","lastUpdated","downloadCount","voteCount","usabilityRating"] if c in df_pub.columns]
                        ss["kaggle_search_df"] = df_pub[keep]
                        st.success("âœ… Kaggle API results shown.")
                elif src == "Hugging Face":
                    from huggingface_hub import list_datasets
                    results = list_datasets(search=query)
                    df_pub = pd.DataFrame([{"Dataset": r.id, "Tags": ", ".join(r.tags)} for r in results[:50]])
                    st.dataframe(df_pub, use_container_width=True, hide_index=True)
                    st.success("âœ… Hugging Face datasets retrieved.")
                elif src == "OpenML":
                    st.markdown(f"[ğŸ“Š OpenML Search â†—ï¸](https://www.openml.org/search?type=data&q={query})")
                elif src == "Public Domain Portals":
                    st.markdown("""
                    - [ğŸŒ data.gov](https://www.data.gov/)
                    - [ğŸ‡ªğŸ‡º data.europa.eu](https://data.europa.eu/)
                    - [ğŸ‡¸ğŸ‡¬ data.gov.sg](https://data.gov.sg/)
                    - [ğŸ‡»ğŸ‡³ data.gov.vn](https://data.gov.vn/)
                    """)
            except Exception as e:
                st.error(f"Search failed: {e}")

    # If we have Kaggle results, show table + import controls
    if src == "Kaggle (API)" and not ss["kaggle_search_df"].empty:
        st.dataframe(ss["kaggle_search_df"], use_container_width=True, hide_index=True)

        with st.expander("â¬‡ï¸ Import Selected Kaggle Dataset", expanded=True):
            refs = ss["kaggle_search_df"]["ref"].astype(str).tolist()
            selected_ref = st.selectbox("Choose a dataset (ref)", refs, key="asset_kaggle_ref")

            kag_dir = os.path.join(RUNS_DIR, "kaggle")
            os.makedirs(kag_dir, exist_ok=True)
            safe_ref = re.sub(r"[^a-zA-Z0-9._/-]+", "_", selected_ref)
            safe_ref_for_file = safe_ref.replace("/", "__")
            dest = os.path.join(kag_dir, safe_ref_for_file)
            os.makedirs(dest, exist_ok=True)

            # Optional: let user set a server-side save folder (relative to project root)
            st.markdown("**Optional server-side save folder (relative to project root)**")
            default_svdir = os.path.join(RUNS_DIR, "kaggle_exports")
            svdir = st.text_input(
                "Save to folder (server-side)",
                value=default_svdir,
                key="asset_kaggle_svdir",
                help="This saves on the server/WSL side (not your desktop). Use Download button below for local Save As."
            )

            # Main import button
            if st.button("ğŸ“¥ Download & Import Selected", key="btn_asset_kaggle_dl", use_container_width=True):
                try:
                    import subprocess
                    cmd = ["kaggle", "datasets", "download", "-d", selected_ref, "-p", dest, "--unzip"]
                    r = subprocess.run(cmd, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                    if r.returncode != 0:
                        raise RuntimeError(r.stderr.strip() or r.stdout.strip())

                    csvs = [f for f in os.listdir(dest) if f.lower().endswith(".csv")]
                    if not csvs:
                        raise FileNotFoundError("No CSV found in the downloaded archive.")
                    fp = os.path.join(dest, csvs[0])

                    # Load & stash for downstream stages + download button
                    df_imp = _safe_read_csv(fp)
                    ss["asset_intake_df"] = df_imp

                    # Save a unified copy with timestamp in RUNS_DIR
                    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
                    uni_fp = os.path.join(RUNS_DIR, f"intake_table.{ts}.csv")
                    df_imp.to_csv(uni_fp, index=False)

                    st.success(f"âœ… Imported {len(df_imp):,} rows from `{selected_ref}`")
                    st.caption(f"Saved unified intake copy: `{uni_fp}`")
                    st.dataframe(df_imp.head(100), use_container_width=True)

                    # â”€â”€ NEW: Local Download (browser) with dataset-name.csv â”€â”€
                    st.markdown("#### ğŸ’¾ Download")
                    default_fname = f"{safe_ref_for_file}.csv"
                    st.download_button(
                        label="â¬‡ï¸ Download CSV",
                        file_name=default_fname,
                        data=df_imp.to_csv(index=False).encode("utf-8"),
                        mime="text/csv",
                        key="asset_kaggle_download"
                    )

                    # â”€â”€ NEW: Optional server-side save with user folder â”€â”€
                    st.markdown("#### ğŸ—‚ï¸ Save on Server (optional)")
                    # sanitize: keep within project root
                    project_root = os.path.abspath(os.path.join(RUNS_DIR, "..", ".."))
                    svdir_abs = os.path.abspath(svdir)
                    if not svdir_abs.startswith(project_root):
                        st.warning("âš ï¸ Path is outside project root; resetting to default exports folder.")
                        svdir_abs = os.path.abspath(default_svdir)

                    os.makedirs(svdir_abs, exist_ok=True)
                    save_name = f"{safe_ref_for_file}.csv"
                    server_save_path = os.path.join(svdir_abs, save_name)

                    if st.button("ğŸ’½ Save CSV on Server", key="btn_asset_kaggle_save_server"):
                        try:
                            df_imp.to_csv(server_save_path, index=False)
                            rel_path = os.path.relpath(server_save_path, start=project_root)
                            st.success(f"âœ… Saved on server: `{server_save_path}`")
                            st.caption(f"(Relative to project root: ./{rel_path})")
                        except Exception as e:
                            st.error(f"Server save failed: {e}")

                except Exception as e:
                    st.error(f"Import failed: {e}")
                    st.info("Tip: check Kaggle auth and try another dataset.")

    
   

    # Quick HF import (optional direct load)
    if src == "Hugging Face":
        st.markdown("#### Or load directly by repo id")
        hf_repo = st.text_input("ğŸ¤— Dataset repo (e.g. uciml/real-estate-valuation)", value="uciml/real-estate-valuation", key="asset_hf_repo")
        if st.button("ğŸ“¥ Load from HF", key="btn_asset_hf_load", use_container_width=True):
            try:
                from datasets import load_dataset
                ds = load_dataset(hf_repo)
                split = next(iter(ds.keys()))
                df_imp = ds[split].to_pandas()
                ss["asset_intake_df"] = df_imp
                uni_fp = os.path.join(RUNS_DIR, f"intake_table.{_ts()}.csv")
                df_imp.to_csv(uni_fp, index=False)
                st.success(f"âœ… Loaded {len(df_imp):,} rows from {hf_repo} (split: {split})")
                st.caption(f"Saved unified intake copy: `{uni_fp}`")
                st.dataframe(df_imp.head(100), use_container_width=True)
            except Exception as e:
                st.error(f"HF load failed: {e}")

    st.divider()
    
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # (A.3) NORMALIZE & GENERATE UNIFIED CSV
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ§¹ Normalize & Combine All Inputs")

    # ---------- helpers ----------
    import io, csv, re
    from pathlib import Path

    def _slug(name: str) -> str:
        return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

    def _read_any_table(uploaded_file) -> pd.DataFrame:
        """Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback."""
        name = uploaded_file.name.lower()

        # Excel
        if name.endswith((".xlsx", ".xls")):
            return pd.read_excel(uploaded_file)

        # Text (CSV/TSV/TXT): try utf-8 then latin-1; sniff delimiter
        raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
        for enc in ("utf-8", "latin-1"):
            try:
                text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
                head = "\n".join(text.splitlines()[:10]) or text
                try:
                    dialect = csv.Sniffer().sniff(head)
                    sep = dialect.delimiter
                except Exception:
                    sep = ","
                return pd.read_csv(io.StringIO(text), sep=sep)
            except Exception:
                continue
        # last resort (python engine)
        return pd.read_csv(io.BytesIO(raw), engine="python")

    # ---------- optional upload right here ----------
    st.markdown("#### â¬†ï¸ Optional: Upload a CSV/TSV/TXT/XLSX to normalize")
    uploaded = st.file_uploader(
        "Upload a dataset file (or skip if you already imported via Kaggle/HF).",
        type=["csv", "tsv", "txt", "xlsx"],
        key="norm_upload_once",
        accept_multiple_files=False
    )

    if uploaded is not None:
        try:
            df_up = _read_any_table(uploaded)
            ss["asset_intake_df"] = df_up
            ss["last_dataset_name"] = Path(uploaded.name).stem  # remember original name
            st.success(f"âœ… Loaded {len(df_up):,} rows from **{uploaded.name}**")
            st.dataframe(df_up.head(100), use_container_width=True, hide_index=True)
        except Exception as e:
            st.error(f"Could not read file: {e}")

    # ---------- normalization source ----------
    df_src = ss.get("asset_intake_df")
    # Best-effort name priority: last_dataset_name (Kaggle/HF/upload) â†’ fallback
    base_name = _slug(ss.get("last_dataset_name") or ss.get("asset_intake_source_name") or "dataset")

    if df_src is None or len(df_src) == 0:
        st.info("Upload/import a dataset first (Kaggle/HF/Upload), then come back to normalize.")
    else:
        with st.expander("âš™ï¸ Normalization options", expanded=False):
            drop_dupes = st.checkbox("Drop duplicate rows", value=True)
            trim_whitespace = st.checkbox("Trim whitespace in string columns", value=True)
            lower_columns = st.checkbox("Lowercase column names", value=True)

        def _normalize(df: pd.DataFrame) -> pd.DataFrame:
            out = df.copy()
            # 1) basic cleanup
            if lower_columns:
                out.columns = [c.strip().lower() for c in out.columns]
            if trim_whitespace:
                for c in out.select_dtypes(include=["object"]).columns:
                    out[c] = out[c].astype(str).str.strip()
            if drop_dupes:
                out = out.drop_duplicates().reset_index(drop=True)
            # (Optional) add any schema harmonization here later
            return out

        if st.button("ğŸ§ª Normalize & Generate Unified CSV", key="btn_normalize", use_container_width=True):
            norm_df = _normalize(df_src)

            # Ensure output dir
            norm_dir = os.path.join(RUNS_DIR, "normalized")
            os.makedirs(norm_dir, exist_ok=True)

            # Build file name: <original>-Normalized.csv   (slug-safe base_name)
            norm_name = f"{base_name}-Normalized.csv"
            norm_path = os.path.join(norm_dir, norm_name)

            # Save to disk with utf-8-sig (friendlier for Excel)
            norm_df.to_csv(norm_path, index=False, encoding="utf-8-sig")

            # Prepare one bytes blob for both download buttons
            _norm_bytes = norm_df.to_csv(index=False).encode("utf-8-sig")
            _rows, _cols = len(norm_df), len(norm_df.columns)
            _size_kb = max(1, int(len(_norm_bytes) / 1024))
            _norm_file_only = Path(norm_path).name

            # Sticky banner: filename â€¢ rowsÃ—cols â€¢ size
            st.markdown(
                f"""
                <div style="
                    position: sticky;
                    top: 64px;
                    z-index: 50;
                    background: rgba(16,185,129,0.10);
                    border: 1px solid #10b981;
                    padding: 12px 16px;
                    border-radius: 12px;
                    margin: 8px 0 14px 0;
                ">
                <b>âœ… Normalized CSV:</b> <code>{_norm_file_only}</code>
                &nbsp;â€¢&nbsp; {_rows:,} rows Ã— {_cols} cols &nbsp;â€¢&nbsp; {_size_kb} KB
                </div>
                """,
                unsafe_allow_html=True
            )

            # BIG centered primary download button (TOP)
            cL, cM, cR = st.columns([1, 2.5, 1])
            with cM:
                st.download_button(
                    "â¬‡ï¸  Download Normalized CSV",
                    data=_norm_bytes,
                    file_name=_norm_file_only,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_norm_top"
                )

            # Copyable saved path
            st.text_input("Saved to (server path)", norm_path, disabled=True, label_visibility="collapsed")

            # Preview table
            st.dataframe(norm_df.head(100), use_container_width=True, hide_index=True)

            # BIG centered primary download button (BOTTOM)
            cL2, cM2, cR2 = st.columns([1, 2.5, 1])
            with cM2:
                st.download_button(
                    "â¬‡ï¸  Download Normalized CSV",
                    data=_norm_bytes,
                    file_name=_norm_file_only,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_norm_bottom"
                )

    # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # # (A.1b) QUICK START â€” Generate Synthetic Data (always visible here)
    # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # st.markdown("### ğŸ² Quick Start: Generate Synthetic Data")
    # c_syn1, c_syn2 = st.columns([2, 1])
    # with c_syn1:
    #     nrows = st.slider("Number of synthetic rows", 20, 1000, 150, step=10, key="slider_synth_rows_A_quick")
    # with c_syn2:
    #     if st.button("ğŸš€ Generate Synthetic Dataset Now", key="btn_generate_synth_A_quick", use_container_width=True):
    #         try:
    #             df_synth = quick_synth(nrows)
    #             ss["asset_intake_df"] = df_synth
    #             os.makedirs("./.tmp_runs", exist_ok=True)
    #             synth_path = f"./.tmp_runs/intake_table_synth_{_ts()}.csv"
    #             df_synth.to_csv(synth_path, index=False, encoding="utf-8-sig")
    #             st.success(f"âœ… Synthetic dataset created ({len(df_synth)} rows). Saved: `{synth_path}`")
    #             st.dataframe(df_synth.head(20), use_container_width=True, hide_index=True)
    #             st.download_button(
    #                 "â¬‡ï¸ Download Synthetic CSV",
    #                 df_synth.to_csv(index=False).encode("utf-8-sig"),
    #                 file_name="synthetic_intake.csv",
    #                 mime="text/csv",
    #                 key="dl_synth_A_quick"
    #             )
    #         except Exception as e:
    #             st.error(f"Synthetic generation failed: {e}")

       
        
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # (A.4) SYNTHETIC DATA GENERATION
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ¤– Generate Synthetic Data (Fallback)")
    nrows = st.slider("Number of synthetic rows", 10, 500, 150, step=10, key="slider_synth_rows")
    if st.button("ğŸ² Generate Synthetic Dataset", key="btn_generate_synth"):
        try:
            df_synth = quick_synth(nrows)
            ss["asset_intake_df"] = df_synth
            os.makedirs("./.tmp_runs", exist_ok=True)
            synth_path = f"./.tmp_runs/intake_table_synth_{datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')}.csv"
            df_synth.to_csv(synth_path, index=False)
            st.success(f"âœ… Synthetic dataset created ({len(df_synth)} rows).")
            st.dataframe(df_synth.head(20), use_container_width=True)
            st.download_button("ğŸ’¾ Download Synthetic CSV", df_synth.to_csv(index=False), "synthetic_intake.csv", "text/csv")
        except Exception as e:
            st.error(f"Synthetic generation failed: {e}")



# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # A â€” INTAKE & EVIDENCE (0..1)
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# with tabA:
#     import hashlib, io, json, os
#     from datetime import datetime, timezone

#     st.subheader("A. Intake & Evidence")
#     st.caption("Steps: **0) Intake & Identity**, **1) Evidence Extraction (OCR/EXIF/GPS)**")

#     # ---------- Local helpers ----------
#     def sha1_of_filelike(fobj) -> str:
#         """Compute SHA1 hash from file-like object without exhausting stream."""
#         pos = fobj.tell() if hasattr(fobj, "tell") else None
#         fobj.seek(0)
#         h = hashlib.sha1()
#         for chunk in iter(lambda: fobj.read(8192), b""):
#             if isinstance(chunk, str):
#                 chunk = chunk.encode("utf-8")
#             h.update(chunk)
#         if pos is not None:
#             fobj.seek(pos)
#         return h.hexdigest()

#     def extract_fake_exif_gps() -> dict:
#         return {
#             "gps": {"lat": 10.7758, "lon": 106.7009},
#             "ts": datetime.now(timezone.utc).isoformat()
#         }
#     def quick_synth(n: int = 150) -> pd.DataFrame:
#         """Generate synthetic intake dataset for demonstration/testing."""
#         import random
#         import numpy as np
#         from datetime import datetime, timedelta, timezone

#         t = datetime.now(timezone.utc)
#         asset_types = ["House", "Apartment", "Car", "Land", "Factory"]
#         cities = ["HCMC", "Hanoi", "Da Nang", "Can Tho", "Hai Phong"]
#         regions = {
#             "HCMC": "South",
#             "Hanoi": "North",
#             "Da Nang": "Central",
#             "Can Tho": "Mekong",
#             "Hai Phong": "North"
#         }

#         rows = []
#         for i in range(n):
#             city = cities[i % len(cities)]
#             region = regions[city]
#             base_value = 120_000 + (i % 17) * 3_500
#             noise = random.uniform(-0.1, 0.1) * base_value

#             # Derived + synthetic realism
#             market_value = base_value + noise
#             age_years = random.randint(0, 40)
#             condition_score = round(max(0.5, 1.0 - (age_years / 100.0) + random.uniform(-0.1, 0.1)), 2)
#             loan_amount = round(market_value * random.uniform(0.4, 0.8))
#             employment_years = random.randint(1, 30)
#             credit_history_years = random.randint(1, 25)
#             delinquencies = random.randint(0, 3)
#             current_loans = random.randint(0, 5)
#             registry_source = random.choice(["gov_registry", "private_registry", "unknown"])
#             evidence_count = random.randint(1, 5)
#             valuation_date = (t - timedelta(days=random.randint(0, 720))).strftime("%Y-%m-%d")

#             rows.append({
#                 "application_id": f"APP_{t.strftime('%H%M%S')}_{i:04d}",
#                 "asset_id": f"A{t.strftime('%M%S')}{i:04d}",
#                 "asset_type": random.choice(asset_types),
#                 "market_value": round(market_value, 2),
#                 "loan_amount": round(loan_amount, 2),
#                 "age_years": age_years,
#                 "condition_score": condition_score,
#                 "employment_years": employment_years,
#                 "credit_history_years": credit_history_years,
#                 "delinquencies": delinquencies,
#                 "current_loans": current_loans,
#                 "city": city,
#                 "region": region,
#                 "registry_source": registry_source,
#                 "evidence_count": evidence_count,
#                 "valuation_date": valuation_date,
#             })

#         df = pd.DataFrame(rows)
#         df["ltv_ratio"] = (df["loan_amount"] / df["market_value"]).round(2)
#         df["legal_penalty_flag"] = np.where(df["registry_source"] == "unknown", 1, 0)
#         return df


#     # def quick_synth(n: int = 150) -> pd.DataFrame:
#     #     t = datetime.now(timezone.utc)
#     #     rows = [{
#     #         "application_id": f"APP_{t.strftime('%H%M%S')}_{i:04d}",
#     #         "asset_id": f"A{t.strftime('%M%S')}{i:04d}",
#     #         "asset_type": ["House","Apartment","Car","Land","Factory"][i % 5],
#     #         "market_value": 120000 + (i % 17) * 3500,
#     #         "age_years": (i % 35),
#     #         "loan_amount": 80000 + (i % 23) * 2500,
#     #         "employment_years": (i % 25),
#     #         "credit_history_years": (i % 20),
#     #         "delinquencies": (i % 3),
#     #         "current_loans": (i % 5),
#     #         "city": ["HCMC","Hanoi","Da Nang","Can Tho","Hai Phong"][i % 5],
#     #     } for i in range(n)]
#     #     return pd.DataFrame(rows)

#     # def synth_why_table() -> pd.DataFrame:
#     #     return pd.DataFrame([
#     #         {"Metric": "PII present", "Why it matters": "Must be masked before feature engineering (privacy-by-design)."},
#     #         {"Metric": "Evidence linked (%)", "Why it matters": "Traceability between assets and documents/photos."},
#     #         {"Metric": "Rows (intake)", "Why it matters": "Sanity-check volume before downstream costs."},
#     #     ])

#     # ---------- A.0 â€” Intake & Identity ----------
#     st.markdown("### **0) Intake & Identity**")

#     # ğŸ“˜ Quick user guide
#     with st.expander("ğŸ“˜ Quick User Guide", expanded=False):
#         st.markdown("""
#         **Goal:** Collect, normalize, and validate all asset data before appraisal.

#         **1ï¸âƒ£ Upload Your Data**
#         - CSV from field agents, real-estate papers, or loan portfolios.
#         - Include key columns: `asset_id`, `asset_type`, `market_value`, `loan_amount`, `city`.

#         **2ï¸âƒ£ Manual Add**
#         - Use to simulate new asset entries for quick testing.

#         **3ï¸âƒ£ Attach Evidence**
#         - Add photos or documents (JPG/PDF). GPS/EXIF auto-parsed.

#         **4ï¸âƒ£ Public Datasets**
#         - Search Kaggle / Hugging Face / Open ML for open valuation benchmarks.
#         """)

#     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#     # ğŸ” Search & Import Public Datasets
#     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#     st.markdown("### ğŸ” Search Public Datasets (Kaggle / Hugging Face / OpenML / Public Portals)")
#     st.caption("Search, preview, or import datasets from open sources â€” useful for model benchmarking or demo generation.")

#     src = st.selectbox(
#         "Select source",
#         ["Kaggle (API)", "Kaggle (Web)", "Hugging Face", "OpenML", "Public Domain Portals"],
#         key="asset_pubsrc"
#     )
#     query = st.text_input(
#         "Search keywords",
#         placeholder="e.g. house prices Vietnam, real estate valuation",
#         key="asset_pubquery"
#     )

#     if st.button("ğŸ” Search dataset", key="btn_asset_pubsearch"):
#         with st.spinner("Searching public datasetsâ€¦"):
#             try:
#                 # Kaggle API mode
#                 if src == "Kaggle (API)":
#                     import subprocess, pandas as pd
#                     cmd = ["kaggle", "datasets", "list", "-s", query, "-v"]
#                     try:
#                         output = subprocess.check_output(cmd, text=True)
#                         lines = output.strip().splitlines()
#                         if len(lines) > 1:
#                             header = [h.strip() for h in lines[0].split(",")]
#                             data = [l.split(",") for l in lines[1:]]
#                             df_pub = pd.DataFrame(data, columns=header)
#                             st.dataframe(df_pub.head(25), use_container_width=True)
#                             st.success("âœ… Kaggle API search complete. Use CLI or download link to import.")
#                         else:
#                             st.warning("No datasets found for your query.")
#                     except Exception as e:
#                         st.error(f"Kaggle API search failed: {e}")
#                         st.info("ğŸ’¡ Tip: Ensure your Kaggle API key (~/.kaggle/kaggle.json) is configured.")

#                 # Kaggle Web mode (no API needed)
#                 elif src == "Kaggle (Web)":
#                     st.markdown(
#                         f"[ğŸŒ Open Kaggle Search â†—ï¸](https://www.kaggle.com/datasets?search={query})"
#                     )

#                 # Hugging Face datasets
#                 elif src == "Hugging Face":
#                     from huggingface_hub import list_datasets
#                     results = list_datasets(search=query)
#                     df_pub = pd.DataFrame(
#                         [{"Dataset": r.id, "Tags": ", ".join(r.tags)} for r in results[:25]]
#                     )
#                     st.dataframe(df_pub, use_container_width=True)
#                     st.success("âœ… Hugging Face datasets retrieved successfully.")

#                 # OpenML
#                 elif src == "OpenML":
#                     st.markdown(
#                         f"[ğŸ“Š Open OpenML Search â†—ï¸](https://www.openml.org/search?type=data&q={query})"
#                     )

#                 # Public domain portals
#                 elif src == "Public Domain Portals":
#                     st.markdown("""
#                     **Global Open Data Portals**
#                     - [ğŸŒ data.gov (US)](https://www.data.gov/)
#                     - [ğŸ‡ªğŸ‡º data.europa.eu (EU)](https://data.europa.eu/)
#                     - [ğŸ‡¸ğŸ‡¬ data.gov.sg (Singapore)](https://data.gov.sg/)
#                     - [ğŸ‡»ğŸ‡³ data.gov.vn (Vietnam)](https://data.gov.vn/)
#                     """)
#             except Exception as e:
#                 st.error(f"âŒ Search failed: {e}")

#     # Optional layout continuation
#     left, right = st.columns([1.4, 1])

    
    
#     # # ğŸ” Search Public Datasets
#     # st.markdown("#### ğŸ” Search Public Datasets (Kaggle / Hugging Face / Open ML)")
#     # src = st.selectbox("Source", ["Kaggle", "Hugging Face Datasets", "Open ML"], key="asset_pubsrc")
#     # query = st.text_input("Search keywords", placeholder="e.g. house prices Vietnam, real estate valuation", key="asset_pubquery")

#     # if st.button("Search dataset", key="btn_asset_pubsearch"):
#     #     with st.spinner("Searching public datasetsâ€¦"):
#     #         try:
#     #             if src == "Hugging Face Datasets":
#     #                 from huggingface_hub import list_datasets
#     #                 results = list_datasets(search=query)
#     #                 df_pub = pd.DataFrame([{"Name": r.id, "Tags": ", ".join(r.tags)} for r in results[:25]])
#     #                 st.dataframe(df_pub)
#     #             elif src == "Kaggle":
#     #                 st.info("Use `kaggle datasets list -s <keywords>` in terminal to explore.")
#     #             else:
#     #                 st.info(f"Open ML search â†’ https://www.openml.org/search?type=data&q={query}")
#     #         except Exception as e:
#     #             st.error(f"Search failed: {e}")

#     # left, right = st.columns([1.4, 1])
    


#     # ---------- A.1 â€” Evidence Extraction ----------
#     st.markdown("### **1) Evidence Extraction (OCR/EXIF/GPS)**")
#     evid = st.file_uploader("Attach evidence (images or PDFs, optional)",
#                             type=["png", "jpg", "jpeg", "pdf"],
#                             accept_multiple_files=True,
#                             key="asset_evidence_files")

#     cA1_1, cA1_2 = st.columns(2)
#     with cA1_1:
#         if st.button("Extract & Index Evidence", key="btn_extract_evidence"):
#             idx_items = []
#             for i, f in enumerate(evid or []):
#                 try:
#                     bio = io.BytesIO(f.read())
#                     file_hash = sha1_of_filelike(bio)
#                     bio.seek(0)
#                     ext = (f.name or "").split(".")[-1].lower()
#                     doc_type = "image" if ext in ("png","jpg","jpeg") else "pdf"
#                     exif = extract_fake_exif_gps()
#                     idx_items.append({
#                         "evidence_id": f"EV-{i+1:04d}",
#                         "file_name": f.name,
#                         "doc_type": doc_type,
#                         "sha1": file_hash,
#                         "exif": exif,
#                     })
#                 except Exception as e:
#                     st.error(f"Error processing {f.name}: {e}")

#             evidence_index = {
#                 "generated_at": datetime.now(timezone.utc).isoformat(),
#                 "count": len(idx_items),
#                 "items": idx_items,
#             }
#             ss["asset_evidence_index"] = evidence_index

#             ev_path = os.path.join("./.tmp_runs", f"evidence_index.{datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')}.json")
#             with open(ev_path, "w", encoding="utf-8") as fp:
#                 json.dump(evidence_index, fp, ensure_ascii=False, indent=2)
#             st.success(f"Saved: `{ev_path}`  â€¢  Items: {len(idx_items)}")
#             st.json(evidence_index)

#     with cA1_2:
#         if ss.get("asset_intake_df") is not None and ss.get("asset_evidence_index") is not None:
#             linked_pct = 1.0 if len(ss["asset_evidence_index"]["items"]) >= 1 else 0.0
#             st.metric("Evidence Linked (â‰¥1)", f"{linked_pct:.0%}")
#         elif ss.get("asset_intake_df") is not None:
#             st.info("Upload evidence and click **Extract & Index Evidence**.")
#         else:
#             st.warning("Build the intake table first (A.0).")


        
#     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#     # ğŸ“Š Synthetic Data Generation
#     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#     st.markdown("### ğŸ“Š Generate Synthetic Data")
#     st.caption("Quickly create a sample intake dataset for testing or demos.")

#     num_rows = st.slider("Number of synthetic rows", min_value=10, max_value=500, value=150, step=10)
#     if st.button("âš™ï¸ Generate Synthetic Intake Dataset", key="btn_generate_synth"):
#         try:
#             df_synth = quick_synth(num_rows)
#             ss["asset_intake_df"] = df_synth
#             st.success(f"âœ… Generated synthetic dataset with {len(df_synth)} rows.")
#             st.dataframe(df_synth.head(20), use_container_width=True)
#             # Optional: auto-save artifact
#             out_path = f"./.tmp_runs/intake_table_synth_{datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')}.csv"
#             os.makedirs("./.tmp_runs", exist_ok=True)
#             df_synth.to_csv(out_path, index=False)
#             st.info(f"Saved to: `{out_path}`")
#         except Exception as e:
#             st.error(f"Synthetic data generation failed: {e}")


#         # ---------- Intake block ----------
#         with left:
#             up_csv = st.file_uploader("Upload Asset CSV", type=["csv"], key="asset_csv")
#             with st.expander("â• Add manual asset row", expanded=False):
#                 m1, m2 = st.columns(2)
#                 asset_type = m1.selectbox("Asset Type", ["House","Apartment","Car","Land","Factory"])
#                 market_value = m2.number_input("Market Value ($)", 0, 10_000_000, 250_000, step=1_000)
#                 age_years = m1.number_input("Age (years)", 0, 100, 10)
#                 loan_amount = m2.number_input("Requested Loan ($)", 0, 10_000_000, 120_000, step=1_000)
#                 employment_years = m1.number_input("Employment Years", 0, 60, 5)
#                 credit_hist_years = m2.number_input("Credit History (years)", 0, 50, 6)
#                 delinq = m1.number_input("Delinquencies", 0, 50, 1)
#                 curr_loans = m2.number_input("Current Loans", 0, 50, 2)
#                 city = m1.text_input("City", "HCMC")
#                 add_row = st.button("Add manual asset row", key="btn_add_manual_row")

#             if st.button("Build Intake Table (fallback to synthetic if empty)", key="btn_build_intake"):
#                 rows = []
#                 # CSV
#                 if up_csv is not None:
#                     try:
#                         rows.append(pd.read_csv(up_csv))
#                     except Exception as e:
#                         st.error(f"CSV parse error: {e}")
#                 # Manual row
#                 if add_row:
#                     rows.append(pd.DataFrame([{
#                         "application_id": f"APP_{datetime.now(timezone.utc).strftime('%H%M%S')}",
#                         "asset_id": f"A{datetime.now(timezone.utc).strftime('%M%S')}",
#                         "asset_type": asset_type, "market_value": market_value, "age_years": age_years,
#                         "loan_amount": loan_amount, "employment_years": employment_years,
#                         "credit_history_years": credit_hist_years, "delinquencies": delinq,
#                         "current_loans": curr_loans, "city": city
#                     }]))
#                 # Synthetic fallback
#                 df = pd.concat(rows, ignore_index=True) if rows else quick_synth(150)
#                 if not rows:
#                     st.info("No inputs provided â€” generated synthetic intake dataset.")

#                 ss["asset_intake_df"] = df
#                 os.makedirs("./.tmp_runs", exist_ok=True)
#                 intake_path = os.path.join("./.tmp_runs", f"intake_table.{datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')}.csv")
#                 df.to_csv(intake_path, index=False)
#                 st.success(f"Saved: `{intake_path}`  â€¢  Rows: {len(df)}")
#                 st.dataframe(df.head(15), use_container_width=True)

#         with right:
#             st.markdown("#### Generated Metrics â€” What & Why")
#             st.dataframe(synth_why_table(), use_container_width=True)

#         st.markdown("---")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# B â€” PRIVACY & FEATURES (2..3)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tabB:
    st.subheader("B. Privacy & Features")
    st.caption("Steps: **2) Anonymize**, **3) Feature Engineering + Comps**")

    import re, math, json, os, time
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    
    # ----------------------------
    # B.2 â€” Anonymize / Sanitize PII
    # ----------------------------
    st.markdown("### **2) Anonymize / Sanitize PII**")

    import io, csv, re, os
    from pathlib import Path

    ss = st.session_state  # make sure this exists globally

    def _slug(name: str) -> str:
        return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

    def _read_any_table(uploaded_file) -> pd.DataFrame:
        """Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback."""
        name = uploaded_file.name.lower()
        if name.endswith((".xlsx", ".xls")):
            return pd.read_excel(uploaded_file)

        raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
        for enc in ("utf-8", "latin-1"):
            try:
                text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
                head = "\n".join(text.splitlines()[:10]) or text
                try:
                    sep = csv.Sniffer().sniff(head).delimiter
                except Exception:
                    sep = ","
                return pd.read_csv(io.StringIO(text), sep=sep)
            except Exception:
                continue
        return pd.read_csv(io.BytesIO(raw), engine="python")

    def _anonymize(df: pd.DataFrame) -> pd.DataFrame:
        """Mask likely-PII text while preserving common join keys."""
        if df is None or df.empty:
            return df
        out = df.copy()
        join_keys = {"loan_id", "asset_id", "application_id"}
        pii_like = re.compile(r"(name|email|phone|addr|address|national|passport|id|nid)", re.I)

        for col in out.columns:
            if col in join_keys:
                continue
            if out[col].dtype == "object" and pii_like.search(col):
                s = out[col].astype(str)
                s = s.str.replace(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+", "[EMAIL]", regex=True)
                s = s.str.replace(r"\b(\+?\d{1,3}[-.\s]?)?\d{7,}\b", "[PHONE]", regex=True)
                s = s.str.replace(r"\b\d{9,16}\b", "[ID]", regex=True)
                out[col] = s
        return out.drop_duplicates().reset_index(drop=True)

    # ---------- Source picker (Stage A or Upload here) ----------
    st.markdown("#### ğŸ”„ Choose data source")
    src_choice = st.radio(
        "Pick how you want to provide data:",
        ["Use data from Stage A", "Upload a new file here"],
        horizontal=True,
        key="b_src_choice"
    )

    df_src, base_name = None, "dataset"

    if src_choice == "Use data from Stage A":
        df_src = ss.get("asset_intake_df")
        if isinstance(df_src, pd.DataFrame) and not df_src.empty:
            base_name = _slug(ss.get("last_dataset_name") or "dataset")
            st.success(f"Using Stage-A dataset: **{base_name}** â€¢ {len(df_src):,} rows")
            st.dataframe(df_src.head(10), use_container_width=True, hide_index=True)
        else:
            st.warning("No data found from Stage A. Upload below instead.")
    else:
        st.markdown(
            """
            <div style="border:2px dashed #22c55e;padding:14px;border-radius:12px;
                        background:rgba(34,197,94,0.06);margin:6px 0 12px 0;">
            <b>â¬†ï¸ Upload CSV/TSV/TXT/XLSX for anonymization</b>
            </div>
            """,
            unsafe_allow_html=True,
        )
        up = st.file_uploader(
            "Upload a dataset file",
            type=["csv","tsv","txt","xlsx"],
            key="b_upload",
            accept_multiple_files=False,
        )
        if up is not None:
            try:
                df_src = _read_any_table(up)
                base_name = _slug(Path(up.name).stem)
                ss["asset_intake_df"] = df_src
                ss["last_dataset_name"] = base_name
                st.success(f"âœ… Loaded {len(df_src):,} rows from **{up.name}**")
                st.dataframe(df_src.head(20), use_container_width=True, hide_index=True)
            except Exception as e:
                st.error(f"Could not read file: {e}")

    if not (isinstance(df_src, pd.DataFrame) and not df_src.empty):
        st.info("Provide data (via Stage A or upload here) to enable anonymization.")
    else:
        if st.button("ğŸ›¡ï¸ Run Anonymization & Export CSV", type="primary", use_container_width=True, key="btn_b_anon"):
            anon_df = _anonymize(df_src)
            out_dir = os.path.join(RUNS_DIR, "normalized")
            os.makedirs(out_dir, exist_ok=True)
            out_name = f"{base_name}-Anonymized.csv"
            out_path = os.path.join(out_dir, out_name)
            anon_df.to_csv(out_path, index=False, encoding="utf-8-sig")
            ss["asset_anon_df"] = anon_df

            _bytes = anon_df.to_csv(index=False).encode("utf-8-sig")

            st.markdown(
                f"""
                <div style="position:sticky;top:64px;z-index:50;background:rgba(59,130,246,0.10);
                            border:1px solid #3b82f6;padding:12px 16px;border-radius:12px;margin:8px 0 14px 0;">
                <b>âœ… Anonymized CSV:</b> <code>{out_name}</code> â€¢ {len(anon_df):,} rows Ã— {len(anon_df.columns)} cols
                </div>
                """,
                unsafe_allow_html=True,
            )

            cL, cM, cR = st.columns([1, 2.6, 1])
            with cM:
                st.download_button(
                    "â¬‡ï¸  Download Anonymized CSV",
                    data=_bytes,
                    file_name=out_name,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_b_anon_top",
                )

            st.text_input("Saved to (server path)", out_path, disabled=True, label_visibility="collapsed")
            st.dataframe(anon_df.head(100), use_container_width=True, hide_index=True)

            cL2, cM2, cR2 = st.columns([1, 2.6, 1])
            with cM2:
                st.download_button(
                    "â¬‡ï¸  Download Anonymized CSV",
                    data=_bytes,
                    file_name=out_name,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_b_anon_bottom",
                )

    st.markdown("---")

    
    

    
    # ----------------------------
    # B.3 â€” Feature Engineering & Comps
    # ----------------------------
    st.markdown("### **3) Feature Engineering & Comps**")

    def feature_engineer(df: pd.DataFrame, evidence_index=None) -> pd.DataFrame:
        """
        Light feature engineering (safe):
        - Ensure city/lat/lon/age_years/delinquencies/current_loans
        - Coerce lat/lon including "10,762" â†’ 10.762
        - Create stable 'geohash' (lat,lon preferred; fallback short city hash)
        - Derive condition_score (0..1) heuristically if inputs exist
        - Ensure legal_penalty numeric
        - Keep join keys up front if present
        """
        if not isinstance(df, pd.DataFrame) or df.empty:
            return pd.DataFrame()

        out = df.copy()

        # Ensure expected columns exist
        for c in ("city", "lat", "lon", "age_years", "delinquencies", "current_loans"):
            if c not in out.columns:
                out[c] = pd.NA

        # Coerce lat/lon
        for c in ("lat", "lon"):
            if out[c].dtype == "object":
                out[c] = out[c].astype(str).str.replace(",", ".", regex=False)
            out[c] = pd.to_numeric(out[c], errors="coerce")

        # Row-wise geokey: prefer lat/lon else short md5(city)
        import hashlib
        def _row_geokey(row) -> str:
            lat = row.get("lat")
            lon = row.get("lon")
            if pd.notna(lat) and pd.notna(lon):
                return f"{float(lat):.3f},{float(lon):.3f}"
            city_val = row.get("city")
            city_txt = "" if pd.isna(city_val) else str(city_val)
            return hashlib.md5(city_txt.encode("utf-8")).hexdigest()[:7]

        out["geohash"] = out.apply(_row_geokey, axis=1).astype(str)

        # Heuristic condition_score (0..1)
        age = pd.to_numeric(out.get("age_years"), errors="coerce").fillna(0.0)
        delinq = pd.to_numeric(out.get("delinquencies"), errors="coerce").fillna(0.0)
        curr_loans = pd.to_numeric(out.get("current_loans"), errors="coerce").fillna(0.0)
        cond = 1.0 - (0.02 * age) - (0.05 * delinq) - (0.03 * curr_loans)
        out["condition_score"] = pd.Series(cond, index=out.index).clip(0.10, 0.98)

        # legal_penalty safe numeric
        if "legal_penalty" not in out.columns:
            out["legal_penalty"] = 0.0
        else:
            out["legal_penalty"] = pd.to_numeric(out["legal_penalty"], errors="coerce").fillna(0.0)

        # Keep join keys in front
        front_cols = [c for c in ["loan_id", "application_id", "asset_id"] if c in out.columns]
        other_cols = [c for c in out.columns if c not in front_cols]
        out = out[front_cols + other_cols]

        return out


    def _fmt_mean(df, col, fmt="{:.2f}"):
        if isinstance(df, pd.DataFrame) and col in df.columns:
            v = pd.to_numeric(df[col], errors="coerce").mean()
            if pd.notna(v):
                return fmt.format(v)
        return "â€”"


    def fetch_and_clean_comps(df_feats: pd.DataFrame) -> dict:
        """Deterministic stub; replace with real comps feed."""
        mv = pd.to_numeric(df_feats.get("market_value", pd.Series(dtype=float)), errors="coerce")
        base = float(mv.median()) if mv.notna().any() else 100000.0
        comps = [{"comp_id": f"C-{i+1:03d}", "price": round(base * (0.95 + 0.02 * i), 2)} for i in range(5)]
        return {"used": comps, "count": len(comps), "median_baseline": base}


    if not isinstance(ss.get("asset_anon_df"), pd.DataFrame) or ss["asset_anon_df"].empty:
        st.info("Run **Anonymization (B.2)** first to prepare inputs for features.")
    else:
        c3a, c3b = st.columns([1.2, 0.8])

        with c3a:
            if st.button("Build Features & Fetch Comps", key="btn_build_features", use_container_width=True):
                # Build features in this rerun and persist
                feats = feature_engineer(ss["asset_anon_df"], ss.get("asset_evidence_index"))
                ss["asset_features_df"] = feats

                # Metrics (from feats)
                m1, m2, m3 = st.columns(3)
                with m1:
                    st.metric("Avg condition_score", _fmt_mean(feats, "condition_score"))
                with m2:
                    st.metric("Avg market_value", _fmt_mean(feats, "market_value", "{:,.0f}"))
                with m3:
                    st.metric("Avg loan_amount", _fmt_mean(feats, "loan_amount", "{:,.0f}"))

                # Persist features.parquet (BytesIO for download)
                features_path = os.path.join(RUNS_DIR, f"features.{_ts()}.parquet")
                feats.to_parquet(features_path, index=False)
                st.success(f"Saved features â†’ `{features_path}`")

                import io as _io
                _buf = _io.BytesIO()
                feats.to_parquet(_buf, index=False)
                st.download_button(
                    "â¬‡ï¸ Download features.parquet",
                    data=_buf.getvalue(),
                    file_name="features.parquet",
                    mime="application/octet-stream",
                    key="dl_features_parquet",
                    use_container_width=True
                )

                # Comps (and persist)
                comps = fetch_and_clean_comps(feats)
                ss["asset_comps_used"] = comps
                comps_path = os.path.join(RUNS_DIR, f"comps_used.{_ts()}.json")
                with open(comps_path, "w", encoding="utf-8") as fp:
                    json.dump(comps, fp, ensure_ascii=False, indent=2)
                st.success(f"Saved comps â†’ `{comps_path}`")

        with c3b:
            # Show last built features (if any) and offer a second download
            df_feats = ss.get("asset_features_df")
            if isinstance(df_feats, pd.DataFrame) and not df_feats.empty:
                import io as _io2
                _buf2 = _io2.BytesIO()
                df_feats.to_parquet(_buf2, index=False)
                st.download_button(
                    "â¬‡ï¸ Download last features.parquet",
                    data=_buf2.getvalue(),
                    file_name="features.parquet",
                    mime="application/octet-stream",
                    key="dl_features_parquet_last",
                    use_container_width=True
                )

    # Outside the columns: show current features + comps if present
    df_feats = ss.get("asset_features_df")
    if isinstance(df_feats, pd.DataFrame) and not df_feats.empty:
        k1, k2, k3 = st.columns(3)
        with k1:
            st.metric("Rows (features)", f"{len(df_feats):,}")
        with k2:
            st.metric("Avg condition_score", _fmt_mean(df_feats, "condition_score"))
        with k3:
            # evidence_count column is optional; show 0 if absent
            ev = pd.to_numeric(df_feats.get("evidence_count", pd.Series([0]*len(df_feats))), errors="coerce").fillna(0)
            st.metric("Evidence count (stub)", int(ev.mean()))

        st.dataframe(df_feats.head(30), use_container_width=True)

    if ss.get("asset_comps_used") is not None:
        st.caption("Comps used (stub)")
        st.json(ss["asset_comps_used"])

    


# ========== 3) AI APPRAISAL & VALUATION ==========
with tabC:
    st.subheader("ğŸ¤– Stage 3 â€” AI Appraisal & Valuation")

    import os, json, numpy as np, requests, pandas as pd, plotly.express as px
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ§­ HOW TO USE THIS STAGE
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("""
    ### ğŸ§­ How to Use This Stage
    1. **Select a model** â€” choose between production, trained, or open-source (Hugging Face) models.  
    2. **Check hardware** â€” confirm your GPU/CPU profile supports the chosen model.  
    3. **Select dataset** â€” use Stage 2 outputs (Features / Anonymized) or fallback synthetic data.  
    4. **Run appraisal** â€” compute AI-based valuation (`fmv`, `ai_adjusted`, `confidence`, `why`).  
    5. **Review outputs** â€” compare customer vs AI results, run projections, dashboards, and reports.  
    6. **Verify ownership** â€” perform Legal / Encumbrance checks (C.5).  
    """)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ§  MODEL FAMILY TABLE
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ§  Model Families & Recommended Use-Cases")

    model_ref = pd.DataFrame([
        {"Category": "Local / Trained", "Model": "LightGBM / XGBoost / CatBoost",
         "Use Case": "Numeric â†’ FMV prediction", "GPU": "CPU OK",
         "Notes": "Fast, explainable baseline model"},
        {"Category": "Production (â­)", "Model": "asset_lgbm-v1 / credit_lr",
         "Use Case": "Enterprise-grade deployed valuation", "GPU": "CPU OK",
         "Notes": "Stable, low-latency predictions"},
        {"Category": "LLM (HF)", "Model": "Mistral 7B / Gemma 2 9B",
         "Use Case": "Text reasoning + narratives", "GPU": "â‰¥ 8 GB",
         "Notes": "Fast reasoning for appraisal explanations"},
        {"Category": "LLM (HF)", "Model": "LLaMA 3 8B / Qwen 2 7B",
         "Use Case": "Multilingual valuation reports", "GPU": "â‰¥ 12 GB",
         "Notes": "Strong contextual generation"},
        {"Category": "LLM (HF)", "Model": "Mixtral 8Ã—7B",
         "Use Case": "High-end MoE valuation", "GPU": "â‰¥ 24 GB",
         "Notes": "Premium precision for portfolios"},
    ])
    st.dataframe(model_ref, use_container_width=True)
    st.markdown("---")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸŸ¢ PRODUCTION MODEL BANNER
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.status_code == 200:
            meta = resp.json()
            if meta.get("has_production"):
                ver = (meta.get("meta") or {}).get("version", "1.x")
                src = (meta.get("meta") or {}).get("source", "production")
                st.success(f"ğŸŸ¢ Production model active â€” version {ver} â€¢ source {src}")
            else:
                st.warning("âš ï¸ No production model promoted yet â€” using baseline.")
        else:
            st.info("â„¹ï¸ Could not fetch production model meta.")
    except Exception:
        st.info("â„¹ï¸ Production meta unavailable.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ§© Model Selection (list all trained models) â€” HARD-CODED TEST
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from datetime import datetime
    import os, shutil, streamlit as st

    # Hardcoded absolute paths for your environment
    trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/trained"
    production_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/production"

    # Debug info
    st.caption(f"ğŸ“‚ Trained dir: `{trained_dir}`")
    st.caption(f"ğŸ“¦ Production dir: `{production_dir}`")

    # Refresh button â€” use unique key for asset
    if st.button("â†» Refresh models", key="asset_refresh_models"):
        st.session_state.pop("asset_selected_model", None)
        st.rerun()

    models = []
    if os.path.isdir(trained_dir):
        for f in os.listdir(trained_dir):
            if f.endswith(".joblib"):
                fpath = os.path.join(trained_dir, f)
                ctime = os.path.getctime(fpath)
                created = datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
                models.append((f, fpath, created))
    else:
        st.error(f"âŒ Trained dir not found: {trained_dir}")

    if models:
        # Sort by creation time (latest first)
        models.sort(key=lambda x: os.path.getctime(x[1]), reverse=True)
        display_names = [f"{m[0]} â€” {m[2]}" for m in models]

        selected_display = st.selectbox("ğŸ“¦ Select trained model to use", display_names, key="asset_model_select")
        selected_model = models[display_names.index(selected_display)][1]
        st.success(f"âœ… Using model: {os.path.basename(selected_model)}")

        st.session_state["asset_selected_model"] = selected_model

        # Promote to production
        if st.button("ğŸš€ Promote this model to Production", key="asset_promote_button"):
            try:
                os.makedirs(production_dir, exist_ok=True)
                prod_path = os.path.join(production_dir, "model.joblib")
                shutil.copy2(selected_model, prod_path)
                st.success(f"âœ… Model promoted to production: {prod_path}")
            except Exception as e:
                st.error(f"âŒ Promotion failed: {e}")
    else:
        st.warning("âš ï¸ No trained models found â€” train one in Stage F first.")


    
    # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # # ğŸ“¦ MODEL SELECTION (Hardcoded Local Paths)
    # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # from datetime import datetime
    # import os, shutil

    # trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/trained"
    # production_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/production"

    # st.caption(f"ğŸ“¦ Trained dir = `{trained_dir}`")
    # st.caption(f"ğŸ“¦ Production dir = `{production_dir}`")

    # # Refresh button
    # if st.button("â†» Refresh models", key="asset_refresh_models"):
    #     st.session_state.pop("asset_selected_model", None)
    #     st.rerun()

    # models = []

    # # Include production model (if any)
    # prod_fp = os.path.join(production_dir, "model.joblib")
    # if os.path.exists(prod_fp):
    #     p_ctime = os.path.getctime(prod_fp)
    #     p_created = datetime.fromtimestamp(p_ctime).strftime("%b %d, %Y %H:%M")
    #     models.append(("â­ Production", prod_fp, p_created, True))

    # # Include trained models
    # if os.path.isdir(trained_dir):
    #     for f in os.listdir(trained_dir):
    #         if f.endswith(".joblib"):
    #             fpath = os.path.join(trained_dir, f)
    #             ctime = os.path.getctime(fpath)
    #             created = datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
    #             models.append((f, fpath, created, False))
    # else:
    #     st.error(f"âŒ Trained dir not found: {trained_dir}")

    # # Render dropdown
    # if models:
    #     models.sort(key=lambda x: (0 if x[3] else 1, -os.path.getctime(x[1])))  # prod first, then newest
    #     display_names = [f"{m[0]} â€” {m[2]}" for m in models]

    #     selected_display = st.selectbox("ğŸ“¦ Select trained or production model", display_names)
    #     selected_model = models[display_names.index(selected_display)][1]
    #     is_prod = models[display_names.index(selected_display)][3]
    #     st.session_state["asset_selected_model"] = selected_model

    #     st.success(f"{'ğŸŸ¢' if is_prod else 'âœ…'} Using model: {os.path.basename(selected_model)}")

    #     if (not is_prod) and st.button("ğŸš€ Promote this model to Production"):
    #         try:
    #             os.makedirs(production_dir, exist_ok=True)
    #             prod_path = os.path.join(production_dir, "model.joblib")
    #             shutil.copy2(selected_model, prod_path)
    #             st.success(f"âœ… Model promoted to production: {prod_path}")
    #         except Exception as e:
    #             st.error(f"âŒ Promotion failed: {e}")
    # else:
    #     st.warning("âš ï¸ No trained models found â€” train one in Stage F first.")

    
    
    # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # # ğŸŸ¢ PRODUCTION MODEL BANNER
    # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # try:
    #     resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
    #     if resp.status_code == 200:
    #         meta = resp.json()
    #         if meta.get("has_production"):
    #             ver = (meta.get("meta") or {}).get("version", "1.x")
    #             src = (meta.get("meta") or {}).get("source", "production")
    #             st.success(f"ğŸŸ¢ Production model active â€” version {ver} â€¢ source {src}")
    #         else:
    #             st.warning("âš ï¸ No production model promoted yet â€” using baseline.")
    #     else:
    #         st.info("â„¹ï¸ Could not fetch production model meta.")
    # except Exception:
    #     st.info("â„¹ï¸ Production meta unavailable.")

    # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 
    # c_ref, _ = st.columns([1, 6])
    # with c_ref:
    #     if st.button("â†» Refresh models", key="asset_models_refresh"):
    #         st.session_state.pop("asset_model_select", None)
    #         st.session_state["_asset_models_bump"] = _now()
    #         st.rerun()

    # models = []
    # if os.path.exists(production_fp):
    #     try:
    #         p_ctime = os.path.getctime(production_fp)
    #         p_created = datetime.fromtimestamp(p_ctime).strftime("%b %d, %Y %H:%M")
    #     except Exception:
    #         p_ctime, p_created = 0.0, "production"
    #     models.append(("â­ Production", production_fp, p_ctime, p_created, True))

    # raw = []
    # for d in trained_dirs:
    #     if os.path.isdir(d):
    #         for f in os.listdir(d):
    #             if f.endswith(".joblib"):
    #                 fpath = os.path.join(d, f)
    #                 try:
    #                     ctime = os.path.getctime(fpath)
    #                     created = datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
    #                 except Exception:
    #                     ctime, created = 0.0, ""
    #                 raw.append((f, fpath, ctime, created, False))

    # newest_by_name = {}
    # for name, path, ctime, created, is_prod in raw:
    #     try:
    #         if os.path.exists(production_fp) and os.path.samefile(path, production_fp):
    #             continue
    #     except Exception:
    #         pass
    #     if (name not in newest_by_name) or (ctime > newest_by_name[name][2]):
    #         newest_by_name[name] = (name, path, ctime, created, is_prod)
    # models.extend(newest_by_name.values())
    # models = sorted(models, key=lambda x: (0 if x[4] else 1, -x[2]))

    # if models:
    #     display_names = [f"{label} â€” {created}" if created else label for (label, _, _, created, _) in models]
    #     default_idx = 0
    #     prev_selected = st.session_state.get("asset_selected_model")
    #     if prev_selected:
    #         for i, (_, path, _, _, _) in enumerate(models):
    #             if path == prev_selected: default_idx = i; break
    #     select_key = f"asset_model_select::{st.session_state.get('_asset_models_bump','')}"
    #     selected_display = st.selectbox("ğŸ“¦ Select trained model", display_names,
    #                                     index=default_idx, key=select_key)
    #     sel_idx = display_names.index(selected_display)
    #     selected_model = models[sel_idx][1]
    #     is_prod = models[sel_idx][4]
    #     st.session_state["asset_selected_model"] = selected_model

    #     st.success(f"{'ğŸŸ¢' if is_prod else 'âœ…'} Using model: {os.path.basename(selected_model)}")
    #     if (not is_prod) and st.button("ğŸš€ Promote to PRODUCTION", key="asset_promote_model"):
    #         try:
    #             r = requests.post(f"{API_URL}/v1/agents/asset_appraisal/training/promote_last", timeout=60)
    #             if r.ok:
    #                 st.success("âœ… Model promoted to PRODUCTION.")
    #                 st.session_state["_asset_models_bump"] = _now(); st.rerun()
    #             else:
    #                 st.error(f"âŒ Promotion failed: {r.status_code} {r.reason}")
    #                 st.code(r.text[:1500])
    #         except Exception as e:
    #             st.error(f"âŒ Promotion error: {e}")
    # else:
    #     st.warning("âš ï¸ No trained models found â€” train one in Stage 5.")

    # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # # ğŸ“¦ MODEL SELECTION (hardcoded for Asset Appraisal)
    # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # from datetime import datetime
    # import os, shutil

    # trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/trained"
    # production_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/production"

    # st.caption(f"ğŸ“¦ Trained dir = `{trained_dir}`")
    # st.caption(f"ğŸ“¦ Production dir = `{production_dir}`")

    # # Refresh button
    # if st.button("â†» Refresh models", key="asset_refresh_models"):
    #     st.session_state.pop("asset_selected_model", None)
    #     st.rerun()

    # # Gather model list
    # models = []
    # if os.path.isdir(trained_dir):
    #     for f in os.listdir(trained_dir):
    #         if f.endswith(".joblib"):
    #             fpath = os.path.join(trained_dir, f)
    #             ctime = os.path.getctime(fpath)
    #             created = datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
    #             models.append((f, fpath, created))
    # else:
    #     st.error(f"âŒ Trained dir not found: {trained_dir}")

    # # Display dropdown
    # if models:
    #     models.sort(key=lambda x: os.path.getctime(x[1]), reverse=True)
    #     display_names = [f"{m[0]} â€” {m[2]}" for m in models]

    #     selected_display = st.selectbox("ğŸ“¦ Select trained or production model", display_names)
    #     selected_model = models[display_names.index(selected_display)][1]
    #     st.success(f"âœ… Using model: {os.path.basename(selected_model)}")

    #     st.session_state["asset_selected_model"] = selected_model

    #     # Promote to production
    #     if st.button("ğŸš€ Promote this model to Production"):
    #         try:
    #             os.makedirs(production_dir, exist_ok=True)
    #             prod_path = os.path.join(production_dir, "model.joblib")
    #             shutil.copy2(selected_model, prod_path)
    #             st.success(f"âœ… Model promoted to production: {prod_path}")
    #         except Exception as e:
    #             st.error(f"âŒ Promotion failed: {e}")
    # else:
    #     st.warning("âš ï¸ No trained models found â€” train one in Stage F first.")

    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ§  LLM + HARDWARE PROFILE (LOCAL + HUGGING FACE)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ§  LLM & Hardware Profile (Local + Hugging Face Models)")

    HF_MODELS = [
        {"Model": "mistralai/Mistral-7B-Instruct-v0.3",
         "Type": "Reasoning / valuation narrative",
         "GPU": "â‰¥ 8 GB", "Notes": "Fast multilingual contextual LLM"},
        {"Model": "google/gemma-2-9b-it",
         "Type": "Instruction-tuned financial reports",
         "GPU": "â‰¥ 12 GB", "Notes": "Great for valuation explanations"},
        {"Model": "meta-llama/Meta-Llama-3-8B-Instruct",
         "Type": "Valuation summarization",
         "GPU": "â‰¥ 12 GB", "Notes": "High accuracy + low hallucination"},
        {"Model": "Qwen/Qwen2-7B-Instruct",
         "Type": "Multilingual reasoning (VN + EN)",
         "GPU": "â‰¥ 12 GB", "Notes": "Excellent for VN asset appraisal"},
        {"Model": "microsoft/Phi-3-mini-4k-instruct",
         "Type": "Compact instruction LLM",
         "GPU": "â‰¤ 8 GB", "Notes": "Fast lightweight valuation logic"},
        {"Model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
         "Type": "MoE premium reasoning",
         "GPU": "â‰¥ 24 GB", "Notes": "Top-tier valuation model"},
        {"Model": "LightAutoML/LightGBM",
         "Type": "Tabular regression baseline",
         "GPU": "CPU OK", "Notes": "Numeric FMV baseline"},
    ]
    st.dataframe(pd.DataFrame(HF_MODELS), use_container_width=True)

    LLM_MODELS = [
        ("Phi-3 Mini (3.8B)", "phi3:3.8b", "CPU 8 GB RAM (fast)"),
        ("Mistral 7B Instruct", "mistral:7b-instruct", "GPU â‰¥ 8 GB (fast)"),
        ("Gemma-2 9B", "gemma2:9b", "GPU â‰¥ 12 GB (high accuracy)"),
        ("LLaMA-3 8B", "llama3:8b-instruct", "GPU â‰¥ 12 GB (context heavy)"),
        ("Qwen-2 7B", "qwen2:7b-instruct", "GPU â‰¥ 12 GB (multilingual)"),
        ("Mixtral 8Ã—7B", "mixtral:8x7b-instruct", "GPU 24-48 GB (batch)"),
    ]
    LLM_LABELS = [l for (l, _, _) in LLM_MODELS]
    LLM_VALUE_BY_LABEL = {l: v for (l, v, _) in LLM_MODELS}
    LLM_HINT_BY_LABEL  = {l: h for (l, _, h) in LLM_MODELS}

    OPENSTACK_FLAVORS = {
        "m4.medium": "4 vCPU / 8 GB RAM (CPU-only small)",
        "m8.large": "8 vCPU / 16 GB RAM (CPU-only medium)",
        "g1.a10.1": "8 vCPU / 32 GB RAM + 1Ã—A10 24 GB",
        "g1.l40.1": "16 vCPU / 64 GB RAM + 1Ã—L40 48 GB",
        "g2.a100.1": "24 vCPU / 128 GB RAM + 1Ã—A100 80 GB",
    }

    with st.expander("ğŸ§  Choose Model & Hardware Profile", expanded=True):
        c1, c2 = st.columns([1.2, 1])
        with c1:
            model_label = st.selectbox(
                "Select Local or HF LLM (for narratives / explanations)",
                LLM_LABELS, index=1, key="asset_llm_label")
            llm_value = LLM_VALUE_BY_LABEL[model_label]
            use_llm = st.checkbox("Use LLM narrative (explanations)",
                                  value=False, key="asset_use_llm")
            st.caption(f"Hint: {LLM_HINT_BY_LABEL[model_label]}")
        with c2:
            flavor = st.selectbox("OpenStack flavor / host profile",
                                  list(OPENSTACK_FLAVORS.keys()), index=0,
                                  key="asset_flavor")
            st.caption(OPENSTACK_FLAVORS[flavor])
        st.caption("These parameters are passed to backend (Ollama / Flowise / RunAI).")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # GPU PROFILE AND DATASET SOURCE (keep existing logic)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### **C.4 â€” Valuation (AI)**")
    gpu_profile = st.selectbox(
        "GPU Profile (for valuation compute)",
        ["CPU (slow)", "GPU: 1Ã—A100", "GPU: 1Ã—H100", "GPU: 2Ã—L40S"],
        index=1, key="asset_gpu_profile_c4")
    ss["asset_gpu_profile"] = gpu_profile

    # Gather candidates from prior stages (they may be None/empty)
    cand_features = ss.get("asset_features_df")
    cand_anon     = ss.get("asset_anon_df")
    cand_intake   = ss.get("asset_intake_df")

    src = st.selectbox(
        "Data source for AI run",
        [
            "Use FEATURES (Stage 2/3)",
            "Use ANON (Stage 2)",
            "Use RAW â†’ auto-sanitize",
            "Use synthetic (fallback)",
        ],
        key="asset_c4_source"
    )

    # Decide df2 explicitly based on choice
    df2 = None
    if src == "Use FEATURES (Stage 2/3)":
        # First non-empty among features â†’ anon â†’ intake
        df2 = first_nonempty_df(cand_features, cand_anon, cand_intake)

    elif src == "Use ANON (Stage 2)":
        df2 = cand_anon

    elif src == "Use RAW â†’ auto-sanitize":
        # If intake exists, sanitize; else leave None
        df2 = anonymize_text_cols(cand_intake) if isinstance(cand_intake, pd.DataFrame) and not cand_intake.empty else None

    else:  # "Use synthetic (fallback)"
        df2 = quick_synth(150)

    # Final safety check
    if not isinstance(df2, pd.DataFrame) or df2.empty:
        st.warning("No usable dataset found. Please complete Stage A (Intake) and Stage B (Privacy/Features), or choose the synthetic fallback.")
        st.stop()

    # Preview selected data
    st.dataframe(df2.head(10), use_container_width=True)



    # Probe API (health & agents)
    with st.expander("ğŸ” Probe API (health & agents)", expanded=False):
        if st.button("Run probe now", key="btn_probe_api"):
            diag = probe_api()
            st.json(diag)

    # Run model button (runtime flavor + gpu_profile included)
    if st.button("ğŸš€ Run AI Appraisal now", key="btn_run_ai"):
        csv_bytes = df2.to_csv(index=False).encode("utf-8")

        form_fields = {
            "use_llm": str(use_llm).lower(),
            "llm": llm_value,
            "flavor": flavor,
            "gpu_profile": gpu_profile,  # NEW: pass GPU profile to backend
            "selected_model": ss.get("asset_selected_model", ""),
            "agent_name": "asset_appraisal",
        }

        with st.spinner("Calling asset agentâ€¦"):
            ok, result = try_run_asset_agent(csv_bytes, form_fields=form_fields, timeout_sec=180)

        if not ok:
            st.error("âŒ Model API error.")
            st.info("Tip: open 'ğŸ” Probe API' above to see health and discovered agent ids.")
            st.code(str(result)[:8000])
            st.stop()

        df_app = result.copy()

        # Ensure core valuation columns per blueprint
        if "ai_adjusted" not in df_app.columns and "market_value" in df_app.columns:
            df_app["ai_adjusted"] = df_app["market_value"]
        if "fmv" not in df_app.columns:
            # heuristics: if model returns fmv, keep; else set fmv ~ ai_adjusted
            df_app["fmv"] = pd.to_numeric(df_app.get("ai_adjusted", np.nan), errors="coerce")
        if "confidence" not in df_app.columns:
            df_app["confidence"] = 80.0
        if "why" not in df_app.columns:
            df_app["why"] = ["Condition, comps, and features (placeholder)"] * len(df_app)

        # Persist valuation artifact
        val_path = os.path.join(RUNS_DIR, f"valuation_ai.{_ts()}.csv")
        df_app.to_csv(val_path, index=False)
        st.success(f"Saved valuation artifact â†’ `{val_path}`")

        # Keep table for downstream steps
        ss["asset_ai_df"] = df_app

        # Display minimal KPIs
        k1, k2, k3 = st.columns(3)
        try:
            k1.metric("Avg FMV", f"{pd.to_numeric(df_app['fmv'], errors='coerce').mean():,.0f}")
        except Exception:
            k1.metric("Avg FMV", "â€”")
        try:
            k2.metric("Avg Confidence", f"{pd.to_numeric(df_app['confidence'], errors='coerce').mean():.2f}")
        except Exception:
            k2.metric("Avg Confidence", "â€”")
        k3.metric("Rows", len(df_app))

        st.markdown("### ğŸ§¾ Valuation Output (preview)")
        cols_first = [c for c in [
            "application_id","asset_id","asset_type","city",
            "fmv","ai_adjusted","confidence","why"
        ] if c in df_app.columns]
        st.dataframe(df_app[cols_first].head(50), use_container_width=True)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Customer vs AI â€” Details & 5-Year Deltas
        # (Place this right after the valuation preview table)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("### ğŸ“‹ Customer & Loan Details (Declared) + AI Alignment")

        import numpy as np
        from datetime import datetime, timezone
        import os

        RUNS_DIR = "./.tmp_runs"
        os.makedirs(RUNS_DIR, exist_ok=True)
        _ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

        ai_df = ss.get("asset_ai_df")
        if ai_df is None or len(ai_df) == 0:
            st.info("Run the AI appraisal first to populate these tables.")
        else:
            # Merge intake (customer-declared) if available
            intake_df = ss.get("asset_intake_df")
            if intake_df is not None and not intake_df.empty:
                # Choose join keys available in both frames
                join_keys = [k for k in ["application_id", "asset_id"] if k in ai_df.columns and k in intake_df.columns]
                if join_keys:
                    merged = intake_df.merge(ai_df, on=join_keys, suffixes=("_cust", "_ai"), how="left")
                else:
                    merged = ai_df.copy()
            else:
                merged = ai_df.copy()

            # Canonical column mapping
            # customer declared value (prefer *_cust if merge happened)
            customer_val_col = "market_value_cust" if "market_value_cust" in merged.columns else (
                "market_value" if "market_value" in merged.columns else None
            )
            # AI value (prefer fmv, fallback ai_adjusted)
            ai_val_col = "fmv" if "fmv" in merged.columns else (
                "ai_adjusted" if "ai_adjusted" in merged.columns else None
            )

            # Build Customer & Loan Details table
            details_cols = [c for c in [
                "application_id","asset_id","asset_type","city",
                customer_val_col,
                "loan_amount",
                ai_val_col, "confidence","why"
            ] if c and c in merged.columns]

            details_tbl = merged[details_cols].copy() if details_cols else merged.copy()

            # Rename for clarity in the UI
            rename_map = {}
            if customer_val_col:
                rename_map[customer_val_col] = "customer_declared_value"
            if ai_val_col:
                rename_map[ai_val_col] = "ai_estimate_value"
            details_tbl = details_tbl.rename(columns=rename_map)

            # Explanation / Source
            selected_model = os.path.basename(str(ss.get("asset_selected_model","") or ""))
            comps_count = int((ss.get("asset_comps_used") or {}).get("count", 0))
            details_tbl["explanation_source"] = details_tbl.apply(
                lambda r: f"Customer input CSV vs AI model {selected_model or 'production'} (comps={comps_count})",
                axis=1
            )

            st.dataframe(details_tbl.head(50), use_container_width=True)

            # Persist details table
            details_path = os.path.join(RUNS_DIR, f"customer_loan_details.{_ts}.csv")
            details_tbl.to_csv(details_path, index=False)
            st.download_button(
                "â¬‡ï¸ Download Customer & Loan Details (CSV)",
                data=details_tbl.to_csv(index=False).encode("utf-8"),
                file_name="customer_loan_details.csv",
                mime="text/csv"
            )

            st.markdown("---")
            st.markdown("### ğŸ“ˆ 5-Year Deltas: Customer vs AI (per-year Î” and %Î”)")

            # Controls for forward projections
            cgr_a, cgr_b = st.columns(2)
            with cgr_a:
                cust_cagr = st.slider("Customer Expected CAGR (%)", min_value=-20, max_value=40, value=5, step=1) / 100.0
            with cgr_b:
                ai_cagr = st.slider("AI Expected CAGR (%)", min_value=-20, max_value=40, value=4, step=1) / 100.0

            if not customer_val_col or not ai_val_col:
                st.warning("Missing base columns to compute deltas. Ensure both customer and AI values exist.")
            else:
                base_cust = merged[customer_val_col].astype(float)
                base_ai   = merged[ai_val_col].astype(float)

                # Build long-format 5-year projection table
                rows = []
                years = [1, 2, 3, 4, 5]
                for idx in range(len(merged)):
                    cust0 = base_cust.iloc[idx]
                    ai0   = base_ai.iloc[idx]
                    app_id = merged.iloc[idx].get("application_id", None)
                    asset_id = merged.iloc[idx].get("asset_id", None)
                    asset_type = merged.iloc[idx].get("asset_type", None)
                    city = merged.iloc[idx].get("city", None)

                    for y in years:
                        cust_y = cust0 * ((1.0 + cust_cagr) ** y) if np.isfinite(cust0) else np.nan
                        ai_y   = ai0   * ((1.0 + ai_cagr) ** y)   if np.isfinite(ai0)   else np.nan
                        delta  = ai_y - cust_y if (np.isfinite(ai_y) and np.isfinite(cust_y)) else np.nan
                        pct    = (delta / cust_y * 100.0) if (np.isfinite(delta) and cust_y not in [0, np.nan]) else np.nan

                        rows.append({
                            "application_id": app_id,
                            "asset_id": asset_id,
                            "asset_type": asset_type,
                            "city": city,
                            "year_ahead": y,
                            "customer_value": cust_y,
                            "ai_value": ai_y,
                            "delta": delta,
                            "delta_pct": pct,
                            "explanation_source": f"Customer CAGR={cust_cagr*100:.1f}% vs AI CAGR={ai_cagr*100:.1f}%; AI model {selected_model or 'production'} (comps={comps_count})"
                        })

                deltas_tbl = pd.DataFrame(rows)

            # Display & export
            # Round for readability
            for c in ["customer_value","ai_value","delta","delta_pct"]:
                if c in deltas_tbl.columns:
                    deltas_tbl[c] = pd.to_numeric(deltas_tbl[c], errors="coerce")

            st.dataframe(deltas_tbl.head(100), use_container_width=True)

            deltas_path = os.path.join(RUNS_DIR, f"valuation_deltas_5y.{_ts}.csv")
            deltas_tbl.to_csv(deltas_path, index=False)
            st.download_button(
                "â¬‡ï¸ Download 5-Year Deltas (CSV)",
                data=deltas_tbl.to_csv(index=False).encode("utf-8"),
                file_name="valuation_deltas_5y.csv",
                mime="text/csv"
            )


        st.markdown("---")
        # ğŸ”’ C.5 â€” Legal/Ownership Verification (encumbrances, liens, fraud)
        st.markdown("### **C.5 â€” Legal/Ownership Verification**")

        def _verify_stub(df_in: pd.DataFrame) -> pd.DataFrame:
            df = df_in.copy()
            if "verification_status" not in df.columns:
                df["verification_status"] = "verified"
            if "encumbrance_flag" not in df.columns:
                df["encumbrance_flag"] = False
            if "verified_owner" not in df.columns:
                df["verified_owner"] = np.where(df.get("asset_type","").astype(str).str.lower().str.contains("car"), "DMV Registry", "Land Registry")
            if "notes" not in df.columns:
                df["notes"] = "Registry check passed (stub)"
            return df

        if st.button("ğŸ” Run Legal/Ownership Checks", key="btn_run_verification"):
            base_df = ss.get("asset_ai_df")
            if base_df is None:
                st.warning("Run valuation first.")
            else:
                verified_df = _verify_stub(base_df)
                ss["asset_verified_df"] = verified_df
                ver_path = os.path.join(RUNS_DIR, f"verification_status.{_ts()}.csv")
                verified_df.to_csv(ver_path, index=False)
                st.success(f"Saved verification artifact â†’ `{ver_path}`")

                v1, v2 = st.columns(2)
                with v1:
                    try:
                        pct = (verified_df["verification_status"] == "verified").mean()
                        st.metric("Verified %", f"{pct:.0%}")
                    except Exception:
                        st.metric("Verified %", "â€”")
                with v2:
                    try:
                        st.metric("Encumbrance Flags", int(pd.to_numeric(verified_df["encumbrance_flag"]).sum()))
                    except Exception:
                        st.metric("Encumbrance Flags", "â€”")

                cols_ver = [c for c in [
                    "application_id","asset_id","verified_owner","verification_status","encumbrance_flag","notes"
                ] if c in verified_df.columns]
                st.dataframe(verified_df[cols_ver].head(50), use_container_width=True)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ğŸ“Š Executive Portfolio Dashboard (Spectacular)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.divider()
        st.subheader("ğŸ“Š Executive Portfolio Dashboard")

        df_src = ss.get("asset_ai_df")
        ft = ss.get("asset_first_table")  # loan-centric projection you already built
        if df_src is None or (hasattr(df_src, "empty") and df_src.empty):
            st.info("Run appraisal to populate the dashboard.")
        else:
            df = df_src.copy()

            # ---- Safe numerics
            def _num(series, default=None):
                s = pd.to_numeric(series, errors="coerce")
                if default is not None:
                    s = s.fillna(default)
                return s

            for c in ["ai_adjusted","realizable_value","loan_amount",
                    "valuation_gap_pct","ltv_ai","ltv_cap","confidence",
                    "condition_score","legal_penalty"]:
                if c in df.columns:
                    df[c] = _num(df[c])

            # ---- KPIs row
            k1, k2, k3, k4, k5 = st.columns(5)
            total_ai        = float(df.get("ai_adjusted", pd.Series(dtype=float)).sum()) if "ai_adjusted" in df.columns else 0.0
            total_realiz    = float(df.get("realizable_value", pd.Series(dtype=float)).sum()) if "realizable_value" in df.columns else 0.0
            avg_conf        = float(df.get("confidence", pd.Series(dtype=float)).mean()) if "confidence" in df.columns else 0.0
            ltv_breach_rate = 0.0
            if {"ltv_ai","ltv_cap"}.issubset(df.columns):
                ltv_breach_rate = float((df["ltv_ai"] > df["ltv_cap"]).mean() * 100)
            approved_cnt = int(df.get("decision","").astype(str).str.lower().eq("approved").sum()) if "decision" in df.columns else 0

            k1.metric("AI Gross Value",       f"${total_ai:,.0f}")
            k2.metric("Realizable Value",     f"${total_realiz:,.0f}")
            k3.metric("Avg Confidence",       f"{avg_conf:.1f}%")
            k4.metric("LTV Breach Rate",      f"{ltv_breach_rate:.1f}%")
            k5.metric("Approved Count",       f"{approved_cnt:,}")

            # ---- Row 1: Top-10 Assets & Decision Mix
            r1c1, r1c2 = st.columns([1.2, 1])
            with r1c1:
                value_col = "realizable_value" if "realizable_value" in df.columns else ("ai_adjusted" if "ai_adjusted" in df.columns else None)
                if value_col:
                    df_top = (df.assign(_val=df[value_col])
                                .sort_values("_val", ascending=False)
                                .head(10))
                    fig_top = px.bar(
                        df_top,
                        x="_val", y=df_top.get("asset_id", df_top.index).astype(str),
                        color="asset_type" if "asset_type" in df_top.columns else None,
                        orientation="h",
                        title=f"Top 10 Assets by {value_col.replace('_',' ').title()}",
                        hover_data=[c for c in ["application_id","asset_id","asset_type","city","_val"] if c in df_top.columns]
                    )
                    fig_top.update_layout(template="plotly_dark", height=380, yaxis_title=None, xaxis_title=value_col)
                    st.plotly_chart(fig_top, use_container_width=True)
            with r1c2:
                names_series = (df["decision"].astype(str).str.title()
                                if "decision" in df.columns
                                else np.where(df.get("policy_breaches","").astype(str).str.len().gt(0),
                                            "Has Breach","No Breach"))
                fig_mix = px.pie(df, names=names_series, title="Decision / Breach Mix")
                fig_mix.update_layout(template="plotly_dark", height=380)
                st.plotly_chart(fig_mix, use_container_width=True)

            # ---- Row 2: By Asset Type & City Concentration
            r2c1, r2c2 = st.columns(2)
            with r2c1:
                if "asset_type" in df.columns:
                    df_type = (df
                            .assign(value=df[value_col] if value_col else 0)
                            .groupby("asset_type", dropna=False)["value"]
                            .sum().sort_values(ascending=False).reset_index())
                    fig_type = px.bar(df_type, x="asset_type", y="value",
                                    title="Value by Asset Type",
                                    text_auto=True)
                    fig_type.update_layout(template="plotly_dark", height=360, xaxis_title=None, yaxis_title="Value")
                    st.plotly_chart(fig_type, use_container_width=True)
            with r2c2:
                if "city" in df.columns and value_col:
                    df_city = (df.groupby("city", dropna=False)[value_col]
                                .sum().sort_values(ascending=False)
                                .head(10).reset_index())
                    fig_city = px.pie(df_city, values=value_col, names="city",
                                    title="Top-10 City Concentration")
                    fig_city.update_layout(template="plotly_dark", height=360)
                    st.plotly_chart(fig_city, use_container_width=True)

            # ---- Row 3: LTV vs Cap & ConditionÃ—Legal Heat
            r3c1, r3c2 = st.columns(2)
            with r3c1:
                if {"ltv_ai","ltv_cap"}.issubset(df.columns):
                    fig_sc = px.scatter(
                        df, x="ltv_cap", y="ltv_ai",
                        color="asset_type" if "asset_type" in df.columns else None,
                        hover_data=[c for c in ["application_id","asset_id","asset_type","city","loan_amount"] if c in df.columns],
                        title="LTV (AI) vs LTV Cap"
                    )
                    try:
                        max_cap = float((df["ltv_cap"].max() or 1.2))
                        fig_sc.add_shape(type="line", x0=0, y0=0, x1=max_cap, y1=max_cap, line=dict(dash="dash"))
                    except Exception:
                        pass
                    fig_sc.update_layout(template="plotly_dark", height=360,
                                        xaxis_title="LTV Cap", yaxis_title="LTV (AI)")
                    st.plotly_chart(fig_sc, use_container_width=True)
            with r3c2:
                if {"condition_score","legal_penalty"}.issubset(df.columns):
                    try:
                        cond_bins  = pd.cut(df["condition_score"], bins=[0,0.70,0.85,1.00], labels=["<0.70","0.70â€“0.85",">0.85"])
                        legal_bins = pd.cut(df["legal_penalty"],  bins=[0,0.97,0.99,1.00], labels=["<0.97","0.97â€“0.99",">=0.99"])
                        heat = (df.assign(cond=cond_bins, legal=legal_bins)
                                .groupby(["cond","legal"]).size().reset_index(name="count"))
                        fig_hm = px.density_heatmap(heat, x="legal", y="cond", z="count",
                                                    title="Condition vs Legal â€” Density")
                        fig_hm.update_layout(template="plotly_dark", height=360)
                        st.plotly_chart(fig_hm, use_container_width=True)
                    except Exception:
                        pass

            # ---- Row 4: City Leaderboard + Per-City Asset List
            st.markdown("### ğŸ™ï¸ City Leaderboard & Assets")
            if "city" in df.columns:
                value_col = value_col or "ai_adjusted"
                city_sum = (df.groupby("city", dropna=False)[value_col]
                            .sum().sort_values(ascending=False).reset_index()
                            .rename(columns={value_col: "total_value"}))
                left, right = st.columns([1, 2])
                with left:
                    st.dataframe(city_sum, use_container_width=True)
                with right:
                    # show top assets per top city
                    top_cities = city_sum["city"].astype(str).head(5).tolist()
                    for city in top_cities:
                        with st.expander(f"ğŸ“ {city} â€” top assets", expanded=False):
                            sub = (df[df["city"].astype(str)==city]
                                .assign(value=df[value_col])
                                .sort_values("value", ascending=False)
                                [[c for c in ["application_id","asset_id","asset_type","value","loan_amount","confidence"] if c in df.columns]]
                                .head(15))
                            st.dataframe(sub, use_container_width=True)


            # ---- Optional Map (if lat/lon present)
            st.markdown("### ğŸ—ºï¸ Map (optional)")
            st.caption("Visualize asset locations â€” map color and style follow the current UI theme.")

            map_cols = [("lat","lon"), ("latitude","longitude"), ("gps_lat","gps_lon")]
            have_map = False

            for la, lo in map_cols:
                if la in df.columns and lo in df.columns:
                    have_map = True
                    map_df = df[[la, lo] + [
                        c for c in ["asset_id","asset_type","city","ai_adjusted","realizable_value","confidence"]
                        if c in df.columns
                    ]].copy()
                    map_df = map_df.rename(columns={la: "lat", lo: "lon"})
                    map_df = map_df.dropna(subset=["lat", "lon"])

                    if not map_df.empty:
                        try:
                            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            # Prefer Plotly (bright light / dark dark)
                            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            import plotly.express as px
                            apply_plotly_mapbox_defaults()
                            style_name = plotly_map_style()

                            fig = px.scatter_mapbox(
                                map_df,
                                lat="lat",
                                lon="lon",
                                hover_name="asset_id" if "asset_id" in map_df.columns else "city",
                                hover_data={c: True for c in ["asset_type","city","ai_adjusted","realizable_value","confidence"] if c in map_df.columns},
                                color_discrete_sequence=["#38bdf8"],
                                zoom=8,
                                height=420,
                            )

                            fig.update_layout(
                                mapbox_style=style_name,
                                margin=dict(l=0, r=0, t=0, b=0),
                                paper_bgcolor="rgba(0,0,0,0)",
                                plot_bgcolor="rgba(0,0,0,0)",
                            )
                            st.plotly_chart(fig, use_container_width=True)

                        except Exception as e:
                            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            # Fallback to pydeck if Plotly unavailable
                            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            import pydeck as pdk
                            view_state = make_pydeck_view_state(
                                lat=float(map_df["lat"].mean()),
                                lon=float(map_df["lon"].mean()),
                                zoom=8
                            )
                            layer = pdk.Layer(
                                "ScatterplotLayer",
                                data=map_df,
                                get_position='[lon, lat]',
                                get_color='[0, 128, 255, 200]',
                                get_radius=120,
                                pickable=True,
                            )
                            deck = pdk.Deck(
                                map_style=pydeck_map_style(),
                                initial_view_state=view_state,
                                layers=[layer],
                                tooltip={"text": "{asset_id} Â· {asset_type}\n{city}\nAI: {ai_adjusted}\nRealiz: {realizable_value}\nConf: {confidence}"}
                            )
                            st.pydeck_chart(deck)
                    else:
                        st.info("â„¹ï¸ No valid coordinates found to display on the map.")
                    break

            if not have_map:
                st.caption("No lat/lon columns found (lat/lon or latitude/longitude or gps_lat/gps_lon). Map hidden.")


            # ---- Exports of aggregates
            st.markdown("#### ğŸ“¤ Export dashboard aggregates")
            exports = {}
            if "asset_type" in df.columns:
                exports["by_asset_type.csv"] = df_type.to_csv(index=False) if 'df_type' in locals() else ""
            if "city" in df.columns and value_col:
                exports["by_city_top10.csv"] = df_city.to_csv(index=False) if 'df_city' in locals() else ""
            if 'df_top' in locals():
                exports["top_assets.csv"] = df_top.drop(columns=["_val"], errors="ignore").to_csv(index=False)

            ex1, ex2, ex3 = st.columns(3)
            for i, (fname, data) in enumerate(exports.items()):
                if not data:
                    continue
                col = [ex1, ex2, ex3][i % 3]
                with col:
                    st.download_button(f"â¬‡ï¸ {fname}", data=data.encode("utf-8"), file_name=fname, mime="text/csv")
            # ========================
            # Stage C â€” AI Appraisal & Valuation
            # ========================
            # Now send AI results to Stage E for review
            if st.button("ğŸ’¬ Review in Stage E"):
                # Store AI appraisal results in session_state for Stage E
                st.session_state["ai_review_df"] = ai_df  # ai_df should be the AI results dataframe from the current stage
                st.session_state["current_stage"] = "human_review"
                st.success("AI results sent to Stage E for human review!")
                st.rerun()  # Trigger a page refresh to go to the next stage




# ========== 4) POLICY & DECISION (Stage D: steps 6â€“7) ==========
with tabD:
    st.subheader("ğŸ§® Stage 4 â€” Policy & Decision (D.6 / D.7)")

    import os, json
    import numpy as np
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)
    def _ts(): return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    # ---- Input table: prefer verified â†’ else AI valuation (safe selector) ----
    base_df = first_nonempty_df(ss.get("asset_verified_df"), ss.get("asset_ai_df"))
    if not is_nonempty_df(base_df):
        st.warning("Run Stage C first (valuation, and optionally verification).")
        st.stop()

    st.caption("Input: valuation + (optional) verification outputs.")

    # â”€â”€ D.6 and D.7 continue here (your existing haircuts / caps / breaches / decision code) â”€â”€


    # -------- D.6 â€” Policy & Haircuts â†’ realizable_value --------
    st.markdown("### **D.6 â€” Policy & Haircuts**")
    p1, p2, p3 = st.columns(3)
    with p1:
        base_haircut_pct = st.slider("Base haircut (%)", 0, 60, 10, 1, key="policy_base_haircut")
    with p2:
        condition_weight = st.slider("Condition multiplier min", 0.50, 1.00, 0.80, 0.01, key="policy_cond_min")
    with p3:
        legal_weight = st.slider("Legal multiplier min", 0.50, 1.00, 0.95, 0.01, key="policy_legal_min")

    if st.button("Apply Haircuts", key="btn_apply_haircuts"):
        df = base_df.copy()

        # Ensure necessary inputs exist
        for col, default in [("ai_adjusted", np.nan), ("condition_score", 0.9), ("legal_penalty", 1.0)]:
            if col not in df.columns:
                df[col] = default

        ai_adj = pd.to_numeric(df["ai_adjusted"], errors="coerce")
        cond   = pd.to_numeric(df["condition_score"], errors="coerce").clip(condition_weight, 1.0)
        legal  = pd.to_numeric(df["legal_penalty"],  errors="coerce").clip(legal_weight, 1.0)
        base_cut = (1.0 - float(base_haircut_pct) / 100.0)

        df["realizable_value"] = ai_adj * cond * legal * base_cut

        # Persist policy_haircuts artifact
        policy_path = os.path.join(RUNS_DIR, f"policy_haircuts.{_ts()}.csv")
        df.to_csv(policy_path, index=False)
        ss["asset_policy_df"] = df
        st.success(f"Saved: `{policy_path}`")

        # KPIs
        k1, k2, k3 = st.columns(3)
        with k1:
            st.metric("Avg Realizable Value", f"{pd.to_numeric(df['realizable_value'], errors='coerce').mean():,.0f}")
        with k2:
            st.metric("Rows", len(df))
        with k3:
            st.metric("Base Haircut", f"{base_haircut_pct}%")

        st.dataframe(df.head(30), use_container_width=True)

    st.markdown("---")

    # -------- D.7 â€” Risk / Decision --------
    st.markdown("### **D.7 â€” Risk / Decision**")

    if ss.get("asset_policy_df") is None:
        st.info("Run D.6 first to compute `realizable_value`.")
    else:
        df = ss["asset_policy_df"].copy()

        # Inputs
        r1, r2, r3 = st.columns(3)
        with r1:
            loan_amount_default = float(pd.to_numeric(df.get("loan_amount", pd.Series([60000])).median()))
            loan_amount = st.number_input("Loan amount (default=median)", value=loan_amount_default, min_value=0.0, step=1000.0, key="risk_loan_amt")
        with r2:
            ltv_mode = st.selectbox("LTV cap mode", ["Fixed cap", "Per asset_type"], index=0, key="risk_ltv_mode")
        with r3:
            fixed_ltv_cap = st.slider("Fixed LTV cap (Ã—)", 0.10, 2.00, 0.80, 0.05, key="risk_ltv_cap_fixed")

        # Per-type caps if requested
        type_caps = {}
        if ltv_mode == "Per asset_type":
            types = sorted(list(map(str, (df.get("asset_type") or pd.Series(["Asset"])).dropna().unique())))[0:10]
            st.caption("Tune LTV caps per asset_type")
            grid = st.columns(4 if len(types) > 3 else max(1, len(types)))
            for i, t in enumerate(types):
                with grid[i % len(grid)]:
                    type_caps[t] = st.number_input(f"{t} cap Ã—", 0.10, 2.00, 0.80, 0.05, key=f"cap_{t}")

        # Thresholds for decisioning
        t1, t2, t3 = st.columns(3)
        with t1:
            min_conf = st.slider("Min confidence (%)", 0, 100, 70, 1, key="risk_min_conf")
        with t2:
            min_cond = st.slider("Min condition_score", 0.60, 1.00, 0.75, 0.01, key="risk_min_cond")
        with t3:
            min_legal = st.slider("Min legal_penalty", 0.80, 1.00, 0.97, 0.01, key="risk_min_legal")

        if st.button("Compute Decision", key="btn_compute_decision"):
            # Compute ltv_ai
            df["ltv_ai"] = pd.to_numeric(loan_amount, errors="coerce") / pd.to_numeric(df.get("ai_adjusted", np.nan), errors="coerce")

            # ltv_cap
            if ltv_mode == "Fixed cap":
                df["ltv_cap"] = float(fixed_ltv_cap)
            else:
                atypes = df.get("asset_type").astype(str) if "asset_type" in df.columns else pd.Series(["Asset"] * len(df))
                df["ltv_cap"] = atypes.map(lambda t: float(type_caps.get(t, fixed_ltv_cap)))

            # Breaches
            conf = pd.to_numeric(df.get("confidence", 100.0), errors="coerce")
            cond = pd.to_numeric(df.get("condition_score", 1.0), errors="coerce")
            legal= pd.to_numeric(df.get("legal_penalty", 1.0),  errors="coerce")
            ltv  = pd.to_numeric(df["ltv_ai"], errors="coerce")
            lcap = pd.to_numeric(df["ltv_cap"], errors="coerce")

            breaches = []
            for i in range(len(df)):
                b = []
                if pd.notna(conf.iat[i]) and conf.iat[i] < min_conf:
                    b.append(f"confidence<{min_conf}%")
                if pd.notna(cond.iat[i]) and cond.iat[i] < min_cond:
                    b.append(f"condition<{min_cond:.2f}")
                if pd.notna(legal.iat[i]) and legal.iat[i] < min_legal:
                    b.append(f"legal<{min_legal:.2f}")
                if pd.notna(ltv.iat[i]) and pd.notna(lcap.iat[i]) and ltv.iat[i] > lcap.iat[i]:
                    b.append("ltv>cap")
                breaches.append(", ".join(b))
            df["policy_breaches"] = breaches

            # Decision rule
            # - reject if LTV>cap OR confidence << min_conf (<= min_conf-10)
            # - review if any breach but not hard reject
            # - approve otherwise
            hard_reject = (
                (ltv > lcap) |
                (pd.to_numeric(conf, errors="coerce") <= (min_conf - 10))
            )
            any_breach = df["policy_breaches"].str.len().gt(0)

            df["decision"] = np.select(
                [
                    hard_reject,
                    any_breach
                ],
                ["reject", "review"],
                default="approve"
            )

            # Persist risk_decision artifact
            risk_path = os.path.join(RUNS_DIR, f"risk_decision.{_ts()}.csv")
            df.to_csv(risk_path, index=False)
            ss["asset_decision_df"] = df
            st.success(f"Saved: `{risk_path}`")

            # KPIs + Table
            k1, k2, k3 = st.columns(3)
            with k1:
                st.metric("Avg LTV (AI)", f"{pd.to_numeric(df['ltv_ai'], errors='coerce').mean():.2f}")
            with k2:
                try:
                    st.metric("Breach Rate", f"{(df['policy_breaches'].str.len().gt(0)).mean():.0%}")
                except Exception:
                    st.metric("Breach Rate", "â€”")
            with k3:
                mix = df["decision"].value_counts(dropna=False)
                st.metric("Approve/Review/Reject", f"{int(mix.get('approve',0))}/{int(mix.get('review',0))}/{int(mix.get('reject',0))}")

            cols_view = [c for c in [
                "application_id","asset_id","asset_type","city",
                "ai_adjusted","realizable_value",
                "loan_amount","ltv_ai","ltv_cap",
                "confidence","condition_score","legal_penalty",
                "policy_breaches","decision"
            ] if c in df.columns]
            st.dataframe(df[cols_view].head(50), use_container_width=True)

            st.download_button(
                "â¬‡ï¸ Download Policy+Decision (CSV)",
                data=df.to_csv(index=False).encode("utf-8"),
                file_name="risk_decision.csv",
                mime="text/csv"
            )



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# E â€” HUMAN REVIEW & FEEDBACK DASHBOARD
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from datetime import datetime, timezone  # ensure available inside this block
import os, glob, json
import numpy as np
import pandas as pd
import plotly.graph_objects as go

with tabE:
    st.subheader("ğŸ§‘â€âš–ï¸ Stage E â€” Human Review & Feedback")
    st.caption("Compare AI-estimated collateral values against business metrics, adjust valuations, and record justification for retraining.")

    
    # Workspace
    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Stage C loader controls (Auto-load + picker)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _find_stage_c_candidates():
        pats = ["valuation_ai*.csv", "valuation_ai*.parquet"]
        files = []
        for pat in pats:
            files.extend(glob.glob(os.path.join(RUNS_DIR, pat)))
        return sorted(files, key=os.path.getmtime, reverse=True)

    if "stage_c_selected_path" not in st.session_state:
        st.session_state["stage_c_selected_path"] = None

    ctrl1, ctrl2, ctrl3 = st.columns([1.2, 1, 2.8])
    with ctrl1:
        btn_autoload = st.button("ğŸ”„ Auto-load latest Stage C", use_container_width=True)
    with ctrl2:
        btn_refresh = st.button("ğŸ” Refresh list", use_container_width=True)
    with ctrl3:
        st.caption("Looks for `valuation_ai*.csv|.parquet` under `./.tmp_runs`")

    if btn_refresh:
        pass  # triggers rerun â†’ list will refresh

    candidates = _find_stage_c_candidates()
    if not candidates:
        st.warning("âš ï¸ No AI appraisal results found. Please complete Stage C first.")
        st.stop()

    # Pick newest on first load or when autoload pressed
    if btn_autoload:
        st.session_state["stage_c_selected_path"] = candidates[0]
    elif not st.session_state["stage_c_selected_path"]:
        st.session_state["stage_c_selected_path"] = candidates[0]
    # Ensure the selected one still exists
    if st.session_state["stage_c_selected_path"] not in candidates:
        st.session_state["stage_c_selected_path"] = candidates[0]

    # Human-friendly label
    def _fmt(p):
        ts = datetime.fromtimestamp(os.path.getmtime(p)).strftime("%Y-%m-%d %H:%M:%S")
        return f"{os.path.basename(p)}  â€¢  {ts}"

    current_idx = candidates.index(st.session_state["stage_c_selected_path"])
    picked = st.selectbox(
        "Stage C output to review",
        options=candidates,
        index=current_idx,
        format_func=_fmt,
    )
    st.session_state["stage_c_selected_path"] = picked

    # Load the selected Stage C table â†’ df_ai
    ai_path = st.session_state["stage_c_selected_path"]
    try:
        if ai_path.lower().endswith(".parquet"):
            df_ai = pd.read_parquet(ai_path)
        else:
            df_ai = pd.read_csv(ai_path)
        st.success(f"âœ… Loaded Stage C: {os.path.basename(ai_path)}  ({len(df_ai)} rows Ã— {df_ai.shape[1]} cols)")
    except Exception as e:
        st.error(f"Failed to read `{ai_path}`: {e}")
        st.stop()

    # Ensure join keys exist to avoid editor KeyErrors later
    for col in ["application_id", "asset_id", "asset_type", "city"]:
        if col not in df_ai.columns:
            df_ai[col] = None

    # Ensure human_value / justification columns for adjustments
    if "human_value" not in df_ai.columns:
        df_ai["human_value"] = pd.to_numeric(df_ai["fmv"], errors="coerce") if "fmv" in df_ai.columns else np.nan
    if "justification" not in df_ai.columns:
        df_ai["justification"] = ""

    
    # # Workspace
    # RUNS_DIR = "./.tmp_runs"
    # os.makedirs(RUNS_DIR, exist_ok=True)

    # # Load latest AI valuation file safely
    # ai_paths = sorted(
    #     [os.path.join(RUNS_DIR, f) for f in os.listdir(RUNS_DIR) if f.startswith("valuation_ai") and f.endswith(".csv")],
    #     key=os.path.getmtime, reverse=True
    # )
    # if not ai_paths:
    #     st.warning("âš ï¸ No AI appraisal results found. Please complete Stage C first.")
    #     st.stop()

    # ai_path = ai_paths[0]
    # try:
    #     df_ai = pd.read_csv(ai_path)
    # except Exception as e:
    #     st.error(f"Failed to read `{ai_path}`: {e}")
    #     st.stop()

    # # Ensure join keys exist to avoid editor KeyErrors later
    # for col in ["application_id", "asset_id", "asset_type", "city"]:
    #     if col not in df_ai.columns:
    #         df_ai[col] = None
    
    # # Inject the Stage C output (AI values) into Stage E for human review
    # # Ensure human_value column exists for adjustments
    # if "human_value" not in df_ai.columns:
    #     df_ai["human_value"] = pd.to_numeric(df_ai["fmv"], errors="coerce") if "fmv" in df_ai.columns else np.nan
    # if "justification" not in df_ai.columns:
    #     df_ai["justification"] = ""

    # # â”€â”€ KPI Overview (safe)
    # st.markdown("### ğŸ“Š Business Metrics Overview")

    # def _safe_mean(df, col, fmt=None):
    #     if isinstance(df, pd.DataFrame) and col in df.columns:
    #         v = pd.to_numeric(df[col], errors="coerce").mean()
    #         if pd.notna(v):
    #             return f"{v:,.0f}" if fmt == "int" else (f"{v:.1f}" if fmt == "1f" else f"{v}")
    #     return "â€”"

    # def _safe_ltv(df):
    #     if all(c in df.columns for c in ("loan_amount", "fmv")):
    #         la = pd.to_numeric(df["loan_amount"], errors="coerce")
    #         fmv = pd.to_numeric(df["fmv"], errors="coerce").replace(0, np.nan)
    #         v = (la / fmv).mean()
    #         if pd.notna(v):
    #             return f"{v:.2f}"
    #     return "â€”"

    # c1, c2, c3, c4 = st.columns(4)
    # with c1: st.metric("Average FMV", _safe_mean(df_ai, "fmv", fmt="int"))
    # with c2: st.metric("Average Confidence", _safe_mean(df_ai, "confidence", fmt="1f") + ("%" if _safe_mean(df_ai, "confidence", fmt="1f") != "â€”" else ""))
    # with c3: st.metric("Average LTV", _safe_ltv(df_ai))
    # with c4: st.metric("Assets", f"{len(df_ai):,}")

    # â”€â”€ Market Projections (safe)
    st.markdown("### ğŸ“ˆ Market Projections")
    horizon = st.select_slider("Projection Horizon (years)", options=[3, 5, 10], value=5)
    growth = st.slider("Expected Market Growth (%)", -10, 25, 4) / 100

    df_proj = df_ai.copy()
    if "fmv" in df_proj.columns:
        fmv_num = pd.to_numeric(df_proj["fmv"], errors="coerce")
        df_proj[f"fmv_proj_{horizon}y"] = (fmv_num * ((1 + growth) ** horizon)).round(0)
        st.line_chart(df_proj[["fmv", f"fmv_proj_{horizon}y"]])
    else:
        st.info("FMV column not found; projection chart will appear after you run Stage C.")

    # â”€â”€ Human Adjustment Table (safe + flexible)
    st.markdown("### âœï¸ Human Adjustments & Justification")

    # Ensure editable columns exist
    if "human_value" not in df_ai.columns:
        df_ai["human_value"] = pd.to_numeric(df_ai["fmv"], errors="coerce") if "fmv" in df_ai.columns else np.nan
    if "justification" not in df_ai.columns:
        df_ai["justification"] = ""

     # Editable columns for the reviewer
    base_editable = ["application_id", "asset_id", "asset_type", "city", "fmv", "ai_adjusted", "confidence", "loan_amount", "human_value", "justification"]
    editable_cols = [c for c in base_editable if c in df_ai.columns]  # filter to present
    if not editable_cols:
        editable_cols = df_ai.columns.tolist()  # last resort: allow full frame
    
    # Display the editable table for human review

    edited = st.data_editor(df_ai[editable_cols], num_rows="dynamic", use_container_width=True)

    # â”€â”€ Agreement / Deviation Gauge + Mismatch list
    st.markdown("### ğŸ¯ Human vs AI Agreement / Deviation")

    # Resolve decision columns if present
    def _first_present(df, candidates):
        return next((c for c in candidates if c in df.columns), None)

    ai_dec_col = _first_present(edited, ["ai_decision", "ai_label", "ai_outcome", "decision_ai"])
    human_dec_col = _first_present(edited, ["human_decision", "human_label", "final_decision", "decision_human"])

    if ai_dec_col and human_dec_col:
        # Agreement gauge (%)
        a = edited[ai_dec_col].astype(str).str.strip().str.lower()
        h = edited[human_dec_col].astype(str).str.strip().str.lower()
        matches = (a == h)
        agree_pct = float(matches.mean() * 100.0) if len(matches) else 0.0

        fig = go.Figure(go.Indicator(
            mode="gauge+number",
            value=round(agree_pct, 2),
            number={'suffix': '%'},
            gauge={
                'axis': {'range': [0, 100]},
                'bar': {'thickness': 0.35},
                'steps': [
                    {'range': [0, 50], 'color': '#fee2e2'},
                    {'range': [50, 80], 'color': '#fef9c3'},
                    {'range': [80, 100], 'color': '#dcfce7'},
                ],
                'threshold': {'line': {'color': '#2563eb', 'width': 4}, 'thickness': 0.9, 'value': round(agree_pct, 2)}
            },
            title={'text': "AI â†” Human Agreement"}
        ))
        st.plotly_chart(fig, use_container_width=True)

        # Mismatch table (if any)
        mis_df = edited.loc[~matches].copy()
        key_cols = [c for c in ["application_id", "asset_id", "asset_type", "city"] if c in edited.columns]
        value_ai = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
        value_hu = _first_present(edited, ["human_value", "reviewed_value", "final_value"])

        if not mis_df.empty:
            show_cols = key_cols + [c for c in [ai_dec_col, human_dec_col, value_ai, value_hu, "justification"] if c]
            show_cols = [c for c in show_cols if c in mis_df.columns]
            st.markdown("#### ğŸ” Mismatches â€” what did humans change?")
            st.dataframe(mis_df[show_cols].head(300), use_container_width=True, hide_index=True)
        else:
            st.success("ğŸ‰ Perfect agreement â€” no mismatches.")
    else:
        # Fall back to deviation score if decisions are not present
        if all(c in edited.columns for c in ("human_value", "fmv")):
            hv = pd.to_numeric(edited["human_value"], errors="coerce")
            fmv = pd.to_numeric(edited["fmv"], errors="coerce").replace(0, np.nan)
            deviation = (hv - fmv).abs() / fmv
            score = max(0.0, 100.0 - float(np.nanmean(deviation) * 200.0)) if len(deviation) else 0.0

            fig = go.Figure(go.Indicator(
                mode="gauge+number",
                value=round(score, 1),
                number={'suffix': ' / 100'},
                gauge={
                    'axis': {'range': [0, 100]},
                    'bar': {'thickness': 0.35},
                    'steps': [
                        {'range': [0, 50], 'color': '#fee2e2'},
                        {'range': [50, 80], 'color': '#fef9c3'},
                        {'range': [80, 100], 'color': '#dcfce7'},
                    ],
                    'threshold': {'line': {'color': '#2563eb', 'width': 4}, 'thickness': 0.9, 'value': round(score, 1)}
                },
                title={'text': "Alignment Score (by value deviation)"}
            ))
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Provide decision columns (ai_decision / human_decision) for agreement gauge, or both FMV and human_value for deviation.")

    
        # â”€â”€ Human Changes Only (colored)
        st.markdown("### ğŸ–ï¸ Human Changes Only (colored)")

        # Reuse helper and edited df from above
        value_ai = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
        value_hu = _first_present(edited, ["human_value", "reviewed_value", "final_value"])
        ai_dec_col = _first_present(edited, ["ai_decision", "ai_label", "ai_outcome", "decision_ai"])
        human_dec_col = _first_present(edited, ["human_decision", "human_label", "final_decision", "decision_human"])

        if value_ai and value_hu:
            ai_vals = pd.to_numeric(edited[value_ai], errors="coerce")
            hu_vals = pd.to_numeric(edited[value_hu], errors="coerce")

            # decisions -> Series aligned to edited.index
            if ai_dec_col and human_dec_col:
                a = edited[ai_dec_col].astype(str).str.strip().str.lower()
                h = edited[human_dec_col].astype(str).str.strip().str.lower()
                dec_changed = (a != h)  # Series
            else:
                dec_changed = pd.Series(False, index=edited.index)

            # justification -> Series aligned
            just_present = edited.get("justification", pd.Series("", index=edited.index)) \
                                .astype(str).str.strip().ne("")

            # treat tiny diffs as equal
            rel_tol = 1e-9
            val_changed = (ai_vals.fillna(np.nan) - hu_vals.fillna(np.nan)).abs() > (
                (ai_vals.abs() + hu_vals.abs()).fillna(0) * rel_tol
            )

            changed_mask = val_changed | dec_changed | just_present
            diff_df = edited.loc[changed_mask].copy()

            if diff_df.empty:
                st.success("ğŸ‰ No human changes detected.")
            else:
                # compute deltas on the FILTERED subset ONLY
                ai_sub = ai_vals.reindex(diff_df.index)
                hu_sub = hu_vals.reindex(diff_df.index)

                diff_df["Î”_value"] = (hu_sub - ai_sub)
                base = ai_sub.replace(0, np.nan)
                diff_df["Î”_%"] = ((diff_df["Î”_value"] / base) * 100.0).round(2)

                key_cols = [c for c in ["application_id", "asset_id", "asset_type", "city"] if c in diff_df.columns]
                show_cols = key_cols + [c for c in [value_ai, value_hu, "Î”_value", "Î”_%", ai_dec_col, human_dec_col, "justification"] if c in diff_df.columns]
                show_df = diff_df[show_cols].copy()

                def _color_row(row):
                    styles = [""] * len(row.index)

                    def _idx(colname):
                        try:
                            return show_df.columns.get_loc(colname)
                        except Exception:
                            return None

                    idx_ai = _idx(value_ai)
                    idx_hu = _idx(value_hu)
                    idx_dv = _idx("Î”_value")
                    idx_dp = _idx("Î”_%")

                    # Value changes
                    try:
                        ai_v = float(row.get(value_ai, np.nan))
                        hu_v = float(row.get(value_hu, np.nan))
                    except Exception:
                        ai_v, hu_v = np.nan, np.nan

                    if pd.notna(ai_v) and pd.notna(hu_v):
                        if hu_v > ai_v:  # green for up
                            for i in [idx_hu, idx_dv, idx_dp]:
                                if i is not None:
                                    styles[i] = "background-color:#dcfce7; color:#065f46; font-weight:600;"
                            if idx_ai is not None:
                                styles[idx_ai] = "background-color:#ecfdf5; color:#064e3b;"
                        elif hu_v < ai_v:  # red for down
                            for i in [idx_hu, idx_dv, idx_dp]:
                                if i is not None:
                                    styles[i] = "background-color:#fee2e2; color:#7f1d1d; font-weight:600;"
                            if idx_ai is not None:
                                styles[idx_ai] = "background-color:#fef2f2; color:#7f1d1d;"

                    # Decision changes â†’ amber
                    if ai_dec_col in show_df.columns and human_dec_col in show_df.columns:
                        ai_d = str(row.get(ai_dec_col, "")).strip().lower()
                        hu_d = str(row.get(human_dec_col, "")).strip().lower()
                        if ai_d != "" and hu_d != "" and ai_d != hu_d:
                            for colname in [ai_dec_col, human_dec_col]:
                                j = _idx(colname)
                                if j is not None:
                                    styles[j] = "background-color:#fef9c3; color:#7c2d12; font-weight:600;"

                    # Justification present â†’ blue
                    if "justification" in show_df.columns:
                        just = str(row.get("justification", "")).strip()
                        if just:
                            j = _idx("justification")
                            if j is not None:
                                styles[j] = "background-color:#e0f2fe; color:#0c4a6e;"

                    return styles

                styled = show_df.style.apply(_color_row, axis=1) \
                                    .format({value_ai: "{:,.0f}", value_hu: "{:,.0f}", "Î”_value": "{:,.0f}", "Î”_%": "{:.2f}%"})
                st.dataframe(styled, use_container_width=True, hide_index=True)
        else:
            st.info("To show the colorful Human-Changes table, ensure value columns exist (e.g., ai_adjusted/fmv and human_value).")

        

    
    # â”€â”€ Export for Retraining
    st.markdown("### ğŸ’¾ Save & Export for Training")
    # Lightweight export view for training: keep keys + AI/Human value/decisions if present
    train_cols_base = ["application_id", "asset_id", "asset_type", "city"]
    ai_val_col = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
    hu_val_col = _first_present(edited, ["human_value", "reviewed_value", "final_value"])
    keep_cols = [c for c in train_cols_base if c in edited.columns] + \
                [c for c in [ai_dec_col, human_dec_col, ai_val_col, hu_val_col, "confidence", "loan_amount", "justification"] if c in edited.columns]
    export_df = edited[keep_cols].copy() if keep_cols else edited.copy()

    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
    out_path = os.path.join(RUNS_DIR, f"reviewed_appraisal.{ts}.csv")

    cE1, cE2 = st.columns([1.2, 1])
    with cE1:
        st.text_input("Will save to (server path)", out_path, label_visibility="collapsed")
    with cE2:
        st.download_button(
            "â¬‡ï¸ Download Human vs AI CSV",
            export_df.to_csv(index=False).encode("utf-8-sig"),
            file_name=os.path.basename(out_path),
            mime="text/csv",
            key="dl_reviewed_appraisal_stageE"
        )

    if st.button("ğŸ’¾ Save Human Feedback (server)", key="btn_save_feedback"):
        try:
            export_df.to_csv(out_path, index=False, encoding="utf-8-sig")
            st.success(f"âœ… Saved human-reviewed data â†’ `{out_path}`")
        except Exception as e:
            st.error(f"Save failed: {e}")




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# F â€” MODEL TRAINING & PROMOTION (A/B with Prod)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tabF:
    import os, json, glob
    from datetime import datetime, timezone
    import numpy as np
    import pandas as pd
    import plotly.graph_objects as go
    import plotly.express as px
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
    from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
    from sklearn.linear_model import LinearRegression
    import joblib
    import shutil

    st.subheader("ğŸ§ª Stage F â€” Model Training & Promotion")
    st.caption("Train or retrain with human feedback, compare against production (A/B), and promote if better.")

    # ---------- helpers ----------
    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    def _rmse(y_true, y_pred):
        return float(np.sqrt(mean_squared_error(y_true, y_pred)))

    def _mape(y_true, y_pred):
        y_true = np.asarray(y_true, dtype=float)
        y_pred = np.asarray(y_pred, dtype=float)
        mask = (y_true != 0) & np.isfinite(y_true) & np.isfinite(y_pred)
        if not mask.any():
            return float("nan")
        return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0)

    def _safe_len_df(x):
        return (0 if not isinstance(x, pd.DataFrame) else len(x))

    # ---------- diagnostics (always visible) ----------
    st.markdown("#### ğŸ” Data availability (snapshots)")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("decision_df",  _safe_len_df(ss.get("asset_decision_df")))
    c2.metric("policy_df",    _safe_len_df(ss.get("asset_policy_df")))
    c3.metric("verified_df",  _safe_len_df(ss.get("asset_verified_df")))
    c4.metric("ai_df",        _safe_len_df(ss.get("asset_ai_df")))

    with st.expander("Load demo portfolio (if earlier stages not run)"):
        if st.button("Load demo portfolio (10 rows)", key="btn_demo_portfolio"):
            rng = np.random.default_rng(42)
            demo = pd.DataFrame({
                "application_id": [f"APP_{i:04d}" for i in range(10)],
                "asset_id":      [f"A{i:04d}" for i in range(10)],
                "asset_type":    rng.choice(["House","Apartment","Car","Land"], 10),
                "city":          rng.choice(["HCMC","Hanoi","Da Nang","Hue"], 10),
                "market_value":  rng.integers(80_000, 800_000, 10),
                "ai_adjusted":   rng.integers(75_000, 820_000, 10),
                "loan_amount":   rng.integers(30_000, 500_000, 10),
                "confidence":    rng.integers(60, 98, 10),
                "condition_score": rng.uniform(0.6, 1.0, 10).round(3),
                "legal_penalty":   rng.uniform(0.95, 1.0, 10).round(3),
                "human_value":   rng.integers(75_000, 820_000, 10),
            })
            ss["asset_decision_df"] = demo
            st.success("Demo portfolio loaded into ss['asset_decision_df'].")

    st.divider()

    # ---------- training data source ----------
    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    # Auto-pick latest reviewed CSV from Stage E
    reviewed = sorted([f for f in os.listdir(RUNS_DIR)
                       if f.startswith("reviewed_appraisal") and f.endswith(".csv")], reverse=True)
    df_train = None
    auto_path = None
    if reviewed:
        auto_path = os.path.join(RUNS_DIR, reviewed[0])
        try:
            df_train = pd.read_csv(auto_path)
        except Exception as e:
            st.warning(f"Could not read `{auto_path}`: {e}")

    st.markdown("#### ğŸ“¥ Training dataset")
    colU1, colU2 = st.columns([1.4, 1])
    with colU1:
        st.text_input("Auto-detected Stage E file", value=(auto_path or "â€”"), disabled=True)
    with colU2:
        up = st.file_uploader("Or upload CSV with human_value", type=["csv"], key="train_csv_upload")

    if up is not None:
        try:
            df_train = pd.read_csv(up)
            st.success(f"Loaded uploaded CSV ({len(df_train)} rows).")
        except Exception as e:
            st.error(f"Upload read failed: {e}")

    if df_train is None or df_train.empty:
        st.warning("âš ï¸ No training data available. Use Stage E to export `reviewed_appraisal*.csv` or upload a CSV above.")
        st.stop()

    st.markdown(f"**Using training rows:** {len(df_train):,}")
    st.dataframe(df_train.head(20), use_container_width=True)

    # ---------- feature building ----------
    st.markdown("#### ğŸ§± Feature selection")
    target_col = "human_value"
    if target_col not in df_train.columns:
        st.error("CSV must include a 'human_value' column (target).")
        st.stop()

    # Exclude obvious leak/IDs/targets from X
    drop_cols = {
        target_col, "fmv", "ai_adjusted",  # avoid leakage; AI numbers used only for comparison
        "ai_decision", "human_decision", "decision", "final_decision",
        "justification", "reviewed_value", "final_value",
        "application_id", "asset_id", "asset_type", "city"
    }
    num_cols = [c for c in df_train.columns
                if c not in drop_cols and pd.api.types.is_numeric_dtype(df_train[c])]

    if not num_cols:
        st.error("No numeric features left after filtering. Please include numeric columns for training.")
        st.stop()

    X = df_train[num_cols].copy()
    y = pd.to_numeric(df_train[target_col], errors="coerce")

    # Drop rows with missing target
    mask = pd.notna(y)
    X, y = X.loc[mask], y.loc[mask]

    # Train/Test split
    test_size = st.slider("Holdout size", 10, 40, 20, step=5) / 100.0
    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, random_state=42)

    # ---------- model choice ----------
    st.markdown("#### ğŸ¤– Choose model")
    model_choice = st.selectbox(
        "Select model algorithm",
        ["GradientBoostingRegressor", "RandomForestRegressor", "LinearRegression"],
        index=0
    )
    ModelCls = {
        "GradientBoostingRegressor": GradientBoostingRegressor,
        "RandomForestRegressor": RandomForestRegressor,
        "LinearRegression": LinearRegression,
    }[model_choice]

    # ---------- train & compare ----------
    if st.button("ğŸš€ Train & Compare (A/B vs Production)", key="btn_train_model"):
        # Train new
        new_model = ModelCls().fit(Xtr, ytr)
        y_pred_new = new_model.predict(Xte)

        # Load production baseline if exists
        prod_model_path = "./agents/asset_appraisal/models/production/model.joblib"
        prod_exists = os.path.exists(prod_model_path)
        y_pred_prod = None
        if prod_exists:
            try:
                prod_model = joblib.load(prod_model_path)
                # guard: try only if shapes align
                y_pred_prod = prod_model.predict(Xte)
            except Exception as e:
                st.warning(f"Production model failed to score holdout: {e}")

        # Metrics
        def _metrics(y_true, y_pred):
            return {
                "MAE": float(mean_absolute_error(y_true, y_pred)),
                "RMSE": _rmse(y_true, y_pred),
                "MAPE%": _mape(y_true, y_pred),
                "R2": float(r2_score(y_true, y_pred)),
            }

        new_m = _metrics(yte, y_pred_new)
        prod_m = _metrics(yte, y_pred_prod) if y_pred_prod is not None else None

        # ===== Dashboard: KPIs & deltas =====
        st.markdown("### ğŸ“Š A/B Metrics (Holdout)")
        k1, k2, k3, k4, k5 = st.columns(5)
        with k1:
            st.metric("New MAE", f"{new_m['MAE']:,.0f}",
                      delta=(f"{(new_m['MAE'] - prod_m['MAE']):+.0f}" if prod_m else None))
        with k2:
            st.metric("New RMSE", f"{new_m['RMSE']:,.0f}",
                      delta=(f"{(new_m['RMSE'] - prod_m['RMSE']):+.0f}" if prod_m else None))
        with k3:
            st.metric("New MAPE", f"{new_m['MAPE%']:.2f}%",
                      delta=(f"{(new_m['MAPE%'] - prod_m['MAPE%']):+.2f}%" if prod_m else None))
        with k4:
            st.metric("New RÂ²", f"{new_m['R2']:.3f}",
                      delta=(f"{(new_m['R2'] - prod_m['R2']):+.3f}" if prod_m else None))
        with k5:
            st.metric("Test rows", f"{len(yte):,}")

        # ===== Plots: Actual vs Pred, Residuals =====
        plot_df = pd.DataFrame({
            "y_true": yte.values,
            "y_pred_new": y_pred_new,
            "y_pred_prod": (y_pred_prod if y_pred_prod is not None else np.full_like(y_pred_new, np.nan))
        })

        # Actual vs Pred overlay
        fig_scatter = go.Figure()
        fig_scatter.add_trace(go.Scatter(
            x=plot_df["y_true"], y=plot_df["y_pred_new"],
            mode="markers", name="New", opacity=0.7
        ))
        if y_pred_prod is not None:
            fig_scatter.add_trace(go.Scatter(
                x=plot_df["y_true"], y=plot_df["y_pred_prod"],
                mode="markers", name="Production", opacity=0.6
            ))
        # diagonal reference
        minv, maxv = np.nanmin(plot_df[["y_true","y_pred_new","y_pred_prod"]].values), np.nanmax(plot_df[["y_true","y_pred_new","y_pred_prod"]].values)
        fig_scatter.add_trace(go.Scatter(x=[minv, maxv], y=[minv, maxv], mode="lines", name="Ideal", line=dict(dash="dash")))
        fig_scatter.update_layout(title="Actual vs Predicted (Holdout)", xaxis_title="Actual", yaxis_title="Predicted")
        st.plotly_chart(fig_scatter, use_container_width=True)

        # Residuals hist
        plot_df["res_new"]  = plot_df["y_true"] - plot_df["y_pred_new"]
        if y_pred_prod is not None:
            plot_df["res_prod"] = plot_df["y_true"] - plot_df["y_pred_prod"]

        fig_res = go.Figure()
        fig_res.add_trace(go.Histogram(x=plot_df["res_new"], name="New", opacity=0.7))
        if y_pred_prod is not None:
            fig_res.add_trace(go.Histogram(x=plot_df["res_prod"], name="Production", opacity=0.6))
        fig_res.update_layout(barmode="overlay", title="Residuals Distribution (Actual - Predicted)")
        fig_res.update_traces(nbinsx=40)
        st.plotly_chart(fig_res, use_container_width=True)

        # ===== Feature importance / coefficients =====
        st.markdown("### ğŸ§  Feature Importance / Coefficients")
        if hasattr(new_model, "feature_importances_"):
            imp = pd.DataFrame({
                "feature": num_cols,
                "importance": new_model.feature_importances_
            }).sort_values("importance", ascending=False)
            st.bar_chart(imp.set_index("feature"))
        elif hasattr(new_model, "coef_"):
            coef = pd.DataFrame({
                "feature": num_cols,
                "coef": np.ravel(new_model.coef_)
            }).sort_values("coef", key=np.abs, ascending=False)
            st.bar_chart(coef.set_index("feature"))
        else:
            st.info("This model does not expose importances/coefficients.")

        # ===== Persist artifacts =====
        #trained_dir = "./agents/asset_appraisal/models/trained"
        trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/trained"
        
        os.makedirs(trained_dir, exist_ok=True)
        ts = _ts()
        model_path = os.path.join(trained_dir, f"{model_choice}_asset_{ts}.joblib")
        joblib.dump(new_model, model_path)

        preds_csv = os.path.join(RUNS_DIR, f"training_preds_{ts}.csv")
        plot_df.to_csv(preds_csv, index=False)

        report = {
            "timestamp": ts,
            "model_choice": model_choice,
            "trained_model_path": model_path,
            "features": num_cols,
            "metrics_new": new_m,
            "metrics_prod": prod_m,
            "holdout_rows": int(len(yte)),
            "source_file": (auto_path or "uploaded"),
            "preds_csv": preds_csv,
        }
        report_path = os.path.join(RUNS_DIR, f"training_report_{ts}.json")
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2)

        st.success(f"âœ… Trained model saved â†’ `{model_path}`")
        st.caption(f"Report â†’ `{report_path}` | Predictions â†’ `{preds_csv}`")

        # Download helpers
        cdl1, cdl2 = st.columns(2)
        with cdl1:
            st.download_button("â¬‡ï¸ Download training report (JSON)",
                               data=json.dumps(report, indent=2).encode("utf-8"),
                               file_name=os.path.basename(report_path),
                               mime="application/json")
        with cdl2:
            st.download_button("â¬‡ï¸ Download holdout predictions (CSV)",
                               data=plot_df.to_csv(index=False).encode("utf-8-sig"),
                               file_name=os.path.basename(preds_csv),
                               mime="text/csv")

        
        
        # ===== Promotion =====
        st.markdown("### ğŸŸ¢ Promote to Production")
        better_hint = ""
        if prod_m is not None and np.isfinite(prod_m["MAE"]):
            delta_mae = new_m["MAE"] - prod_m["MAE"]
            better_hint = "âœ… New MAE is lower than production â€” promotion recommended." if delta_mae < 0 else "âš ï¸ New MAE is not lower than production."
            st.info(better_hint)

        if st.button("ğŸ“¤ Promote this trained model to Production", key="btn_promote_prod"):
            # Define the production directory path
            prod_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/production"
            os.makedirs(prod_dir, exist_ok=True)

            # Copy the trained model to production folder
            shutil.copy(model_path, os.path.join(prod_dir, "model.joblib"))

            # Prepare meta information for the promoted model
            meta = {
                "model_path": model_path,
                "promoted_at": datetime.now(timezone.utc).isoformat(),
                "ab_report": report,  # Add your AB report or other relevant info
            }

            # Define the meta path for writing
            meta_path = os.path.join(prod_dir, "production_meta.json")

            try:
                # Write the meta data to production_meta.json
                with open(meta_path, "w", encoding="utf-8") as f:
                    json.dump(meta, f, indent=2)
                st.success(f"ğŸŸ¢ Model promoted to production: {os.path.join(prod_dir, 'model.joblib')}")
                st.caption(f"Production metadata saved in {meta_path}")
            except Exception as e:
                # Catch any errors while creating the meta file
                st.error(f"âŒ Error while creating production meta file: {e}")
                st.warning("Please check the directory permissions or try again.")

        
        
 

    