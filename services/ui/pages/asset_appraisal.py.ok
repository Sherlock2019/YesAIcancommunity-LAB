#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¦ Asset Appraisal Agent â€” Full E2E Flow (Inputs â†’ Anonymize â†’ AI â†’ Human Review â†’ Training)
Author:  Nguyen Dzoan
Version: 2025-11-01

Includes:
- Stage 1: CSV + evidence (images/PDFs) + manual row; synthetic fallback + "why" table
- Stage 2: Explicit anonymization pipeline (RAW & ANON kept)
- Stage 3: AI appraisal with runtime flavor selector, agent discovery+probe, rule_reasons when backend omits
  + Production banner + asset-trained model selector + promote inside Stage 3
- Stage 4: Human Review with AIâ†”Human agreement gauge; export feedback CSV
- Stage 5: Training (upload feedback) â†’ Train candidate â†’ Promote to PRODUCTION
"""

import os
import io
import re
import json
from datetime import datetime, timezone  # âœ… clean, safe, supports datetime.now()
from pathlib import Path
from typing import Any, Dict

# â”€â”€ Third-party
import requests
import numpy as np
import pandas as pd
import streamlit as st
import streamlit.components.v1 as components
import plotly.express as px
import plotly.graph_objects as go
import csv
import zipfile  # âœ… ADD THIS






def apply_theme(theme: str = "dark"):
    import streamlit as st

    st.markdown("""
    <style>
    /* ===============================================
       ğŸŒ™ MACOS BLUE DARK THEME â€” GLOBAL BASE
    =============================================== */
    html, body, [data-testid="stAppViewContainer"] {
        background: radial-gradient(circle at 20% 20%, #0b0f16, #060a12 85%) !important;
        color: #f8fafc !important;
        font-family: "Inter","SF Pro Display","Segoe UI",system-ui,sans-serif !important;
    }

    h1,h2,h3,h4,h5,h6 {
        color: #f8fafc !important;
        font-weight: 700 !important;
        letter-spacing: -0.02em !important;
    }

    p, li, label, span, div {
        color: #e2e8f0 !important;
    }
    small, .stCaption { color: #94a3b8 !important; }

    a, a:link, a:visited { color: #339dff !important; }
    a:hover { color: #60a5fa !important; text-decoration: underline; }

    hr {
        border: none !important;
        height: 1px !important;
        background: linear-gradient(90deg,transparent,#007aff,transparent) !important;
    }

    /* ===============================================
       ğŸ§± CONTAINERS & CARDS
    =============================================== */
    .stMarkdown, .stContainer, .stAlert, [class*="stCard"], [class*="block-container"] {
        background: #0f172a !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 12px !important;
        box-shadow: 0 4px 16px rgba(0,0,0,0.5) !important;
    }

    /* ===============================================
       ğŸ”˜ BUTTONS â€” macOS BLUE
    =============================================== */
    button[kind="primary"], .stButton>button, .stDownloadButton>button, .stDownloadButton button {
        background: linear-gradient(180deg,#007aff,#005ecb) !important;
        color: #ffffff !important;
        border: 1px solid #0051b8 !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
        padding: 0.5rem 1rem !important;
        box-shadow: 0 4px 10px rgba(0,122,255,0.35),
                    inset 0 -1px 0 rgba(255,255,255,0.2) !important;
        transition: all 0.25s ease-in-out !important;
    }
    button[kind="primary"]:hover, .stButton>button:hover, .stDownloadButton>button:hover {
        background: linear-gradient(180deg,#339dff,#006ae6) !important;
        box-shadow: 0 4px 14px rgba(0,122,255,0.45) !important;
        transform: translateY(-1px) !important;
    }
    button[kind="primary"]:active, .stButton>button:active, .stDownloadButton>button:active {
        background: linear-gradient(180deg,#004fc4,#0042a8) !important;
        box-shadow: inset 0 2px 6px rgba(0,122,255,0.3) !important;
        transform: translateY(0) !important;
    }
    .stButton button[disabled], .stDownloadButton button[disabled] {
        background: #1e293b !important;
        color: #64748b !important;
        border: 1px solid #334155 !important;
    }

    /* ===============================================
    ğŸ§  INPUTS (Text, Select, Number) & FOCUS STATE
    =============================================== */
    .stTextInput>div>div>input,
    .stSelectbox>div>div>div,
    .stNumberInput input {
        background: #111827 !important;
        color: #f8fafc !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 8px !important;
        padding: 6px 10px !important;
        transition: all 0.25s ease;
    }
    .stTextInput>div>div>input:focus,
    .stSelectbox>div>div>div:focus-within,
    .stNumberInput input:focus {
        outline: none !important;
        border-color: #007aff !important;
        box-shadow: 0 0 0 2px rgba(0,122,255,0.4) !important;
    }
    ::placeholder {
        color: #9ca3af !important;
        opacity: 1 !important;
    }
    /* ===============================================
   ğŸ› DROPDOWN MENUS
    =============================================== */
    [data-baseweb="popover"], [role="listbox"] {
        background: #0f172a !important;
        color: #f8fafc !important;
        border: 1px solid #1e3a8a !important;
        box-shadow: 0 4px 20px rgba(0,0,0,0.6) !important;
    }
    [data-baseweb="menu-item"] {
        background: #0f172a !important;
        color: #f8fafc !important;
    }
    [data-baseweb="menu-item"]:hover {
        background: #1e3a8a !important;
        color: #ffffff !important;
    }
    /* ===============================================
    ğŸ§­ SIDEBAR THEME
    =============================================== */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg,#0d1320,#060a12) !important;
        border-right: 1px solid #1e3a8a !important;
        color: #f8fafc !important;
    }

    
    /* ===============================================
       â˜‘ï¸ CHECKBOXES / RADIOS / SLIDERS
    =============================================== */
    input[type="checkbox"], input[type="radio"] {
        accent-color: #007aff !important;
    }
    .stSlider [role="slider"] {
        background-color: #007aff !important;
    }

    /* ===============================================
       ğŸ—‚ï¸ TABS
    =============================================== */
    .stTabs [data-baseweb="tab-list"] button {
        color: #e2e8f0 !important;
        background: #111827 !important;
        border: 1px solid #1e293b !important;
        border-radius: 10px !important;
        font-weight: 500 !important;
        margin-right: 4px !important;
    }
    .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {
        background: #007aff !important;
        color: #ffffff !important;
        box-shadow: 0 0 12px rgba(0,122,255,0.4) !important;
    }

    /* ===============================================
       ğŸ§­ EXPANDERS / ACCORDIONS
    =============================================== */
    .streamlit-expanderHeader {
        background: linear-gradient(90deg,#0d284d,#0a1f3a) !important;
        color: #dbeafe !important;
        border: 1px solid #1e3a5f !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
    }
    .streamlit-expanderContent {
        background: #0f172a !important;
        color: #e2e8f0 !important;
        border: 1px solid #1e3a5f !important;
        border-radius: 0 0 8px 8px !important;
    }

    /* ===============================================
       ğŸ“Š METRIC CARDS (st.metric)
    =============================================== */
    [data-testid="stMetric"] {
        background: linear-gradient(180deg,#0b1220,#101a2c) !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 10px !important;
        box-shadow: inset 0 0 10px rgba(255,255,255,0.03),
                    0 3px 10px rgba(0,0,0,0.6) !important;
        padding: 10px 14px !important;
        text-align: center !important;
    }
    div[data-testid="stMetricLabel"] {
        color: #94a3b8 !important;
        font-size: 0.85rem !important;
        font-weight: 500 !important;
    }
    div[data-testid="stMetricValue"] {
        color: #ffffff !important;
        font-size: 1.3rem !important;
        font-weight: 600 !important;
    }

    /* ===============================================
       ğŸ“Š METRIC COMPARISON TABLE â€” FINAL
    =============================================== */
    [data-testid="stDataFrame"] {
        background: radial-gradient(circle at 50% 50%, #0b1220, #060a12 90%) !important;
        border: 1px solid #1e3a8a !important;
        border-radius: 12px !important;
        box-shadow:
            0 0 14px rgba(0,0,0,0.6) inset,
            0 4px 18px rgba(0,0,0,0.7),
            0 0 12px rgba(0,122,255,0.15) !important;
        margin-top: 12px !important;
        padding: 8px !important;
    }
    [data-testid="stDataFrame"] thead tr th {
        background: linear-gradient(90deg,#004fc4,#007aff) !important;
        color: #ffffff !important;
        border-bottom: 2px solid #007aff !important;
        font-weight: 700 !important;
        text-transform: uppercase !important;
        letter-spacing: 0.02em !important;
        font-size: 0.92rem !important;
        padding: 10px 14px !important;
    }
    [data-testid="stDataFrame"] tbody tr {
        background-color: #0b1220 !important;
        color: #ffffff !important;
        transition: background 0.25s ease;
    }
    [data-testid="stDataFrame"] tbody tr:nth-child(even) {
        background-color: #101a2c !important;
    }
    [data-testid="stDataFrame"] tbody tr:hover {
        background-color: #112a52 !important;
        box-shadow: 0 0 8px rgba(0,122,255,0.25) inset !important;
    }
    [data-testid="stDataFrame"] tbody td {
        border-top: 1px solid #1e3a8a !important;
        color: #ffffff !important;
        padding: 9px 14px !important;
        font-size: 0.95rem !important;
        font-weight: 500 !important;
    }
    [data-testid="stDataFrame"] tbody td:last-child {
        color: #60a5fa !important;
        font-weight: 500 !important;
    }

    /* ===============================================
       ğŸ“ FILE UPLOADER
    =============================================== */
    [data-testid="stFileUploaderDropzone"] {
        background: rgba(255,255,255,0.03) !important;
        border: 1px dashed #1e3a8a !important;
        border-radius: 10px !important;
        color: #cbd5e1 !important;
        transition: all 0.25s ease;
    }
    [data-testid="stFileUploaderDropzone"]:hover {
        border-color: #007aff !important;
        background: rgba(0,122,255,0.1) !important;
    }

    /* ===============================================
       âš ï¸ ALERT BOXES
    =============================================== */
    [data-testid^="stAlert"] {
        border-radius: 10px !important;
        border: 1px solid #1e3a8a !important;
        color: #e2e8f0 !important;
        box-shadow: 0 3px 15px rgba(0,0,0,0.4) !important;
    }
    [data-testid="stAlertInfo"]    { background: linear-gradient(145deg,#0d1829,#10243d)!important; }
    [data-testid="stAlertSuccess"] { background: linear-gradient(145deg,#0f2414,#183820)!important; }
    [data-testid="stAlertError"]   { background: linear-gradient(145deg,#2b1617,#1a0c0d)!important; }
    [data-testid="stAlertWarning"] { background: linear-gradient(145deg,#2f2a10,#1c1a0a)!important; }

    </style>
    """, unsafe_allow_html=True)














# def apply_theme(theme: str = "dark"):
#     import streamlit as st

#     # =======================
#     # ğŸ¨ COLOR PALETTE
#     # =======================
#     if theme == "light":
#         bg, text, subtext = "#ffffff", "#0f172a", "#334155"
#         card, border = "#f8fafc", "#e2e8f0"
#         accent, accent2 = "#2563eb", "#22c55e"
#         tab_bg, table_bg = "#eef2ff", "#ffffff"
#         table_head_bg, table_head_tx = "#e2e8f0", "#0f172a"
#     else:
#         bg, text, subtext = "#0b0f16", "#f8fafc", "#cbd5e1"
#         card, border = "#111827", "#1e293b"
#         accent, accent2 = "#60a5fa", "#22c55e"
#         tab_bg, table_bg = "#1e293b", "#111827"
#         table_head_bg, table_head_tx = "#1f2937", "#f8fafc"

#     # =======================
#     # ğŸ’… GLOBAL STYLES
#     # =======================
#     st.markdown(f"""
#     <style>

#     /* GLOBAL BACKGROUND */
#     .stApp {{
#         background: radial-gradient(circle at 20% 20%, {bg}, #090d13 90%) !important;
#         color: {text} !important;
#         font-family: 'Inter','Segoe UI',system-ui,sans-serif !important;
#     }}

#     /* HEADERS */
#     h1,h2,h3,h4,h5,h6 {{
#         color:{text}!important;
#         font-weight:700!important;
#         letter-spacing:-0.02em;
#     }}

#     /* LABELS & TEXT */
#     label, .stMarkdown p, .stMarkdown li, span {{
#         color:{subtext}!important;
#         opacity:1!important;
#     }}

#     /* INPUTS */
#     .stTextInput>div>div>input,
#     .stSelectbox>div>div>div,
#     .stNumberInput input {{
#         background:{card}!important;
#         color:{text}!important;
#         border:1px solid {border}!important;
#         border-radius:8px!important;
#     }}

#     /* PLACEHOLDERS / DISABLED TEXT */
#     .stTextInput>div>div>input::placeholder,
#     [disabled], .stCheckbox label, .stRadio label {{
#         color:#9ca3af!important;
#         opacity:1!important;
#     }}

#     /* BUTTONS */
#     .stButton>button {{
#         background:linear-gradient(90deg,{accent},#3b82f6)!important;
#         color:#fff!important;
#         border:none!important;
#         border-radius:10px!important;
#         font-weight:600!important;
#         padding:0.5rem 1rem!important;
#         box-shadow:0 3px 10px rgba(0,0,0,0.3)!important;
#         transition:all .25s ease;
#     }}
#     .stButton>button:hover {{
#         transform:translateY(-2px)!important;
#         box-shadow:0 5px 15px rgba(96,165,250,0.4)!important;
#     }}

#     /* DOWNLOAD BUTTONS */
#     a.stDownloadButton>button {{
#         background:#1e3a8a!important;
#         color:#fff!important;
#         border:1px solid #3b82f6!important;
#         border-radius:6px!important;
#         font-weight:600!important;
#     }}
#     a.stDownloadButton>button:hover {{
#         background:#2563eb!important;
#         box-shadow:0 0 8px rgba(37,99,235,0.6)!important;
#     }}

#     /* METRIC BOXES */
#     [data-testid="stMetric"] {{
#         background:{card}!important;
#         border:1px solid {border}!important;
#         border-radius:10px!important;
#         padding:10px 14px!important;
#         color:{text}!important;
#         box-shadow:inset 0 0 6px rgba(255,255,255,0.03),
#                     0 3px 10px rgba(0,0,0,0.4)!important;
#     }}
#     div[data-testid="stMetricLabel"],
#     div[data-testid="stMetricValue"] {{
#         color:{text}!important;
#         opacity:1!important;
#         font-weight:600!important;
#     }}

#     /* TABS */
#     .stTabs [data-baseweb="tab-list"] button {{
#         color:{text}!important;
#         background:{tab_bg}!important;
#         border:1px solid {border}!important;
#         border-radius:10px!important;
#         font-weight:500!important;
#     }}
#     .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {{
#         background:{accent}!important;
#         color:#fff!important;
#         box-shadow:0 0 10px rgba(96,165,250,0.4)!important;
#     }}

#     /* EXPANDERS (How-to panels) */
#     .streamlit-expanderHeader {{
#         background:linear-gradient(90deg,#10243d,#0e1b2b)!important;
#         color:#dbeafe!important;
#         font-weight:600!important;
#         border:1px solid #1e3a5f!important;
#         border-radius:8px!important;
#     }}
#     .streamlit-expanderContent {{
#         background:#0d1624!important;
#         color:#e2e8f0!important;
#         border:1px solid #1a2d4a!important;
#         border-radius:0 0 8px 8px!important;
#     }}

#     /* ALERT / INFO / SUCCESS PANELS */
#     [data-testid^="stAlert"] {{
#         border-radius:10px!important;
#         border:1px solid {border}!important;
#         color:{text}!important;
#         box-shadow:0 4px 20px rgba(0,0,0,0.3)!important;
#     }}
#     [data-testid="stAlertInfo"]    {{ background:linear-gradient(145deg,#0d1829,#10243d)!important; }}
#     [data-testid="stAlertSuccess"] {{ background:linear-gradient(145deg,#0e1f14,#153524)!important; }}
#     [data-testid="stAlertWarning"] {{ background:linear-gradient(145deg,#2f2a10,#1c1a0a)!important; }}
#     [data-testid="stAlertError"]   {{ background:linear-gradient(145deg,#2b1617,#1a0c0d)!important; }}

#     /* TABLES */
#     [data-testid="stDataFrame"] thead tr th {{
#         background:{table_head_bg}!important;
#         color:{table_head_tx}!important;
#         border-bottom:2px solid {accent}!important;
#         font-weight:700!important;
#     }}

#     /* DIVIDER */
#     hr {{
#         border:none!important;
#         height:1px!important;
#         background:linear-gradient(90deg,transparent,{accent},transparent)!important;
#     }}

#     </style>
#     """, unsafe_allow_html=True)



# # Custom pro  Theme 

# def apply_theme(theme: str = "light"):
#     """Apply Rackspace-class professional dark/light Streamlit theme."""

#     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#     # ğŸ¨ COLOR PALETTE
#     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#     if theme == "light":
#         bg, text, subtext = "#ffffff", "#0f172a", "#334155"
#         card, border = "#f8fafc", "#e2e8f0"
#         accent, accent2 = "#2563eb", "#22c55e"
#         tab_bg, table_bg = "#eef2ff", "#ffffff"
#         table_head_bg, table_head_tx = "#e2e8f0", "#0f172a"
#         info_bg, success_bg, warn_bg, error_bg = "#f1f5f9", "#dcfce7", "#fef9c3", "#fee2e2"
#     else:  # ğŸŒ™ DARK MODE â€” refined for clarity & elegance
#         bg, text, subtext = "#0b0f16", "#f8fafc", "#cbd5e1"
#         card, border = "#111827", "#1e293b"
#         accent, accent2 = "#60a5fa", "#22c55e"
#         tab_bg, table_bg = "#1e293b", "#111827"
#         table_head_bg, table_head_tx = "#1f2937", "#f8fafc"
#         info_bg, success_bg, warn_bg, error_bg = "#162130", "#10291c", "#2f2a10", "#2b1617"

#     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#     # ğŸ’… CSS STYLE OVERRIDES
#     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#     st.markdown(f"""
#     <style>

#       /* GLOBAL APP BACKGROUND */
#       .stApp {{
#         background: radial-gradient(circle at 20% 20%, {bg}, #090d13 90%) !important;
#         color: {text} !important;
#         font-family: 'Inter', 'Segoe UI', system-ui, sans-serif !important;
#       }}

#       /* HEADERS & TITLES */
#       h1, h2, h3, h4, h5, h6,
#       .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 {{
#         color: {text} !important;
#         font-weight: 700 !important;
#         letter-spacing: -0.02em;
#         text-shadow: 0 0 10px rgba(0,0,0,0.3);
#       }}
#       h1 {{ font-size: 1.8rem !important; }}
#       h2 {{ font-size: 1.4rem !important; }}
#       h3 {{ font-size: 1.15rem !important; }}

#       /* TEXT & PARAGRAPHS */
#       .stMarkdown p, .stMarkdown li, .stCaption, small {{
#         color: {subtext} !important;
#         opacity: 1 !important;
#         font-size: 0.96rem !important;
#       }}

#       /* INFO / SUCCESS / WARNING / ERROR PANELS */
#       [data-testid="stNotification"], .stAlert {{
#         border-radius: 10px !important;
#         border: 1px solid {border} !important;
#         color: {text} !important;
#         opacity: 1 !important;
#         box-shadow: 0 4px 20px rgba(0,0,0,0.3) !important;
#         backdrop-filter: blur(6px);
#       }}
#       [data-testid="stAlertInfo"]    {{ background: linear-gradient(145deg,{info_bg},#111a27)!important; }}
#       [data-testid="stAlertSuccess"] {{ background: linear-gradient(145deg,{success_bg},#0d2015)!important; }}
#       [data-testid="stAlertWarning"] {{ background: linear-gradient(145deg,{warn_bg},#201c0d)!important; }}
#       [data-testid="stAlertError"]   {{ background: linear-gradient(145deg,{error_bg},#1a0c0d)!important; }}
#       [data-testid^="stAlert"] * {{ opacity:1!important; color:{text}!important; }}

#       /* EMPHASIS / STEP NUMBERS */
#       strong,b,span[style*="font-weight:700"],
#       .stMarkdown ul li::marker, .stMarkdown ol li::marker {{
#         color:{accent}!important;
#         opacity:1!important;
#       }}

#       /* BUTTONS */
#       .stButton>button {{
#         background: linear-gradient(90deg,{accent},#3b82f6)!important;
#         color:#fff!important;
#         border:none!important;
#         border-radius:10px!important;
#         font-weight:600!important;
#         padding:0.5rem 1rem!important;
#         box-shadow:0 3px 10px rgba(0,0,0,0.3)!important;
#         transition:all 0.25s ease;
#       }}
#       .stButton>button:hover {{
#         transform:translateY(-2px)!important;
#         box-shadow:0 5px 20px rgba(96,165,250,0.4)!important;
#         filter:brightness(1.1)!important;
#       }}

#       /* INPUTS & SELECTBOXES */
#       .stTextInput>div>div>input,
#       .stSelectbox>div>div>div,
#       .stNumberInput input {{
#         background:{card}!important;
#         color:{text}!important;
#         border:1px solid {border}!important;
#         border-radius:6px!important;
#       }}
#       .stSelectbox label, .stNumberInput label {{
#         color:{subtext}!important;
#       }}

#       /* TABS */
#       .stTabs [data-baseweb="tab-list"] button {{
#         color:{text}!important;
#         background:{tab_bg}!important;
#         border:1px solid {border}!important;
#         border-radius:10px!important;
#         margin-right:4px!important;
#         font-weight:500!important;
#       }}
#       .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {{
#         background:{accent}!important;
#         color:#fff!important;
#         box-shadow:0 0 10px rgba(96,165,250,0.5)!important;
#         font-weight:700!important;
#       }}

#       /* METRIC CARDS */
#       [data-testid="stMetric"] {{
#         background:{card}!important;
#         border:1px solid {border}!important;
#         border-radius:10px!important;
#         color:{text}!important;
#         padding:10px 14px!important;
#         box-shadow:inset 0 0 6px rgba(255,255,255,0.03), 0 3px 10px rgba(0,0,0,0.4)!important;
#       }}
#       div[data-testid="stMetricLabel"],
#       div[data-testid="stMetricValue"] {{
#         color:{text}!important; opacity:1!important;
#       }}

#       /* DATAFRAMES / TABLES */
#       [data-testid="stDataFrame"] {{
#         background:{table_bg}!important;
#         color:{text}!important;
#         border:1px solid {border}!important;
#         border-radius:10px!important;
#         box-shadow:0 2px 15px rgba(0,0,0,0.3)!important;
#       }}
#       [data-testid="stDataFrame"] thead tr th {{
#         background:{table_head_bg}!important;
#         color:{table_head_tx}!important;
#         border-bottom:2px solid {accent}!important;
#         font-weight:700!important;
#       }}

#       /* DOWNLOAD BUTTONS */
#       a.stDownloadButton>button {{
#         background:{accent}!important;
#         color:white!important;
#         border-radius:8px!important;
#         font-weight:600!important;
#       }}
#       a.stDownloadButton>button:hover {{
#         filter:brightness(1.15)!important;
#         box-shadow:0 0 10px rgba(96,165,250,0.6)!important;
#       }}

#       /* DIVIDERS & LABELS */
#       hr,.stMarkdown hr {{ border-color:{border}!important; }}
#       label[for*="Select"], label[for*="TextInput"] {{ color:{subtext}!important; }}

#     </style>
#     """, unsafe_allow_html=True)


    


# def apply_theme(theme: str = "light"):
#     if theme == "light":
#         bg      = "#ffffff"
#         text    = "#0f172a"
#         subtext = "#334155"
#         card    = "#f8fafc"
#         border  = "#e2e8f0"
#         accent  = "#2563eb"
#         accent2 = "#22c55e"
#         tab_bg  = "#eef2ff"
#         table_bg= "#ffffff"
#         table_head_bg = "#e2e8f0"
#         table_head_tx = "#0f172a"
#     else:  # ğŸŒ™ dark mode â€” improved contrast
#         bg      = "#0b0f16"      # slightly lighter than before
#         text    = "#e2e8f0"      # main text bright
#         subtext = "#cbd5e1"      # labels and muted text still visible
#         card    = "#111827"      # panels/cards
#         border  = "#1e293b"      # border contrast
#         accent  = "#60a5fa"      # primary accent (bright blue)
#         accent2 = "#22c55e"      # success green
#         tab_bg  = "#1e293b"      # tab background
#         table_bg= "#111827"
#         table_head_bg = "#1f2937"
#         table_head_tx = "#f8fafc"

#     st.markdown(f"""
#     <style>
#       /* App bg + text */
#       .stApp {{
#         background-color: {bg} !important;
#         color: {text} !important;
#       }}

#       /* Text elements (force clarity) */
#       .stMarkdown, .stMarkdown p, .stMarkdown li, .stCaption, .st-emotion-cache-16idsys,
#       div[data-testid="stMetricLabel"], div[data-testid="stMetricDelta"], div[data-testid="stMetricValue"] {{
#         color: {text} !important;
#         opacity: 1 !important;
#       }}

#       /* Subtext / captions */
#       .st-emotion-cache-1v0mbdj, .stCaption, small {{
#         color: {subtext} !important;
#       }}

#       /* Buttons */
#       .stButton>button {{
#         background-color: {accent} !important;
#         color: white !important;
#         border-radius: 8px !important;
#         font-weight: 600 !important;
#         border: 1px solid {border} !important;
#       }}
#       .stButton>button:hover {{
#         filter: brightness(1.1);
#       }}

#       /* Tabs */
#       .stTabs [data-baseweb="tab-list"] button {{
#         color: {text} !important;
#         background: {tab_bg} !important;
#         border-radius: 10px !important;
#         margin-right: 4px !important;
#         border: 1px solid {border} !important;
#       }}
#       .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {{
#         background-color: {accent} !important;
#         color: #ffffff !important;
#       }}

#       /* DataFrames / Tables */
#       [data-testid="stDataFrame"] {{
#         background-color: {table_bg} !important;
#         color: {text} !important;
#         border-radius: 10px !important;
#         border: 1px solid {border} !important;
#         box-shadow: 0 4px 18px rgba(0,0,0,0.4) !important;
#       }}
#       [data-testid="stDataFrame"] thead tr th {{
#         background: {table_head_bg} !important;
#         color: {table_head_tx} !important;
#         font-weight: 700 !important;
#         border-bottom: 2px solid {accent} !important;
#       }}
#       [data-testid="stDataFrameCell"] {{
#         background-color: {table_bg} !important;
#         color: {text} !important;
#         border-color: {border} !important;
#       }}

#       /* Metrics (make them readable) */
#       [data-testid="stMetric"] {{
#         background-color: {card} !important;
#         padding: 8px 12px !important;
#         border-radius: 8px !important;
#         border: 1px solid {border} !important;
#       }}

#       /* Horizontal rules */
#       hr, .stMarkdown hr {{
#         border-color: {border} !important;
#       }}
#     </style>
#     """, unsafe_allow_html=True)


# #THEME SWITCHER

# def apply_theme(theme: str = "light"):
#     # Keep palette compact so it's easy to tune
#     if theme == "light":
#         bg      = "#ffffff"
#         text    = "#0f172a"
#         subtext = "#334155"
#         card    = "#f8fafc"
#         border  = "#e2e8f0"
#         accent  = "#2563eb"
#         accent2 = "#22c55e"
#         tab_bg  = "#eef2ff"
#         table_bg= "#ffffff"
#         table_head_bg = "#e2e8f0"
#         table_head_tx = "#0f172a"
#     else:  # dark
#         bg      = "#0E1117"
#         text    = "#f1f5f9"
#         subtext = "#93a4b8"
#         card    = "#0f172a"
#         border  = "#334155"
#         accent  = "#3b82f6"
#         accent2 = "#22c55e"
#         tab_bg  = "#111418"
#         table_bg= "#0f172a"
#         table_head_bg = "#1e293b"
#         table_head_tx = "#93c5fd"

#     st.markdown(f"""
#     <style>
#       /* App bg + text */
#       .stApp {{
#         background: {bg} !important;
#         color: {text} !important;
#       }}
#       .stCaption, .stMarkdown p, .stMarkdown li, .st-emotion-cache-16idsys {{
#         color: {subtext} !important;
#       }}

#       /* Buttons */
#       .stButton>button {{
#         background-color: {accent} !important;
#         color: white !important;
#         border-radius: 8px !important;
#         font-weight: 600 !important;
#         border: 1px solid {border} !important;
#       }}
#       .stButton>button:hover {{
#         filter: brightness(0.95);
#       }}

#       /* Tabs */
#       .stTabs [data-baseweb="tab-list"] button {{
#         color: {text} !important;
#         background: {tab_bg} !important;
#         border-radius: 10px !important;
#         margin-right: 4px !important;
#         border: 1px solid {border} !important;
#       }}
#       .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {{
#         background-color: {accent} !important;
#         color: #ffffff !important;
#       }}

#       /* Dataframe/Data Editor container */
#       [data-testid="stDataFrame"] {{
#         background-color: {table_bg} !important;
#         color: {text} !important;
#         border-radius: 10px !important;
#         border: 1px solid {border} !important;
#         box-shadow: 0 4px 18px rgba(0,0,0,0.2) !important;
#       }}
#       [data-testid="stDataFrame"] thead tr th {{
#         background: {table_head_bg} !important;
#         color: {table_head_tx} !important;
#         font-weight: 700 !important;
#         border-bottom: 2px solid {accent} !important;
#       }}
#       [data-testid="stDataFrameCell"]:not([data-testid="stDataFrameCellEditable"]) {{
#         background-color: {table_bg} !important;
#         color: {text} !important;
#         border-color: {border} !important;
#       }}
#       [data-testid="stDataFrameCellEditable"] textarea {{
#         background-color: {card} !important;
#         color: {text} !important;
#         border: 1px solid {border} !important;
#         border-radius: 6px !important;
#       }}
#       [data-testid="stDataFrameCellEditable"]:focus-within textarea,
#       [data-testid="stDataFrameCellEditable"]:hover textarea {{
#         border-color: {accent2} !important;
#         box-shadow: 0 0 0 2px rgba(34,197,94,0.35) !important;
#       }}

#       /* Horizontal rules */
#       hr, .stMarkdown hr {{
#         border-color: {border} !important;
#       }}
#     </style>
#     """, unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§­ THEME BOOTSTRAP â€” Default to DARK
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import streamlit as st

# Define default session theme early
if "ui_theme" not in st.session_state:
    st.session_state["ui_theme"] = "dark"

# Apply immediately before any Streamlit renders
apply_theme(st.session_state["ui_theme"])

# âœ… Then continue with Streamlit config
st.set_page_config(page_title="Asset Appraisal Agent", layout="wide")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PAGE CONFIG â€” must be the first Streamlit call
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(page_title="Asset Appraisal Agent", layout="wide")
ss = st.session_state

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SESSION DEFAULTS (idempotent)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _init_defaults():
    ss.setdefault("asset_logged_in", False)
    ss.setdefault("asset_stage", "login")   # login â†’ asset_flow
    ss.setdefault("asset_user", {"name": "Guest", "email": None})
    # Working tables/artifacts per our matrix (placeholders)
    ss.setdefault("asset_intake_df", None)
    ss.setdefault("asset_evidence_index", None)
    ss.setdefault("asset_anon_df", None)
    ss.setdefault("asset_features_df", None)
    ss.setdefault("asset_comps_used", None)
    ss.setdefault("asset_valued_df", None)
    ss.setdefault("asset_verified_df", None)
    ss.setdefault("asset_policy_df", None)
    ss.setdefault("asset_decision_df", None)
    ss.setdefault("asset_human_review_df", None)
    ss.setdefault("asset_feedback_csv", None)
    ss.setdefault("asset_trained_model_meta", None)
    ss.setdefault("asset_gpu_profile", None)  # will be set only in C.4
    os.makedirs("./.tmp_runs", exist_ok=True)

_init_defaults()

def render_nav_bar_app():
    st.markdown(
        "<div style='display:flex;gap:12px;align-items:center'>"
        "<a href='?stage=agents' class='macbtn'>ğŸ¤– Agents</a>"
        "<span style='opacity:.6'>/</span>"
        "<span>ğŸ›ï¸ Asset Appraisal Agent</span>"
        "</div>",
        unsafe_allow_html=True,
    )


# ---- Global runs dir (used everywhere) ----
RUNS_DIR = os.path.abspath("./.tmp_runs")
os.makedirs(RUNS_DIR, exist_ok=True)




# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # THEME INJECTION (Dark + Sidebar hide, with MutationObserver)
# # Run immediately on every script execution to avoid flicker on rerun
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# import streamlit.components.v1 as components # <--- IMPORT IS HERE
# # 1) CSS (global, persistent)
# # Note: Removed the inject_dark_theme_once() function and session state check
# st.markdown("""
#     <style>
#       :root { color-scheme: dark; } /* Hint built-ins */
#       html, body, .stApp {
#         background-color:#0f172a !important;
#         color:#e5e7eb !important;
#         font-family: 'Inter', system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
#       }
#       /* Sidebar off + container padding */
#       [data-testid="stSidebar"], section[data-testid="stSidebar"], nav[data-testid="stSidebarNav"] { display:none !important; }
#       [data-testid="stAppViewContainer"] { margin-left:0 !important; padding-left:0 !important; }
#       /* Headings */
#       h1, h2, h3, h4, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 { color:#e5e7eb !important; }
#       /* Tabs */
#       .stTabs [data-baseweb="tab-list"] { gap:6px; border-bottom:1px solid #1f2937; }
#       .stTabs [data-baseweb="tab"] {
#         background:#0b1222; border:1px solid #1f2937; border-bottom:none;
#         padding:10px 14px; border-top-left-radius:10px; border-top-right-radius:10px; color:#cbd5e1;
#       }
#       .stTabs [aria-selected="true"] { background:#111827 !important; color:#e5e7eb !important; }
#       /* Inputs */
#       .stTextInput input, .stNumberInput input, .stSelectbox [data-baseweb="select"] > div {
#         background:#0b1222 !important; color:#e5e7eb !important; border:1px solid #1f2937 !important;
#       }
#       /* Buttons */
#       .stButton button {
#         background:linear-gradient(180deg,#1f3b57 0%,#0e1f33 100%) !important;
#         color:#e6f3ff !important; border:1px solid #1d2b3a !important; border-radius:10px !important;
#         box-shadow:0 0 10px rgba(56,189,248,.15);
#       }
#       .stButton button:hover { filter:brightness(1.05); box-shadow:0 0 16px rgba(56,189,248,.25); }
#       /* Metrics */
#       div[data-testid="stMetric"] { background:#0b1222; border:1px solid #1f2937; border-radius:12px; padding:10px 12px; }
#       div[data-testid="stMetricValue"] { color:#38bdf8 !important; }
#       /* Tables */
#       .stDataFrame, .stTable { background:#0b1222 !important; border:1px solid #1f2937 !important; border-radius:10px !important; }
#       /* Expanders */
#       details { background:#0b1222 !important; border:1px solid #1f2937 !important; border-radius:10px !important; padding:6px 10px !important; }
#       /* Plotly */
#       .js-plotly-plot .plotly .main-svg { background-color:transparent !important; }
#     </style>
#     """, unsafe_allow_html=True)

# # 2) JS observer to re-assert dark after Streamlit mutates
# components.html("""
#     <script>
#       (function() {
#         const apply = () => {
#           try {
#             const root = parent.document.documentElement;
#             const app = parent.document.querySelector('.stApp');
#             if (root) root.style.setProperty('color-scheme','dark');
#             if (app) app.classList.add('dark-hold');
#           } catch(e) {}
#         };
#         apply();
#         const mo = new MutationObserver(apply);
#         mo.observe(parent.document.documentElement, {childList:true, subtree:true});
#       })();
#     </script>
#     """, height=0)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API CONFIG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

API_URL = os.getenv("API_URL", "http://localhost:8090")

# Default fallbacks (will be superseded by discovery)

ASSET_AGENT_IDS = [a.strip() for a in os.getenv("ASSET_AGENT_IDS", "asset_appraisal,asset").split(",") if a.strip()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NAV (reliable jump to Home / Agents from a page)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _set_query_params_safe(**kwargs):
    # New API (Streamlit â‰¥1.40)
    try:
        for k, v in kwargs.items():
            st.query_params[k] = v
        return True
    except Exception:
        pass
    # Older versions
    try:
        st.experimental_set_query_params(**kwargs)
        return True
    except Exception:
        return False

def _go_stage(target_stage: str):
    # 1) let app.pyâ€™s router know what to show
    st.session_state["stage"] = target_stage

    # 2) preferred: jump to main app file
    try:
        # path is relative to the run root when you launch:
        #   streamlit run services/ui/app.py
        st.switch_page("app.py")
        return
    except Exception:
        pass

    # 3) fallback: set query param and rerun so app.py picks it up
    _set_query_params_safe(stage=target_stage)
    st.rerun()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UTILITIES â€” DataFrame selection helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---- DataFrame selection helpers (avoid boolean ambiguity) ----
def first_nonempty_df(*candidates):
    """Return the first candidate that is a non-empty pandas DataFrame, else None."""
    for df in candidates:
        if isinstance(df, pd.DataFrame) and not df.empty:
            return df
    return None

def is_nonempty_df(x) -> bool:
    return isinstance(x, pd.DataFrame) and not x.empty

def render_nav_bar_app():
    stage = st.session_state.get("stage", "landing")

    # three columns: home, agents, theme toggle
    c1, c2, c3 = st.columns([1, 1, 2.5])

    with c1:
        if st.button("ğŸ  Back to Home", key=f"btn_home_{stage}"):
            _go_stage("landing")
            st.stop()

    with c2:
        if st.button("ğŸ¤– Back to Agents", key=f"btn_agents_{stage}"):
            _go_stage("agents")
            st.stop()

    with c3:
        is_dark = (ss.get("ui_theme", "dark") == "dark")
        new_is_dark = st.toggle("ğŸŒ™ Dark mode", value=is_dark, key="ui_theme_toggle", help="Switch theme")
        new_theme = "dark" if new_is_dark else "light"
        if new_theme != ss["ui_theme"]:
            ss["ui_theme"] = new_theme
            apply_theme(ss["ui_theme"])
            st.rerun()

    st.markdown("---")




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GEO UTILITIES: EXIF GPS, Geocode, Geohash   â† PASTE START
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from typing import Optional, Tuple

def _exif_to_degrees(value):
    try:
        d = float(value[0][0]) / float(value[0][1])
        m = float(value[1][0]) / float(value[1][1])
        s = float(value[2][0]) / float(value[2][1])
        return d + (m / 60.0) + (s / 3600.0)
    except Exception:
        return None

def extract_gps_from_image(path: str) -> Optional[Tuple[float, float]]:
    try:
        from PIL import Image
        from PIL.ExifTags import TAGS, GPSTAGS
        img = Image.open(path)
        exif = img._getexif() or {}
        tagged = {TAGS.get(k, k): v for k, v in exif.items()}
        gps_info = tagged.get("GPSInfo")
        if not gps_info:
            return None
        gps_data = {GPSTAGS.get(k, k): v for k, v in gps_info.items()}
        lat = _exif_to_degrees(gps_data.get("GPSLatitude"))
        lon = _exif_to_degrees(gps_data.get("GPSLongitude"))
        if lat is None or lon is None:
            return None
        lat_ref = gps_data.get("GPSLatitudeRef", "N")
        lon_ref = gps_data.get("GPSLongitudeRef", "E")
        if lat_ref == "S": lat = -lat
        if lon_ref == "W": lon = -lon
        return (lat, lon)
    except Exception:
        return None

_GEOCODE_CACHE_PATH = "./.tmp_runs/geocode_cache.json"

def _load_geocode_cache():
    try:
        with open(_GEOCODE_CACHE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _save_geocode_cache(cache: dict):
    os.makedirs("./.tmp_runs", exist_ok=True)
    with open(_GEOCODE_CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def geocode_freeform(addr: str) -> Optional[Tuple[float, float]]:
    """Nominatim via geopy; cached locally. Returns None if offline."""
    try:
        cache = _load_geocode_cache()
        key = addr.strip().lower()
        if key in cache:
            v = cache[key]
            return (v["lat"], v["lon"])
        from geopy.geocoders import Nominatim
        geolocator = Nominatim(user_agent="asset-appraisal-agent")
        loc = geolocator.geocode(addr, timeout=10)
        if not loc:
            return None
        cache[key] = {"lat": float(loc.latitude), "lon": float(loc.longitude)}
        _save_geocode_cache(cache)
        return (float(loc.latitude), float(loc.longitude))
    except Exception:
        return None

def geohash_decode(s: str) -> Optional[Tuple[float, float]]:
    try:
        import geohash  # pip install python-geohash
        lat, lon = geohash.decode(s)
        return (float(lat), float(lon))
    except Exception:
        return None



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SESSION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ss = st.session_state
# Theme default
ss.setdefault("ui_theme", "dark")   # "dark" or "light"
ss.setdefault("asset_stage", "login")
ss.setdefault("asset_logged_in", False)
ss.setdefault("asset_user", None)

# Stage caches
ss.setdefault("asset_raw_df", None)     # Stage 1 raw (after CSV/manual merge)
ss.setdefault("asset_evidence", [])     # evidence filenames (images/pdfs)
ss.setdefault("asset_anon_df", None)    # Stage 2 anonymized
ss.setdefault("asset_stage2_df", None)  # Stage 3 input (resolved source)
ss.setdefault("asset_ai_df", None)      # Stage 3 AI output
ss.setdefault("asset_selected_model", None)  # trained model path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HELPERS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def anonymize_text_cols(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for col in out.columns:
        if out[col].dtype == "object":
            out[col] = (
                out[col].astype(str)
                .apply(lambda x: re.sub(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+", "[EMAIL]", x))
            )
    return out

def quick_synth(rows: int = 150) -> pd.DataFrame:
    """Generate asset rows + finance metrics for demo/backup."""
    rng = np.random.default_rng(42)
    cities = [
        ("Hanoi", 21.0285, 105.8542),
        ("HCMC", 10.7769, 106.7009),
        ("Da Nang", 16.0544, 108.2022),
        ("Hue", 16.4637, 107.5909),
        ("Can Tho", 10.0452, 105.7469),
    ]
    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, rows + 1)],
        "asset_id": [f"A{i:04d}" for i in range(1, rows + 1)],
        "asset_type": rng.choice(["House","Apartment","Car","Land","Factory"], rows),
        "age_years": rng.integers(1, 40, rows),
        "market_value": rng.integers(50_000, 2_000_000, rows),
        "condition_score": rng.uniform(0.6, 1.0, rows),
        "legal_penalty": rng.uniform(0.95, 1.0, rows),          # legal/title risk adj
        "employment_years": rng.integers(0, 30, rows),
        "credit_history_years": rng.integers(0, 25, rows),
        "delinquencies": rng.integers(0, 6, rows),
        "current_loans": rng.integers(0, 8, rows),
        "loan_amount": rng.integers(10_000, 200_000, rows),
        "customer_type": rng.choice(["bank","non-bank"], rows, p=[0.7,0.3]),
    })
    cdf = pd.DataFrame(cities, columns=["city","lat","lon"])
    df["city"] = rng.choice(cdf["city"], rows)
    df = df.merge(cdf, on="city", how="left")
    df["depreciation_rate"] = (1 - df["condition_score"]) * 100
    df["market_segment"] = np.where(df["market_value"] > 500_000, "High", "Mass")
    df["DTI"] = rng.uniform(0.05, 0.9, rows)
    df["LTV"] = np.clip(df["loan_amount"] / np.maximum(df["market_value"], 1), 0.05, 1.5)
    df["evidence_files"] = [[] for _ in range(rows)]
    return df

def synth_why_table() -> pd.DataFrame:
    return pd.DataFrame([
        {"Metric": "DTI", "Why": "Debt service relative to income â€” proxy for payability."},
        {"Metric": "LTV", "Why": "Loan vs asset value â€” proxy for collateral adequacy."},
        {"Metric": "condition_score", "Why": "Asset physical state impacts fair value/depreciation."},
        {"Metric": "legal_penalty", "Why": "Legal/title flags reduce realizable value."},
        {"Metric": "employment_years / credit_history_years", "Why": "Stability/track record."},
        {"Metric": "delinquencies / current_loans", "Why": "Current risk pressure."},
        {"Metric": "market_segment / city / lat,lon", "Why": "Market & location effects on pricing."},
    ])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DATAFRAME SELECTION (avoid boolean ambiguity)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def first_nonempty_df(*candidates):
    """Return the first candidate that is a non-empty pandas DataFrame, else None."""
    for df in candidates:
        if isinstance(df, pd.DataFrame) and not df.empty:
            return df
    return None



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UNIVERSAL INGEST + NORMALIZATION HELPERS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _slug(name: str) -> str:
    return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

def _ts() -> str:
    return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

def _read_any_table(uploaded_file) -> pd.DataFrame:
    """
    Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback.
    Accepts Streamlit UploadedFile or a file-like object.
    """
    name = getattr(uploaded_file, "name", "").lower()

    # Excel first
    if name.endswith((".xlsx", ".xls")):
        return pd.read_excel(uploaded_file)

    # Text (CSV/TSV/TXT): try utf-8, then latin-1; sniff delimiter.
    raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
    for enc in ("utf-8", "latin-1"):
        try:
            text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
            lines = text.splitlines()
            sample = "\n".join(lines[:5]) if lines else ""
            try:
                dialect = csv.Sniffer().sniff(sample) if sample else csv.excel
                sep = getattr(dialect, "delimiter", ",")
            except Exception:
                sep = ","
            return pd.read_csv(io.StringIO(text), sep=sep)
        except Exception:
            continue
    # last resort
    return pd.read_csv(io.BytesIO(raw), engine="python")

def _normalize_for_agents(df: pd.DataFrame) -> pd.DataFrame:
    """
    Light normalization for credit/asset agents.
    Creates a consistent thin schema if columns exist; leaves extras intact.
    """
    out = df.copy()

    # alias map (extend freely)
    aliases = {
        "application_id": ["application_id", "app_id", "loan_id", "id", "request_id"],
        "asset_id":       ["asset_id", "property_id", "house_id", "assetid"],
        "asset_type":     ["asset_type", "type", "category"],
        "address":        ["address", "addr", "street", "location"],
        "city":           ["city", "town"],
        "state":          ["state", "province", "region"],
        "country":        ["country"],
        "price":          ["price", "value", "market_value", "listing_price", "sale_price"],
        "bedrooms":       ["bedrooms", "beds"],
        "bathrooms":      ["bathrooms", "baths"],
        "parking_space":  ["parking_space", "parking", "garage"],
        "title":          ["title", "name"],
    }

    # rename by first matching alias
    rename_map = {}
    cols = set(out.columns)
    for target, cands in aliases.items():
        for c in cands:
            if c in cols:
                rename_map[c] = target
                break
    out = out.rename(columns=rename_map)

    # ensure presence of common columns
    required = ["application_id", "asset_id", "asset_type", "address", "city", "state", "price"]
    for col in required:
        if col not in out.columns:
            out[col] = None

    # numeric coercions
    for col in ("price", "bedrooms", "bathrooms", "parking_space"):
        if col in out.columns:
            out[col] = pd.to_numeric(out[col], errors="coerce")

    # provenance
    out["source_dataset"] = st.session_state.get("asset_intake_source_name", "uploaded")
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# THEME SYSTEM (Light/Dark CSS + map style)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from textwrap import dedent

THEME_VARS = {
    "light": {
        "bg": "#FFFFFF",
        "panel": "#F8FAFC",
        "text": "#0F172A",
        "muted": "#475569",
        "primary": "#2563EB",
        "success": "#16A34A",
        "warn": "#D97706",
        "danger": "#DC2626",
        "accent": "#0EA5E9",
        "stripe": "#F1F5F9",
        "shadow": "0 6px 24px rgba(15,23,42,0.08)",
    },
    "dark": {
        "bg": "#0B1020",
        "panel": "#101727",
        "text": "#E5E7EB",
        "muted": "#94A3B8",
        "primary": "#60A5FA",
        "success": "#22C55E",
        "warn": "#FBBF24",
        "danger": "#F87171",
        "accent": "#38BDF8",
        "stripe": "#111827",
        "shadow": "0 8px 30px rgba(0,0,0,0.35)",
    },
}

def _theme_css(theme: str) -> str:
    t = THEME_VARS[theme]
    return dedent(f"""
    <style>
      /* Fonts: Inter + JetBrains Mono */
      @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=JetBrains+Mono:wght@400;600&display=swap');

      :root {{
        --bg: {t['bg']};
        --panel: {t['panel']};
        --text: {t['text']};
        --muted: {t['muted']};
        --primary: {t['primary']};
        --success: {t['success']};
        --warn: {t['warn']};
        --danger: {t['danger']};
        --accent: {t['accent']};
        --stripe: {t['stripe']};
        --shadow: {t['shadow']};
        --radius: 14px;
      }}

      html, body, .stApp {{
        background: var(--bg) !important;
        color: var(--text) !important;
        font-family: 'Inter', system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif !important;
      }}

      /* Panel-like boxes (use .left-box/.right-box or your custom containers) */
      .left-box, .right-box, .stExpander, .stTabs [data-baseweb="tab-highlight"] {{
        background: var(--panel) !important;
        border-radius: var(--radius);
        box-shadow: var(--shadow);
      }}

      /* Headings */
      h1, h2, h3, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 {{
        color: var(--text) !important;
        font-weight: 700;
        letter-spacing: -0.01em;
      }}

      /* Buttons */
      .stButton>button, button[kind="primary"] {{
        background: var(--primary) !important;
        color: #fff !important;
        border-radius: 12px !important;
        border: none !important;
        box-shadow: 0 3px 12px rgba(37,99,235,0.35) !important;
      }}
      .stDownloadButton button {{
        background: var(--success) !important;
        color: #fff !important;
        border-radius: 12px !important;
        border: none !important;
        box-shadow: 0 3px 12px rgba(34,197,94,0.35) !important;
      }}

      /* Tables (dataframe) */
      .stDataFrame thead tr th {{
        background: var(--panel) !important;
        color: var(--muted) !important;
        font-weight: 600 !important;
      }}
      .stDataFrame tbody tr:nth-child(odd) {{
        background: var(--stripe) !important;
      }}

      /* Chips / small badges */
      .chip {{
        display:inline-block; padding:4px 10px; border-radius:999px;
        background: var(--panel); color: var(--muted); border:1px solid rgba(148,163,184,0.35);
      }}

      /* Code font */
      code, pre, .stCodeBlock, .st-emotion-cache-ffhzg2 {{
        font-family: 'JetBrains Mono', ui-monospace, SFMono-Regular, Menlo, Consolas, monospace !important;
      }}
      
      /* NEW  Optional: hide sidebar without touching theme */
      [data-testid="stSidebar"], section[data-testid="stSidebar"], nav[data-testid="stSidebarNav"] {{
        display: none !important;
      }}
      [data-testid="stAppViewContainer"] {{
        margin-left: 0 !important;
        padding-left: 0 !important;
      }}
      
      
    </style>
    """)

def apply_theme(theme: str = None):
    """Inject CSS theme; defaults to session theme."""
    theme = theme or st.session_state.get("ui_theme", "light")
    if theme not in THEME_VARS:
        theme = "light"
    st.markdown(_theme_css(theme), unsafe_allow_html=True)



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAP THEME HELPERS (Mapbox style + token + adapters)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import streamlit as st

def get_mapbox_token() -> str | None:
    """Find a Mapbox token from secrets or env. Return None if not set."""
    try:
        tok = st.secrets.get("MAPBOX_TOKEN")
    except Exception:
        tok = None
    if not tok:
        tok = os.environ.get("MAPBOX_TOKEN") or os.environ.get("MAPBOX_ACCESS_TOKEN")
    return tok or None


def plotly_map_style() -> str:
    """
    Return a Plotly-compatible map style.
    - Uses bright style in light mode even without a Mapbox token.
    - Falls back to CARTO 'positron' if Mapbox token not set.
    """
    theme = st.session_state.get("ui_theme", "light")
    token = get_mapbox_token()
    if token:
        return "light" if theme == "light" else "dark"
    else:
        # fallback to open CARTO tiles (bright)
        return "carto-positron" if theme == "light" else "carto-darkmatter"


def get_map_style() -> str:
    """
    Return a Mapbox style URL (for pydeck only).
    Light â†’ bright, Dark â†’ dark. Defaults to light for safety.
    """
    theme = st.session_state.get("ui_theme", "light")
    return "mapbox://styles/mapbox/light-v11" if theme == "light" else "mapbox://styles/mapbox/dark-v11"


def apply_plotly_mapbox_defaults():
    """Set Plotly's Mapbox token globally (if available)."""
    import plotly.express as px
    token = get_mapbox_token()
    if token:
        px.set_mapbox_access_token(token)
    else:
        st.info("â„¹ï¸ Mapbox token not set â€” using free bright map style (carto-positron).")


def make_pydeck_view_state(lat=10.7769, lon=106.7009, zoom=10, pitch=0, bearing=0):
    import pydeck as pdk
    return pdk.ViewState(latitude=lat, longitude=lon, zoom=zoom, pitch=pitch, bearing=bearing)


def pydeck_map_style() -> str:
    """pydeck uses the same Mapbox style URLs when a token is available."""
    return get_map_style()


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # MAP THEME HELPERS (Mapbox style + token + adapters)
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# import os

# def get_mapbox_token() -> str | None:
#     """Find a Mapbox token from secrets or env. Return None if not set."""
#     # Prefer Streamlit secrets if present
#     tok = None
#     try:
#         tok = st.secrets.get("MAPBOX_TOKEN")  # type: ignore[attr-defined]
#     except Exception:
#         pass
#     if not tok:
#         tok = os.environ.get("MAPBOX_TOKEN") or os.environ.get("MAPBOX_ACCESS_TOKEN")
#     return tok or None

# def get_map_style() -> str:
#     """
#     Return a Mapbox style URL matched to current theme.
#     Light â†’ bright, Dark â†’ dark. Defaults to light for safety.
#     """
#     theme = st.session_state.get("ui_theme", "light")
#     return "mapbox://styles/mapbox/light-v11" if theme == "light" else "mapbox://styles/mapbox/dark-v11"

# # Optional: convenience adapters for Plotly + pydeck
# def apply_plotly_mapbox_defaults():
#     """
#     Set Plotly's Mapbox token globally. Call once before creating px/scatter_mapbox etc.
#     """
#     import plotly.express as px  # local import to avoid hard dependency at import time
#     token = get_mapbox_token()
#     if token:
#         px.set_mapbox_access_token(token)
#     else:
#         st.info("â„¹ï¸ Mapbox token not set. Set MAPBOX_TOKEN in env or st.secrets to enable styled basemaps.")

# def make_pydeck_view_state(lat=10.7769, lon=106.7009, zoom=10, pitch=0, bearing=0):
#     import pydeck as pdk
#     return pdk.ViewState(latitude=lat, longitude=lon, zoom=zoom, pitch=pitch, bearing=bearing)

# def pydeck_map_style() -> str:
#     """
#     pydeck uses the same Mapbox style URLs when a token is available.
#     """
#     return get_map_style()



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AGENT DISCOVERY & PROBE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _safe_get_json(url: str, timeout: int = 8):
    try:
        r = requests.get(url, timeout=timeout)
        if r.ok:
            try:
                return True, r.json()
            except Exception as e:
                return False, f"parse error: {e}\nBody:\n{r.text[:2000]}"
        return False, f"{r.status_code} {r.reason}\nBody:\n{r.text[:2000]}"
    except Exception as e:
        return False, f"request error: {e}"

def discover_asset_agents() -> list[str]:
    """Try common discovery endpoints and extract agent ids. Cache in session."""
    cached = st.session_state.get("asset_agent_ids")
    if isinstance(cached, list) and cached:
        return cached

    candidates = []

    # 1) /v1/agents (prefer)
    ok, data = _safe_get_json(f"{API_URL}/v1/agents")
    if ok:
        try:
            if isinstance(data, dict) and "agents" in data:
                items = data["agents"]
                if isinstance(items, list):
                    for it in items:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name") or it.get("agent") or it.get("slug")
                            if aid: candidates.append(aid)
            elif isinstance(data, list):
                for it in data:
                    if isinstance(it, str):
                        candidates.append(it)
                    elif isinstance(it, dict):
                        aid = it.get("id") or it.get("name")
                        if aid: candidates.append(aid)
        except Exception:
            pass

    # 2) /v1/agents/list (alt)
    if not candidates:
        ok2, data2 = _safe_get_json(f"{API_URL}/v1/agents/list")
        if ok2:
            try:
                if isinstance(data2, dict):
                    for k in ("agents", "data", "items"):
                        if k in data2 and isinstance(data2[k], list):
                            for it in data2[k]:
                                if isinstance(it, str):
                                    candidates.append(it)
                                elif isinstance(it, dict):
                                    aid = it.get("id") or it.get("name")
                                    if aid: candidates.append(aid)
                elif isinstance(data2, list):
                    for it in data2:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name")
                            if aid: candidates.append(aid)
            except Exception:
                pass

    # 3) /v1/health (sometimes lists agents)
    if not candidates:
        ok3, data3 = _safe_get_json(f"{API_URL}/v1/health")
        if ok3 and isinstance(data3, dict):
            for k in ("agents", "services", "available_agents"):
                val = data3.get(k)
                if isinstance(val, list):
                    for it in val:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name")
                            if aid: candidates.append(aid)

    discovered = [c for c in dict.fromkeys(candidates) if c]  # de-dupe
    if not discovered:
        discovered = ASSET_AGENT_IDS[:]  # fallback to env/defaults

    st.session_state["asset_agent_ids"] = discovered
    return discovered

def probe_api() -> dict:
    """Collect quick diagnostics for UI."""
    diag = {}
    for path in ("/v1/health", "/v1/agents", "/v1/agents/list"):
        ok, data = _safe_get_json(f"{API_URL}{path}")
        diag[path] = data if ok else {"error": data}
    diag["API_URL"] = API_URL
    diag["discovered_agents"] = discover_asset_agents()
    return diag

# NEW: run_id extractor for various API payload shapes
def _extract_run_id(obj) -> str | None:
    """Find a run_id in a nested dict/list API response."""
    if isinstance(obj, dict):
        rid = obj.get("run_id")
        if isinstance(rid, str) and rid:
            return rid
        for k in ("data", "meta", "result", "payload"):
            v = obj.get(k)
            if isinstance(v, dict):
                rid = v.get("run_id")
                if isinstance(rid, str) and rid:
                    return rid
    elif isinstance(obj, list):
        for it in obj:
            rid = _extract_run_id(it)
            if rid:
                return rid
    return None

def try_run_asset_agent(csv_bytes: bytes, form_fields: dict, timeout_sec: int = 180):
    """
    Discover agent ids, then try each. Rebuild multipart for each attempt.
    Preferred: use run_id to GET merged CSV and DataFrame it.
    Fallback: normalize 'result' only (not whole JSON).

    Returns (ok: bool, DataFrame | error_string)
    """
    agent_ids = discover_asset_agents()
    errors = []
    for agent_id in agent_ids:
        files = {"file": ("asset_verified.csv", io.BytesIO(csv_bytes), "text/csv")}
        url = f"{API_URL}/v1/agents/{agent_id}/run"
        try:
            resp = requests.post(url, files=files, data=form_fields, timeout=timeout_sec)
        except Exception as e:
            errors.append(f"[{agent_id}] request error: {e}")
            continue

        if resp.ok:
            body_text = resp.text[:4000]
            try:
                payload = resp.json()
            except Exception as e:
                errors.append(f"[{agent_id}] parse error: {e}\nBody:\n{body_text}")
                continue

            rid = _extract_run_id(payload)
            if rid:
                # Preferred: fetch merged CSV
                try:
                    r_csv = requests.get(f"{API_URL}/v1/runs/{rid}/report?format=csv", timeout=60)
                    if r_csv.ok:
                        df = pd.read_csv(io.BytesIO(r_csv.content))
                        st.session_state["asset_last_run_id"] = rid
                        st.session_state["asset_last_runner"] = ((payload.get("meta") or {}).get("runner_used"))
                        return True, df
                    else:
                        errors.append(
                            f"[{agent_id}] report GET {r_csv.status_code} {r_csv.reason} for run_id={rid}\n"
                            f"Body:\n{r_csv.text[:2000]}"
                        )
                except Exception as e:
                    errors.append(f"[{agent_id}] report GET error for run_id={rid}: {e}")

            # Fallback: try to render just 'result'
            result_part = payload.get("result")
            if isinstance(result_part, list):
                try:
                    df = pd.json_normalize(result_part)
                    return True, df
                except Exception as e:
                    errors.append(f"[{agent_id}] fallback normalize error: {e}\nBody:\n{body_text}")
            elif isinstance(result_part, dict):
                try:
                    df = pd.json_normalize(result_part)
                    return True, df
                except Exception as e:
                    errors.append(f"[{agent_id}] fallback normalize error: {e}\nBody:\n{body_text}")
            else:
                errors.append(f"[{agent_id}] no run_id and empty/unknown 'result'.\nBody:\n{body_text}")
        else:
            errors.append(f"[{agent_id}] {resp.status_code} {resp.reason}\nBody:\n{resp.text[:2000]}")

    return False, "All agent attempts failed (discovered=" + ", ".join(agent_ids) + "):\n" + "\n\n".join(errors)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LOGIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ss["asset_stage"] == "login" and not ss["asset_logged_in"]:
    render_nav_bar_app()
    st.title("ğŸ” Login to AI Asset Appraisal Platform")
    c1, c2, c3 = st.columns([1,1,1])
    with c1:
        user = st.text_input("Username", placeholder="e.g. dzoan")
    with c2:
        email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
    with c3:
        pwd = st.text_input("Password", type="password", placeholder="Enter any password")
    if st.button("Login", key="btn_asset_login", use_container_width=True):
        if (user or "").strip() and (email or "").strip():
            ss["asset_user"] = {
                "name": user.strip(),
                "email": email.strip(),
                "timestamp": datetime.now(timezone.utc).isoformat(),  # âœ… fixed
            }
            ss["asset_logged_in"] = True
            ss["asset_stage"] = "asset_flow"
            st.rerun()
        else:
            st.error("âš ï¸ Please fill all fields before continuing.")
    st.stop()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WORKFLOW (Aâ†’G)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ss.get("asset_logged_in") and ss.get("asset_stage") in ("asset_flow", "asset_agent"):
    render_nav_bar_app()
    st.title("ğŸ›ï¸ Asset Appraisal Agent")
    st.caption(
        "Aâ†’G pipeline â€” Intake â†’ Privacy â†’ Valuation â†’ Policy â†’ Human Review â†’ Model Training â†’ Reporting "
        f"| ğŸ‘‹ {ss['asset_user']['name']}"
    )


    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TABS (A..G) â€” Live tabs
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tabA, tabB, tabC, tabD, tabE, tabF, tabG, tabH = st.tabs([
    "ğŸŸ¦ A) Intake & Evidence",
    "ğŸŸ© B) Privacy & Features",
    "ğŸŸ¨ C) Valuation & Verification",
    "ğŸŸ§ D) Policy & Decision",
    "ğŸŸª E) Human Review & Feedback",
    "ğŸŸ« F) Model Training & Promotion",
    "ğŸŸ« G) Deployment & Export ğŸš€",     # âœ… corrected label (was double F)
    "â¬œ H) Reporting & Handoff ğŸ§¾"      # âœ… make this Stage H
    ])


    # Runtime tip
    st.caption(
        "ğŸ“˜ Tip: Move sequentially from Aâ†’G or revisit individual stages. "
        "If a stage reports missing data, rerun the previous one or load demo data."
    )

else:
    st.warning("Please log in first to access the Asset Appraisal workflow.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸŸ¦ STAGE A â€” INTAKE & EVIDENCE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tabA:
    import io, os, json, hashlib, pandas as pd
    from datetime import datetime, timezone
    
    
    ss = st.session_state  # âœ… make 'ss' available in this scope
    st.subheader("A. Intake & Evidence")
    st.caption("Steps: (1) Upload / Import, (2) Normalize, (3) Generate unified intake CSV")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ“˜ Quick User Guide (updated)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ“˜ Quick User Guide", expanded=False):
        st.markdown("""
        **Goal:** Collect, normalize, and unify all asset-related data before appraisal.

        **1ï¸âƒ£ Upload Your Data**
        - Upload **field agent reports**, **loan lists with collateral**, and **legal property documents**.
        - Supported: `.csv`, `.xlsx`, `.zip` (evidence images/docs).

        **2ï¸âƒ£ Import Open Data**
        - Search **Kaggle** or **Hugging Face** for relevant valuation datasets.
        - You can mix public + internal uploads â€” AI will normalize columns.

        **3ï¸âƒ£ Normalize**
        - After upload/import, click **"Normalize Data"** to merge and standardize features.
        - Output: `intake_table.csv` ready for Stage B (Anonymization).

        **4ï¸âƒ£ Generate Synthetic Data**
        - If no input data is available, the AI can synthesize a demo dataset representing:
          `asset_id, asset_type, city, market_value, loan_amount, legal_source, condition_score`.

        **5ï¸âƒ£ Output**
        - A unified CSV file is produced â†’ download or proceed directly to **Stage B**.
        """)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # (A.1) UPLOAD ZONE â€” Human Inputs
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ“¤ Upload Data Files (Field Agents / Loans / Legal Docs)")
    uploaded_files = st.file_uploader(
        "Upload multiple files",
        type=["csv", "xlsx", "zip"],
        accept_multiple_files=True,
        key="asset_upload_files"
    )

    uploaded_dfs = []
    if uploaded_files:
        for f in uploaded_files:
            try:
                if f.name.endswith(".csv"):
                    df = pd.read_csv(f)
                elif f.name.endswith(".xlsx"):
                    df = pd.read_excel(f)
                else:
                    st.info(f"ğŸ“¦ Skipping non-tabular file: {f.name}")
                    continue
                st.success(f"âœ… Loaded `{f.name}` ({len(df)} rows, {len(df.columns)} cols)")
                uploaded_dfs.append(df)
            except Exception as e:
                st.error(f"âŒ Failed to read {f.name}: {e}")


    # â”€â”€New  global runs dir (shared across stages) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    RUNS_DIR = os.path.abspath("./.tmp_runs")
    os.makedirs(RUNS_DIR, exist_ok=True)
    
   

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # (A.2) PUBLIC DATASETS â€” Kaggle / HF / OpenML
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸŒ Import Public Datasets (Kaggle / Hugging Face / OpenML / Portals)")

    # keep a place to persist search results across reruns
    ss.setdefault("kaggle_search_df", pd.DataFrame())

    src = st.selectbox(
        "Select source",
        ["Kaggle (API)", "Hugging Face", "OpenML", "Public Domain Portals"],
        key="asset_pubsrc"
    )
    query = st.text_input("Search keywords", "house prices real estate valuation", key="asset_pubquery")

    # helpers
    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    
    def _safe_read_csv(fp: str) -> pd.DataFrame:
        """
        Robust CSV reader:
        - Tries multiple encodings (utf-8, utf-8-sig, cp1252, latin-1)
        - Tries common separators
        - Skips bad rows rather than crashing
        """
        encodings = ["utf-8", "utf-8-sig", "cp1252", "latin-1"]
        seps = [",", ";", "\t", "|"]

        last_err = None
        for enc in encodings:
            for sep in seps:
                try:
                    df_try = pd.read_csv(
                        fp,
                        encoding=enc,
                        sep=sep,
                        engine="python",
                        on_bad_lines="skip",   # pandas >=1.3
                    )
                    # Require at least 2 columns to consider it valid
                    if df_try.shape[1] >= 2:
                        return df_try
                except Exception as e:
                    last_err = e
                    continue

        # Final fallback: read bytes, decode with latin-1 replacement, then parse in-memory
        try:
            with open(fp, "rb") as f:
                raw = f.read()
            text = raw.decode("latin-1", errors="replace")
            for sep in seps:
                try:
                    return pd.read_csv(
                        io.StringIO(text),
                        sep=sep,
                        engine="python",
                        on_bad_lines="skip",
                    )
                except Exception:
                    pass
        except Exception as e:
            last_err = e

        raise RuntimeError(f"Could not parse CSV with common encodings/separators. Last error: {last_err}")

    


    # Kaggle search
    if st.button("ğŸ” Search dataset", key="btn_asset_pubsearch"):
        with st.spinner("Searching datasets..."):
            try:
                if src == "Kaggle (API)":
                    import subprocess, io
                    cmd = ["kaggle", "datasets", "list", "-s", query, "-v"]  # -v => CSV output
                    out = subprocess.run(cmd, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                    if out.returncode != 0:
                        st.error(f"Kaggle CLI failed: {out.stderr.strip() or out.stdout.strip()}")
                        st.info("ğŸ’¡ Ensure ~/.kaggle/kaggle.json exists and has chmod 600.")
                        ss["kaggle_search_df"] = pd.DataFrame()
                    else:
                        df_pub = pd.read_csv(io.StringIO(out.stdout))
                        keep = [c for c in ["ref","title","size","lastUpdated","downloadCount","voteCount","usabilityRating"] if c in df_pub.columns]
                        ss["kaggle_search_df"] = df_pub[keep]
                        st.success("âœ… Kaggle API results shown.")
                elif src == "Hugging Face":
                    from huggingface_hub import list_datasets
                    results = list_datasets(search=query)
                    df_pub = pd.DataFrame([{"Dataset": r.id, "Tags": ", ".join(r.tags)} for r in results[:50]])
                    st.dataframe(df_pub, use_container_width=True, hide_index=True)
                    st.success("âœ… Hugging Face datasets retrieved.")
                elif src == "OpenML":
                    st.markdown(f"[ğŸ“Š OpenML Search â†—ï¸](https://www.openml.org/search?type=data&q={query})")
                elif src == "Public Domain Portals":
                    st.markdown("""
                    - [ğŸŒ data.gov](https://www.data.gov/)
                    - [ğŸ‡ªğŸ‡º data.europa.eu](https://data.europa.eu/)
                    - [ğŸ‡¸ğŸ‡¬ data.gov.sg](https://data.gov.sg/)
                    - [ğŸ‡»ğŸ‡³ data.gov.vn](https://data.gov.vn/)
                    """)
            except Exception as e:
                st.error(f"Search failed: {e}")

    # If we have Kaggle results, show table + import controls
    if src == "Kaggle (API)" and not ss["kaggle_search_df"].empty:
        st.dataframe(ss["kaggle_search_df"], use_container_width=True, hide_index=True)

        with st.expander("â¬‡ï¸ Import Selected Kaggle Dataset", expanded=True):
            refs = ss["kaggle_search_df"]["ref"].astype(str).tolist()
            selected_ref = st.selectbox("Choose a dataset (ref)", refs, key="asset_kaggle_ref")

            kag_dir = os.path.join(RUNS_DIR, "kaggle")
            os.makedirs(kag_dir, exist_ok=True)
            safe_ref = re.sub(r"[^a-zA-Z0-9._/-]+", "_", selected_ref)
            safe_ref_for_file = safe_ref.replace("/", "__")
            dest = os.path.join(kag_dir, safe_ref_for_file)
            os.makedirs(dest, exist_ok=True)

            # Optional: let user set a server-side save folder (relative to project root)
            st.markdown("**Optional server-side save folder (relative to project root)**")
            default_svdir = os.path.join(RUNS_DIR, "kaggle_exports")
            svdir = st.text_input(
                "Save to folder (server-side)",
                value=default_svdir,
                key="asset_kaggle_svdir",
                help="This saves on the server/WSL side (not your desktop). Use Download button below for local Save As."
            )

            # Main import button
            if st.button("ğŸ“¥ Download & Import Selected", key="btn_asset_kaggle_dl", use_container_width=True):
                try:
                    import subprocess
                    cmd = ["kaggle", "datasets", "download", "-d", selected_ref, "-p", dest, "--unzip"]
                    r = subprocess.run(cmd, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                    if r.returncode != 0:
                        raise RuntimeError(r.stderr.strip() or r.stdout.strip())

                    csvs = [f for f in os.listdir(dest) if f.lower().endswith(".csv")]
                    if not csvs:
                        raise FileNotFoundError("No CSV found in the downloaded archive.")
                    fp = os.path.join(dest, csvs[0])

                    # Load & stash for downstream stages + download button
                    df_imp = _safe_read_csv(fp)
                    ss["asset_intake_df"] = df_imp

                    # Save a unified copy with timestamp in RUNS_DIR
                    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
                    uni_fp = os.path.join(RUNS_DIR, f"intake_table.{ts}.csv")
                    df_imp.to_csv(uni_fp, index=False)

                    st.success(f"âœ… Imported {len(df_imp):,} rows from `{selected_ref}`")
                    st.caption(f"Saved unified intake copy: `{uni_fp}`")
                    st.dataframe(df_imp.head(100), use_container_width=True)

                    # â”€â”€ NEW: Local Download (browser) with dataset-name.csv â”€â”€
                    st.markdown("#### ğŸ’¾ Download")
                    default_fname = f"{safe_ref_for_file}.csv"
                    st.download_button(
                        label="â¬‡ï¸ Download CSV",
                        file_name=default_fname,
                        data=df_imp.to_csv(index=False).encode("utf-8"),
                        mime="text/csv",
                        key="asset_kaggle_download"
                    )

                    # â”€â”€ NEW: Optional server-side save with user folder â”€â”€
                    st.markdown("#### ğŸ—‚ï¸ Save on Server (optional)")
                    # sanitize: keep within project root
                    project_root = os.path.abspath(os.path.join(RUNS_DIR, "..", ".."))
                    svdir_abs = os.path.abspath(svdir)
                    if not svdir_abs.startswith(project_root):
                        st.warning("âš ï¸ Path is outside project root; resetting to default exports folder.")
                        svdir_abs = os.path.abspath(default_svdir)

                    os.makedirs(svdir_abs, exist_ok=True)
                    save_name = f"{safe_ref_for_file}.csv"
                    server_save_path = os.path.join(svdir_abs, save_name)

                    if st.button("ğŸ’½ Save CSV on Server", key="btn_asset_kaggle_save_server"):
                        try:
                            df_imp.to_csv(server_save_path, index=False)
                            rel_path = os.path.relpath(server_save_path, start=project_root)
                            st.success(f"âœ… Saved on server: `{server_save_path}`")
                            st.caption(f"(Relative to project root: ./{rel_path})")
                        except Exception as e:
                            st.error(f"Server save failed: {e}")

                except Exception as e:
                    st.error(f"Import failed: {e}")
                    st.info("Tip: check Kaggle auth and try another dataset.")

    
   

    # Quick HF import (optional direct load)
    if src == "Hugging Face":
        st.markdown("#### Or load directly by repo id")
        hf_repo = st.text_input("ğŸ¤— Dataset repo (e.g. uciml/real-estate-valuation)", value="uciml/real-estate-valuation", key="asset_hf_repo")
        if st.button("ğŸ“¥ Load from HF", key="btn_asset_hf_load", use_container_width=True):
            try:
                from datasets import load_dataset
                ds = load_dataset(hf_repo)
                split = next(iter(ds.keys()))
                df_imp = ds[split].to_pandas()
                ss["asset_intake_df"] = df_imp
                uni_fp = os.path.join(RUNS_DIR, f"intake_table.{_ts()}.csv")
                df_imp.to_csv(uni_fp, index=False)
                st.success(f"âœ… Loaded {len(df_imp):,} rows from {hf_repo} (split: {split})")
                st.caption(f"Saved unified intake copy: `{uni_fp}`")
                st.dataframe(df_imp.head(100), use_container_width=True)
            except Exception as e:
                st.error(f"HF load failed: {e}")

    st.divider()
    
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # (A.3) NORMALIZE & GENERATE UNIFIED CSV
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ§¹ Normalize & Combine All Inputs")

    # ---------- helpers ----------
    import io, csv, re
    from pathlib import Path

    def _slug(name: str) -> str:
        return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

    def _read_any_table(uploaded_file) -> pd.DataFrame:
        """Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback."""
        name = uploaded_file.name.lower()

        # Excel
        if name.endswith((".xlsx", ".xls")):
            return pd.read_excel(uploaded_file)

        # Text (CSV/TSV/TXT): try utf-8 then latin-1; sniff delimiter
        raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
        for enc in ("utf-8", "latin-1"):
            try:
                text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
                head = "\n".join(text.splitlines()[:10]) or text
                try:
                    dialect = csv.Sniffer().sniff(head)
                    sep = dialect.delimiter
                except Exception:
                    sep = ","
                return pd.read_csv(io.StringIO(text), sep=sep)
            except Exception:
                continue
        # last resort (python engine)
        return pd.read_csv(io.BytesIO(raw), engine="python")

    # ---------- optional upload right here ----------
    st.markdown("#### â¬†ï¸ Optional: Upload a CSV/TSV/TXT/XLSX to normalize")
    uploaded = st.file_uploader(
        "Upload a dataset file (or skip if you already imported via Kaggle/HF).",
        type=["csv", "tsv", "txt", "xlsx"],
        key="norm_upload_once",
        accept_multiple_files=False
    )

    if uploaded is not None:
        try:
            df_up = _read_any_table(uploaded)
            ss["asset_intake_df"] = df_up
            ss["last_dataset_name"] = Path(uploaded.name).stem  # remember original name
            st.success(f"âœ… Loaded {len(df_up):,} rows from **{uploaded.name}**")
            st.dataframe(df_up.head(100), use_container_width=True, hide_index=True)
        except Exception as e:
            st.error(f"Could not read file: {e}")

    # ---------- normalization source ----------
    df_src = ss.get("asset_intake_df")
    # Best-effort name priority: last_dataset_name (Kaggle/HF/upload) â†’ fallback
    base_name = _slug(ss.get("last_dataset_name") or ss.get("asset_intake_source_name") or "dataset")

    if df_src is None or len(df_src) == 0:
        st.info("Upload/import a dataset first (Kaggle/HF/Upload), then come back to normalize.")
    else:
        with st.expander("âš™ï¸ Normalization options", expanded=False):
            drop_dupes = st.checkbox("Drop duplicate rows", value=True)
            trim_whitespace = st.checkbox("Trim whitespace in string columns", value=True)
            lower_columns = st.checkbox("Lowercase column names", value=True)

        def _normalize(df: pd.DataFrame) -> pd.DataFrame:
            out = df.copy()
            # 1) basic cleanup
            if lower_columns:
                out.columns = [c.strip().lower() for c in out.columns]
            if trim_whitespace:
                for c in out.select_dtypes(include=["object"]).columns:
                    out[c] = out[c].astype(str).str.strip()
            if drop_dupes:
                out = out.drop_duplicates().reset_index(drop=True)
            # (Optional) add any schema harmonization here later
            return out

        if st.button("ğŸ§ª Normalize & Generate Unified CSV", key="btn_normalize", use_container_width=True):
            norm_df = _normalize(df_src)

            # Ensure output dir
            norm_dir = os.path.join(RUNS_DIR, "normalized")
            os.makedirs(norm_dir, exist_ok=True)

            # Build file name: <original>-Normalized.csv   (slug-safe base_name)
            norm_name = f"{base_name}-Normalized.csv"
            norm_path = os.path.join(norm_dir, norm_name)

            # Save to disk with utf-8-sig (friendlier for Excel)
            norm_df.to_csv(norm_path, index=False, encoding="utf-8-sig")

            # Prepare one bytes blob for both download buttons
            _norm_bytes = norm_df.to_csv(index=False).encode("utf-8-sig")
            _rows, _cols = len(norm_df), len(norm_df.columns)
            _size_kb = max(1, int(len(_norm_bytes) / 1024))
            _norm_file_only = Path(norm_path).name

            # Sticky banner: filename â€¢ rowsÃ—cols â€¢ size
            st.markdown(
                f"""
                <div style="
                    position: sticky;
                    top: 64px;
                    z-index: 50;
                    background: rgba(16,185,129,0.10);
                    border: 1px solid #10b981;
                    padding: 12px 16px;
                    border-radius: 12px;
                    margin: 8px 0 14px 0;
                ">
                <b>âœ… Normalized CSV:</b> <code>{_norm_file_only}</code>
                &nbsp;â€¢&nbsp; {_rows:,} rows Ã— {_cols} cols &nbsp;â€¢&nbsp; {_size_kb} KB
                </div>
                """,
                unsafe_allow_html=True
            )

            # BIG centered primary download button (TOP)
            cL, cM, cR = st.columns([1, 2.5, 1])
            with cM:
                st.download_button(
                    "â¬‡ï¸  Download Normalized CSV",
                    data=_norm_bytes,
                    file_name=_norm_file_only,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_norm_top"
                )

            # Copyable saved path
            st.text_input("Saved to (server path)", norm_path, disabled=True, label_visibility="collapsed")

            # Preview table
            st.dataframe(norm_df.head(100), use_container_width=True, hide_index=True)

            # BIG centered primary download button (BOTTOM)
            cL2, cM2, cR2 = st.columns([1, 2.5, 1])
            with cM2:
                st.download_button(
                    "â¬‡ï¸  Download Normalized CSV",
                    data=_norm_bytes,
                    file_name=_norm_file_only,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_norm_bottom"
                )

    # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # # (A.1b) QUICK START â€” Generate Synthetic Data (always visible here)
    # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # st.markdown("### ğŸ² Quick Start: Generate Synthetic Data")
    # c_syn1, c_syn2 = st.columns([2, 1])
    # with c_syn1:
    #     nrows = st.slider("Number of synthetic rows", 20, 1000, 150, step=10, key="slider_synth_rows_A_quick")
    # with c_syn2:
    #     if st.button("ğŸš€ Generate Synthetic Dataset Now", key="btn_generate_synth_A_quick", use_container_width=True):
    #         try:
    #             df_synth = quick_synth(nrows)
    #             ss["asset_intake_df"] = df_synth
    #             os.makedirs("./.tmp_runs", exist_ok=True)
    #             synth_path = f"./.tmp_runs/intake_table_synth_{_ts()}.csv"
    #             df_synth.to_csv(synth_path, index=False, encoding="utf-8-sig")
    #             st.success(f"âœ… Synthetic dataset created ({len(df_synth)} rows). Saved: `{synth_path}`")
    #             st.dataframe(df_synth.head(20), use_container_width=True, hide_index=True)
    #             st.download_button(
    #                 "â¬‡ï¸ Download Synthetic CSV",
    #                 df_synth.to_csv(index=False).encode("utf-8-sig"),
    #                 file_name="synthetic_intake.csv",
    #                 mime="text/csv",
    #                 key="dl_synth_A_quick"
    #             )
    #         except Exception as e:
    #             st.error(f"Synthetic generation failed: {e}")

       
        
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # (A.4) SYNTHETIC DATA GENERATION
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ¤– Generate Synthetic Data (Fallback)")
    nrows = st.slider("Number of synthetic rows", 10, 500, 150, step=10, key="slider_synth_rows")
    if st.button("ğŸ² Generate Synthetic Dataset", key="btn_generate_synth"):
        try:
            df_synth = quick_synth(nrows)
            ss["asset_intake_df"] = df_synth
            os.makedirs("./.tmp_runs", exist_ok=True)
            synth_path = f"./.tmp_runs/intake_table_synth_{datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')}.csv"
            df_synth.to_csv(synth_path, index=False)
            st.success(f"âœ… Synthetic dataset created ({len(df_synth)} rows).")
            st.dataframe(df_synth.head(20), use_container_width=True)
            st.download_button("ğŸ’¾ Download Synthetic CSV", df_synth.to_csv(index=False), "synthetic_intake.csv", "text/csv")
        except Exception as e:
            st.error(f"Synthetic generation failed: {e}")





# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# B â€” PRIVACY & FEATURES (2..3)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tabB:
    st.subheader("B. Privacy & Features")
    st.caption("Steps: **2) Anonymize**, **3) Feature Engineering + Comps**")

    import re, math, json, os, time
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    
    # ----------------------------
    # B.2 â€” Anonymize / Sanitize PII
    # ----------------------------
    st.markdown("### **2) Anonymize / Sanitize PII**")

    import io, csv, re, os
    from pathlib import Path

    ss = st.session_state  # make sure this exists globally

    def _slug(name: str) -> str:
        return re.sub(r"[^a-zA-Z0-9._-]+", "_", (name or "dataset")).strip("_").lower()

    def _read_any_table(uploaded_file) -> pd.DataFrame:
        """Robust reader for CSV/TSV/TXT/XLSX with encoding + delimiter fallback."""
        name = uploaded_file.name.lower()
        if name.endswith((".xlsx", ".xls")):
            return pd.read_excel(uploaded_file)

        raw = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
        for enc in ("utf-8", "latin-1"):
            try:
                text = raw.decode(enc) if isinstance(raw, (bytes, bytearray)) else raw
                head = "\n".join(text.splitlines()[:10]) or text
                try:
                    sep = csv.Sniffer().sniff(head).delimiter
                except Exception:
                    sep = ","
                return pd.read_csv(io.StringIO(text), sep=sep)
            except Exception:
                continue
        return pd.read_csv(io.BytesIO(raw), engine="python")

    def _anonymize(df: pd.DataFrame) -> pd.DataFrame:
        """Mask likely-PII text while preserving common join keys."""
        if df is None or df.empty:
            return df
        out = df.copy()
        join_keys = {"loan_id", "asset_id", "application_id"}
        pii_like = re.compile(r"(name|email|phone|addr|address|national|passport|id|nid)", re.I)

        for col in out.columns:
            if col in join_keys:
                continue
            if out[col].dtype == "object" and pii_like.search(col):
                s = out[col].astype(str)
                s = s.str.replace(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+", "[EMAIL]", regex=True)
                s = s.str.replace(r"\b(\+?\d{1,3}[-.\s]?)?\d{7,}\b", "[PHONE]", regex=True)
                s = s.str.replace(r"\b\d{9,16}\b", "[ID]", regex=True)
                out[col] = s
        return out.drop_duplicates().reset_index(drop=True)

    # ---------- Source picker (Stage A or Upload here) ----------
    st.markdown("#### ğŸ”„ Choose data source")
    src_choice = st.radio(
        "Pick how you want to provide data:",
        ["Use data from Stage A", "Upload a new file here"],
        horizontal=True,
        key="b_src_choice"
    )

    df_src, base_name = None, "dataset"

    if src_choice == "Use data from Stage A":
        df_src = ss.get("asset_intake_df")
        if isinstance(df_src, pd.DataFrame) and not df_src.empty:
            base_name = _slug(ss.get("last_dataset_name") or "dataset")
            st.success(f"Using Stage-A dataset: **{base_name}** â€¢ {len(df_src):,} rows")
            st.dataframe(df_src.head(10), use_container_width=True, hide_index=True)
        else:
            st.warning("No data found from Stage A. Upload below instead.")
    else:
        st.markdown(
            """
            <div style="border:2px dashed #22c55e;padding:14px;border-radius:12px;
                        background:rgba(34,197,94,0.06);margin:6px 0 12px 0;">
            <b>â¬†ï¸ Upload CSV/TSV/TXT/XLSX for anonymization</b>
            </div>
            """,
            unsafe_allow_html=True,
        )
        up = st.file_uploader(
            "Upload a dataset file",
            type=["csv","tsv","txt","xlsx"],
            key="b_upload",
            accept_multiple_files=False,
        )
        if up is not None:
            try:
                df_src = _read_any_table(up)
                base_name = _slug(Path(up.name).stem)
                ss["asset_intake_df"] = df_src
                ss["last_dataset_name"] = base_name
                st.success(f"âœ… Loaded {len(df_src):,} rows from **{up.name}**")
                st.dataframe(df_src.head(20), use_container_width=True, hide_index=True)
            except Exception as e:
                st.error(f"Could not read file: {e}")

    if not (isinstance(df_src, pd.DataFrame) and not df_src.empty):
        st.info("Provide data (via Stage A or upload here) to enable anonymization.")
    else:
        if st.button("ğŸ›¡ï¸ Run Anonymization & Export CSV", type="primary", use_container_width=True, key="btn_b_anon"):
            anon_df = _anonymize(df_src)
            out_dir = os.path.join(RUNS_DIR, "normalized")
            os.makedirs(out_dir, exist_ok=True)
            out_name = f"{base_name}-Anonymized.csv"
            out_path = os.path.join(out_dir, out_name)
            anon_df.to_csv(out_path, index=False, encoding="utf-8-sig")
            ss["asset_anon_df"] = anon_df

            _bytes = anon_df.to_csv(index=False).encode("utf-8-sig")

            st.markdown(
                f"""
                <div style="position:sticky;top:64px;z-index:50;background:rgba(59,130,246,0.10);
                            border:1px solid #3b82f6;padding:12px 16px;border-radius:12px;margin:8px 0 14px 0;">
                <b>âœ… Anonymized CSV:</b> <code>{out_name}</code> â€¢ {len(anon_df):,} rows Ã— {len(anon_df.columns)} cols
                </div>
                """,
                unsafe_allow_html=True,
            )

            cL, cM, cR = st.columns([1, 2.6, 1])
            with cM:
                st.download_button(
                    "â¬‡ï¸  Download Anonymized CSV",
                    data=_bytes,
                    file_name=out_name,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_b_anon_top",
                )

            st.text_input("Saved to (server path)", out_path, disabled=True, label_visibility="collapsed")
            st.dataframe(anon_df.head(100), use_container_width=True, hide_index=True)

            cL2, cM2, cR2 = st.columns([1, 2.6, 1])
            with cM2:
                st.download_button(
                    "â¬‡ï¸  Download Anonymized CSV",
                    data=_bytes,
                    file_name=out_name,
                    mime="text/csv",
                    type="primary",
                    use_container_width=True,
                    key="dl_b_anon_bottom",
                )

    st.markdown("---")

    
    

    
    # ----------------------------
    # B.3 â€” Feature Engineering & Comps
    # ----------------------------
    st.markdown("### **3) Feature Engineering & Comps**")

    def feature_engineer(df: pd.DataFrame, evidence_index=None) -> pd.DataFrame:
        """
        Light feature engineering (safe):
        - Ensure city/lat/lon/age_years/delinquencies/current_loans
        - Coerce lat/lon including "10,762" â†’ 10.762
        - Create stable 'geohash' (lat,lon preferred; fallback short city hash)
        - Derive condition_score (0..1) heuristically if inputs exist
        - Ensure legal_penalty numeric
        - Keep join keys up front if present
        """
        if not isinstance(df, pd.DataFrame) or df.empty:
            return pd.DataFrame()

        out = df.copy()

        # Ensure expected columns exist
        for c in ("city", "lat", "lon", "age_years", "delinquencies", "current_loans"):
            if c not in out.columns:
                out[c] = pd.NA

        # Coerce lat/lon
        for c in ("lat", "lon"):
            if out[c].dtype == "object":
                out[c] = out[c].astype(str).str.replace(",", ".", regex=False)
            out[c] = pd.to_numeric(out[c], errors="coerce")

        # Row-wise geokey: prefer lat/lon else short md5(city)
        import hashlib
        def _row_geokey(row) -> str:
            lat = row.get("lat")
            lon = row.get("lon")
            if pd.notna(lat) and pd.notna(lon):
                return f"{float(lat):.3f},{float(lon):.3f}"
            city_val = row.get("city")
            city_txt = "" if pd.isna(city_val) else str(city_val)
            return hashlib.md5(city_txt.encode("utf-8")).hexdigest()[:7]

        out["geohash"] = out.apply(_row_geokey, axis=1).astype(str)

        # Heuristic condition_score (0..1)
        age = pd.to_numeric(out.get("age_years"), errors="coerce").fillna(0.0)
        delinq = pd.to_numeric(out.get("delinquencies"), errors="coerce").fillna(0.0)
        curr_loans = pd.to_numeric(out.get("current_loans"), errors="coerce").fillna(0.0)
        cond = 1.0 - (0.02 * age) - (0.05 * delinq) - (0.03 * curr_loans)
        out["condition_score"] = pd.Series(cond, index=out.index).clip(0.10, 0.98)

        # legal_penalty safe numeric
        if "legal_penalty" not in out.columns:
            out["legal_penalty"] = 0.0
        else:
            out["legal_penalty"] = pd.to_numeric(out["legal_penalty"], errors="coerce").fillna(0.0)

        # Keep join keys in front
        front_cols = [c for c in ["loan_id", "application_id", "asset_id"] if c in out.columns]
        other_cols = [c for c in out.columns if c not in front_cols]
        out = out[front_cols + other_cols]

        return out


    def _fmt_mean(df, col, fmt="{:.2f}"):
        if isinstance(df, pd.DataFrame) and col in df.columns:
            v = pd.to_numeric(df[col], errors="coerce").mean()
            if pd.notna(v):
                return fmt.format(v)
        return "â€”"


    def fetch_and_clean_comps(df_feats: pd.DataFrame) -> dict:
        """Deterministic stub; replace with real comps feed."""
        mv = pd.to_numeric(df_feats.get("market_value", pd.Series(dtype=float)), errors="coerce")
        base = float(mv.median()) if mv.notna().any() else 100000.0
        comps = [{"comp_id": f"C-{i+1:03d}", "price": round(base * (0.95 + 0.02 * i), 2)} for i in range(5)]
        return {"used": comps, "count": len(comps), "median_baseline": base}


    if not isinstance(ss.get("asset_anon_df"), pd.DataFrame) or ss["asset_anon_df"].empty:
        st.info("Run **Anonymization (B.2)** first to prepare inputs for features.")
    else:
        c3a, c3b = st.columns([1.2, 0.8])

        with c3a:
            if st.button("Build Features & Fetch Comps", key="btn_build_features", use_container_width=True):
                # Build features in this rerun and persist
                feats = feature_engineer(ss["asset_anon_df"], ss.get("asset_evidence_index"))
                ss["asset_features_df"] = feats

                # Metrics (from feats)
                m1, m2, m3 = st.columns(3)
                with m1:
                    st.metric("Avg condition_score", _fmt_mean(feats, "condition_score"))
                with m2:
                    st.metric("Avg market_value", _fmt_mean(feats, "market_value", "{:,.0f}"))
                with m3:
                    st.metric("Avg loan_amount", _fmt_mean(feats, "loan_amount", "{:,.0f}"))

                # Persist features.parquet (BytesIO for download)
                features_path = os.path.join(RUNS_DIR, f"features.{_ts()}.parquet")
                feats.to_parquet(features_path, index=False)
                st.success(f"Saved features â†’ `{features_path}`")

                import io as _io
                _buf = _io.BytesIO()
                feats.to_parquet(_buf, index=False)
                st.download_button(
                    "â¬‡ï¸ Download features.parquet",
                    data=_buf.getvalue(),
                    file_name="features.parquet",
                    mime="application/octet-stream",
                    key="dl_features_parquet",
                    use_container_width=True
                )

                # Comps (and persist)
                comps = fetch_and_clean_comps(feats)
                ss["asset_comps_used"] = comps
                comps_path = os.path.join(RUNS_DIR, f"comps_used.{_ts()}.json")
                with open(comps_path, "w", encoding="utf-8") as fp:
                    json.dump(comps, fp, ensure_ascii=False, indent=2)
                st.success(f"Saved comps â†’ `{comps_path}`")

        with c3b:
            # Show last built features (if any) and offer a second download
            df_feats = ss.get("asset_features_df")
            if isinstance(df_feats, pd.DataFrame) and not df_feats.empty:
                import io as _io2
                _buf2 = _io2.BytesIO()
                df_feats.to_parquet(_buf2, index=False)
                st.download_button(
                    "â¬‡ï¸ Download last features.parquet",
                    data=_buf2.getvalue(),
                    file_name="features.parquet",
                    mime="application/octet-stream",
                    key="dl_features_parquet_last",
                    use_container_width=True
                )

    # Outside the columns: show current features + comps if present
    df_feats = ss.get("asset_features_df")
    if isinstance(df_feats, pd.DataFrame) and not df_feats.empty:
        k1, k2, k3 = st.columns(3)
        with k1:
            st.metric("Rows (features)", f"{len(df_feats):,}")
        with k2:
            st.metric("Avg condition_score", _fmt_mean(df_feats, "condition_score"))
        with k3:
            # evidence_count column is optional; show 0 if absent
            ev = pd.to_numeric(df_feats.get("evidence_count", pd.Series([0]*len(df_feats))), errors="coerce").fillna(0)
            st.metric("Evidence count (stub)", int(ev.mean()))

        st.dataframe(df_feats.head(30), use_container_width=True)

    if ss.get("asset_comps_used") is not None:
        st.caption("Comps used (stub)")
        st.json(ss["asset_comps_used"])

    


# ========== 3) AI APPRAISAL & VALUATION ==========
with tabC:
    st.subheader("ğŸ¤– Stage 3 â€” AI Appraisal & Valuation")

    import os, json, numpy as np, requests, pandas as pd, plotly.express as px
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ§­ HOW TO USE THIS STAGE
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("""
    ### ğŸ§­ How to Use This Stage
    1. **Select a model** â€” choose between production, trained, or open-source (Hugging Face) models.  
    2. **Check hardware** â€” confirm your GPU/CPU profile supports the chosen model.  
    3. **Select dataset** â€” use Stage 2 outputs (Features / Anonymized) or fallback synthetic data.  
    4. **Run appraisal** â€” compute AI-based valuation (`fmv`, `ai_adjusted`, `confidence`, `why`).  
    5. **Review outputs** â€” compare customer vs AI results, run projections, dashboards, and reports.  
    6. **Verify ownership** â€” perform Legal / Encumbrance checks (C.5).  
    """)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ§  MODEL FAMILY TABLE
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ§  Model Families & Recommended Use-Cases")

    model_ref = pd.DataFrame([
        {"Category": "Local / Trained", "Model": "LightGBM / XGBoost / CatBoost",
         "Use Case": "Numeric â†’ FMV prediction", "GPU": "CPU OK",
         "Notes": "Fast, explainable baseline model"},
        {"Category": "Production (â­)", "Model": "asset_lgbm-v1 / credit_lr",
         "Use Case": "Enterprise-grade deployed valuation", "GPU": "CPU OK",
         "Notes": "Stable, low-latency predictions"},
        {"Category": "LLM (HF)", "Model": "Mistral 7B / Gemma 2 9B",
         "Use Case": "Text reasoning + narratives", "GPU": "â‰¥ 8 GB",
         "Notes": "Fast reasoning for appraisal explanations"},
        {"Category": "LLM (HF)", "Model": "LLaMA 3 8B / Qwen 2 7B",
         "Use Case": "Multilingual valuation reports", "GPU": "â‰¥ 12 GB",
         "Notes": "Strong contextual generation"},
        {"Category": "LLM (HF)", "Model": "Mixtral 8Ã—7B",
         "Use Case": "High-end MoE valuation", "GPU": "â‰¥ 24 GB",
         "Notes": "Premium precision for portfolios"},
    ])
    st.dataframe(model_ref, use_container_width=True)
    st.markdown("---")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸŸ¢ PRODUCTION MODEL BANNER
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.status_code == 200:
            meta = resp.json()
            if meta.get("has_production"):
                ver = (meta.get("meta") or {}).get("version", "1.x")
                src = (meta.get("meta") or {}).get("source", "production")
                st.success(f"ğŸŸ¢ Production model active â€” version {ver} â€¢ source {src}")
            else:
                st.warning("âš ï¸ No production model promoted yet â€” using baseline.")
        else:
            st.info("â„¹ï¸ Could not fetch production model meta.")
    except Exception:
        st.info("â„¹ï¸ Production meta unavailable.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ§© Model Selection (list all trained models) â€” HARD-CODED TEST
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from datetime import datetime
    import os, shutil, streamlit as st

    # Hardcoded absolute paths for your environment
    trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/trained"
    production_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/production"

    # Debug info
    st.caption(f"ğŸ“‚ Trained dir: `{trained_dir}`")
    st.caption(f"ğŸ“¦ Production dir: `{production_dir}`")

    # Refresh button â€” use unique key for asset
    if st.button("â†» Refresh models", key="asset_refresh_models"):
        st.session_state.pop("asset_selected_model", None)
        st.rerun()

    models = []
    if os.path.isdir(trained_dir):
        for f in os.listdir(trained_dir):
            if f.endswith(".joblib"):
                fpath = os.path.join(trained_dir, f)
                ctime = os.path.getctime(fpath)
                created = datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
                models.append((f, fpath, created))
    else:
        st.error(f"âŒ Trained dir not found: {trained_dir}")

    if models:
        # Sort by creation time (latest first)
        models.sort(key=lambda x: os.path.getctime(x[1]), reverse=True)
        display_names = [f"{m[0]} â€” {m[2]}" for m in models]

        selected_display = st.selectbox("ğŸ“¦ Select trained model to use", display_names, key="asset_model_select")
        selected_model = models[display_names.index(selected_display)][1]
        st.success(f"âœ… Using model: {os.path.basename(selected_model)}")

        st.session_state["asset_selected_model"] = selected_model

        # Promote to production
        if st.button("ğŸš€ Promote this model to Production", key="asset_promote_button"):
            try:
                os.makedirs(production_dir, exist_ok=True)
                prod_path = os.path.join(production_dir, "model.joblib")
                shutil.copy2(selected_model, prod_path)
                st.balloons()
                st.success(f"âœ… Model promoted to production: {prod_path}")
            except Exception as e:
                st.error(f"âŒ Promotion failed: {e}")
    else:
        st.warning("âš ï¸ No trained models found â€” train one in Stage F first.")


    

    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ§  LLM + HARDWARE PROFILE (LOCAL + HUGGING FACE)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ§  LLM & Hardware Profile (Local + Hugging Face Models)")

    HF_MODELS = [
        {"Model": "mistralai/Mistral-7B-Instruct-v0.3",
         "Type": "Reasoning / valuation narrative",
         "GPU": "â‰¥ 8 GB", "Notes": "Fast multilingual contextual LLM"},
        {"Model": "google/gemma-2-9b-it",
         "Type": "Instruction-tuned financial reports",
         "GPU": "â‰¥ 12 GB", "Notes": "Great for valuation explanations"},
        {"Model": "meta-llama/Meta-Llama-3-8B-Instruct",
         "Type": "Valuation summarization",
         "GPU": "â‰¥ 12 GB", "Notes": "High accuracy + low hallucination"},
        {"Model": "Qwen/Qwen2-7B-Instruct",
         "Type": "Multilingual reasoning (VN + EN)",
         "GPU": "â‰¥ 12 GB", "Notes": "Excellent for VN asset appraisal"},
        {"Model": "microsoft/Phi-3-mini-4k-instruct",
         "Type": "Compact instruction LLM",
         "GPU": "â‰¤ 8 GB", "Notes": "Fast lightweight valuation logic"},
        {"Model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
         "Type": "MoE premium reasoning",
         "GPU": "â‰¥ 24 GB", "Notes": "Top-tier valuation model"},
        {"Model": "LightAutoML/LightGBM",
         "Type": "Tabular regression baseline",
         "GPU": "CPU OK", "Notes": "Numeric FMV baseline"},
    ]
    st.dataframe(pd.DataFrame(HF_MODELS), use_container_width=True)

    LLM_MODELS = [
        ("Phi-3 Mini (3.8B)", "phi3:3.8b", "CPU 8 GB RAM (fast)"),
        ("Mistral 7B Instruct", "mistral:7b-instruct", "GPU â‰¥ 8 GB (fast)"),
        ("Gemma-2 9B", "gemma2:9b", "GPU â‰¥ 12 GB (high accuracy)"),
        ("LLaMA-3 8B", "llama3:8b-instruct", "GPU â‰¥ 12 GB (context heavy)"),
        ("Qwen-2 7B", "qwen2:7b-instruct", "GPU â‰¥ 12 GB (multilingual)"),
        ("Mixtral 8Ã—7B", "mixtral:8x7b-instruct", "GPU 24-48 GB (batch)"),
    ]
    LLM_LABELS = [l for (l, _, _) in LLM_MODELS]
    LLM_VALUE_BY_LABEL = {l: v for (l, v, _) in LLM_MODELS}
    LLM_HINT_BY_LABEL  = {l: h for (l, _, h) in LLM_MODELS}

    OPENSTACK_FLAVORS = {
        "m4.medium": "4 vCPU / 8 GB RAM (CPU-only small)",
        "m8.large": "8 vCPU / 16 GB RAM (CPU-only medium)",
        "g1.a10.1": "8 vCPU / 32 GB RAM + 1Ã—A10 24 GB",
        "g1.l40.1": "16 vCPU / 64 GB RAM + 1Ã—L40 48 GB",
        "g2.a100.1": "24 vCPU / 128 GB RAM + 1Ã—A100 80 GB",
    }

    with st.expander("ğŸ§  Choose Model & Hardware Profile", expanded=True):
        c1, c2 = st.columns([1.2, 1])
        with c1:
            model_label = st.selectbox(
                "Select Local or HF LLM (for narratives / explanations)",
                LLM_LABELS, index=1, key="asset_llm_label")
            llm_value = LLM_VALUE_BY_LABEL[model_label]
            use_llm = st.checkbox("Use LLM narrative (explanations)",
                                  value=False, key="asset_use_llm")
            st.caption(f"Hint: {LLM_HINT_BY_LABEL[model_label]}")
        with c2:
            flavor = st.selectbox("OpenStack flavor / host profile",
                                  list(OPENSTACK_FLAVORS.keys()), index=0,
                                  key="asset_flavor")
            st.caption(OPENSTACK_FLAVORS[flavor])
        st.caption("These parameters are passed to backend (Ollama / Flowise / RunAI).")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # GPU PROFILE AND DATASET SOURCE (keep existing logic)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### **C.4 â€” Valuation (AI)**")
    gpu_profile = st.selectbox(
        "GPU Profile (for valuation compute)",
        ["CPU (slow)", "GPU: 1Ã—A100", "GPU: 1Ã—H100", "GPU: 2Ã—L40S"],
        index=1, key="asset_gpu_profile_c4")
    ss["asset_gpu_profile"] = gpu_profile

    # Gather candidates from prior stages (they may be None/empty)
    cand_features = ss.get("asset_features_df")
    cand_anon     = ss.get("asset_anon_df")
    cand_intake   = ss.get("asset_intake_df")

    src = st.selectbox(
        "Data source for AI run",
        [
            "Use FEATURES (Stage 2/3)",
            "Use ANON (Stage 2)",
            "Use RAW â†’ auto-sanitize",
            "Use synthetic (fallback)",
        ],
        key="asset_c4_source"
    )

    # Decide df2 explicitly based on choice
    df2 = None
    if src == "Use FEATURES (Stage 2/3)":
        # First non-empty among features â†’ anon â†’ intake
        df2 = first_nonempty_df(cand_features, cand_anon, cand_intake)

    elif src == "Use ANON (Stage 2)":
        df2 = cand_anon

    elif src == "Use RAW â†’ auto-sanitize":
        # If intake exists, sanitize; else leave None
        df2 = anonymize_text_cols(cand_intake) if isinstance(cand_intake, pd.DataFrame) and not cand_intake.empty else None

    else:  # "Use synthetic (fallback)"
        df2 = quick_synth(150)

    # Final safety check
    if not isinstance(df2, pd.DataFrame) or df2.empty:
        st.warning("No usable dataset found. Please complete Stage A (Intake) and Stage B (Privacy/Features), or choose the synthetic fallback.")
        st.stop()

    # Preview selected data
    st.dataframe(df2.head(10), use_container_width=True)



    # Probe API (health & agents)
    with st.expander("ğŸ” Probe API (health & agents)", expanded=False):
        if st.button("Run probe now", key="btn_probe_api"):
            diag = probe_api()
            st.json(diag)

    # Run model button (runtime flavor + gpu_profile included)
    if st.button("ğŸš€ Run AI Appraisal now", key="btn_run_ai"):
        csv_bytes = df2.to_csv(index=False).encode("utf-8")

        form_fields = {
            "use_llm": str(use_llm).lower(),
            "llm": llm_value,
            "flavor": flavor,
            "gpu_profile": gpu_profile,  # NEW: pass GPU profile to backend
            "selected_model": ss.get("asset_selected_model", ""),
            "agent_name": "asset_appraisal",
        }

        with st.spinner("Calling asset agentâ€¦"):
            ok, result = try_run_asset_agent(csv_bytes, form_fields=form_fields, timeout_sec=180)

        if not ok:
            st.error("âŒ Model API error.")
            st.info("Tip: open 'ğŸ” Probe API' above to see health and discovered agent ids.")
            st.code(str(result)[:8000])
            st.stop()

        df_app = result.copy()

        # Ensure core valuation columns per blueprint
        if "ai_adjusted" not in df_app.columns and "market_value" in df_app.columns:
            df_app["ai_adjusted"] = df_app["market_value"]
        if "fmv" not in df_app.columns:
            # heuristics: if model returns fmv, keep; else set fmv ~ ai_adjusted
            df_app["fmv"] = pd.to_numeric(df_app.get("ai_adjusted", np.nan), errors="coerce")
        if "confidence" not in df_app.columns:
            df_app["confidence"] = 80.0
        if "why" not in df_app.columns:
            df_app["why"] = ["Condition, comps, and features (placeholder)"] * len(df_app)

        # Persist valuation artifact
        val_path = os.path.join(RUNS_DIR, f"valuation_ai.{_ts()}.csv")
        df_app.to_csv(val_path, index=False)
        
        st.success(f"Saved valuation artifact â†’ `{val_path}`")

        # # âœ… PATCH: Save Stage C valuation table for Stage H (use df_app, not ai_df)
        # try:
        #     st.session_state["asset_ai_df"] = df_app.copy()
        #     ss["asset_ai_df"] = df_app.copy()
        #     st.info("âœ… Stage C valuation stored for Stage D / E / H.")
        # except Exception as e:
        #     st.warning(f"Could not store Stage C output: {e}")

        # st.success(f"Saved valuation artifact â†’ `{val_path}`")
        # # âœ… PATCH: Save Stage C valuation table for Stage H
        # try:
        #     st.session_state["asset_ai_df"] = ai_df.copy()
        #     st.info("âœ… Stage C results stored for Stage H portfolio view.")
        # except Exception as e:
        #     st.warning(f"Could not store Stage C output: {e}")


        # Keep table for downstream steps
        ss["asset_ai_df"] = df_app

        # Display minimal KPIs
        k1, k2, k3 = st.columns(3)
        try:
            k1.metric("Avg FMV", f"{pd.to_numeric(df_app['fmv'], errors='coerce').mean():,.0f}")
        except Exception:
            k1.metric("Avg FMV", "â€”")
        try:
            k2.metric("Avg Confidence", f"{pd.to_numeric(df_app['confidence'], errors='coerce').mean():.2f}")
        except Exception:
            k2.metric("Avg Confidence", "â€”")
        k3.metric("Rows", len(df_app))

        st.markdown("### ğŸ§¾ Valuation Output (preview)")
        cols_first = [c for c in [
            "application_id","asset_id","asset_type","city",
            "fmv","ai_adjusted","confidence","why"
        ] if c in df_app.columns]
        st.dataframe(df_app[cols_first].head(50), use_container_width=True)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Customer vs AI â€” Details & 5-Year Deltas
        # (Place this right after the valuation preview table)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("### ğŸ“‹ Customer & Loan Details (Declared) + AI Alignment")

        import numpy as np
        from datetime import datetime, timezone
        import os

        RUNS_DIR = "./.tmp_runs"
        os.makedirs(RUNS_DIR, exist_ok=True)
        _ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

        
        # âœ… Save Stage C output for Stage H (single source of truth)
        st.session_state["asset_ai_df"] = df_app.copy()
        ss["asset_ai_df"] = df_app.copy()
        st.info("âœ… Stage C valuation stored for Stage D / E / H.")

        # âœ… Use the saved valuation table
        ai_df = st.session_state["asset_ai_df"]

        # âœ… Merge intake (customer-declared) if available
        intake_df = ss.get("asset_intake_df")
        if intake_df is not None and not intake_df.empty:
            join_keys = [
                k for k in ["application_id", "asset_id"]
                if k in ai_df.columns and k in intake_df.columns
            ]
            if join_keys:
                merged = intake_df.merge(
                    ai_df, on=join_keys,
                    suffixes=("_cust", "_ai"),
                    how="left"
                )
            else:
                merged = ai_df.copy()
        else:
            merged = ai_df.copy()

        # Optional: show preview table
        st.markdown("### ğŸ” Stage C Output Preview")
        st.dataframe(merged, use_container_width=True)

        
        # # âœ… Save Stage C output for Stage H (using df_app, not undefined ai_df)
        # st.session_state["asset_ai_df"] = df_app.copy()
        # #ai_df = df_app.copy()
        # ai_df = st.session_state["asset_ai_df"]
        

        

        # âœ… Merge intake (customer-declared) if available
        intake_df = ss.get("asset_intake_df")
        if intake_df is not None and not intake_df.empty:
            # Choose join keys available in both frames
            join_keys = [
                k for k in ["application_id", "asset_id"]
                if k in ai_df.columns and k in intake_df.columns
            ]
            if join_keys:
                merged = intake_df.merge(
                    ai_df, on=join_keys,
                    suffixes=("_cust", "_ai"),
                    how="left"
                )
            else:
                merged = ai_df.copy()
        else:
            merged = ai_df.copy()

        
        # # âœ… Save Stage C output for Stage H
        # #st.session_state["asset_ai_df"] = ai_df.copy()
        # ai_df = ss.get("asset_ai_df")
        # if ai_df is None or len(ai_df) == 0:
        #     st.info("Run the AI appraisal first to populate these tables.")
        # else:
        #     # Merge intake (customer-declared) if available
        #     intake_df = ss.get("asset_intake_df")
        #     if intake_df is not None and not intake_df.empty:
        #         # Choose join keys available in both frames
        #         join_keys = [k for k in ["application_id", "asset_id"] if k in ai_df.columns and k in intake_df.columns]
        #         if join_keys:
        #             merged = intake_df.merge(ai_df, on=join_keys, suffixes=("_cust", "_ai"), how="left")
        #         else:
        #             merged = ai_df.copy()
        #     else:
        #         merged = ai_df.copy()

            # Canonical column mapping
            # customer declared value (prefer *_cust if merge happened)
            customer_val_col = "market_value_cust" if "market_value_cust" in merged.columns else (
                "market_value" if "market_value" in merged.columns else None
            )
            # AI value (prefer fmv, fallback ai_adjusted)
            ai_val_col = "fmv" if "fmv" in merged.columns else (
                "ai_adjusted" if "ai_adjusted" in merged.columns else None
            )

            # Build Customer & Loan Details table
            details_cols = [c for c in [
                "application_id","asset_id","asset_type","city",
                customer_val_col,
                "loan_amount",
                ai_val_col, "confidence","why"
            ] if c and c in merged.columns]

            details_tbl = merged[details_cols].copy() if details_cols else merged.copy()

            # Rename for clarity in the UI
            rename_map = {}
            if customer_val_col:
                rename_map[customer_val_col] = "customer_declared_value"
            if ai_val_col:
                rename_map[ai_val_col] = "ai_estimate_value"
            details_tbl = details_tbl.rename(columns=rename_map)

            # Explanation / Source
            selected_model = os.path.basename(str(ss.get("asset_selected_model","") or ""))
            comps_count = int((ss.get("asset_comps_used") or {}).get("count", 0))
            details_tbl["explanation_source"] = details_tbl.apply(
                lambda r: f"Customer input CSV vs AI model {selected_model or 'production'} (comps={comps_count})",
                axis=1
            )

            st.dataframe(details_tbl.head(50), use_container_width=True)

            # Persist details table
            details_path = os.path.join(RUNS_DIR, f"customer_loan_details.{_ts}.csv")
            details_tbl.to_csv(details_path, index=False)
            st.download_button(
                "â¬‡ï¸ Download Customer & Loan Details (CSV)",
                data=details_tbl.to_csv(index=False).encode("utf-8"),
                file_name="customer_loan_details.csv",
                mime="text/csv"
            )

            st.markdown("---")
            st.markdown("### ğŸ“ˆ 5-Year Deltas: Customer vs AI (per-year Î” and %Î”)")

            # Controls for forward projections
            cgr_a, cgr_b = st.columns(2)
            with cgr_a:
                cust_cagr = st.slider("Customer Expected CAGR (%)", min_value=-20, max_value=40, value=5, step=1) / 100.0
            with cgr_b:
                ai_cagr = st.slider("AI Expected CAGR (%)", min_value=-20, max_value=40, value=4, step=1) / 100.0

            if not customer_val_col or not ai_val_col:
                st.warning("Missing base columns to compute deltas. Ensure both customer and AI values exist.")
            else:
                base_cust = merged[customer_val_col].astype(float)
                base_ai   = merged[ai_val_col].astype(float)

                # Build long-format 5-year projection table
                rows = []
                years = [1, 2, 3, 4, 5]
                for idx in range(len(merged)):
                    cust0 = base_cust.iloc[idx]
                    ai0   = base_ai.iloc[idx]
                    app_id = merged.iloc[idx].get("application_id", None)
                    asset_id = merged.iloc[idx].get("asset_id", None)
                    asset_type = merged.iloc[idx].get("asset_type", None)
                    city = merged.iloc[idx].get("city", None)

                    for y in years:
                        cust_y = cust0 * ((1.0 + cust_cagr) ** y) if np.isfinite(cust0) else np.nan
                        ai_y   = ai0   * ((1.0 + ai_cagr) ** y)   if np.isfinite(ai0)   else np.nan
                        delta  = ai_y - cust_y if (np.isfinite(ai_y) and np.isfinite(cust_y)) else np.nan
                        pct    = (delta / cust_y * 100.0) if (np.isfinite(delta) and cust_y not in [0, np.nan]) else np.nan

                        rows.append({
                            "application_id": app_id,
                            "asset_id": asset_id,
                            "asset_type": asset_type,
                            "city": city,
                            "year_ahead": y,
                            "customer_value": cust_y,
                            "ai_value": ai_y,
                            "delta": delta,
                            "delta_pct": pct,
                            "explanation_source": f"Customer CAGR={cust_cagr*100:.1f}% vs AI CAGR={ai_cagr*100:.1f}%; AI model {selected_model or 'production'} (comps={comps_count})"
                        })

                deltas_tbl = pd.DataFrame(rows)

            # Display & export
            # Round for readability
            for c in ["customer_value","ai_value","delta","delta_pct"]:
                if c in deltas_tbl.columns:
                    deltas_tbl[c] = pd.to_numeric(deltas_tbl[c], errors="coerce")

            st.dataframe(deltas_tbl.head(100), use_container_width=True)

            deltas_path = os.path.join(RUNS_DIR, f"valuation_deltas_5y.{_ts}.csv")
            deltas_tbl.to_csv(deltas_path, index=False)
            st.download_button(
                "â¬‡ï¸ Download 5-Year Deltas (CSV)",
                data=deltas_tbl.to_csv(index=False).encode("utf-8"),
                file_name="valuation_deltas_5y.csv",
                mime="text/csv"
            )


        st.markdown("---")
        # ğŸ”’ C.5 â€” Legal/Ownership Verification (encumbrances, liens, fraud)
        st.markdown("### **C.5 â€” Legal/Ownership Verification**")

        def _verify_stub(df_in: pd.DataFrame) -> pd.DataFrame:
            df = df_in.copy()
            if "verification_status" not in df.columns:
                df["verification_status"] = "verified"
            if "encumbrance_flag" not in df.columns:
                df["encumbrance_flag"] = False
            if "verified_owner" not in df.columns:
                df["verified_owner"] = np.where(df.get("asset_type","").astype(str).str.lower().str.contains("car"), "DMV Registry", "Land Registry")
            if "notes" not in df.columns:
                df["notes"] = "Registry check passed (stub)"
            return df

        if st.button("ğŸ” Run Legal/Ownership Checks", key="btn_run_verification"):
            base_df = ss.get("asset_ai_df")
            if base_df is None:
                st.warning("Run valuation first.")
            else:
                verified_df = _verify_stub(base_df)
                ss["asset_verified_df"] = verified_df
                ver_path = os.path.join(RUNS_DIR, f"verification_status.{_ts()}.csv")
                verified_df.to_csv(ver_path, index=False)
                st.success(f"Saved verification artifact â†’ `{ver_path}`")

                v1, v2 = st.columns(2)
                with v1:
                    try:
                        pct = (verified_df["verification_status"] == "verified").mean()
                        st.metric("Verified %", f"{pct:.0%}")
                    except Exception:
                        st.metric("Verified %", "â€”")
                with v2:
                    try:
                        st.metric("Encumbrance Flags", int(pd.to_numeric(verified_df["encumbrance_flag"]).sum()))
                    except Exception:
                        st.metric("Encumbrance Flags", "â€”")

                cols_ver = [c for c in [
                    "application_id","asset_id","verified_owner","verification_status","encumbrance_flag","notes"
                ] if c in verified_df.columns]
                st.dataframe(verified_df[cols_ver].head(50), use_container_width=True)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ğŸ“Š Executive Portfolio Dashboard (Spectacular)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.divider()
        st.subheader("ğŸ“Š Executive Portfolio Dashboard")

        df_src = ss.get("asset_ai_df")
        ft = ss.get("asset_first_table")  # loan-centric projection you already built
        if df_src is None or (hasattr(df_src, "empty") and df_src.empty):
            st.info("Run appraisal to populate the dashboard.")
        else:
            df = df_src.copy()

            # ---- Safe numerics
            def _num(series, default=None):
                s = pd.to_numeric(series, errors="coerce")
                if default is not None:
                    s = s.fillna(default)
                return s

            for c in ["ai_adjusted","realizable_value","loan_amount",
                    "valuation_gap_pct","ltv_ai","ltv_cap","confidence",
                    "condition_score","legal_penalty"]:
                if c in df.columns:
                    df[c] = _num(df[c])

            # ---- KPIs row
            k1, k2, k3, k4, k5 = st.columns(5)
            total_ai        = float(df.get("ai_adjusted", pd.Series(dtype=float)).sum()) if "ai_adjusted" in df.columns else 0.0
            total_realiz    = float(df.get("realizable_value", pd.Series(dtype=float)).sum()) if "realizable_value" in df.columns else 0.0
            avg_conf        = float(df.get("confidence", pd.Series(dtype=float)).mean()) if "confidence" in df.columns else 0.0
            ltv_breach_rate = 0.0
            if {"ltv_ai","ltv_cap"}.issubset(df.columns):
                ltv_breach_rate = float((df["ltv_ai"] > df["ltv_cap"]).mean() * 100)
            approved_cnt = int(df.get("decision","").astype(str).str.lower().eq("approved").sum()) if "decision" in df.columns else 0

            k1.metric("AI Gross Value",       f"${total_ai:,.0f}")
            k2.metric("Realizable Value",     f"${total_realiz:,.0f}")
            k3.metric("Avg Confidence",       f"{avg_conf:.1f}%")
            k4.metric("LTV Breach Rate",      f"{ltv_breach_rate:.1f}%")
            k5.metric("Approved Count",       f"{approved_cnt:,}")

            # ---- Row 1: Top-10 Assets & Decision Mix
            r1c1, r1c2 = st.columns([1.2, 1])
            with r1c1:
                value_col = "realizable_value" if "realizable_value" in df.columns else ("ai_adjusted" if "ai_adjusted" in df.columns else None)
                if value_col:
                    df_top = (df.assign(_val=df[value_col])
                                .sort_values("_val", ascending=False)
                                .head(10))
                    fig_top = px.bar(
                        df_top,
                        x="_val", y=df_top.get("asset_id", df_top.index).astype(str),
                        color="asset_type" if "asset_type" in df_top.columns else None,
                        orientation="h",
                        title=f"Top 10 Assets by {value_col.replace('_',' ').title()}",
                        hover_data=[c for c in ["application_id","asset_id","asset_type","city","_val"] if c in df_top.columns]
                    )
                    fig_top.update_layout(template="plotly_dark", height=380, yaxis_title=None, xaxis_title=value_col)
                    st.plotly_chart(fig_top, use_container_width=True)
            with r1c2:
                names_series = (df["decision"].astype(str).str.title()
                                if "decision" in df.columns
                                else np.where(df.get("policy_breaches","").astype(str).str.len().gt(0),
                                            "Has Breach","No Breach"))
                fig_mix = px.pie(df, names=names_series, title="Decision / Breach Mix")
                fig_mix.update_layout(template="plotly_dark", height=380)
                st.plotly_chart(fig_mix, use_container_width=True)

            # ---- Row 2: By Asset Type & City Concentration
            r2c1, r2c2 = st.columns(2)
            with r2c1:
                if "asset_type" in df.columns:
                    df_type = (df
                            .assign(value=df[value_col] if value_col else 0)
                            .groupby("asset_type", dropna=False)["value"]
                            .sum().sort_values(ascending=False).reset_index())
                    fig_type = px.bar(df_type, x="asset_type", y="value",
                                    title="Value by Asset Type",
                                    text_auto=True)
                    fig_type.update_layout(template="plotly_dark", height=360, xaxis_title=None, yaxis_title="Value")
                    st.plotly_chart(fig_type, use_container_width=True)
            with r2c2:
                if "city" in df.columns and value_col:
                    df_city = (df.groupby("city", dropna=False)[value_col]
                                .sum().sort_values(ascending=False)
                                .head(10).reset_index())
                    fig_city = px.pie(df_city, values=value_col, names="city",
                                    title="Top-10 City Concentration")
                    fig_city.update_layout(template="plotly_dark", height=360)
                    st.plotly_chart(fig_city, use_container_width=True)

            # ---- Row 3: LTV vs Cap & ConditionÃ—Legal Heat
            r3c1, r3c2 = st.columns(2)
            with r3c1:
                if {"ltv_ai","ltv_cap"}.issubset(df.columns):
                    fig_sc = px.scatter(
                        df, x="ltv_cap", y="ltv_ai",
                        color="asset_type" if "asset_type" in df.columns else None,
                        hover_data=[c for c in ["application_id","asset_id","asset_type","city","loan_amount"] if c in df.columns],
                        title="LTV (AI) vs LTV Cap"
                    )
                    try:
                        max_cap = float((df["ltv_cap"].max() or 1.2))
                        fig_sc.add_shape(type="line", x0=0, y0=0, x1=max_cap, y1=max_cap, line=dict(dash="dash"))
                    except Exception:
                        pass
                    fig_sc.update_layout(template="plotly_dark", height=360,
                                        xaxis_title="LTV Cap", yaxis_title="LTV (AI)")
                    st.plotly_chart(fig_sc, use_container_width=True)
            with r3c2:
                if {"condition_score","legal_penalty"}.issubset(df.columns):
                    try:
                        cond_bins  = pd.cut(df["condition_score"], bins=[0,0.70,0.85,1.00], labels=["<0.70","0.70â€“0.85",">0.85"])
                        legal_bins = pd.cut(df["legal_penalty"],  bins=[0,0.97,0.99,1.00], labels=["<0.97","0.97â€“0.99",">=0.99"])
                        heat = (df.assign(cond=cond_bins, legal=legal_bins)
                                .groupby(["cond","legal"]).size().reset_index(name="count"))
                        fig_hm = px.density_heatmap(heat, x="legal", y="cond", z="count",
                                                    title="Condition vs Legal â€” Density")
                        fig_hm.update_layout(template="plotly_dark", height=360)
                        st.plotly_chart(fig_hm, use_container_width=True)
                    except Exception:
                        pass

            # ---- Row 4: City Leaderboard + Per-City Asset List
            st.markdown("### ğŸ™ï¸ City Leaderboard & Assets")
            if "city" in df.columns:
                value_col = value_col or "ai_adjusted"
                city_sum = (df.groupby("city", dropna=False)[value_col]
                            .sum().sort_values(ascending=False).reset_index()
                            .rename(columns={value_col: "total_value"}))
                left, right = st.columns([1, 2])
                with left:
                    st.dataframe(city_sum, use_container_width=True)
                with right:
                    # show top assets per top city
                    top_cities = city_sum["city"].astype(str).head(5).tolist()
                    for city in top_cities:
                        with st.expander(f"ğŸ“ {city} â€” top assets", expanded=False):
                            sub = (df[df["city"].astype(str)==city]
                                .assign(value=df[value_col])
                                .sort_values("value", ascending=False)
                                [[c for c in ["application_id","asset_id","asset_type","value","loan_amount","confidence"] if c in df.columns]]
                                .head(15))
                            st.dataframe(sub, use_container_width=True)


            # ---- Optional Map (if lat/lon present)
            st.markdown("### ğŸ—ºï¸ Map (optional)")
            st.caption("Visualize asset locations â€” map color and style follow the current UI theme.")

            map_cols = [("lat","lon"), ("latitude","longitude"), ("gps_lat","gps_lon")]
            have_map = False

            for la, lo in map_cols:
                if la in df.columns and lo in df.columns:
                    have_map = True
                    map_df = df[[la, lo] + [
                        c for c in ["asset_id","asset_type","city","ai_adjusted","realizable_value","confidence"]
                        if c in df.columns
                    ]].copy()
                    map_df = map_df.rename(columns={la: "lat", lo: "lon"})
                    map_df = map_df.dropna(subset=["lat", "lon"])

                    if not map_df.empty:
                        try:
                            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            # Prefer Plotly (bright light / dark dark)
                            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            import plotly.express as px
                            apply_plotly_mapbox_defaults()
                            style_name = plotly_map_style()

                            fig = px.scatter_mapbox(
                                map_df,
                                lat="lat",
                                lon="lon",
                                hover_name="asset_id" if "asset_id" in map_df.columns else "city",
                                hover_data={c: True for c in ["asset_type","city","ai_adjusted","realizable_value","confidence"] if c in map_df.columns},
                                color_discrete_sequence=["#38bdf8"],
                                zoom=8,
                                height=420,
                            )

                            fig.update_layout(
                                mapbox_style=style_name,
                                margin=dict(l=0, r=0, t=0, b=0),
                                paper_bgcolor="rgba(0,0,0,0)",
                                plot_bgcolor="rgba(0,0,0,0)",
                            )
                            st.plotly_chart(fig, use_container_width=True)

                        except Exception as e:
                            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            # Fallback to pydeck if Plotly unavailable
                            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            import pydeck as pdk
                            view_state = make_pydeck_view_state(
                                lat=float(map_df["lat"].mean()),
                                lon=float(map_df["lon"].mean()),
                                zoom=8
                            )
                            layer = pdk.Layer(
                                "ScatterplotLayer",
                                data=map_df,
                                get_position='[lon, lat]',
                                get_color='[0, 128, 255, 200]',
                                get_radius=120,
                                pickable=True,
                            )
                            deck = pdk.Deck(
                                map_style=pydeck_map_style(),
                                initial_view_state=view_state,
                                layers=[layer],
                                tooltip={"text": "{asset_id} Â· {asset_type}\n{city}\nAI: {ai_adjusted}\nRealiz: {realizable_value}\nConf: {confidence}"}
                            )
                            st.pydeck_chart(deck)
                    else:
                        st.info("â„¹ï¸ No valid coordinates found to display on the map.")
                    break

            if not have_map:
                st.caption("No lat/lon columns found (lat/lon or latitude/longitude or gps_lat/gps_lon). Map hidden.")


            # ---- Exports of aggregates
            st.markdown("#### ğŸ“¤ Export dashboard aggregates")
            exports = {}
            if "asset_type" in df.columns:
                exports["by_asset_type.csv"] = df_type.to_csv(index=False) if 'df_type' in locals() else ""
            if "city" in df.columns and value_col:
                exports["by_city_top10.csv"] = df_city.to_csv(index=False) if 'df_city' in locals() else ""
            if 'df_top' in locals():
                exports["top_assets.csv"] = df_top.drop(columns=["_val"], errors="ignore").to_csv(index=False)

            ex1, ex2, ex3 = st.columns(3)
            for i, (fname, data) in enumerate(exports.items()):
                if not data:
                    continue
                col = [ex1, ex2, ex3][i % 3]
                with col:
                    st.download_button(f"â¬‡ï¸ {fname}", data=data.encode("utf-8"), file_name=fname, mime="text/csv")
                    
            
            # âœ… NEW: Export full AI decision file for Stage E (Human Review)
            st.markdown("### ğŸ§¾ Export AI Decision for Human Review (Stage E)")

            if 'ai_df' in locals() and isinstance(ai_df, pd.DataFrame) and not ai_df.empty:
                ai_export_name = f"ai_decision_stageC_{datetime.now().strftime('%Y%m%d-%H%M%S')}.csv"
                ai_csv_data = ai_df.to_csv(index=False, encoding="utf-8-sig")

                st.download_button(
                    "â¬‡ï¸ Export AI Decisions (send to Stage E)",
                    data=ai_csv_data,
                    file_name=ai_export_name,
                    mime="text/csv",
                    key="dl_ai_stagec_export"
                )
            else:
                st.info("AI table (ai_df) not available â€” run valuation first.")

            # ========================
            # Stage C â€” AI Appraisal & Valuation
            # ========================
            # Now send AI results to Stage E for review
            if st.button("ğŸ’¬ Review in Stage E"):
                # Store AI appraisal results in session_state for Stage E
                st.session_state["ai_review_df"] = ai_df  # ai_df should be the AI results dataframe from the current stage
                st.session_state["current_stage"] = "human_review"
                st.success("AI results sent to Stage E for human review!")
                st.rerun()

            
            # # ========================
            # # Stage C â€” AI Appraisal & Valuation
            # # ========================
            # # Now send AI results to Stage E for review
            # if st.button("ğŸ’¬ Review in Stage E"):
            #     # Store AI appraisal results in session_state for Stage E
            #     st.session_state["ai_review_df"] = ai_df  # ai_df should be the AI results dataframe from the current stage
            #     st.session_state["current_stage"] = "human_review"
            #     st.success("AI results sent to Stage E for human review!")
            #     st.rerun()  # Trigger a page refresh to go to the next stage




# ========== 4) POLICY & DECISION (Stage D: steps 6â€“7) ==========
with tabD:
    st.subheader("ğŸ§® Stage 4 â€” Policy & Decision (D.6 / D.7)")

    import os, json
    import numpy as np
    from datetime import datetime, timezone

    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)
    def _ts(): return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    # ---- Input table: prefer verified â†’ else AI valuation (safe selector) ----
    base_df = first_nonempty_df(ss.get("asset_verified_df"), ss.get("asset_ai_df"))
    if not is_nonempty_df(base_df):
        st.warning("Run Stage C first (valuation, and optionally verification).")
        st.stop()

    st.caption("Input: valuation + (optional) verification outputs.")

    # â”€â”€ D.6 and D.7 continue here (your existing haircuts / caps / breaches / decision code) â”€â”€

    # -------- D.6 â€” Policy & Haircuts â†’ realizable_value --------
    st.markdown("### **D.6 â€” Policy & Haircuts**")
    p1, p2, p3 = st.columns(3)
    with p1:
        base_haircut_pct = st.slider("Base haircut (%)", 0, 60, 10, 1, key="policy_base_haircut")
    with p2:
        condition_weight = st.slider("Condition multiplier min", 0.50, 1.00, 0.80, 0.01, key="policy_cond_min")
    with p3:
        legal_weight = st.slider("Legal multiplier min", 0.50, 1.00, 0.95, 0.01, key="policy_legal_min")

    if st.button("Apply Haircuts", key="btn_apply_haircuts"):
        df = base_df.copy()

        # Ensure necessary inputs exist
        for col, default in [("ai_adjusted", np.nan), ("condition_score", 0.9), ("legal_penalty", 1.0)]:
            if col not in df.columns:
                df[col] = default

        ai_adj = pd.to_numeric(df["ai_adjusted"], errors="coerce")
        cond   = pd.to_numeric(df["condition_score"], errors="coerce").clip(condition_weight, 1.0)
        legal  = pd.to_numeric(df["legal_penalty"],  errors="coerce").clip(legal_weight, 1.0)
        base_cut = (1.0 - float(base_haircut_pct) / 100.0)

        df["realizable_value"] = ai_adj * cond * legal * base_cut

        # Persist artifact
        policy_path = os.path.join(RUNS_DIR, f"policy_haircuts.{_ts()}.csv")
        df.to_csv(policy_path, index=False)

        # âœ… Save Stage D policy results for Stage H
        try:
            # Save into BOTH namespaces safely
            st.session_state["asset_policy_df"] = df.copy()
            ss["asset_policy_df"] = df.copy()

            st.info("âœ… Stage D policy results stored for Stage H.")
        except Exception as e:
            st.warning(f"Could not store Stage D output: {e}")

        st.success(f"Saved: `{policy_path}`")

        # KPIs
        k1, k2, k3 = st.columns(3)
        with k1:
            st.metric(
                "Avg Realizable Value",
                f"{pd.to_numeric(df['realizable_value'], errors='coerce').mean():,.0f}"
            )
        with k2:
            st.metric("Rows", len(df))
        with k3:
            st.metric("Base Haircut", f"{base_haircut_pct}%")

        st.dataframe(df.head(30), use_container_width=True)

        
        # # âœ… Save Stage D policy results for Stage H
        # try:
        #     st.session_state["asset_policy_df"] = df.copy()
        #     ss["asset_policy_df"] = df.copy()
        #     st.info("âœ… Stage D policy results stored for Stage H.")
        # except Exception as e:
        #     st.warning(f"Could not store Stage D output: {e}")

        # st.success(f"Saved: `{policy_path}`")

        # # KPIs
        # k1, k2, k3 = st.columns(3)
        # with k1:
        #     st.metric("Avg Realizable Value", f"{pd.to_numeric(df['realizable_value'], errors='coerce').mean():,.0f}")
        # with k2:
        #     st.metric("Rows", len(df))
        # with k3:
        #     st.metric("Base Haircut", f"{base_haircut_pct}%")

        # st.dataframe(df.head(30), use_container_width=True)

    # # -------- D.6 â€” Policy & Haircuts â†’ realizable_value --------
    # st.markdown("### **D.6 â€” Policy & Haircuts**")
    # p1, p2, p3 = st.columns(3)
    # with p1:
    #     base_haircut_pct = st.slider("Base haircut (%)", 0, 60, 10, 1, key="policy_base_haircut")
    # with p2:
    #     condition_weight = st.slider("Condition multiplier min", 0.50, 1.00, 0.80, 0.01, key="policy_cond_min")
    # with p3:
    #     legal_weight = st.slider("Legal multiplier min", 0.50, 1.00, 0.95, 0.01, key="policy_legal_min")

    # if st.button("Apply Haircuts", key="btn_apply_haircuts"):
    #     df = base_df.copy()

    #     # Ensure necessary inputs exist
    #     for col, default in [("ai_adjusted", np.nan), ("condition_score", 0.9), ("legal_penalty", 1.0)]:
    #         if col not in df.columns:
    #             df[col] = default

    #     ai_adj = pd.to_numeric(df["ai_adjusted"], errors="coerce")
    #     cond   = pd.to_numeric(df["condition_score"], errors="coerce").clip(condition_weight, 1.0)
    #     legal  = pd.to_numeric(df["legal_penalty"],  errors="coerce").clip(legal_weight, 1.0)
    #     base_cut = (1.0 - float(base_haircut_pct) / 100.0)

    #     df["realizable_value"] = ai_adj * cond * legal * base_cut

    #     # Persist policy_haircuts artifact
    #     policy_path = os.path.join(RUNS_DIR, f"policy_haircuts.{_ts()}.csv")
    #     df.to_csv(policy_path, index=False)
        
    #     # âœ… Save Stage D policy for Stage H
    #     st.session_state["asset_policy_df"] = policy_df.copy()

    #     ss["asset_policy_df"] = df
    #     st.success(f"Saved: `{policy_path}`")

    #     # KPIs
    #     k1, k2, k3 = st.columns(3)
    #     with k1:
    #         st.metric("Avg Realizable Value", f"{pd.to_numeric(df['realizable_value'], errors='coerce').mean():,.0f}")
    #     with k2:
    #         st.metric("Rows", len(df))
    #     with k3:
    #         st.metric("Base Haircut", f"{base_haircut_pct}%")

    #     st.dataframe(df.head(30), use_container_width=True)

    # st.markdown("---")

    # -------- D.7 â€” Risk / Decision --------
    st.markdown("### **D.7 â€” Risk / Decision**")

    if ss.get("asset_policy_df") is None:
        st.info("Run D.6 first to compute `realizable_value`.")
    else:
        df = ss["asset_policy_df"].copy()

        # Inputs
        r1, r2, r3 = st.columns(3)
        with r1:
            loan_amount_default = float(pd.to_numeric(df.get("loan_amount", pd.Series([60000])).median()))
            loan_amount = st.number_input("Loan amount (default=median)", value=loan_amount_default, min_value=0.0, step=1000.0, key="risk_loan_amt")
        with r2:
            ltv_mode = st.selectbox("LTV cap mode", ["Fixed cap", "Per asset_type"], index=0, key="risk_ltv_mode")
        with r3:
            fixed_ltv_cap = st.slider("Fixed LTV cap (Ã—)", 0.10, 2.00, 0.80, 0.05, key="risk_ltv_cap_fixed")

        # Per-type caps if requested
        type_caps = {}
        if ltv_mode == "Per asset_type":
            types = sorted(list(map(str, (df.get("asset_type") or pd.Series(["Asset"])).dropna().unique())))[0:10]
            st.caption("Tune LTV caps per asset_type")
            grid = st.columns(4 if len(types) > 3 else max(1, len(types)))
            for i, t in enumerate(types):
                with grid[i % len(grid)]:
                    type_caps[t] = st.number_input(f"{t} cap Ã—", 0.10, 2.00, 0.80, 0.05, key=f"cap_{t}")

        # Thresholds for decisioning
        t1, t2, t3 = st.columns(3)
        with t1:
            min_conf = st.slider("Min confidence (%)", 0, 100, 70, 1, key="risk_min_conf")
        with t2:
            min_cond = st.slider("Min condition_score", 0.60, 1.00, 0.75, 0.01, key="risk_min_cond")
        with t3:
            min_legal = st.slider("Min legal_penalty", 0.80, 1.00, 0.97, 0.01, key="risk_min_legal")

        if st.button("Compute Decision", key="btn_compute_decision"):
            # Compute ltv_ai
            df["ltv_ai"] = pd.to_numeric(loan_amount, errors="coerce") / pd.to_numeric(df.get("ai_adjusted", np.nan), errors="coerce")

            # ltv_cap
            if ltv_mode == "Fixed cap":
                df["ltv_cap"] = float(fixed_ltv_cap)
            else:
                atypes = df.get("asset_type").astype(str) if "asset_type" in df.columns else pd.Series(["Asset"] * len(df))
                df["ltv_cap"] = atypes.map(lambda t: float(type_caps.get(t, fixed_ltv_cap)))

            # Breaches
            conf = pd.to_numeric(df.get("confidence", 100.0), errors="coerce")
            cond = pd.to_numeric(df.get("condition_score", 1.0), errors="coerce")
            legal= pd.to_numeric(df.get("legal_penalty", 1.0),  errors="coerce")
            ltv  = pd.to_numeric(df["ltv_ai"], errors="coerce")
            lcap = pd.to_numeric(df["ltv_cap"], errors="coerce")

            breaches = []
            for i in range(len(df)):
                b = []
                if pd.notna(conf.iat[i]) and conf.iat[i] < min_conf:
                    b.append(f"confidence<{min_conf}%")
                if pd.notna(cond.iat[i]) and cond.iat[i] < min_cond:
                    b.append(f"condition<{min_cond:.2f}")
                if pd.notna(legal.iat[i]) and legal.iat[i] < min_legal:
                    b.append(f"legal<{min_legal:.2f}")
                if pd.notna(ltv.iat[i]) and pd.notna(lcap.iat[i]) and ltv.iat[i] > lcap.iat[i]:
                    b.append("ltv>cap")
                breaches.append(", ".join(b))
            df["policy_breaches"] = breaches

            # Decision rule
            # - reject if LTV>cap OR confidence << min_conf (<= min_conf-10)
            # - review if any breach but not hard reject
            # - approve otherwise
            hard_reject = (
                (ltv > lcap) |
                (pd.to_numeric(conf, errors="coerce") <= (min_conf - 10))
            )
            any_breach = df["policy_breaches"].str.len().gt(0)

            df["decision"] = np.select(
                [
                    hard_reject,
                    any_breach
                ],
                ["reject", "review"],
                default="approve"
            )

            # Persist risk_decision artifact
            risk_path = os.path.join(RUNS_DIR, f"risk_decision.{_ts()}.csv")
            df.to_csv(risk_path, index=False)
            ss["asset_decision_df"] = df
            st.success(f"Saved: `{risk_path}`")

            # KPIs + Table
            k1, k2, k3 = st.columns(3)
            with k1:
                st.metric("Avg LTV (AI)", f"{pd.to_numeric(df['ltv_ai'], errors='coerce').mean():.2f}")
            with k2:
                try:
                    st.metric("Breach Rate", f"{(df['policy_breaches'].str.len().gt(0)).mean():.0%}")
                except Exception:
                    st.metric("Breach Rate", "â€”")
            with k3:
                mix = df["decision"].value_counts(dropna=False)
                st.metric("Approve/Review/Reject", f"{int(mix.get('approve',0))}/{int(mix.get('review',0))}/{int(mix.get('reject',0))}")

            cols_view = [c for c in [
                "application_id","asset_id","asset_type","city",
                "ai_adjusted","realizable_value",
                "loan_amount","ltv_ai","ltv_cap",
                "confidence","condition_score","legal_penalty",
                "policy_breaches","decision"
            ] if c in df.columns]
            st.dataframe(df[cols_view].head(50), use_container_width=True)

            st.download_button(
                "â¬‡ï¸ Download Policy+Decision (CSV)",
                data=df.to_csv(index=False).encode("utf-8"),
                file_name="risk_decision.csv",
                mime="text/csv"
            )



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# E â€” HUMAN REVIEW & FEEDBACK DASHBOARD
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from datetime import datetime, timezone  # ensure available inside this block
import os, glob, json
import numpy as np
import pandas as pd
import plotly.graph_objects as go

with tabE:
    st.subheader("ğŸ§‘â€âš–ï¸ Stage E â€” Human Review & Feedback")
    st.caption("Compare AI-estimated collateral values against business metrics, adjust valuations, and record justification for retraining.")

    
    # Workspace
    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Stage C loader controls (Auto-load + picker)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _find_stage_c_candidates():
        pats = ["valuation_ai*.csv", "valuation_ai*.parquet"]
        files = []
        for pat in pats:
            files.extend(glob.glob(os.path.join(RUNS_DIR, pat)))
        return sorted(files, key=os.path.getmtime, reverse=True)

    if "stage_c_selected_path" not in st.session_state:
        st.session_state["stage_c_selected_path"] = None

    ctrl1, ctrl2, ctrl3 = st.columns([1.2, 1, 2.8])
    with ctrl1:
        btn_autoload = st.button("ğŸ”„ Auto-load latest Stage C", use_container_width=True)
    with ctrl2:
        btn_refresh = st.button("ğŸ” Refresh list", use_container_width=True)
    with ctrl3:
        st.caption("Looks for `valuation_ai*.csv|.parquet` under `./.tmp_runs`")

    if btn_refresh:
        pass  # triggers rerun â†’ list will refresh

    candidates = _find_stage_c_candidates()
    if not candidates:
        st.warning("âš ï¸ No AI appraisal results found. Please complete Stage C first.")
        st.stop()

    # Pick newest on first load or when autoload pressed
    if btn_autoload:
        st.session_state["stage_c_selected_path"] = candidates[0]
    elif not st.session_state["stage_c_selected_path"]:
        st.session_state["stage_c_selected_path"] = candidates[0]
    # Ensure the selected one still exists
    if st.session_state["stage_c_selected_path"] not in candidates:
        st.session_state["stage_c_selected_path"] = candidates[0]

    # Human-friendly label
    def _fmt(p):
        ts = datetime.fromtimestamp(os.path.getmtime(p)).strftime("%Y-%m-%d %H:%M:%S")
        return f"{os.path.basename(p)}  â€¢  {ts}"

    current_idx = candidates.index(st.session_state["stage_c_selected_path"])
    picked = st.selectbox(
        "Stage C output to review",
        options=candidates,
        index=current_idx,
        format_func=_fmt,
    )
    st.session_state["stage_c_selected_path"] = picked

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # âœ… NEW: Direct Upload of Stage C Export (CSV)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ“¤ Or Upload Stage C Export")
    uploaded_c = st.file_uploader(
        "Upload a Stage C file (valuation_ai*.csv)",
        type=["csv"],
        key="stage_c_upload"
    )

    if uploaded_c is not None:
        try:
            df_ai = pd.read_csv(uploaded_c)
            #st.success(f"âœ… Imported uploaded file ({len[df_ai)} rows).")
            st.success(f"âœ… Imported uploaded file ({len(df_ai)} rows).")


            temp_path = os.path.join(RUNS_DIR, f"uploaded_stage_c_{datetime.now().timestamp()}.csv")
            df_ai.to_csv(temp_path, index=False, encoding="utf-8-sig")

            st.session_state["stage_c_selected_path"] = temp_path
            st.session_state["df_ai_current"] = df_ai.copy()

            st.rerun()

        except Exception as e:
            st.error(f"Upload failed: {e}")
    

    # Load the selected Stage C table â†’ df_ai
    ai_path = st.session_state["stage_c_selected_path"]
    try:
        if ai_path.lower().endswith(".parquet"):
            df_ai = pd.read_parquet(ai_path)
        else:
            df_ai = pd.read_csv(ai_path)
        st.success(f"âœ… Loaded Stage C: {os.path.basename(ai_path)}  ({len(df_ai)} rows Ã— {df_ai.shape[1]} cols)")
    except Exception as e:
        st.error(f"Failed to read `{ai_path}`: {e}")
        st.stop()

    # Ensure join keys exist to avoid editor KeyErrors later
    for col in ["application_id", "asset_id", "asset_type", "city"]:
        if col not in df_ai.columns:
            df_ai[col] = None

    # Ensure human_value / justification columns for adjustments
    if "human_value" not in df_ai.columns:
        df_ai["human_value"] = pd.to_numeric(df_ai["fmv"], errors="coerce") if "fmv" in df_ai.columns else np.nan
    if "justification" not in df_ai.columns:
        df_ai["justification"] = ""

    

    # â”€â”€ Market Projections (safe)
    st.markdown("### ğŸ“ˆ Market Projections")
    horizon = st.select_slider("Projection Horizon (years)", options=[3, 5, 10], value=5)
    growth = st.slider("Expected Market Growth (%)", -10, 25, 4) / 100

    df_proj = df_ai.copy()
    if "fmv" in df_proj.columns:
        fmv_num = pd.to_numeric(df_proj["fmv"], errors="coerce")
        df_proj[f"fmv_proj_{horizon}y"] = (fmv_num * ((1 + growth) ** horizon)).round(0)
        st.line_chart(df_proj[["fmv", f"fmv_proj_{horizon}y"]])
    else:
        st.info("FMV column not found; projection chart will appear after you run Stage C.")

    # âœ… Helper: return the first column present in dataframe
    def _first_present(df, candidates):
        for c in candidates:
            if c in df.columns:
                return c
        return None


    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # âœ… Human Adjustment Table (LIVE + REFRESH SAFE)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### âœï¸ Human Adjustments & Justification")

    


    # Ensure editable columns exist
    if "human_value" not in df_ai.columns:
        df_ai["human_value"] = pd.to_numeric(df_ai["fmv"], errors="coerce") if "fmv" in df_ai.columns else np.nan
    if "justification" not in df_ai.columns:
        df_ai["justification"] = ""

     # Editable columns for the reviewer
    base_editable = ["application_id", "asset_id", "asset_type", "city", "fmv", "ai_adjusted", "confidence", "loan_amount", "human_value", "justification"]
    editable_cols = [c for c in base_editable if c in df_ai.columns]  # filter to present
    if not editable_cols:
        editable_cols = df_ai.columns.tolist()  # last resort: allow full frame
    
    # Display the editable table for human review

    edited = st.data_editor(df_ai[editable_cols], num_rows="dynamic", use_container_width=True)

    # â”€â”€ Agreement / Deviation Gauge + Mismatch list
    st.markdown("### ğŸ¯ Human vs AI Agreement / Deviation")

    # Resolve decision columns if present
    def _first_present(df, candidates):
        return next((c for c in candidates if c in df.columns), None)

    ai_dec_col = _first_present(edited, ["ai_decision", "ai_label", "ai_outcome", "decision_ai"])
    human_dec_col = _first_present(edited, ["human_decision", "human_label", "final_decision", "decision_human"])

    if ai_dec_col and human_dec_col:
        # Agreement gauge (%)
        a = edited[ai_dec_col].astype(str).str.strip().str.lower()
        h = edited[human_dec_col].astype(str).str.strip().str.lower()
        matches = (a == h)
        agree_pct = float(matches.mean() * 100.0) if len(matches) else 0.0

        fig = go.Figure(go.Indicator(
            mode="gauge+number",
            value=round(agree_pct, 2),
            number={'suffix': '%'},
            gauge={
                'axis': {'range': [0, 100]},
                'bar': {'thickness': 0.35},
                'steps': [
                    {'range': [0, 50], 'color': '#fee2e2'},
                    {'range': [50, 80], 'color': '#fef9c3'},
                    {'range': [80, 100], 'color': '#dcfce7'},
                ],
                'threshold': {'line': {'color': '#2563eb', 'width': 4}, 'thickness': 0.9, 'value': round(agree_pct, 2)}
            },
            title={'text': "AI â†” Human Agreement"}
        ))
        st.plotly_chart(fig, use_container_width=True)

        # Mismatch table (if any)
        mis_df = edited.loc[~matches].copy()
        key_cols = [c for c in ["application_id", "asset_id", "asset_type", "city"] if c in edited.columns]
        value_ai = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
        value_hu = _first_present(edited, ["human_value", "reviewed_value", "final_value"])

        if not mis_df.empty:
            show_cols = key_cols + [c for c in [ai_dec_col, human_dec_col, value_ai, value_hu, "justification"] if c]
            show_cols = [c for c in show_cols if c in mis_df.columns]
            st.markdown("#### ğŸ” Mismatches â€” what did humans change?")
            st.dataframe(mis_df[show_cols].head(300), use_container_width=True, hide_index=True)
        else:
            st.success("ğŸ‰ Perfect agreement â€” no mismatches.")
    else:
        # Fall back to deviation score if decisions are not present
        if all(c in edited.columns for c in ("human_value", "fmv")):
            hv = pd.to_numeric(edited["human_value"], errors="coerce")
            fmv = pd.to_numeric(edited["fmv"], errors="coerce").replace(0, np.nan)
            deviation = (hv - fmv).abs() / fmv
            score = max(0.0, 100.0 - float(np.nanmean(deviation) * 200.0)) if len(deviation) else 0.0

            fig = go.Figure(go.Indicator(
                mode="gauge+number",
                value=round(score, 1),
                number={'suffix': ' / 100'},
                gauge={
                    'axis': {'range': [0, 100]},
                    'bar': {'thickness': 0.35},
                    'steps': [
                        {'range': [0, 50], 'color': '#fee2e2'},
                        {'range': [50, 80], 'color': '#fef9c3'},
                        {'range': [80, 100], 'color': '#dcfce7'},
                    ],
                    'threshold': {'line': {'color': '#2563eb', 'width': 4}, 'thickness': 0.9, 'value': round(score, 1)}
                },
                title={'text': "Alignment Score (by value deviation)"}
            ))
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Provide decision columns (ai_decision / human_decision) for agreement gauge, or both FMV and human_value for deviation.")

    
        # â”€â”€ Human Changes Only (colored)
        st.markdown("### ğŸ–ï¸ Human Changes Only (colored)")

        # Reuse helper and edited df from above
        value_ai = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
        value_hu = _first_present(edited, ["human_value", "reviewed_value", "final_value"])
        ai_dec_col = _first_present(edited, ["ai_decision", "ai_label", "ai_outcome", "decision_ai"])
        human_dec_col = _first_present(edited, ["human_decision", "human_label", "final_decision", "decision_human"])

        if value_ai and value_hu:
            ai_vals = pd.to_numeric(edited[value_ai], errors="coerce")
            hu_vals = pd.to_numeric(edited[value_hu], errors="coerce")

            # decisions -> Series aligned to edited.index
            if ai_dec_col and human_dec_col:
                a = edited[ai_dec_col].astype(str).str.strip().str.lower()
                h = edited[human_dec_col].astype(str).str.strip().str.lower()
                dec_changed = (a != h)  # Series
            else:
                dec_changed = pd.Series(False, index=edited.index)

            # justification -> Series aligned
            just_present = edited.get("justification", pd.Series("", index=edited.index)) \
                                .astype(str).str.strip().ne("")

            # treat tiny diffs as equal
            rel_tol = 1e-9
            val_changed = (ai_vals.fillna(np.nan) - hu_vals.fillna(np.nan)).abs() > (
                (ai_vals.abs() + hu_vals.abs()).fillna(0) * rel_tol
            )

            changed_mask = val_changed | dec_changed | just_present
            diff_df = edited.loc[changed_mask].copy()

            if diff_df.empty:
                st.success("ğŸ‰ No human changes detected.")
            else:
                # compute deltas on the FILTERED subset ONLY
                ai_sub = ai_vals.reindex(diff_df.index)
                hu_sub = hu_vals.reindex(diff_df.index)

                diff_df["Î”_value"] = (hu_sub - ai_sub)
                base = ai_sub.replace(0, np.nan)
                diff_df["Î”_%"] = ((diff_df["Î”_value"] / base) * 100.0).round(2)

                key_cols = [c for c in ["application_id", "asset_id", "asset_type", "city"] if c in diff_df.columns]
                show_cols = key_cols + [c for c in [value_ai, value_hu, "Î”_value", "Î”_%", ai_dec_col, human_dec_col, "justification"] if c in diff_df.columns]
                show_df = diff_df[show_cols].copy()

                def _color_row(row):
                    styles = [""] * len(row.index)

                    def _idx(colname):
                        try:
                            return show_df.columns.get_loc(colname)
                        except Exception:
                            return None

                    idx_ai = _idx(value_ai)
                    idx_hu = _idx(value_hu)
                    idx_dv = _idx("Î”_value")
                    idx_dp = _idx("Î”_%")

                    # Value changes
                    try:
                        ai_v = float(row.get(value_ai, np.nan))
                        hu_v = float(row.get(value_hu, np.nan))
                    except Exception:
                        ai_v, hu_v = np.nan, np.nan

                    if pd.notna(ai_v) and pd.notna(hu_v):
                        if hu_v > ai_v:  # green for up
                            for i in [idx_hu, idx_dv, idx_dp]:
                                if i is not None:
                                    styles[i] = "background-color:#dcfce7; color:#065f46; font-weight:600;"
                            if idx_ai is not None:
                                styles[idx_ai] = "background-color:#ecfdf5; color:#064e3b;"
                        elif hu_v < ai_v:  # red for down
                            for i in [idx_hu, idx_dv, idx_dp]:
                                if i is not None:
                                    styles[i] = "background-color:#fee2e2; color:#7f1d1d; font-weight:600;"
                            if idx_ai is not None:
                                styles[idx_ai] = "background-color:#fef2f2; color:#7f1d1d;"

                    # Decision changes â†’ amber
                    if ai_dec_col in show_df.columns and human_dec_col in show_df.columns:
                        ai_d = str(row.get(ai_dec_col, "")).strip().lower()
                        hu_d = str(row.get(human_dec_col, "")).strip().lower()
                        if ai_d != "" and hu_d != "" and ai_d != hu_d:
                            for colname in [ai_dec_col, human_dec_col]:
                                j = _idx(colname)
                                if j is not None:
                                    styles[j] = "background-color:#fef9c3; color:#7c2d12; font-weight:600;"

                    # Justification present â†’ blue
                    if "justification" in show_df.columns:
                        just = str(row.get("justification", "")).strip()
                        if just:
                            j = _idx("justification")
                            if j is not None:
                                styles[j] = "background-color:#e0f2fe; color:#0c4a6e;"

                    return styles

                styled = show_df.style.apply(_color_row, axis=1) \
                                    .format({value_ai: "{:,.0f}", value_hu: "{:,.0f}", "Î”_value": "{:,.0f}", "Î”_%": "{:.2f}%"})
                st.dataframe(styled, use_container_width=True, hide_index=True)
        else:
            st.info("To show the colorful Human-Changes table, ensure value columns exist (e.g., ai_adjusted/fmv and human_value).")

        

    
    # â”€â”€ Export for Retraining
    st.markdown("### ğŸ’¾ Save & Export for Training")
    # Lightweight export view for training: keep keys + AI/Human value/decisions if present
    train_cols_base = ["application_id", "asset_id", "asset_type", "city"]
    ai_val_col = _first_present(edited, ["ai_adjusted", "fmv", "predicted_value"])
    hu_val_col = _first_present(edited, ["human_value", "reviewed_value", "final_value"])
    keep_cols = [c for c in train_cols_base if c in edited.columns] + \
                [c for c in [ai_dec_col, human_dec_col, ai_val_col, hu_val_col, "confidence", "loan_amount", "justification"] if c in edited.columns]
    export_df = edited[keep_cols].copy() if keep_cols else edited.copy()

    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
    out_path = os.path.join(RUNS_DIR, f"reviewed_appraisal.{ts}.csv")

    cE1, cE2 = st.columns([1.2, 1])
    with cE1:
        st.text_input("Will save to (server path)", out_path, label_visibility="collapsed")
    with cE2:
        st.download_button(
            "â¬‡ï¸ Download Human vs AI CSV",
            export_df.to_csv(index=False).encode("utf-8-sig"),
            file_name=os.path.basename(out_path),
            mime="text/csv",
            key="dl_reviewed_appraisal_stageE"
        )

    if st.button("ğŸ’¾ Save Human Feedback (server)", key="btn_save_feedback"):
        try:
            export_df.to_csv(out_path, index=False, encoding="utf-8-sig")
            st.success(f"âœ… Saved human-reviewed data â†’ `{out_path}`")
        except Exception as e:
            st.error(f"Save failed: {e}")


# # ============================================================
# # âœ… STAGE F FOOTER FUNCTION â€” must be defined BEFORE Stage F
# # ============================================================

def render_stage_f_footer(
    new_m, prod_m, RUNS_DIR, model_path, report,
    df_train=None, yte=None, y_pred_new=None
):
    import streamlit as st
    import pandas as pd
    import numpy as np
    import os, json, glob, shutil, zipfile
    from datetime import datetime, timezone
    import plotly.express as px

    st.markdown("## ğŸ§­ Executive Model Evaluation Dashboard (Stage F)")

    # -----------------------------------------
    # âœ… Compute deltas
    # -----------------------------------------
    if prod_m:
        delta_mae = (prod_m["MAE"] - new_m["MAE"]) / prod_m["MAE"] * 100
        delta_rmse = (prod_m["RMSE"] - new_m["RMSE"]) / prod_m["RMSE"] * 100
        delta_mape = (prod_m["MAPE%"] - new_m["MAPE%"]) / prod_m["MAPE%"] * 100
        delta_r2  = (new_m["R2"] - prod_m["R2"]) * 100

        improved = {
            "MAE": delta_mae > 0,
            "RMSE": delta_rmse > 0,
            "MAPE%": delta_mape > 0,
            "R2": delta_r2 > 0
        }

        # Main headline message
        headline = f"âœ… The new model outperforms the production model by **{delta_mae:+.1f}% MAE** and **{delta_r2:+.2f} RÂ² points**."
        headline_color = "#D1FAE5"  # greenish
        reward_phrase = "âœ” This is a strong improvement and beneficial for production use."
    else:
        headline = "ğŸŸ¢ First model trained â€” this will become the initial production baseline."
        headline_color = "#DBEAFE"  # blueish
        reward_phrase = "âœ” You can safely promote this model."

    # -----------------------------------------
    # âœ… WHAT â€” Big one-sentence discovery
    # -----------------------------------------
    st.markdown(
        f"""
        <div style="
            padding: 18px;
            border-radius: 12px;
            background-color: {headline_color};
            font-size: 1.3rem;
            font-weight: 600;
        ">
        {headline}
        </div>
        """,
        unsafe_allow_html=True
    )

    # -----------------------------------------
    # âœ… SO WHAT â€” Why does this matter?
    # -----------------------------------------
    st.markdown("### ğŸ§ SO WHAT â€” Why does this matter?")
    if prod_m:
        st.write(
            f"""
            The new model shows measurable improvements across key financial and ML metrics:

            - **MAE** (Average absolute error) improved by **{delta_mae:+.1f}%**  
            - **RMSE** (Hard penalties on large mismatches) improved by **{delta_rmse:+.1f}%**  
            - **MAPE** (Percentage error relative to asset value) improved by **{delta_mape:+.1f}%**  
            - **RÂ²** (How well the model explains variance) improved by **{delta_r2:+.2f} points**  

            These metrics together mean:
            - âœ… More accurate valuation predictions  
            - âœ… Smaller high-error outliers  
            - âœ… Better stability with fewer â€œshocksâ€  
            - âœ… Higher confidence for underwriting, credit, and collateral decisions  
            """
        )
    else:
        st.info(
            """
            Since there is **no existing production model**, this trained model becomes 
            the best available baseline for your valuation pipeline.
            """
        )

    # -----------------------------------------
    # âœ… KEY COMPARISON TABLE
    # -----------------------------------------
    st.markdown("### ğŸ“Š Metric Comparison (New vs Production)")

    if prod_m:
        df_cmp = pd.DataFrame([
            ["MAE",   f"{new_m['MAE']:,.0f}",   f"{prod_m['MAE']:,.0f}",   f"{delta_mae:+.1f}%",  "Lower is better"],
            ["RMSE",  f"{new_m['RMSE']:,.0f}",  f"{prod_m['RMSE']:,.0f}",  f"{delta_rmse:+.1f}%", "Penalizes large errors"],
            ["MAPE%", f"{new_m['MAPE%']:.2f}%", f"{prod_m['MAPE%']:.2f}%", f"{delta_mape:+.1f}%", "Percent accuracy"],
            ["RÂ²",    f"{new_m['R2']:.3f}",     f"{prod_m['R2']:.3f}",     f"{delta_r2:+.2f}",    "Explained variance"],
        ], columns=["Metric", "New Model", "Production", "Î” (Change)", "Meaning"])
    else:
        df_cmp = pd.DataFrame([
            ["MAE",   f"{new_m['MAE']:,.0f}",   "â€”",  "â€”", "Lower is better"],
            ["RMSE",  f"{new_m['RMSE']:,.0f}",  "â€”",  "â€”", "Penalizes large errors"],
            ["MAPE%", f"{new_m['MAPE%']:.2f}%", "â€”",  "â€”", "Percent accuracy"],
            ["RÂ²",    f"{new_m['R2']:.3f}",     "â€”",  "â€”", "Explained variance"],
        ], columns=["Metric", "New Model", "Production", "Î” (Change)", "Meaning"])

    st.table(df_cmp)

    # -----------------------------------------
    # âœ… NOW WHAT â€” Recommended Action
    # -----------------------------------------
    st.markdown("### ğŸš€ NOW WHAT â€” Recommended Next Action")

    if not prod_m or (delta_mae > 0 and delta_r2 > 0):
        st.success(
            f"""
            ### âœ… Recommendation: **Promote the new model to production.**

            {reward_phrase}

            #### Why?
            - It reduces valuation errors.
            - It improves consistency and confidence scores.
            - It captures market variance better (higher RÂ²).
            - It reduces underwriting risk.
            - It generates more stable predictions for credit, risk & collateral workflows.
            """
        )
        promote_ready = True
    else:
        st.warning(
            f"""
            ### âš ï¸ Recommendation: **Do NOT promote yet.**

            Some metrics degrade when compared to production.

            #### Before promoting:
            - Tune hyperparameters  
            - Add more diverse training samples  
            - Validate anomalies / outliers  
            - Re-check human_value labels from Stage E  
            """
        )
        promote_ready = False

    # -----------------------------------------
    # âœ… Next Steps Checklist
    # -----------------------------------------
    st.markdown("### âœ… Next Steps Checklist")

    if promote_ready:
        st.markdown(
            """
            âœ… Promote to production  
            âœ… Export ZIP bundle  
            âœ… Notify Credit / Risk agents  
            âœ… Schedule monitoring in Stage I  
            âœ… Optional: widen training dataset  
            """
        )
    else:
        st.markdown(
            """
            ğŸ”„ Retrain with more data  
            ğŸ§¹ Clean labeling inconsistencies  
            ğŸ” Inspect outliers via residual plots  
            ğŸ”§ Try Gradient Boosting or Random Forest  
            """
        )

    # -----------------------------------------
    # âœ… Show Drift Trend (mini chart)
    # -----------------------------------------
    st.markdown("### ğŸ“ˆ Performance Trend (MAE & RÂ² over time)")
    reports = sorted(glob.glob(os.path.join(RUNS_DIR, "training_report_*.json")), reverse=True)[:10]
    trend = []
    for f in reports:
        try:
            with open(f) as jf:
                rep = json.load(jf)
            trend.append({
                "timestamp": rep["timestamp"],
                "MAE": rep["metrics_new"]["MAE"],
                "R2": rep["metrics_new"]["R2"],
            })
        except:
            pass

    if trend:
        df_tr = pd.DataFrame(trend).sort_values("timestamp")
        st.line_chart(df_tr.set_index("timestamp")[["MAE", "R2"]])

    # -----------------------------------------
    # âœ… Promotion Button
    # -----------------------------------------
    st.markdown("### ğŸ“¤ Promote to Production")

    if st.button("âœ… Promote This Model Now"):
        try:
            #prod_dir = "./agents/asset_appraisal/models/production"
            prod_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/production"

            
            os.makedirs(prod_dir, exist_ok=True)

            shutil.copy(model_path, os.path.join(prod_dir, "model.joblib"))
            json.dump(
                {"model_path": model_path, "promoted_at": datetime.now(timezone.utc).isoformat(), "report": report},
                open(os.path.join(prod_dir, "production_meta.json"), "w"),
                indent=2
            )
            st.balloons()
            st.success("âœ… Model promoted successfully!")
        except Exception as e:
            st.error(f"âŒ Promotion failed: {e}")
    



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# F â€” MODEL TRAINING & PROMOTION (A/B with Prod)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tabF:
    import os, json, glob
    from datetime import datetime, timezone
    import numpy as np
    import pandas as pd
    import plotly.graph_objects as go
    import plotly.express as px
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
    from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
    from sklearn.linear_model import LinearRegression
    import joblib
    import shutil
    from pathlib import Path


    st.subheader("ğŸ§ª Stage F â€” Model Training & Promotion")
    st.caption("Train or retrain with human feedback, compare against production (A/B), and promote if better.")
    
    
    # -----------------------------------------
    # âœ… HOW TO USE THIS TRAINING STAGE (Dark / Collapsible)
    # -----------------------------------------
    with st.expander("ğŸ§­ How this stage works", expanded=False):
        st.markdown("""
        <div style="
            padding:18px;
            border-radius:12px;
            background:linear-gradient(145deg,#0d1829,#10243d);
            border:1px solid #1e3a5f;
            color:#e2e8f0;
            font-size:1.05rem;
            line-height:1.55;
        ">
        <b>ğŸ“˜ How this stage works:</b><br><br>
        You are now in <b>Stage F</b>, where your appraisal model is trained, compared, and prepared for production.
        <br>This stage takes the <b>human-reviewed values</b> produced in Stage E and builds a model that predicts
        future valuations with better accuracy.
        <br><br><b>âœ… Follow these steps:</b><br>
        <b>1ï¸âƒ£ Load Training Data</b><br>â€¢ Auto-detect latest <code>reviewed_appraisal*.csv</code> from Stage E.<br>
        â€¢ Or upload CSV with <code>human_value</code> labels.
        <br><br><b>2ï¸âƒ£ Select Features</b><br>â€¢ Numeric columns auto-selected; leakage columns excluded.
        <br><br><b>3ï¸âƒ£ Choose a Model</b><br>Select GradientBoosting, RandomForest, or LinearRegression (fast cycle).
        <br><br><b>4ï¸âƒ£ Train & Compare</b><br>â€¢ Train on data â†’ evaluate holdout â†’ A/B compare if baseline exists.
        <br><br><b>5ï¸âƒ£ Review Metrics & Insights</b><br>â€¢ Actual vs predicted charts, residuals, importance, summary.
        <br><br><b>6ï¸âƒ£ Save / Promote / Export</b><br>â€¢ Promote best model â†’ Stage G ZIP bundle.
        <br><br><b>ğŸ¯ Goal:</b> Produce a model thatâ€™s <b>more accurate, more stable, and more explainable</b>.
        </div>
        """, unsafe_allow_html=True)

    # # -----------------------------------------
    # # âœ… HOW TO USE THIS TRAINING STAGE
    # # -----------------------------------------
    # st.markdown("""
    # <div style="
    #     padding: 18px;
    #     border-radius: 12px;
    #     background-color: #EFF6FF;
    #     border-left: 6px solid #2563EB;
    #     font-size: 1.05rem;
    # ">
    # <b>ğŸ“˜ How this stage works:</b><br><br>

    # You are now in <b>Stage F</b>, where your appraisal model is trained, compared, and prepared for production.

    # This stage takes the <b>human-reviewed values</b> produced in Stage E and builds a model that predicts future valuations with better accuracy.

    # <br><br>

    # <b>âœ… Follow these steps:</b><br>

    # <b>1ï¸âƒ£ Load Training Data</b><br>
    # â€¢ The system auto-detects your latest <code>reviewed_appraisal*.csv</code> from Stage E.  
    # â€¢ If you prefer, upload a new CSV containing <code>human_value</code> labels.  

    # <br>

    # <b>2ï¸âƒ£ Select Features</b><br>
    # â€¢ Numeric and relevant features are automatically selected.  
    # â€¢ ID columns and leakage columns (asset_id, ai_adjusted, etc.) are excluded.

    # <br>

    # <b>3ï¸âƒ£ Choose a Model</b><br>
    # Select an algorithm (GradientBoosting, RandomForest, LinearRegression).  
    # The system will auto-tune nothingâ€”this is a fast-iteration training cycle.

    # <br>

    # <b>4ï¸âƒ£ Train & Compare</b><br>
    # â€¢ The model trains on your data.  
    # â€¢ A holdout test set evaluates performance.  
    # â€¢ If a production model exists, an A/B comparison is displayed.  

    # <br>

    # <b>5ï¸âƒ£ Review Metrics & Insights</b><br>
    # â€¢ Actual vs predicted charts  
    # â€¢ Residual distributions  
    # â€¢ Feature importance analysis  
    # â€¢ Executive summary (WHAT â†’ SO WHAT â†’ NOW WHAT)  
    # â€¢ AI recommendation (promote / retrain)

    # <br>

    # <b>6ï¸âƒ£ Save, Promote or Export</b><br>
    # â€¢ Save trained models  
    # â€¢ Promote to production (Stage G uses it for ZIP packaging)  
    # â€¢ Export full ZIP bundles for deployment (AWS S3, Swift, GitHub)

    # <br><br>

    # <b>ğŸ¯ Goal of Stage F:</b><br>
    # Produce a model that is <b>more accurate, more stable, and more explainable</b> than your current production baseline â€” and ready for deployment in Stage G.
    # </div>
    # """, unsafe_allow_html=True)

    

    # ---------- helpers ----------
    def _ts():
        return datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    def _rmse(y_true, y_pred):
        return float(np.sqrt(mean_squared_error(y_true, y_pred)))

    def _mape(y_true, y_pred):
        y_true = np.asarray(y_true, dtype=float)
        y_pred = np.asarray(y_pred, dtype=float)
        mask = (y_true != 0) & np.isfinite(y_true) & np.isfinite(y_pred)
        if not mask.any():
            return float("nan")
        return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0)

    def _safe_len_df(x):
        return (0 if not isinstance(x, pd.DataFrame) else len(x))

    # ---------- diagnostics (always visible) ----------
    st.markdown("#### ğŸ” Data availability (snapshots)")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("decision_df",  _safe_len_df(ss.get("asset_decision_df")))
    c2.metric("policy_df",    _safe_len_df(ss.get("asset_policy_df")))
    c3.metric("verified_df",  _safe_len_df(ss.get("asset_verified_df")))
    c4.metric("ai_df",        _safe_len_df(ss.get("asset_ai_df")))

    with st.expander("Load demo portfolio (if earlier stages not run)"):
        if st.button("Load demo portfolio (10 rows)", key="btn_demo_portfolio"):
            rng = np.random.default_rng(42)
            demo = pd.DataFrame({
                "application_id": [f"APP_{i:04d}" for i in range(10)],
                "asset_id":      [f"A{i:04d}" for i in range(10)],
                "asset_type":    rng.choice(["House","Apartment","Car","Land"], 10),
                "city":          rng.choice(["HCMC","Hanoi","Da Nang","Hue"], 10),
                "market_value":  rng.integers(80_000, 800_000, 10),
                "ai_adjusted":   rng.integers(75_000, 820_000, 10),
                "loan_amount":   rng.integers(30_000, 500_000, 10),
                "confidence":    rng.integers(60, 98, 10),
                "condition_score": rng.uniform(0.6, 1.0, 10).round(3),
                "legal_penalty":   rng.uniform(0.95, 1.0, 10).round(3),
                "human_value":   rng.integers(75_000, 820_000, 10),
            })
            ss["asset_decision_df"] = demo
            st.success("Demo portfolio loaded into ss['asset_decision_df'].")

    st.divider()

    # ---------- training data source ----------
    RUNS_DIR = "./.tmp_runs"
    os.makedirs(RUNS_DIR, exist_ok=True)

    # Auto-pick latest reviewed CSV from Stage E
    reviewed = sorted([f for f in os.listdir(RUNS_DIR)
                       if f.startswith("reviewed_appraisal") and f.endswith(".csv")], reverse=True)
    df_train = None
    auto_path = None
    if reviewed:
        auto_path = os.path.join(RUNS_DIR, reviewed[0])
        try:
            df_train = pd.read_csv(auto_path)
        except Exception as e:
            st.warning(f"Could not read `{auto_path}`: {e}")

    st.markdown("#### ğŸ“¥ Training dataset")
    colU1, colU2 = st.columns([1.4, 1])
    with colU1:
        st.text_input("Auto-detected Stage E file", value=(auto_path or "â€”"), disabled=True)
    with colU2:
        up = st.file_uploader("Or upload CSV with human_value", type=["csv"], key="train_csv_upload")

    if up is not None:
        try:
            df_train = pd.read_csv(up)
            st.success(f"Loaded uploaded CSV ({len(df_train)} rows).")
        except Exception as e:
            st.error(f"Upload read failed: {e}")

    if df_train is None or df_train.empty:
        st.warning("âš ï¸ No training data available. Use Stage E to export `reviewed_appraisal*.csv` or upload a CSV above.")
        st.stop()

    st.markdown(f"**Using training rows:** {len(df_train):,}")
    st.dataframe(df_train.head(20), use_container_width=True)

    # ---------- feature building ----------
    st.markdown("#### ğŸ§± Feature selection")
    target_col = "human_value"
    if target_col not in df_train.columns:
        st.error("CSV must include a 'human_value' column (target).")
        st.stop()

    # Exclude obvious leak/IDs/targets from X
    drop_cols = {
        target_col, "fmv", "ai_adjusted",  # avoid leakage; AI numbers used only for comparison
        "ai_decision", "human_decision", "decision", "final_decision",
        "justification", "reviewed_value", "final_value",
        "application_id", "asset_id", "asset_type", "city"
    }
    num_cols = [c for c in df_train.columns
                if c not in drop_cols and pd.api.types.is_numeric_dtype(df_train[c])]

    if not num_cols:
        st.error("No numeric features left after filtering. Please include numeric columns for training.")
        st.stop()

    X = df_train[num_cols].copy()
    y = pd.to_numeric(df_train[target_col], errors="coerce")

    # Drop rows with missing target
    mask = pd.notna(y)
    X, y = X.loc[mask], y.loc[mask]

    # Train/Test split
    test_size = st.slider("Holdout size", 10, 40, 20, step=5) / 100.0
    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, random_state=42)

    # ---------- model choice ----------
    st.markdown("#### ğŸ¤– Choose model")
    model_choice = st.selectbox(
        "Select model algorithm",
        ["GradientBoostingRegressor", "RandomForestRegressor", "LinearRegression"],
        index=0
    )
    ModelCls = {
        "GradientBoostingRegressor": GradientBoostingRegressor,
        "RandomForestRegressor": RandomForestRegressor,
        "LinearRegression": LinearRegression,
    }[model_choice]

    # ---------- train & compare ----------
    if st.button("ğŸš€ Train & Compare (A/B vs Production)", key="btn_train_model"):
        # Train new
        new_model = ModelCls().fit(Xtr, ytr)
        y_pred_new = new_model.predict(Xte)

        # Load production baseline if exists
        prod_model_path = "./agents/asset_appraisal/models/production/model.joblib"
        prod_exists = os.path.exists(prod_model_path)
        y_pred_prod = None
        if prod_exists:
            try:
                prod_model = joblib.load(prod_model_path)
                # guard: try only if shapes align
                y_pred_prod = prod_model.predict(Xte)
            except Exception as e:
                st.warning(f"Production model failed to score holdout: {e}")

        # Metrics
        def _metrics(y_true, y_pred):
            return {
                "MAE": float(mean_absolute_error(y_true, y_pred)),
                "RMSE": _rmse(y_true, y_pred),
                "MAPE%": _mape(y_true, y_pred),
                "R2": float(r2_score(y_true, y_pred)),
            }

        new_m = _metrics(yte, y_pred_new)
        prod_m = _metrics(yte, y_pred_prod) if y_pred_prod is not None else None

        # ===== Dashboard: KPIs & deltas =====
        st.markdown("### ğŸ“Š A/B Metrics (Holdout)")
        k1, k2, k3, k4, k5 = st.columns(5)
        with k1:
            st.metric("New MAE", f"{new_m['MAE']:,.0f}",
                      delta=(f"{(new_m['MAE'] - prod_m['MAE']):+.0f}" if prod_m else None))
        with k2:
            st.metric("New RMSE", f"{new_m['RMSE']:,.0f}",
                      delta=(f"{(new_m['RMSE'] - prod_m['RMSE']):+.0f}" if prod_m else None))
        with k3:
            st.metric("New MAPE", f"{new_m['MAPE%']:.2f}%",
                      delta=(f"{(new_m['MAPE%'] - prod_m['MAPE%']):+.2f}%" if prod_m else None))
        with k4:
            st.metric("New RÂ²", f"{new_m['R2']:.3f}",
                      delta=(f"{(new_m['R2'] - prod_m['R2']):+.3f}" if prod_m else None))
        with k5:
            st.metric("Test rows", f"{len(yte):,}")

        # ===== Plots: Actual vs Pred, Residuals =====
        plot_df = pd.DataFrame({
            "y_true": yte.values,
            "y_pred_new": y_pred_new,
            "y_pred_prod": (y_pred_prod if y_pred_prod is not None else np.full_like(y_pred_new, np.nan))
        })

        # Actual vs Pred overlay
        fig_scatter = go.Figure()
        fig_scatter.add_trace(go.Scatter(
            x=plot_df["y_true"], y=plot_df["y_pred_new"],
            mode="markers", name="New", opacity=0.7
        ))
        if y_pred_prod is not None:
            fig_scatter.add_trace(go.Scatter(
                x=plot_df["y_true"], y=plot_df["y_pred_prod"],
                mode="markers", name="Production", opacity=0.6
            ))
        # diagonal reference
        minv, maxv = np.nanmin(plot_df[["y_true","y_pred_new","y_pred_prod"]].values), np.nanmax(plot_df[["y_true","y_pred_new","y_pred_prod"]].values)
        fig_scatter.add_trace(go.Scatter(x=[minv, maxv], y=[minv, maxv], mode="lines", name="Ideal", line=dict(dash="dash")))
        fig_scatter.update_layout(title="Actual vs Predicted (Holdout)", xaxis_title="Actual", yaxis_title="Predicted")
        st.plotly_chart(fig_scatter, use_container_width=True)

        # Residuals hist
        plot_df["res_new"]  = plot_df["y_true"] - plot_df["y_pred_new"]
        if y_pred_prod is not None:
            plot_df["res_prod"] = plot_df["y_true"] - plot_df["y_pred_prod"]

        fig_res = go.Figure()
        fig_res.add_trace(go.Histogram(x=plot_df["res_new"], name="New", opacity=0.7))
        if y_pred_prod is not None:
            fig_res.add_trace(go.Histogram(x=plot_df["res_prod"], name="Production", opacity=0.6))
        fig_res.update_layout(barmode="overlay", title="Residuals Distribution (Actual - Predicted)")
        fig_res.update_traces(nbinsx=40)
        st.plotly_chart(fig_res, use_container_width=True)

        # ===== Feature importance / coefficients =====
        st.markdown("### ğŸ§  Feature Importance / Coefficients")
        if hasattr(new_model, "feature_importances_"):
            imp = pd.DataFrame({
                "feature": num_cols,
                "importance": new_model.feature_importances_
            }).sort_values("importance", ascending=False)
            st.bar_chart(imp.set_index("feature"))
        elif hasattr(new_model, "coef_"):
            coef = pd.DataFrame({
                "feature": num_cols,
                "coef": np.ravel(new_model.coef_)
            }).sort_values("coef", key=np.abs, ascending=False)
            st.bar_chart(coef.set_index("feature"))
        else:
            st.info("This model does not expose importances/coefficients.")

        # ===== Persist artifacts =====
        #trained_dir = "./agents/asset_appraisal/models/trained"
        trained_dir = "/home/dzoan/AI-AIGENTbythePeoplesANDBOX/HUGKAG/agents/asset_appraisal/models/trained"
        
        os.makedirs(trained_dir, exist_ok=True)
        ts = _ts()
        model_path = os.path.join(trained_dir, f"{model_choice}_asset_{ts}.joblib")
        joblib.dump(new_model, model_path)

        preds_csv = os.path.join(RUNS_DIR, f"training_preds_{ts}.csv")
        plot_df.to_csv(preds_csv, index=False)

        report = {
            "timestamp": ts,
            "model_choice": model_choice,
            "trained_model_path": model_path,
            "features": num_cols,
            "metrics_new": new_m,
            "metrics_prod": prod_m,
            "holdout_rows": int(len(yte)),
            "source_file": (auto_path or "uploaded"),
            "preds_csv": preds_csv,
        }
        report_path = os.path.join(RUNS_DIR, f"training_report_{ts}.json")
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2)

        st.success(f"âœ… Trained model saved â†’ `{model_path}`")
        st.caption(f"Report â†’ `{report_path}` | Predictions â†’ `{preds_csv}`")

        # Download helpers
        cdl1, cdl2 = st.columns(2)
        with cdl1:
            st.download_button("â¬‡ï¸ Download training report (JSON)",
                               data=json.dumps(report, indent=2).encode("utf-8"),
                               file_name=os.path.basename(report_path),
                               mime="application/json")
        with cdl2:
            st.download_button("â¬‡ï¸ Download holdout predictions (CSV)",
                               data=plot_df.to_csv(index=False).encode("utf-8-sig"),
                               file_name=os.path.basename(preds_csv),
                               mime="text/csv")
        # âœ… âœ… âœ… CALL DASHBOARD â€” FIX FOR YOUR ISSUE
        render_stage_f_footer(
            new_m=new_m,
            prod_m=prod_m,
            RUNS_DIR=RUNS_DIR,
            model_path=model_path,
            report=report,
            df_train=df_train,
            yte=yte,
            y_pred_new=y_pred_new
        )
        
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… Helper functions required by Stage G
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_nonempty_df(x):
    import pandas as pd
    return isinstance(x, pd.DataFrame) and not x.empty

def first_nonempty_df(*candidates):
    for c in candidates:
        if is_nonempty_df(c):
            return c
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# G â€” DEPLOYMENT & DISTRIBUTION STAGE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tabG:
    import os, json, hashlib, zipfile, requests
    from datetime import datetime, timezone
    from pathlib import Path
    import streamlit as st

    st.title("ğŸš€ Stage G â€” Deployment & Distribution")
    st.caption("Package â†’ Verify â†’ Upload â†’ Release â†’ Distribute to Credit / Legal / Risk units.")
    EXPORT_DIR = Path("./exports")
    EXPORT_DIR.mkdir(exist_ok=True)

    st.markdown("## ğŸ“¦ Build Project Bundle (Model + Reports + Artifacts)")
    build_zip_name = f"asset_project_bundle_{_ts()}.zip"
    build_zip_path = EXPORT_DIR / build_zip_name

    if st.button("â¬‡ï¸ Build & Download Project ZIP", key="btn_build_stage_g_zip"):
        try:
            with zipfile.ZipFile(build_zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
                for root, dirs, files in os.walk(RUNS_DIR):
                    for f in files:
                        full = os.path.join(root, f)
                        arc = os.path.relpath(full, RUNS_DIR)
                        zf.write(full, f"runs/{arc}")

                if os.path.exists("./agents/asset_appraisal/models/production"):
                    for root, dirs, files in os.walk("./agents/asset_appraisal/models/production"):
                        for f in files:
                            full = os.path.join(root, f)
                            zf.write(full, f"production_models/{f}")

                if os.path.exists("./agents/asset_appraisal/models/trained"):
                    for root, dirs, files in os.walk("./agents/asset_appraisal/models/trained"):
                        for f in files:
                            full = os.path.join(root, f)
                            zf.write(full, f"trained_models/{f}")

                latest_report = sorted(Path(RUNS_DIR).glob("training_report_*.json"), reverse=True)
                if latest_report:
                    zf.write(latest_report[0], "training_report.json")

            st.success(f"âœ… Exported: {build_zip_name}")
            with open(build_zip_path, "rb") as fp:
                st.download_button(
                    "â¬‡ï¸ Download ZIP Now",
                    data=fp,
                    file_name=build_zip_name,
                    mime="application/zip",
                    use_container_width=True,
                    key="btn_download_stage_g_zip",
                )
        except Exception as e:
            st.error(f"âŒ ZIP creation failed: {e}")

    # ---------------------------------------------
    # 1) Load the latest ZIP bundle created in Stage F
    # ---------------------------------------------
    st.markdown("## ğŸ“¦ 1) Project Package (Generated in Stage F)")

    # Find ZIP files
    zip_files = sorted(EXPORT_DIR.glob("asset_project_bundle_*.zip"), reverse=True)
    
    if not zip_files:
        st.warning("âš ï¸ No project ZIP found. Run Stage F and export a bundle first.")
        st.stop()

    latest_zip = zip_files[0]

    st.success(f"âœ… Latest bundle detected: `{latest_zip.name}`")
    st.caption(f"Size: **{latest_zip.stat().st_size/1e6:.2f} MB**")
    
    # # Show preview
    # with zipfile.ZipFile(latest_zip, "r") as z:
    #     preview = z.namelist()[:20]
    #     st.code("\n".join(preview), language="text")
    

    # ---------------------------------------------
    # 2) Integrity Check (SHA256)
    # ---------------------------------------------
    st.markdown("## âœ… 2) File Integrity Check (SHA256)")

    sha256 = hashlib.sha256(latest_zip.read_bytes()).hexdigest()
    st.code(sha256)

    checksum_path = latest_zip.with_suffix(".sha256")
    checksum_path.write_text(sha256)
    st.caption(f"Checksum written â†’ `{checksum_path.name}`")

    # Simple signature
    sig_path = latest_zip.with_suffix(".sig")
    sig_path.write_text(f"AI-Agent-Hub signed @ {datetime.now(timezone.utc).isoformat()}")
    st.caption(f"Signature stub â†’ `{sig_path.name}`")


    # ---------------------------------------------
    # 3) Upload Targets (S3 / Swift / GitHub Release)
    # ---------------------------------------------
    st.markdown("## â˜ï¸ 3) Upload / Publish Package")

    dest = st.radio(
        "Choose destination",
        ["AWS S3", "OpenStack Swift", "GitHub Release"],
        horizontal=True
    )

    if dest == "AWS S3":
        st.info("Upload to S3 (requires AWS credentials)")
        bucket = st.text_input("Bucket Name", "my-ai-models")
        key = st.text_input("Object Key", latest_zip.name)

        if st.button("â¬†ï¸ Upload to S3"):
            try:
                import boto3
                s3 = boto3.client("s3")
                s3.upload_file(str(latest_zip), bucket, key)
                st.success(f"âœ… Uploaded to `s3://{bucket}/{key}`")
            except Exception as e:
                st.error(f"âŒ Failed: {e}")

    elif dest == "OpenStack Swift":
        st.info("Upload to Swift (requires Swift credentials)")
        container = st.text_input("Container Name", "ai-models")
        if st.button("â¬†ï¸ Upload to Swift"):
            try:
                from swiftclient.service import SwiftService, SwiftUploadObject
                with SwiftService() as swift:
                    swift.upload(container, [SwiftUploadObject(str(latest_zip))])
                st.success(f"âœ… Uploaded to Swift container `{container}`")
            except Exception as e:
                st.error(f"âŒ Failed: {e}")

    elif dest == "GitHub Release":
        st.info("Publish as a GitHub release asset")
        repo = st.text_input("Repo (owner/repo)", "RackspaceAI/asset-appraisal-agent")
        token = st.text_input("GitHub Personal Access Token", type="password")
        tag = datetime.now().strftime("v%Y%m%d-%H%M%S")

        if st.button("â¬†ï¸ Publish Release on GitHub"):
            try:
                headers = {
                    "Authorization": f"token {token}",
                    "Accept": "application/vnd.github+json",
                }

                # Create release
                r = requests.post(
                    f"https://api.github.com/repos/{repo}/releases",
                    headers=headers,
                    json={"tag_name": tag, "name": f"Release {tag}", 
                          "body": "Automated export from Stage G"}
                )
                r.raise_for_status()

                upload_url = r.json()["upload_url"].split("{")[0]

                # Upload asset
                with open(latest_zip, "rb") as f:
                    ur = requests.post(
                        f"{upload_url}?name={latest_zip.name}",
                        headers={**headers, "Content-Type": "application/zip"},
                        data=f,
                    )
                ur.raise_for_status()

                st.success(f"âœ… GitHub Release `{tag}` published successfully!")
            except Exception as e:
                st.error(f"âŒ Failed: {e}")


    # ---------------------------------------------
    # 4) Deployment Audit Log
    # ---------------------------------------------
    st.markdown("## ğŸ§¾ 4) Deployment Audit Log")

    audit = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "export_file": latest_zip.name,
        "checksum": sha256,
        "target": dest,
    }

    audit_path = EXPORT_DIR / "deployment_audit.jsonl"
    with open(audit_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(audit) + "\n")

    st.success(f"Audit record added â†’ `{audit_path.name}`")


    # ---------------------------------------------
    # 5) Next Steps Checklist
    # ---------------------------------------------
    st.markdown("## âœ… 5) Next Steps for DevOps / IT")

    st.markdown("""
    ### âœ” For Credit Underwriting
    - Import CSV assets into the Credit Appraisal Agent  
    - Validate LTV, confidence, breaches  
    - Promote selected assets for loan approval  

    ### âœ” For Legal & Compliance
    - Use verification subset (ownership, encumbrances)  
    - Run through Legal Verification Agent  
    - Flag encumbrances & fraud paths  

    ### âœ” For Risk Management
    - Use realizable_value, condition_score, legal_penalty  
    - Re-run LTV stress tests  
    - Update risk dashboards monthly  

    ### âœ” For DevOps / Platform Teams
    - Push ZIP to GitHub / Swift / S3  
    - Deploy production model into RunAI / SageMaker / OpenStack MLOps  
    - Update production_meta.json  
    """)

    st.info("Stage G is complete â€” continue to Stage H for Inter-Department Handoff.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… STAGE H â€” Executive Dashboard + Handoff Export
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with tabH:
    import os, json, zipfile
    from pathlib import Path
    from datetime import datetime, timezone
    import pandas as pd
    import numpy as np
    import streamlit as st
    import plotly.express as px
    import plotly.graph_objects as go

    st.markdown("## ğŸ§­ Stage H â€” Unified Portfolio, Insights & Handoff Export")
    st.caption("Executive summary â€¢ Asset insights â€¢ Fraud/risk signals â€¢ Department deliverables")


    
    # ---------------------------------------------------------
    # âœ… Required outputs from earlier stages â€” MUST COME FIRST
    # ---------------------------------------------------------
    ai_df        = st.session_state.get("asset_ai_df")
    policy_df    = st.session_state.get("asset_policy_df")
    decision_df  = st.session_state.get("asset_decision_df")

    missing = []

    if ai_df is None or ai_df.empty:
        missing.append("Stage C (valuation)")
    if decision_df is None or decision_df.empty:
        missing.append("Stage D (risk & decision)")

    if missing:
        st.error("âš ï¸ Missing required data: " + ", ".join(missing))
        st.info("Please run the missing stages before returning to Stage H.")
        st.stop()

    # âœ… Only now is dfv allowed to be created
    dfv = decision_df.copy()


    # ---------------------------------------------------------
    # âœ… STATUS LABEL (Validated / Risky / Fraud)
    # ---------------------------------------------------------
    def label_row(r):
        if r.get("fraud_flag") in [True, "True", 1]:
            return "FRAUD"
        if r.get("encumbrance_flag") in [True, "True", 1]:
            return "ENCUMBERED"
        if str(r.get("decision", "")).lower() == "reject":
            return "RISKY"
        if str(r.get("policy_breaches", "")).strip():
            return "RISKY"
        return "VALIDATED"

    dfv["status"] = dfv.apply(label_row, axis=1)

    # ---------------------------------------------------------
    # âœ… EXECUTIVE SUMMARY METRICS
    # ---------------------------------------------------------
    st.markdown("### ğŸ“Š Executive Summary")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Assets", len(dfv))
    with col2:
        st.metric("Validated", (dfv["status"] == "VALIDATED").sum())
    with col3:
        st.metric("Risky", (dfv["status"] == "RISKY").sum())
    with col4:
        st.metric("Fraud / Encumbered", (dfv["status"].isin(["FRAUD","ENCUMBERED"])).sum())

    # ---------------------------------------------------------
    # âœ… HEATMAP â€” Asset Risk & Fraud Signals
    # ---------------------------------------------------------
    st.markdown("### ğŸ”¥ Fraud / Risk Heatmap")
    
    try:
        hm = dfv[["confidence", "ltv_ai"]].copy()
        hm = hm.dropna()

        fig_hm = px.density_heatmap(
            hm, x="confidence", y="ltv_ai",
            nbinsx=30, nbinsy=30,
            color_continuous_scale="YlOrRd",
            title="Fraud/Anomaly Density â€” (Low confidence + High LTV = Hot Zones)"
        )
        st.plotly_chart(fig_hm, use_container_width=True)
    except Exception:
        st.info("Heatmap unavailable until confidence / LTV data is complete.")

    # ---------------------------------------------------------
    # âœ… MARKET INSIGHTS â€” CITY LEVEL DISTRIBUTION
    # ---------------------------------------------------------
    st.markdown("### ğŸŒ Asset Distribution by City")

    if "city" in dfv.columns:
        fig_city = px.histogram(
            dfv, x="city", color="status",
            title="Asset Count per City by Status",
            barmode="group"
        )
        st.plotly_chart(fig_city, use_container_width=True)

    # ---------------------------------------------------------
    # âœ… VALUE INSIGHTS â€” Realizable Value Curve
    # ---------------------------------------------------------
    st.markdown("### ğŸ’° Value Distribution â€” FMV vs Realizable Value")

    if "realizable_value" in dfv.columns:
        fig_val = go.Figure()
        fig_val.add_trace(go.Violin(y=dfv["fmv"], name="FMV", box_visible=True))
        fig_val.add_trace(go.Violin(y=dfv["realizable_value"], name="Realizable", box_visible=True))
        st.plotly_chart(fig_val, use_container_width=True)

    # ---------------------------------------------------------
    # âœ… FULL PORTFOLIO TABLE
    # ---------------------------------------------------------
    st.markdown("### ğŸ“‚ Unified Portfolio (with status)")
    st.dataframe(dfv, use_container_width=True)

    # ---------------------------------------------------------
    # âœ… DEPARTMENT HANDOFF EXPORTS (bulletproof)
    # ---------------------------------------------------------
    st.markdown("## ğŸ¦ Department Handoff Packages")
    st.caption("Each team receives only what they need. Clear, simple, compliant.")

    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")

    HANDOFF_DIR = Path("./handoff")
    ZIP_DIR      = HANDOFF_DIR / "zips"
    HANDOFF_DIR.mkdir(exist_ok=True)
    ZIP_DIR.mkdir(exist_ok=True)

    # ---------------------------
    # âœ… CREDIT APPRAISAL EXPORT
    # ---------------------------
    credit_cols = [ 
        "application_id","asset_id","asset_type","city",
        "ai_adjusted","fmv","realizable_value",
        "loan_amount","ltv_ai","ltv_cap",
        "decision","policy_breaches"
    ]
    credit = dfv[[c for c in credit_cols if c in dfv.columns]].copy()
    credit_path = HANDOFF_DIR / f"credit_appraisal_{ts}.csv"
    credit.to_csv(credit_path, index=False)

    # Download button
    with open(credit_path, "rb") as f:
        st.download_button("â¬‡ï¸ Credit Appraisal CSV", f, file_name=credit_path.name, mime="text/csv")

    # ---------------------------
    # âœ… LEGAL / TITLE EXPORT
    # ---------------------------
    legal_cols = [
        "application_id","asset_id","verified_owner",
        "encumbrance_flag","legal_penalty","condition_score","notes"
    ]
    legal = dfv[[c for c in legal_cols if c in dfv.columns]].copy()
    legal_path = HANDOFF_DIR / f"legal_pack_{ts}.csv"
    legal.to_csv(legal_path, index=False)

    with open(legal_path, "rb") as f:
        st.download_button("â¬‡ï¸ Legal & Title CSV", f, file_name=legal_path.name, mime="text/csv")

    # ---------------------------
    # âœ… RISK MANAGEMENT EXPORT
    # ---------------------------
    risk_cols = [
        "application_id","asset_id","confidence",
        "ltv_ai","ltv_cap","policy_breaches","decision","status"
    ]
    risk = dfv[[c for c in risk_cols if c in dfv.columns]].copy()
    risk_path = HANDOFF_DIR / f"risk_management_{ts}.csv"
    risk.to_csv(risk_path, index=False)

    with open(risk_path, "rb") as f:
        st.download_button("â¬‡ï¸ Risk Management CSV", f, file_name=risk_path.name, mime="text/csv")

    # ---------------------------
    # âœ… CUSTOMER SERVICE EXPORT
    # ---------------------------
    cust_cols = [
        "application_id","asset_id","asset_type","city",
        "fmv","ai_adjusted","decision","status","why"
    ]
    cust = dfv[[c for c in cust_cols if c in dfv.columns]].copy()
    cust_path = HANDOFF_DIR / f"customer_service_{ts}.csv"
    cust.to_csv(cust_path, index=False)

    with open(cust_path, "rb") as f:
        st.download_button("â¬‡ï¸ Customer Service CSV", f, file_name=cust_path.name, mime="text/csv")

    # ---------------------------
    # âœ… PORTFOLIO SUMMARY
    # ---------------------------
    portfolio_path = HANDOFF_DIR / f"portfolio_{ts}.csv"
    dfv.to_csv(portfolio_path, index=False)

    with open(portfolio_path, "rb") as f:
        st.download_button("â¬‡ï¸ Portfolio Summary CSV", f, file_name=portfolio_path.name, mime="text/csv")

    # ---------------------------
    # âœ… AUDIT RECORD
    # ---------------------------
    audit = {
        "timestamp": ts,
        "rows": len(dfv),
        "status": dfv["status"].value_counts().to_dict(),
        "avg_confidence": float(dfv["confidence"].mean() if "confidence" in dfv else 0.0),
    }
    audit_path = HANDOFF_DIR / f"audit_{ts}.json"
    with open(audit_path, "w") as f:
        json.dump(audit, f, indent=2)

    with open(audit_path, "rb") as f:
        st.download_button("â¬‡ï¸ Audit Record (JSON)", f, file_name=audit_path.name, mime="application/json")

    # # ---------------------------
    # # âœ… FULL ZIP BUNDLE
    # # ---------------------------
    # zip_path = ZIP_DIR / f"handoff_bundle_{ts}.zip"
    # with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
    #     for fp in [credit_path, legal_path, risk_path, cust_path, portfolio_path, audit_path]:
    #         zf.write(fp, arcname=os.path.basename(fp))

    # with open(zip_path, "rb") as f:
    #     st.download_button("â¬‡ï¸ Download FULL Handoff ZIP", f,
    #                     file_name=zip_path.name, mime="application/zip",
    #                     use_container_width=True)

    
    
   
    # ---------------------------------------------------------
    # âœ… ZIP bundle
    # ---------------------------------------------------------
    zip_path = ZIP_DIR / f"handoff_bundle_{ts}.zip"
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
        for fp in [credit_path, legal_path, risk_path, cust_path, portfolio_path, audit_path]:
            zf.write(fp, arcname=os.path.basename(fp))

    st.markdown("### ğŸ“¦ Download Unified Handoff Bundle")
    with open(zip_path, "rb") as f:
        st.download_button(
            "â¬‡ï¸ Download Full Handoff ZIP",
            data=f,
            file_name=os.path.basename(zip_path),
            mime="application/zip",
            use_container_width=True
        )
